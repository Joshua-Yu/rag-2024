Published Date,Link,Title,Summary,First Author
2024-06-06T07:01:50Z,http://arxiv.org/abs/2406.03790v2,"End-to-End Trainable Retrieval-Augmented Generation for Relation
  Extraction","This paper addresses a crucial challenge in retrieval-augmented
generation-based relation extractors; the end-to-end training is not applicable
to conventional retrieval-augmented generation due to the non-differentiable
nature of instance retrieval. This problem prevents the instance retrievers
from being optimized for the relation extraction task, and conventionally it
must be trained with an objective different from that for relation extraction.
To address this issue, we propose a novel End-to-end Trainable
Retrieval-Augmented Generation (ETRAG), which allows end-to-end optimization of
the entire model, including the retriever, for the relation extraction
objective by utilizing a differentiable selection of the $k$ nearest instances.
We evaluate the relation extraction performance of ETRAG on the TACRED dataset,
which is a standard benchmark for relation extraction. ETRAG demonstrates
consistent improvements against the baseline model as retrieved instances are
added. Furthermore, the analysis of instances retrieved by the end-to-end
trained retriever confirms that the retrieved instances contain common relation
labels or entities with the query and are specialized for the target task. Our
findings provide a promising foundation for future research on
retrieval-augmented generation and the broader applications of text generation
in Natural Language Processing.",Kohei Makino
2024-05-12T09:48:28Z,http://arxiv.org/abs/2405.13002v1,DuetRAG: Collaborative Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) methods augment the input of Large
Language Models (LLMs) with relevant retrieved passages, reducing factual
errors in knowledge-intensive tasks. However, contemporary RAG approaches
suffer from irrelevant knowledge retrieval issues in complex domain questions
(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to
low-quality generations. To address this issue, we propose a novel
Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our
bootstrapping philosophy is to simultaneously integrate the domain fintuning
and RAG models to improve the knowledge retrieval quality, thereby enhancing
generation quality. Finally, we demonstrate DuetRAG' s matches with expert
human researchers on HotPot QA.",Dian Jiao
2024-07-04T14:20:12Z,http://arxiv.org/abs/2407.03955v1,Meta-prompting Optimized Retrieval-augmented Generation,"Retrieval-augmented generation resorts to content retrieved from external
sources in order to leverage the performance of large language models in
downstream tasks. The excessive volume of retrieved content, the possible
dispersion of its parts, or their out of focus range may happen nevertheless to
eventually have a detrimental rather than an incremental effect. To mitigate
this issue and improve retrieval-augmented generation, we propose a method to
refine the retrieved content before it is included in the prompt by resorting
to meta-prompting optimization. Put to empirical test with the demanding
multi-hop question answering task from the StrategyQA dataset, the evaluation
results indicate that this method outperforms a similar retrieval-augmented
system but without this method by over 30%.",João Rodrigues
2024-12-19T21:14:54Z,http://arxiv.org/abs/2412.15404v1,"A Retrieval-Augmented Generation Framework for Academic Literature
  Navigation in Data Science","In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.",Ahmet Yasin Aytar
2023-09-04T08:28:44Z,http://arxiv.org/abs/2309.01431v2,Benchmarking Large Language Models in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) is a promising approach for mitigating
the hallucination of large language models (LLMs). However, existing research
lacks rigorous evaluation of the impact of retrieval-augmented generation on
different large language models, which make it challenging to identify the
potential bottlenecks in the capabilities of RAG for different LLMs. In this
paper, we systematically investigate the impact of Retrieval-Augmented
Generation on large language models. We analyze the performance of different
large language models in 4 fundamental abilities required for RAG, including
noise robustness, negative rejection, information integration, and
counterfactual robustness. To this end, we establish Retrieval-Augmented
Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and
Chinese. RGB divides the instances within the benchmark into 4 separate
testbeds based on the aforementioned fundamental abilities required to resolve
the case. Then we evaluate 6 representative LLMs on RGB to diagnose the
challenges of current LLMs when applying RAG. Evaluation reveals that while
LLMs exhibit a certain degree of noise robustness, they still struggle
significantly in terms of negative rejection, information integration, and
dealing with false information. The aforementioned assessment outcomes indicate
that there is still a considerable journey ahead to effectively apply RAG to
LLMs.",Jiawei Chen
2024-05-30T09:50:38Z,http://arxiv.org/abs/2405.19893v1,"Similarity is Not All You Need: Endowing Retrieval Augmented Generation
  with Multi Layered Thoughts","In recent years, large language models (LLMs) have made remarkable
achievements in various domains. However, the untimeliness and cost of
knowledge updates coupled with hallucination issues of LLMs have curtailed
their applications in knowledge intensive tasks, where retrieval augmented
generation (RAG) can be of help. Nevertheless, existing retrieval augmented
models typically use similarity as a bridge between queries and documents and
follow a retrieve then read procedure. In this work, we argue that similarity
is not always the panacea and totally relying on similarity would sometimes
degrade the performance of retrieval augmented generation. To this end, we
propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented
Generation framework. To begin with, beyond existing similarity oriented
thought, we embrace a small scale utility model that draws supervision from an
LLM for utility oriented thought and further come up with a smarter model by
comprehensively combining the similarity and utility oriented thoughts.
Furthermore, given the fact that the retrieved document set tends to be huge
and using them in isolation makes it difficult to capture the commonalities and
characteristics among them, we propose to make an LLM as a task adaptive
summarizer to endow retrieval augmented generation with compactness-oriented
thought. Finally, with multi layered thoughts from the precedent stages, an LLM
is called for knowledge augmented generation. Extensive experiments on
knowledge-intensive tasks have demonstrated the superiority of MetRag.",Chunjing Gan
2023-08-01T12:04:50Z,http://arxiv.org/abs/2308.00479v1,"Retrieval Augmented Generation and Representative Vector Summarization
  for large unstructured textual data in Medical Education","Large Language Models are increasingly being used for various tasks including
content generation and as chatbots. Despite their impressive performances in
general tasks, LLMs need to be aligned when applying for domain specific tasks
to mitigate the problems of hallucination and producing harmful answers.
Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a
non-parametric knowledgebases to LLMs. Applications of RAG in the field of
medical education are discussed in this paper. A combined extractive and
abstractive summarization method for large unstructured textual data using
representative vectors is proposed.",S. S. Manathunga
2023-12-12T23:22:57Z,http://arxiv.org/abs/2312.07796v1,"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge
  Gaps","The paper presents a methodology for uncovering knowledge gaps on the
internet using the Retrieval Augmented Generation (RAG) model. By simulating
user search behaviour, the RAG system identifies and addresses gaps in
information retrieval systems. The study demonstrates the effectiveness of the
RAG system in generating relevant suggestions with a consistent accuracy of
93%. The methodology can be applied in various fields such as scientific
discovery, educational enhancement, research development, market analysis,
search engine optimisation, and content development. The results highlight the
value of identifying and understanding knowledge gaps to guide future
endeavours.",Joan Figuerola Hurtado
2024-03-03T21:24:35Z,http://arxiv.org/abs/2403.01616v2,"Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models","This paper presents our contributions towards advancing the state of
Vietnamese language understanding and generation through the development and
dissemination of open datasets and pre-trained models for Vietnamese
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).",Nguyen Quang Duc
2024-06-18T09:53:37Z,http://arxiv.org/abs/2406.12449v1,"Retrieval-Augmented Generation for Generative Artificial Intelligence in
  Medicine","Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care.",Rui Yang
2024-06-21T07:52:01Z,http://arxiv.org/abs/2406.14938v1,Towards Retrieval Augmented Generation over Large Video Libraries,"Video content creators need efficient tools to repurpose content, a task that
often requires complex manual or automated searches. Crafting a new video from
large video libraries remains a challenge. In this paper we introduce the task
of Video Library Question Answering (VLQA) through an interoperable
architecture that applies Retrieval Augmented Generation (RAG) to video
libraries. We propose a system that uses large language models (LLMs) to
generate search queries, retrieving relevant video moments indexed by speech
and visual metadata. An answer generation module then integrates user queries
with this metadata to produce responses with specific video timestamps. This
approach shows promise in multimedia content retrieval, and AI-assisted video
content creation.",Yannis Tevissen
2024-07-09T09:46:23Z,http://arxiv.org/abs/2407.06718v1,"A Simple Architecture for Enterprise Large Language Model Applications
  based on Role based security and Clearance Levels using Retrieval-Augmented
  Generation or Mixture of Experts","This study proposes a simple architecture for Enterprise application for
Large Language Models (LLMs) for role based security and NATO clearance levels.
Our proposal aims to address the limitations of current LLMs in handling
security and information access. The proposed architecture could be used while
utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of
experts models (MoE). It could be used only with RAG, or only with MoE or with
both of them. Using roles and security clearance level of the user, documents
in RAG and experts in MoE are filtered. This way information leakage is
prevented.",Atilla Özgür
2024-07-12T16:18:00Z,http://arxiv.org/abs/2407.09394v1,"PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with
  User-Centric Agents","Large Language Models (LLMs) struggle with generating reliable outputs due to
outdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG)
models address this by enhancing LLMs with external knowledge, but often fail
to personalize the retrieval process. This paper introduces PersonaRAG, a novel
framework incorporating user-centric agents to adapt retrieval and generation
based on real-time user data and interactions. Evaluated across various
question answering datasets, PersonaRAG demonstrates superiority over baseline
models, providing tailored answers to user needs. The results suggest promising
directions for user-adapted information retrieval systems.",Saber Zerhoudi
2024-10-02T23:14:29Z,http://arxiv.org/abs/2410.03771v1,"SeeSay: An Assistive Device for the Visually Impaired Using Retrieval
  Augmented Generation","In this paper, we present SeeSay, an assistive device designed for
individuals with visual impairments. This system leverages large language
models (LLMs) for speech recognition and visual querying. It effectively
identifies, records, and responds to the user's environment by providing audio
guidance using retrieval-augmented generation (RAG). Our experiments
demonstrate the system's capability to recognize its surroundings and respond
to queries with audio feedback in diverse settings. We hope that the SeeSay
system will facilitate users' comprehension and recollection of their
surroundings, thereby enhancing their environmental perception, improving
navigational capabilities, and boosting overall independence.",Melody Yu
2024-10-28T09:55:52Z,http://arxiv.org/abs/2410.20878v1,"AutoRAG: Automated Framework for optimization of Retrieval Augmented
  Generation Pipeline","Using LLMs (Large Language Models) in conjunction with external documents has
made RAG (Retrieval-Augmented Generation) an essential technology. Numerous
techniques and modules for RAG are being researched, but their performance can
vary across different datasets. Finding RAG modules that perform well on
specific datasets is challenging. In this paper, we propose the AutoRAG
framework, which automatically identifies suitable RAG modules for a given
dataset. AutoRAG explores and approximates the optimal combination of RAG
modules for the dataset. Additionally, we share the results of optimizing a
dataset using AutoRAG. All experimental results and data are publicly available
and can be accessed through our GitHub repository
https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",Dongkyu Kim
2024-12-16T21:09:28Z,http://arxiv.org/abs/2412.12358v1,"BioRAGent: A Retrieval-Augmented Generation System for Showcasing
  Generative Query Expansion and Domain-Specific Search for Scientific Q&A","We present BioRAGent, an interactive web-based retrieval-augmented generation
(RAG) system for biomedical question answering. The system uses large language
models (LLMs) for query expansion, snippet extraction, and answer generation
while maintaining transparency through citation links to the source documents
and displaying generated queries for further editing. Building on our
successful participation in the BioASQ 2024 challenge, we demonstrate how
few-shot learning with LLMs can be effectively applied for a professional
search setting. The system supports both direct short paragraph style responses
and responses with inline citations. Our demo is available online, and the
source code is publicly accessible through GitHub.",Samy Ateia
2024-11-27T18:27:07Z,http://arxiv.org/abs/2411.18583v1,"Automated Literature Review Using NLP Techniques and LLM-Based
  Retrieval-Augmented Generation","This research presents and compares multiple approaches to automate the
generation of literature reviews using several Natural Language Processing
(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language
Model (LLM). The ever-increasing number of research articles provides a huge
challenge for manual literature review. It has resulted in an increased demand
for automation. Developing a system capable of automatically generating the
literature reviews from only the PDF files as input is the primary objective of
this research work. The effectiveness of several Natural Language Processing
(NLP) strategies, such as the frequency-based method (spaCy), the transformer
model (Simple T5), and retrieval-augmented generation (RAG) with Large Language
Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR
dataset is chosen for this research experiment and three distinct techniques
are utilized to implement three different systems for auto-generating the
literature reviews. The ROUGE scores are used for the evaluation of all three
systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo
achieved the highest ROUGE-1 score, 0.364. The transformer model comes in
second place and spaCy is at the last position. Finally, a graphical user
interface is created for the best system based on the large language model.",Nurshat Fateh Ali
2023-09-26T19:23:54Z,http://arxiv.org/abs/2309.15217v1,RAGAS: Automated Evaluation of Retrieval Augmented Generation,"We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With RAGAs, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.",Shahul Es
2023-11-07T18:03:23Z,http://arxiv.org/abs/2311.04177v1,"Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for
  Retrieval Augmented Generation","Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,
(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of
performing amazing tasks typically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not improve over time; they neither
acquire new knowledge nor learn from their successes or failures. Some
approaches to improving the intelligence of LLMs include fine-tuning models
based on problem-solving performance (Zelikman et al., 2022), and building
bigger and more sophisticated models (Bubeck et al., 2023). However, these
methods have the drawback of requiring substantial data and computational
resources to retrain existing models. In this paper, we explore the use of
Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to
improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale
Memory for Retrieval Augmented Generation), a system that learns from its
successes without incurring high training costs. We demonstrate that the
storage and subsequent retrieval of reasoning chains have a positive influence
on performance in grade-school math problems.",Eric Melz
2024-02-13T12:40:39Z,http://arxiv.org/abs/2402.08416v1,Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning,"Large Language Models~(LLMs) have gained immense popularity and are being
increasingly applied in various domains. Consequently, ensuring the security of
these models is of paramount importance. Jailbreak attacks, which manipulate
LLMs to generate malicious content, are recognized as a significant
vulnerability. While existing research has predominantly focused on direct
jailbreak attacks on LLMs, there has been limited exploration of indirect
methods. The integration of various plugins into LLMs, notably Retrieval
Augmented Generation~(RAG), which enables LLMs to incorporate external
knowledge bases into their response generation such as GPTs, introduces new
avenues for indirect jailbreak attacks.
  To fill this gap, we investigate indirect jailbreak attacks on LLMs,
particularly GPTs, introducing a novel attack vector named Retrieval Augmented
Generation Poisoning. This method, Pandora, exploits the synergy between LLMs
and RAG through prompt manipulation to generate unexpected responses. Pandora
uses maliciously crafted content to influence the RAG process, effectively
initiating jailbreak attacks. Our preliminary tests show that Pandora
successfully conducts jailbreak attacks in four different scenarios, achieving
higher success rates than direct attacks, with 64.3\% for GPT-3.5 and 34.8\%
for GPT-4.",Gelei Deng
2024-03-21T07:47:57Z,http://arxiv.org/abs/2403.14197v1,"Context Quality Matters in Training Fusion-in-Decoder for Extractive
  Open-Domain Question Answering","Retrieval-augmented generation models augment knowledge encoded in a language
model by providing additional relevant external knowledge (context) during
generation. Although it has been shown that the quantity and quality of context
impact the performance of retrieval-augmented generation models during
inference, limited research explores how these characteristics affect model
training. This paper explores how context quantity and quality during model
training affect the performance of Fusion-in-Decoder (FiD), the
state-of-the-art retrieval-augmented generation model, in extractive
open-domain question answering tasks. Experimental results suggest that FiD
models overfit to context quality during training and show suboptimal
performance when evaluated on different context quality. Through the
experimental results, we also reveal FiD models trained with different context
quality have different cross-attention distribution patterns. Specifically, as
context quality during training increases, FiD models tend to attend more
uniformly to each passage in context. Finally, based on these observations, we
propose a method to mitigate overfitting to specific context quality by
introducing bias to the cross-attention distribution, which we demonstrate to
be effective in improving the performance of FiD models on different context
quality.",Kosuke Akimoto
2024-03-31T08:58:54Z,http://arxiv.org/abs/2404.00610v1,RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,"Large Language Models (LLMs) exhibit remarkable capabilities but are prone to
generating inaccurate or hallucinatory responses. This limitation stems from
their reliance on vast pretraining datasets, making them susceptible to errors
in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation
(RAG) addresses this by incorporating external, relevant documents into the
response generation process, thus leveraging non-parametric knowledge alongside
LLMs' in-context learning abilities. However, existing RAG implementations
primarily focus on initial input for context retrieval, overlooking the nuances
of ambiguous or complex queries that necessitate further clarification or
decomposition for accurate responses. To this end, we propose learning to
Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper,
endeavoring to enhance the model by equipping it with capabilities for explicit
rewriting, decomposition, and disambiguation. Our experimental results indicate
that our method, when applied to a 7B Llama2 model, surpasses the previous
state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA
datasets, and also demonstrates enhanced performance in handling complex,
multi-hop QA datasets. Our code is available at
https://github.com/chanchimin/RQ-RAG.",Chi-Min Chan
2024-05-26T10:11:40Z,http://arxiv.org/abs/2405.16506v2,GRAG: Graph Retrieval-Augmented Generation,"Naive Retrieval-Augmented Generation (RAG) focuses on individual documents
during retrieval and, as a result, falls short in handling networked documents
which are very popular in many applications such as citation graphs, social
media, and knowledge graphs. To overcome this limitation, we introduce Graph
Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges
in retrieving textual subgraphs and integrating the joint textual and
topological information into Large Language Models (LLMs) to enhance its
generation. To enable efficient textual subgraph retrieval, we propose a novel
divide-and-conquer strategy that retrieves the optimal subgraph structure in
linear time. To achieve graph context-aware generation, incorporate textual
graphs into LLMs through two complementary views-the text view and the graph
view-enabling LLMs to more effectively comprehend and utilize the graph
context. Extensive experiments on graph reasoning benchmarks demonstrate that
in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach
significantly outperforms current state-of-the-art RAG methods.",Yuntong Hu
2024-05-28T12:18:50Z,http://arxiv.org/abs/2405.18111v3,"ATM: Adversarial Tuning Multi-agent System Makes a Robust
  Retrieval-Augmented Generator","Large language models (LLMs) are proven to benefit a lot from
retrieval-augmented generation (RAG) in alleviating hallucinations confronted
with knowledge-intensive questions. RAG adopts information retrieval techniques
to inject external knowledge from semantic-relevant documents as input
contexts. However, since today's Internet is flooded with numerous noisy and
fabricating content, it is inevitable that RAG systems are vulnerable to these
noises and prone to respond incorrectly. To this end, we propose to optimize
the retrieval-augmented Generator with an Adversarial Tuning Multi-agent system
(ATM). The ATM steers the Generator to have a robust perspective of useful
documents for question answering with the help of an auxiliary Attacker agent
through adversarially tuning the agents for several iterations. After rounds of
multi-agent iterative tuning, the Generator can eventually better discriminate
useful documents amongst fabrications. The experimental results verify the
effectiveness of ATM and we also observe that the Generator can achieve better
performance compared to the state-of-the-art baselines.",Junda Zhu
2024-06-03T17:31:06Z,http://arxiv.org/abs/2406.01549v2,"An Information Bottleneck Perspective for Effective Noise Filtering on
  Retrieval-Augmented Generation","Retrieval-augmented generation integrates the capabilities of large language
models with relevant information retrieved from an extensive corpus, yet
encounters challenges when confronted with real-world noisy data. One recent
solution is to train a filter module to find relevant content but only achieve
suboptimal noise compression. In this paper, we propose to introduce the
information bottleneck theory into retrieval-augmented generation. Our approach
involves the filtration of noise by simultaneously maximizing the mutual
information between compression and ground output, while minimizing the mutual
information between compression and retrieved passage. In addition, we derive
the formula of information bottleneck to facilitate its application in novel
comprehensive evaluations, the selection of supervised fine-tuning data, and
the construction of reinforcement learning rewards. Experimental results
demonstrate that our approach achieves significant improvements across various
question answering datasets, not only in terms of the correctness of answer
generation but also in the conciseness with $2.5\%$ compression rate.",Kun Zhu
2024-06-19T06:19:48Z,http://arxiv.org/abs/2406.13249v2,"R^2AG: Incorporating Retrieval Information into Retrieval Augmented
  Generation","Retrieval augmented generation (RAG) has been applied in many scenarios to
augment large language models (LLMs) with external documents provided by
retrievers. However, a semantic gap exists between LLMs and retrievers due to
differences in their training objectives and architectures. This misalignment
forces LLMs to passively accept the documents provided by the retrievers,
leading to incomprehension in the generation process, where the LLMs are
burdened with the task of distinguishing these documents using their inherent
knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill
this gap by incorporating Retrieval information into Retrieval Augmented
Generation. Specifically, R$^2$AG utilizes the nuanced features from the
retrievers and employs a R$^2$-Former to capture retrieval information. Then, a
retrieval-aware prompting strategy is designed to integrate retrieval
information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios
where LLMs and retrievers are frozen. Extensive experiments across five
datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our
analysis reveals that retrieval information serves as an anchor to aid LLMs in
the generation process, thereby filling the semantic gap.",Fuda Ye
2024-06-19T16:42:57Z,http://arxiv.org/abs/2406.13692v2,"Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented
  Generation","Retrieval-augmented language models (RALMs) have shown strong performance and
wide applicability in knowledge-intensive tasks. However, there are significant
trustworthiness concerns as RALMs are prone to generating unfaithful outputs,
including baseless information or contradictions with the retrieved context.
This paper proposes SynCheck, a lightweight monitor that leverages fine-grained
decoding dynamics including sequence likelihood, uncertainty quantification,
context influence, and semantic alignment to synchronously detect unfaithful
sentences. By integrating efficiently measurable and complementary signals,
SynCheck enables accurate and immediate feedback and intervention, achieving
0.85 AUROC in detecting faithfulness errors across six long-form
retrieval-augmented generation tasks, improving prior best method by 4%.
Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding
algorithm guided by beam search for long-form retrieval-augmented generation.
Empirical results demonstrate that FOD outperforms traditional strategies such
as abstention, reranking, or contrastive decoding significantly in terms of
faithfulness, achieving over 10% improvement across six datasets.",Di Wu
2024-07-06T02:22:25Z,http://arxiv.org/abs/2407.04925v1,RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations,"Massive Open Online Courses (MOOCs) have significantly enhanced educational
accessibility by offering a wide variety of courses and breaking down
traditional barriers related to geography, finance, and time. However, students
often face difficulties navigating the vast selection of courses, especially
when exploring new fields of study. Driven by this challenge, researchers have
been exploring course recommender systems to offer tailored guidance that
aligns with individual learning preferences and career aspirations. These
systems face particular challenges in effectively addressing the ``cold start''
problem for new users. Recent advancements in recommender systems suggest
integrating large language models (LLMs) into the recommendation process to
enhance personalized recommendations and address the ``cold start'' problem.
Motivated by these advancements, our study introduces RAMO (Retrieval-Augmented
Generation for MOOCs), a system specifically designed to overcome the ``cold
start'' challenges of traditional course recommender systems. The RAMO system
leverages the capabilities of LLMs, along with Retrieval-Augmented Generation
(RAG)-facilitated contextual understanding, to provide course recommendations
through a conversational interface, aiming to enhance the e-learning
experience.",Jiarui Rao
2024-07-25T13:47:01Z,http://arxiv.org/abs/2407.18044v1,"The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented
  Generation","Digital health chatbots powered by Large Language Models (LLMs) have the
potential to significantly improve personal health management for chronic
conditions by providing accessible and on-demand health coaching and
question-answering. However, these chatbots risk providing unverified and
inaccurate information because LLMs generate responses based on patterns
learned from diverse internet data. Retrieval Augmented Generation (RAG) can
help mitigate hallucinations and inaccuracies in LLM responses by grounding it
on reliable content. However, efficiently and accurately retrieving most
relevant set of content for real-time user questions remains a challenge. In
this work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a
novel approach that pre-computes a database of potential queries from a content
base using LLMs. For an incoming patient question, QB-RAG efficiently matches
it against this pre-generated query database using vector search, improving
alignment between user questions and the content. We establish a theoretical
foundation for QB-RAG and provide a comparative analysis of existing retrieval
enhancement techniques for RAG systems. Finally, our empirical evaluation
demonstrates that QB-RAG significantly improves the accuracy of healthcare
question answering, paving the way for robust and trustworthy LLM applications
in digital health.",Eric Yang
2024-08-16T05:15:12Z,http://arxiv.org/abs/2408.08535v1,"CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for
  Advanced Retrieval-Augmented Generation in Fact-Checking","Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.",Rong-Ching Chang
2024-09-24T07:24:01Z,http://arxiv.org/abs/2409.15815v1,"AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For
  Asthma Patient Support","Asthma rates have risen globally, driven by environmental and lifestyle
factors. Access to immediate medical care is limited, particularly in
developing countries, necessitating automated support systems. Large Language
Models like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have
advanced natural language processing in general and question answering in
particular, however, they are prone to producing factually incorrect responses
(i.e. hallucinations). Retrieval-augmented generation systems, integrating
curated documents, can improve large language models' performance and reduce
the incidence of hallucination. We introduce AsthmaBot, a multi-lingual,
multi-modal retrieval-augmented generation system for asthma support.
Evaluation of an asthma-related frequently asked questions dataset shows
AsthmaBot's efficacy. AsthmaBot has an added interactive and intuitive
interface that integrates different data modalities (text, images, videos) to
make it accessible to the larger public. AsthmaBot is available online via
\url{asthmabot.datanets.org}.",Adil Bahaj
2024-10-15T06:39:35Z,http://arxiv.org/abs/2410.11321v1,Self-adaptive Multimodal Retrieval-Augmented Generation,"Traditional Retrieval-Augmented Generation (RAG) methods are limited by their
reliance on a fixed number of retrieved documents, often resulting in
incomplete or noisy information that undermines task performance. Although
recent adaptive approaches alleviated these problems, their application in
intricate and real-world multimodal tasks remains limited. To address these, we
propose a new approach called Self-adaptive Multimodal Retrieval-Augmented
Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG
not only dynamically filters relevant documents based on the input query,
including image captions when needed, but also verifies the quality of both the
retrieved documents and the output. Extensive experimental results show that
SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy
and response generation. By further ablation experiments and effectiveness
analysis, SAM-RAG maintains high recall quality while improving overall task
performance in multimodal RAG task. Our codes are available at
https://github.com/SAM-RAG/SAM_RAG.",Wenjia Zhai
2024-10-17T06:57:29Z,http://arxiv.org/abs/2410.13272v1,"FRAG: Toward Federated Vector Database Management for Collaborative and
  Secure Retrieval-Augmented Generation","This paper introduces \textit{Federated Retrieval-Augmented Generation
(FRAG)}, a novel database management paradigm tailored for the growing needs of
retrieval-augmented generation (RAG) systems, which are increasingly powered by
large-language models (LLMs). FRAG enables mutually-distrusted parties to
collaboratively perform Approximate $k$-Nearest Neighbor (ANN) searches on
encrypted query vectors and encrypted data stored in distributed vector
databases, all while ensuring that no party can gain any knowledge about the
queries or data of others. Achieving this paradigm presents two key challenges:
(i) ensuring strong security guarantees, such as Indistinguishability under
Chosen-Plaintext Attack (IND-CPA), under practical assumptions (e.g., we avoid
overly optimistic assumptions like non-collusion among parties); and (ii)
maintaining performance overheads comparable to traditional, non-federated RAG
systems. To address these challenges, FRAG employs a single-key homomorphic
encryption protocol that simplifies key management across mutually-distrusted
parties. Additionally, FRAG introduces a \textit{multiplicative caching}
technique to efficiently encrypt floating-point numbers, significantly
improving computational performance in large-scale federated environments. We
provide a rigorous security proof using standard cryptographic reductions and
demonstrate the practical scalability and efficiency of FRAG through extensive
experiments on both benchmark and real-world datasets.",Dongfang Zhao
2024-11-01T08:02:09Z,http://arxiv.org/abs/2411.00437v1,"E2E-AFG: An End-to-End Model with Adaptive Filtering for
  Retrieval-Augmented Generation","Retrieval-augmented generation methods often neglect the quality of content
retrieved from external knowledge bases, resulting in irrelevant information or
potential misinformation that negatively affects the generation results of
large language models. In this paper, we propose an end-to-end model with
adaptive filtering for retrieval-augmented generation (E2E-AFG), which
integrates answer existence judgment and text generation into a single
end-to-end framework. This enables the model to focus more effectively on
relevant content while reducing the influence of irrelevant information and
generating accurate answers. We evaluate E2E-AFG on six representative
knowledge-intensive language datasets, and the results show that it
consistently outperforms baseline models across all tasks, demonstrating the
effectiveness and robustness of the proposed approach.",Yun Jiang
2024-11-14T06:19:18Z,http://arxiv.org/abs/2411.09213v1,"Comprehensive and Practical Evaluation of Retrieval-Augmented Generation
  Systems for Medical Question Answering","Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.",Nghia Trung Ngo
2024-11-25T06:48:38Z,http://arxiv.org/abs/2411.16133v1,Context Awareness Gate For Retrieval Augmented Generation,"Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach
to mitigate the limitations of large language models (LLMs) in answering
domain-specific questions. Previous research has predominantly focused on
improving the accuracy and quality of retrieved data chunks to enhance the
overall performance of the generation pipeline. However, despite ongoing
advancements, the critical issue of retrieving irrelevant information -- which
can impair the ability of the model to utilize its internal knowledge
effectively -- has received minimal attention. In this work, we investigate the
impact of retrieving irrelevant information in open-domain question answering,
highlighting its significant detrimental effect on the quality of LLM outputs.
To address this challenge, we propose the Context Awareness Gate (CAG)
architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt
based on whether the user query necessitates external context retrieval.
Additionally, we introduce the Vector Candidates method, a core mathematical
component of CAG that is statistical, LLM-independent, and highly scalable. We
further examine the distributions of relationships between contexts and
questions, presenting a statistical analysis of these distributions. This
analysis can be leveraged to enhance the context retrieval process in Retrieval
Augmented Generation (RAG) systems.",Mohammad Hassan Heydari
2024-12-19T02:17:35Z,http://arxiv.org/abs/2412.14457v1,VISA: Retrieval Augmented Generation with Visual Source Attribution,"Generation with source attribution is important for enhancing the
verifiability of retrieval-augmented generation (RAG) systems. However,
existing approaches in RAG primarily link generated content to document-level
references, making it challenging for users to locate evidence among multiple
content-rich retrieved documents. To address this challenge, we propose
Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel
approach that combines answer generation with visual source attribution.
Leveraging large vision-language models (VLMs), VISA identifies the evidence
and highlights the exact regions that support the generated answers with
bounding boxes in the retrieved document screenshots. To evaluate its
effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia
webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the
medical domain. Experimental results demonstrate the effectiveness of VISA for
visual source attribution on documents' original look, as well as highlighting
the challenges for improvement. Code, data, and model checkpoints will be
released.",Xueguang Ma
2024-12-17T15:40:08Z,http://arxiv.org/abs/2412.15272v1,"SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven
  Retrieval-Augmented Generation","Recent advancements in large language models (LLMs) have shown impressive
versatility across various tasks. To eliminate its hallucinations,
retrieval-augmented generation (RAG) has emerged as a powerful approach,
leveraging external knowledge sources like knowledge graphs (KGs). In this
paper, we study the task of KG-driven RAG and propose a novel Similar Graph
Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively
addresses the challenge of aligning query texts and KG structures through a
two-stage process: (1) query-to-pattern, which uses an LLM to transform queries
into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the
alignment between the pattern and candidate subgraphs using a graph semantic
distance (GSD) metric. We also develop an optimized retrieval algorithm that
efficiently identifies the top-$k$ subgraphs within 1-second latency on a
10-million-scale KG. Extensive experiments show that SimGRAG outperforms
state-of-the-art KG-driven RAG methods in both question answering and fact
verification, offering superior plug-and-play usability and scalability.",Yuzheng Cai
2023-05-03T21:40:54Z,http://arxiv.org/abs/2305.02437v3,Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory,"With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation~(we define this as
primal problem). The traditional approach for memory retrieval involves
selecting memory that exhibits the highest similarity to the input. However,
this method is constrained by the quality of the fixed corpus from which memory
is retrieved. In this paper, by exploring the duality of the primal problem:
better generation also prompts better memory, we propose a novel framework,
selfmem, which addresses this limitation by iteratively employing a
retrieval-augmented generator to create an unbounded memory pool and using a
memory selector to choose one output as memory for the subsequent generation
round. This enables the model to leverage its own output, referred to as
self-memory, for improved generation. We evaluate the effectiveness of selfmem
on three distinct text generation tasks: neural machine translation,
abstractive text summarization, and dialogue generation, under two generation
paradigms: fine-tuned small model and few-shot LLM. Our approach achieves
state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),
and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in
enhancing retrieval-augmented generation models. Furthermore, we conduct
thorough analyses of each component in the selfmem framework to identify
bottlenecks and provide insights for future research.",Xin Cheng
2023-05-11T17:13:40Z,http://arxiv.org/abs/2305.06983v2,Active Retrieval Augmented Generation,"Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing retrieval
augmented LMs employ a retrieve-and-generate setup that only retrieves
information once based on the input. This is limiting, however, in more general
scenarios involving generation of long texts, where continually gathering
information throughout generation is essential. In this work, we provide a
generalized view of active retrieval augmented generation, methods that
actively decide when and what to retrieve across the course of the generation.
We propose Forward-Looking Active REtrieval augmented generation (FLARE), a
generic method which iteratively uses a prediction of the upcoming sentence to
anticipate future content, which is then utilized as a query to retrieve
relevant documents to regenerate the sentence if it contains low-confidence
tokens. We test FLARE along with baselines comprehensively over 4 long-form
knowledge-intensive generation tasks/datasets. FLARE achieves superior or
competitive performance on all tasks, demonstrating the effectiveness of our
method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",Zhengbao Jiang
2023-07-12T04:44:31Z,http://arxiv.org/abs/2307.05915v2,"Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval
  Augmented Generation Models for Open Book Question-Answering","We propose a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation (RAG) model to the target domain using
supervised fine-tuning and reinforcement learning with synthetic feedback in a
few-shot setting. This, we hypothesize, will yield an aligned, uncertainty
calibrated model that is competitive with GPT-4 based in-context retrieval
augmented generation in generating relevant answers at lower serving costs. The
framework's synthetic generation pipeline will generate synthetic training data
comprising <passage, question, answer> tuples using an open-source LLM and a
novel consistency filtering scheme. The pipeline will be designed to generate
both abstractive and extractive questions that span the entire corpus. The
framework proposes to fine-tune a smaller RAG model comprising a dense
retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In
parallel, the framework will train a Reward model to score domain grounded
answers higher than hallucinated answers using an a priori relevance ordering
of synthetically assembled samples. In the next phase, the framework will align
the RAG model with the target domain using reinforcement learning (Proximal
Policy Optimization). This step may improve the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework will calibrate the model's uncertainty for extractive
question-answers.",C. S. Krishna
2024-01-29T04:36:39Z,http://arxiv.org/abs/2401.15884v3,Corrective Retrieval Augmented Generation,"Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.",Shi-Qi Yan
2024-02-20T17:44:06Z,http://arxiv.org/abs/2402.13178v2,Benchmarking Retrieval-Augmented Generation for Medicine,"While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the ""lost-in-the-middle"" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.",Guangzhi Xiong
2024-03-15T07:45:37Z,http://arxiv.org/abs/2403.10081v3,"DRAGIN: Dynamic Retrieval Augmented Generation based on the Information
  Needs of Large Language Models","Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",Weihang Su
2024-05-22T16:15:17Z,http://arxiv.org/abs/2405.13792v2,"xRAG: Extreme Context Compression for Retrieval-augmented Generation
  with One Token","This paper introduces xRAG, an innovative context compression method tailored
for retrieval-augmented generation. xRAG reinterprets document embeddings in
dense retrieval--traditionally used solely for retrieval--as features from the
retrieval modality. By employing a modality fusion methodology, xRAG seamlessly
integrates these embeddings into the language model representation space,
effectively eliminating the need for their textual counterparts and achieving
an extreme compression rate. In xRAG, the only trainable component is the
modality bridge, while both the retriever and the language model remain frozen.
This design choice allows for the reuse of offline-constructed document
embeddings and preserves the plug-and-play nature of retrieval augmentation.
Experimental results demonstrate that xRAG achieves an average improvement of
over 10% across six knowledge-intensive tasks, adaptable to various language
model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts
configuration. xRAG not only significantly outperforms previous context
compression methods but also matches the performance of uncompressed models on
several datasets, while reducing overall FLOPs by a factor of 3.53. Our work
pioneers new directions in retrieval-augmented generation from the perspective
of multimodality fusion, and we hope it lays the foundation for future
efficient and scalable retrieval-augmented systems",Xin Cheng
2024-06-11T15:15:33Z,http://arxiv.org/abs/2406.07348v3,"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented
  Generation for Question-Answering","Retrieval-Augmented Generation (RAG) has recently demonstrated the
performance of Large Language Models (LLMs) in the knowledge-intensive tasks
such as Question-Answering (QA). RAG expands the query context by incorporating
external knowledge bases to enhance the response accuracy. However, it would be
inefficient to access LLMs multiple times for each query and unreliable to
retrieve all the relevant documents by a single query. We have found that even
though there is low relevance between some critical documents and query, it is
possible to retrieve the remaining documents by combining parts of the
documents with the query. To mine the relevance, a two-stage retrieval
framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is
proposed to improve document retrieval recall and the accuracy of answers while
maintaining efficiency. Additionally, a compact classifier is applied to two
different selection strategies to determine the contribution of the retrieved
documents to answering the query and retrieve the relatively relevant
documents. Meanwhile, DR-RAG call the LLMs only once, which significantly
improves the efficiency of the experiment. The experimental results on
multi-hop QA datasets show that DR-RAG can significantly improve the accuracy
of the answers and achieve new progress in QA systems.",Zijian Hei
2024-07-24T12:27:33Z,http://arxiv.org/abs/2407.21055v1,"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications","Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.",Cui Long
2024-09-06T13:06:29Z,http://arxiv.org/abs/2409.13707v1,"Retrieval Augmented Generation-Based Incident Resolution Recommendation
  System for IT Support","Clients wishing to implement generative AI in the domain of IT Support and
AIOps face two critical issues: domain coverage and model size constraints due
to model choice limitations. Clients might choose to not use larger proprietary
models such as GPT-4 due to cost and privacy concerns and so are limited to
smaller models with potentially less domain coverage that do not generalize to
the client's domain. Retrieval augmented generation is a common solution that
addresses both of these issues: a retrieval system first retrieves the
necessary domain knowledge which a smaller generative model leverages as
context for generation. We present a system developed for a client in the IT
Support domain for support case solution recommendation that combines retrieval
augmented generation (RAG) for answer generation with an encoder-only model for
classification and a generative large language model for query generation. We
cover architecture details, data collection and annotation, development journey
and preliminary validations, expected final deployment process and evaluation
plans, and finally lessons learned.",Paulina Toro Isaza
2024-10-15T14:51:45Z,http://arxiv.org/abs/2410.22353v1,"RuleRAG: Rule-guided retrieval-augmented generation with language models
  for question answering","Retrieval-augmented generation (RAG) framework has shown promising potential
in knowledge-intensive question answering (QA) by retrieving external corpus
and generating based on augmented context. However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-Guided
Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic
rules as demonstrations for in-context learning (RuleRAG-ICL) to guide
retrievers to retrieve logically related documents in the directions of rules
and uniformly guide generators to generate answers attributed by the guidance
of the same set of rules. Moreover, the combination of queries and rules can be
further used as supervised fine-tuning data to update retrievers and generators
(RuleRAG-FT) to achieve better rule-based instruction following capability,
leading to retrieve more supportive results and generate more acceptable
answers. To emphasize the attribution of rules, we construct five rule-aware QA
benchmarks, including three temporal and two static scenarios, and equip
RuleRAG with several kinds of retrievers and generators. Experiments
demonstrate that training-free RuleRAG-ICL effectively improves the retrieval
quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in
exact match scores over standard RAG on average across the five benchmarks, and
further fine-tuned RuleRAG-FT consistently yields more significant performance
enhancement. Extensive analyses indicate that RuleRAG scales well with
increasing numbers of retrieved documents and exhibits generalization ability
for untrained rules.",Zhongwu Chen
2024-11-01T17:11:16Z,http://arxiv.org/abs/2411.00744v1,"CORAG: A Cost-Constrained Retrieval Optimization System for
  Retrieval-Augmented Generation","Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.",Ziting Wang
2024-11-25T16:10:05Z,http://arxiv.org/abs/2411.16523v1,"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology
  Report Generation","In the current paradigm of image captioning, deep learning models are trained
to generate text from image embeddings of latent features. We challenge the
assumption that these latent features ought to be high-dimensional vectors
which require model fine tuning to handle. Here we propose Label Boosted
Retrieval Augmented Generation (LaB-RAG), a text-based approach to image
captioning that leverages image descriptors in the form of categorical labels
to boost standard retrieval augmented generation (RAG) with pretrained large
language models (LLMs). We study our method in the context of radiology report
generation (RRG), where the task is to generate a clinician's report detailing
their observations from a set of radiological images, such as X-rays. We argue
that simple linear classifiers over extracted image embeddings can effectively
transform X-rays into text-space as radiology-specific labels. In combination
with standard RAG, we show that these derived text labels can be used with
general-domain LLMs to generate radiology reports. Without ever training our
generative language model or image feature encoder models, and without ever
directly ""showing"" the LLM an X-ray, we demonstrate that LaB-RAG achieves
better results across natural language and radiology language metrics compared
with other retrieval-based RRG methods, while attaining competitive results
compared to other fine-tuned vision-language RRG models. We further present
results of our experiments with various components of LaB-RAG to better
understand our method. Finally, we critique the use of a popular RRG metric,
arguing it is possible to artificially inflate its results without true
data-leakage.",Steven Song
2024-12-08T07:18:19Z,http://arxiv.org/abs/2412.05838v1,"A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation
  Across Diverse Data","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
incorporating external, domain-specific data into the generative process. While
LLMs are highly capable, they often rely on static, pre-trained datasets,
limiting their ability to integrate dynamic or private data. Traditional RAG
systems typically use a single-agent architecture to handle query generation,
data retrieval, and response synthesis. However, this approach becomes
inefficient when dealing with diverse data sources, such as relational
databases, document stores, and graph databases, often leading to performance
bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system
to address these limitations. Specialized agents, each optimized for a specific
data source, handle query generation for relational, NoSQL, and document-based
systems. These agents collaborate within a modular framework, with query
execution delegated to an environment designed for compatibility across various
database types. This distributed approach enhances query efficiency, reduces
token overhead, and improves response accuracy by ensuring that each agent
focuses on its specialized task. The proposed system is scalable and adaptable,
making it ideal for generative AI workflows that require integration with
diverse, dynamic, or private data sources. By leveraging specialized agents and
a modular execution environment, the system provides an efficient and robust
solution for handling complex, heterogeneous data environments in generative AI
applications.",Aniruddha Salve
2024-12-17T18:42:21Z,http://arxiv.org/abs/2412.13163v2,C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System,"Organizations seeking to utilize Large Language Models (LLMs) for knowledge
querying and analysis often encounter challenges in maintaining an LLM
fine-tuned on targeted, up-to-date information that keeps answers relevant and
grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible
solution for organizations looking to overcome the challenges of maintaining
proprietary models and to help reduce LLM hallucinations in their query
responses. However, RAG comes with its own issues regarding scaling data
pipelines across tiered-access and disparate data sources. In many scenarios,
it is necessary to query beyond a single data silo to provide richer and more
relevant context for an LLM. Analyzing data sources within and across
organizational trust boundaries is often limited by complex data-sharing
policies that prohibit centralized data storage, therefore, inhibit the fast
and effective setup and scaling of RAG solutions. In this paper, we introduce
Confidential Computing (CC) techniques as a solution for secure Federated
Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG
system (C-FedRAG) enables secure connection and scaling of a RAG workflows
across a decentralized network of data providers by ensuring context
confidentiality. We also demonstrate how to implement a C-FedRAG system using
the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and
MIRAGE benchmarking dataset.",Parker Addison
2024-03-31T12:01:34Z,http://arxiv.org/abs/2404.00657v1,Observations on Building RAG Systems for Technical Documents,"Retrieval augmented generation (RAG) for technical documents creates
challenges as embeddings do not often capture domain information. We review
prior art for important factors affecting RAG and perform experiments to
highlight best practices and potential challenges to build RAG systems for
technical documents.",Sumit Soman
2021-02-09T04:50:35Z,http://arxiv.org/abs/2102.04643v1,"Efficient Retrieval Augmented Generation from Unstructured Knowledge for
  Task-Oriented Dialog","This paper summarizes our work on the first track of the ninth Dialog System
Technology Challenge (DSTC 9), ""Beyond Domain APIs: Task-oriented
Conversational Modeling with Unstructured Knowledge Access"". The goal of the
task is to generate responses to user turns in a task-oriented dialog that
require knowledge from unstructured documents. The task is divided into three
subtasks: detection, selection and generation. In order to be compute
efficient, we formulate the selection problem in terms of hierarchical
classification steps. We achieve our best results with this model.
Alternatively, we employ siamese sequence embedding models, referred to as
Dense Knowledge Retrieval, to retrieve relevant documents. This method further
reduces the computation time by a factor of more than 100x at the cost of
degradation in R@1 of 5-6% compared to the first model. Then for either
approach, we use Retrieval Augmented Generation to generate responses based on
multiple selected snippets and we show how the method can be used to fine-tune
trained embeddings.",David Thulke
2022-11-14T02:00:32Z,http://arxiv.org/abs/2211.07067v1,"Retrieval-Augmented Generative Question Answering for Event Argument
  Extraction","Event argument extraction has long been studied as a sequential prediction
problem with extractive-based methods, tackling each argument in isolation.
Although recent work proposes generation-based methods to capture
cross-argument dependency, they require generating and post-processing a
complicated target sequence (template). Motivated by these observations and
recent pretrained language models' capabilities of learning from
demonstrations. We propose a retrieval-augmented generative QA model (R-GQA)
for event argument extraction. It retrieves the most similar QA pair and
augments it as prompt to the current example's context, then decodes the
arguments as answers. Our approach outperforms substantially prior methods
across various settings (i.e. fully supervised, domain transfer, and fewshot
learning). Finally, we propose a clustering-based sampling strategy (JointEnc)
and conduct a thorough analysis of how different strategies influence the
few-shot learning performance. The implementations are available at https://
github.com/xinyadu/RGQA",Xinya Du
2023-09-27T21:26:03Z,http://arxiv.org/abs/2309.16035v3,"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering","Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks such as medical question answering
(QA). In addition, LLMs tend to function as ""black-boxes"", making it
challenging to modify their behavior. To address the problem, our work employs
a transparent process of retrieval augmented generation (RAG), aiming to
improve LLM responses without the need for fine-tuning or retraining.
Specifically, we propose a comprehensive retrieval strategy to extract medical
facts from an external knowledge base, and then inject them into the LLM's
query prompt. Focusing on medical QA, we evaluate the impact of different
retrieval models and the number of facts on LLM performance using the
MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited
an accuracy improvement from 44.46% to 48.54%. This work underscores the
potential of RAG to enhance LLM performance, offering a practical approach to
mitigate the challenges posed by black-box LLMs.",Yucheng Shi
2023-11-16T00:39:39Z,http://arxiv.org/abs/2311.09476v2,"ARES: An Automated Evaluation Framework for Retrieval-Augmented
  Generation Systems","Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. By creating its own synthetic training data, ARES finetunes
lightweight LM judges to assess the quality of individual RAG components. To
mitigate potential prediction errors, ARES utilizes a small set of
human-annotated datapoints for prediction-powered inference (PPI). Across eight
different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES
accurately evaluates RAG systems while using only a few hundred human
annotations during evaluation. Furthermore, ARES judges remain effective across
domain shifts, proving accurate even after changing the type of queries and/or
documents used in the evaluated RAG systems. We make our code and datasets
publicly available on Github.",Jon Saad-Falcon
2023-11-29T15:02:46Z,http://arxiv.org/abs/2311.17696v4,"How to Build an AI Tutor that Can Adapt to Any Course and Provide
  Accurate Answers Using Large Language Model and Retrieval-Augmented
  Generation","This paper proposes a low-code solution to build an AI tutor that leverages
advanced AI techniques to provide accurate and contextually relevant responses
in a personalized learning environment. The OpenAI Assistants API allows AI
Tutor to easily embed, store, retrieve, and manage files and chat history,
enabling a low-code solution. Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG) technology generate sophisticated answers
based on course-specific materials. The application efficiently organizes and
retrieves relevant information through vector embedding and similarity-based
retrieval algorithms. The AI Tutor prototype demonstrates its ability to
generate relevant, accurate answers with source citations. It represents a
significant advancement in technology-enhanced tutoring systems, democratizing
access to high-quality, customized educational support in higher education.",Chenxi Dong
2023-12-09T23:33:16Z,http://arxiv.org/abs/2312.05708v1,Context Tuning for Retrieval Augmented Generation,"Large language models (LLMs) have the remarkable ability to solve new tasks
with just a few examples, but they need access to the right tools. Retrieval
Augmented Generation (RAG) addresses this problem by retrieving a list of
relevant tools for a given task. However, RAG's tool retrieval step requires
all the required information to be explicitly present in the query. This is a
limitation, as semantic search, the widely adopted tool retrieval method, can
fail when the query is incomplete or lacks context. To address this limitation,
we propose Context Tuning for RAG, which employs a smart context retrieval
system to fetch relevant information that improves both tool retrieval and plan
generation. Our lightweight context retrieval model uses numerical,
categorical, and habitual usage signals to retrieve and rank context items. Our
empirical results demonstrate that context tuning significantly enhances
semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for
context retrieval and tool retrieval tasks respectively, and resulting in an
11.6% increase in LLM-based planner accuracy. Additionally, we show that our
proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART
outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at
plan generation, even after tool retrieval, reduces hallucination.",Raviteja Anantha
2023-12-08T18:50:20Z,http://arxiv.org/abs/2312.07559v2,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,"Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA's matches expert human researchers on LitQA.",Jakub Lála
2023-12-19T23:11:06Z,http://arxiv.org/abs/2312.13303v2,"RealGen: Retrieval Augmented Generation for Controllable Traffic
  Scenarios","Simulation plays a crucial role in the development of autonomous vehicles
(AVs) due to the potential risks associated with real-world testing. Although
significant progress has been made in the visual aspects of simulators,
generating complex behavior among agents remains a formidable challenge. It is
not only imperative to ensure realism in the scenarios generated but also
essential to incorporate preferences and conditions to facilitate controllable
generation for AV training and evaluation. Traditional methods, mainly relying
on memorizing the distribution of training datasets, often fall short in
generating unseen scenarios. Inspired by the success of retrieval augmented
generation in large language models, we present RealGen, a novel
retrieval-based in-context learning framework for traffic scenario generation.
RealGen synthesizes new scenarios by combining behaviors from multiple
retrieved examples in a gradient-free way, which may originate from templates
or tagged scenarios. This in-context learning framework endows versatile
generative capabilities, including the ability to edit scenarios, compose
various behaviors, and produce critical scenarios. Evaluations show that
RealGen offers considerable flexibility and controllability, marking a new
direction in the field of controllable traffic scenario generation. Check our
project website for more information: https://realgen.github.io.",Wenhao Ding
2024-01-03T00:09:34Z,http://arxiv.org/abs/2401.01469v1,"Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation","Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.",Walid Saba
2024-01-03T17:01:44Z,http://arxiv.org/abs/2401.01835v1,"Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework
  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)","Addressing the complexity of comprehensive information retrieval, this study
introduces an innovative, iterative retrieval-augmented generation system. Our
approach uniquely integrates a vector-space driven re-ranking mechanism with
concurrent brainstorming to expedite the retrieval of highly relevant
documents, thereby streamlining the generation of potential queries. This sets
the stage for our novel hybrid process, which synergistically combines
hypothesis formulation with satisfying decision-making strategy to determine
content adequacy, leveraging a chain of thought-based prompting technique. This
unified hypothesize-satisfied phase intelligently distills information to
ascertain whether user queries have been satisfactorily addressed. Upon
reaching this criterion, the system refines its output into a concise
representation, maximizing conceptual density with minimal verbosity. The
iterative nature of the workflow enhances process efficiency and accuracy.
Crucially, the concurrency within the brainstorming phase significantly
accelerates recursive operations, facilitating rapid convergence to solution
satisfaction. Compared to conventional methods, our system demonstrates a
marked improvement in computational time and cost-effectiveness. This research
advances the state-of-the-art in intelligent retrieval systems, setting a new
benchmark for resource-efficient information extraction and abstraction in
knowledge-intensive applications.",Arash Shahmansoori
2024-01-11T12:04:11Z,http://arxiv.org/abs/2401.05856v1,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",Scott Barnett
2024-01-20T14:59:43Z,http://arxiv.org/abs/2401.11246v1,"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented
  Generation in Niche Domains, Exemplified by Korean Medicine","We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.",Bongsu Kang
2024-01-23T09:54:36Z,http://arxiv.org/abs/2401.12599v1,"Revolutionizing Retrieval-Augmented Generation with Enhanced PDF
  Structure Recognition","With the rapid development of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) has become a predominant method in the
field of professional knowledge-based question answering. Presently, major
foundation model companies have opened up Embedding and Chat API interfaces,
and frameworks like LangChain have already integrated the RAG process. It
appears that the key models and steps in RAG have been resolved, leading to the
question: are professional knowledge QA systems now approaching perfection?
This article discovers that current primary methods depend on the premise of
accessing high-quality text corpora. However, since professional documents are
mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts
the effectiveness of professional knowledge-based QA. We conducted an empirical
RAG experiment across hundreds of questions from the corresponding real-world
professional documents. The results show that, ChatDOC, a RAG system equipped
with a panoptic and pinpoint PDF parser, retrieves more accurate and complete
segments, and thus better answers. Empirical experiments show that ChatDOC is
superior to baseline on nearly 47% of questions, ties for 38% of cases, and
falls short on only 15% of cases. It shows that we may revolutionize RAG with
enhanced PDF structure recognition.",Demiao Lin
2024-01-26T08:23:29Z,http://arxiv.org/abs/2402.01717v1,"From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical
  Regulatory Compliance Process","Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.",Jaewoong Kim
2024-01-31T22:06:07Z,http://arxiv.org/abs/2402.03367v2,RAG-Fusion: a New Take on Retrieval-Augmented Generation,"Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.",Zackary Rackauckas
2024-02-05T11:58:56Z,http://arxiv.org/abs/2402.05128v2,"Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation","Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.",Hessa Abdulrahman Alawwad
2024-02-05T22:35:42Z,http://arxiv.org/abs/2402.05131v3,Financial Report Chunking for Effective Retrieval Augmented Generation,"Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question & Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.",Antonio Jimeno Yepes
2024-02-18T15:41:31Z,http://arxiv.org/abs/2402.11626v1,Metacognitive Retrieval-Augmented Large Language Models,"Retrieval-augmented generation have become central in natural language
processing due to their efficacy in generating factual content. While
traditional methods employ single-time retrieval, more recent approaches have
shifted towards multi-time retrieval for multi-hop reasoning tasks. However,
these strategies are bound by predefined reasoning steps, potentially leading
to inaccuracies in response generation. This paper introduces MetaRAG, an
approach that combines the retrieval-augmented generation process with
metacognition. Drawing from cognitive psychology, metacognition allows an
entity to self-reflect and critically evaluate its cognitive processes. By
integrating this, MetaRAG enables the model to monitor, evaluate, and plan its
response strategies, enhancing its introspective reasoning abilities. Through a
three-step metacognitive regulation pipeline, the model can identify
inadequacies in initial cognitive responses and fixes them. Empirical
evaluations show that MetaRAG significantly outperforms existing methods.",Yujia Zhou
2024-02-19T02:48:44Z,http://arxiv.org/abs/2402.11794v1,"Unveiling the Magic: Investigating Attention Distillation in
  Retrieval-augmented Generation","Retrieval-augmented generation framework can address the limitations of large
language models by enabling real-time knowledge updates for more accurate
answers. An efficient way in the training phase of retrieval-augmented models
is attention distillation, which uses attention scores as a supervision signal
instead of manually annotated query-document pairs. Despite its growing
popularity, the detailed mechanisms behind the success of attention
distillation remain unexplored, particularly the specific patterns it leverages
to benefit training. In this paper, we address this gap by conducting a
comprehensive review of attention distillation workflow and identifying key
factors influencing the learning quality of retrieval-augmented language
models. We further propose indicators for optimizing models' training methods
and avoiding ineffective training.",Zizhong Li
2024-02-19T07:06:52Z,http://arxiv.org/abs/2402.11891v1,"FeB4RAG: Evaluating Federated Search in the Context of Retrieval
  Augmented Generation","Federated search systems aggregate results from multiple search engines,
selecting appropriate sources to enhance result quality and align with user
intent. With the increasing uptake of Retrieval-Augmented Generation (RAG)
pipelines, federated search can play a pivotal role in sourcing relevant
information across heterogeneous data sources to generate informed responses.
However, existing datasets, such as those developed in the past TREC FedWeb
tracks, predate the RAG paradigm shift and lack representation of modern
information retrieval challenges. To bridge this gap, we present FeB4RAG, a
novel dataset specifically designed for federated search within RAG frameworks.
This dataset, derived from 16 sub-collections of the widely used \beir
benchmarking collection, includes 790 information requests (akin to
conversational queries) tailored for chatbot applications, along with top
results returned by each resource and associated LLM-derived relevance
judgements. Additionally, to support the need for this collection, we
demonstrate the impact on response generation of a high quality federated
search system for RAG compared to a naive approach to federated search. We do
so by comparing answers generated through the RAG pipeline through a
qualitative side-by-side comparison. Our collection fosters and supports the
development and evaluation of new federated search methods, especially in the
context of RAG pipelines.",Shuai Wang
2024-02-26T09:59:04Z,http://arxiv.org/abs/2402.16457v2,"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for
  Short-form Open-Domain Question Answering","Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine
the necessity of retrieval for queries instead of retrieving indiscriminately
to enhance the efficiency and relevance of the sourced information. However,
previous works largely overlook the evaluation of ARAG approaches, leading to
their effectiveness being understudied. This work presents a benchmark,
RetrievalQA, comprising 1,271 short-form questions covering new world and
long-tail knowledge. The knowledge necessary to answer the questions is absent
from LLMs; therefore, external information must be retrieved to answer
correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG
methods. We observe that calibration-based methods heavily rely on threshold
tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable
retrieval decisions. Based on our findings, we propose Time-Aware Adaptive
Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the
necessity of retrieval without calibration or additional training. The dataset
and code will be available at https://github.com/hyintell/RetrievalQA",Zihan Zhang
2024-02-23T18:35:15Z,http://arxiv.org/abs/2402.16893v1,"The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)","Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.",Shenglai Zeng
2024-02-26T12:56:17Z,http://arxiv.org/abs/2403.00820v1,"Retrieval Augmented Generation Systems: Automatic Dataset Creation,
  Evaluation and Boolean Agent Setup","Retrieval Augmented Generation (RAG) systems have seen huge popularity in
augmenting Large-Language Model (LLM) outputs with domain specific and time
sensitive data. Very recently a shift is happening from simple RAG setups that
query a vector database for additional information with every user input to
more sophisticated forms of RAG. However, different concrete approaches compete
on mostly anecdotal evidence at the moment. In this paper we present a rigorous
dataset creation and evaluation workflow to quantitatively compare different
RAG strategies. We use a dataset created this way for the development and
evaluation of a boolean agent RAG setup: A system in which a LLM can decide
whether to query a vector database or not, thus saving tokens on questions that
can be answered with internal knowledge. We publish our code and generated
dataset online.",Tristan Kenneweg
2024-03-07T06:38:41Z,http://arxiv.org/abs/2403.04256v1,Federated Recommendation via Hybrid Retrieval Augmented Generation,"Federated Recommendation (FR) emerges as a novel paradigm that enables
privacy-preserving recommendations. However, traditional FR systems usually
represent users/items with discrete identities (IDs), suffering from
performance degradation due to the data sparsity and heterogeneity in FR. On
the other hand, Large Language Models (LLMs) as recommenders have proven
effective across various recommendation scenarios. Yet, LLM-based recommenders
encounter challenges such as low inference efficiency and potential
hallucination, compromising their performance in real-world scenarios. To this
end, we propose GPT-FedRec, a federated recommendation framework leveraging
ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.
GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval
process, mining ID-based user patterns and text-based item features. Next, the
retrieved results are converted into text prompts and fed into GPT for
re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims
to extract generalized features from data and exploit pretrained knowledge
within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the
RAG approach also prevents LLM hallucination, improving the recommendation
performance for real-world users. Experimental results on diverse benchmark
datasets demonstrate the superior performance of GPT-FedRec against
state-of-the-art baseline methods.",Huimin Zeng
2024-03-08T21:09:20Z,http://arxiv.org/abs/2403.05676v1,"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System
  Co-design","Retrieval-augmented generation (RAG) can enhance the generation quality of
large language models (LLMs) by incorporating external token databases.
However, retrievals from large databases can constitute a substantial portion
of the overall generation time, particularly when retrievals are periodically
performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design
approach to reduce generation latency and enhance generation quality. PipeRAG
integrates (1) pipeline parallelism to enable concurrent retrieval and
generation processes, (2) flexible retrieval intervals to maximize the
efficiency of pipeline parallelism, and (3) a performance model to
automatically balance retrieval quality and latency based on the generation
states and underlying hardware. Our evaluation shows that, by combining the
three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in
end-to-end generation latency while improving generation quality. These
promising results showcase the effectiveness of co-designing algorithms with
underlying systems, paving the way for the adoption of PipeRAG in future RAG
systems.",Wenqi Jiang
2024-03-13T18:47:00Z,http://arxiv.org/abs/2403.08904v1,"Detecting Hallucination and Coverage Errors in Retrieval Augmented
  Generation for Controversial Topics","We explore a strategy to handle controversial topics in LLM-based chatbots
based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the
absence of a single true answer and surface multiple perspectives. We frame
this as retrieval augmented generation, where perspectives are retrieved from a
knowledge base and the LLM is tasked with generating a fluent and faithful
response from the given perspectives. As a starting point, we use a
deterministic retrieval system and then focus on common LLM failure modes that
arise during this approach to text generation, namely hallucination and
coverage errors. We propose and evaluate three methods to detect such errors
based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our
results demonstrate that LLM-based classifiers, even when trained only on
synthetic errors, achieve high error detection performance, with ROC AUC scores
of 95.3% for hallucination and 90.5% for coverage error detection on
unambiguous error cases. We show that when no training data is available, our
other methods still yield good results on hallucination (84.0%) and coverage
error (85.2%) detection.",Tyler A. Chang
2024-03-14T02:26:31Z,http://arxiv.org/abs/2403.09040v2,"RAGGED: Towards Informed Design of Retrieval Augmented Generation
  Systems","Retrieval-augmented generation (RAG) can significantly improve the
performance of language models (LMs) by providing additional context for tasks
such as document-based question answering (DBQA). However, the effectiveness of
RAG is highly dependent on its configuration. To systematically find the
optimal configuration, we introduce RAGGED, a framework for analyzing RAG
configurations across various DBQA tasks. Using the framework, we discover
distinct LM behaviors in response to varying context quantities, context
qualities, and retrievers. For instance, while some models are robust to noisy
contexts, monotonically performing better with more contexts, others are more
noise-sensitive and can effectively use only a few contexts before declining in
performance. This framework also provides a deeper analysis of these
differences by evaluating the LMs' sensitivity to signal and noise under
specific context quality conditions. Using RAGGED, researchers and
practitioners can derive actionable insights about how to optimally configure
their RAG systems for their specific question-answering tasks.",Jennifer Hsia
2024-02-25T11:22:19Z,http://arxiv.org/abs/2403.12077v1,"Evaluating Robustness of Generative Search Engine on Adversarial Factual
  Questions","Generative search engines have the potential to transform how people seek
information online, but generated responses from existing large language models
(LLMs)-backed generative search engines may not always be accurate.
Nonetheless, retrieval-augmented generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable part of a claim. To this end, we propose evaluating the
robustness of generative search engines in the realistic and high-risk setting,
where adversaries have only black-box system access and seek to deceive the
model into returning incorrect responses. Through a comprehensive human
evaluation of various generative search engines, such as Bing Chat,
PerplexityAI, and YouChat across diverse queries, we demonstrate the
effectiveness of adversarial factual questions in inducing incorrect responses.
Moreover, retrieval-augmented generation exhibits a higher susceptibility to
factual errors compared to LLMs without retrieval. These findings highlight the
potential security risks of these systems and emphasize the need for rigorous
evaluation before deployment.",Xuming Hu
2024-03-18T15:19:17Z,http://arxiv.org/abs/2403.15450v1,Loops On Retrieval Augmented Generation (LoRAG),"This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new
framework designed to enhance the quality of retrieval-augmented text
generation through the incorporation of an iterative loop mechanism. The
architecture integrates a generative model, a retrieval mechanism, and a
dynamic loop module, allowing for iterative refinement of the generated text
through interactions with relevant information retrieved from the input
context. Experimental evaluations on benchmark datasets demonstrate that LoRAG
surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score,
and perplexity, showcasing its effectiveness in achieving both coherence and
relevance in generated text. The qualitative assessment further illustrates
LoRAG's capability to produce contextually rich and coherent outputs. This
research contributes valuable insights into the potential of iterative loops in
mitigating challenges in text generation, positioning LoRAG as a promising
advancement in the field.",Ayush Thakur
2024-03-24T21:02:35Z,http://arxiv.org/abs/2403.16295v1,"LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation","With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.",Ashish Chouhan
2024-03-27T08:42:31Z,http://arxiv.org/abs/2403.18350v2,"Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language","The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).",Ali Mahboub
2024-03-28T17:07:02Z,http://arxiv.org/abs/2403.19584v1,"Img2Loc: Revisiting Image Geolocalization using Multi-modality
  Foundation Models and Image-based Retrieval-Augmented Generation","Geolocating precise locations from images presents a challenging problem in
computer vision and information retrieval.Traditional methods typically employ
either classification, which dividing the Earth surface into grid cells and
classifying images accordingly, or retrieval, which identifying locations by
matching images with a database of image-location pairs. However,
classification-based approaches are limited by the cell size and cannot yield
precise predictions, while retrieval-based systems usually suffer from poor
search quality and inadequate coverage of the global landscape at varied scale
and aggregation levels. To overcome these drawbacks, we present Img2Loc, a
novel system that redefines image geolocalization as a text generation task.
This is achieved using cutting-edge large multi-modality models like GPT4V or
LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based
representations to generate an image-based coordinate query database. It then
uniquely combines query results with images itself, forming elaborate prompts
customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and
YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art
models but does so without any model training.",Zhongliang Zhou
2024-04-04T15:21:22Z,http://arxiv.org/abs/2404.03514v2,"Embedding-Informed Adaptive Retrieval-Augmented Generation of Large
  Language Models","Retrieval-augmented large language models (LLMs) have been remarkably
competent in various NLP tasks. However, it was observed by previous works that
retrieval is not always helpful, especially when the LLM is already
knowledgeable on the query to answer. Motivated by this, Adaptive
Retrieval-Augmented Generation (ARAG) studies retrieving only when the
knowledge asked by the query is absent in the LLM. Previous works of ARAG
either require accessing the pre-training corpus or prompting with additional
model inferences. Aiming to avoid such drawbacks, we propose to determine
whether the model is knowledgeable on a query via inspecting the
(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such
embeddings capture rich information on the model's intrinsic knowledge base,
which enables an efficient way of judging the necessity to retrieve from an
external corpus. Extensive experiments demonstrate our ARAG approach's superior
performance across various benchmarks.",Chengkai Huang
2024-04-04T21:47:43Z,http://arxiv.org/abs/2404.04302v1,"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs
  for Legal Question Answering","Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)
output by providing prior knowledge as context to input. This is beneficial for
knowledge-intensive and expert reliant tasks, including legal
question-answering, which require evidence to validate generated text outputs.
We highlight that Case-Based Reasoning (CBR) presents key opportunities to
structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,
where CBR cycle's initial retrieval stage, its indexing vocabulary, and
similarity knowledge containers are used to enhance LLM queries with
contextually relevant cases. This integration augments the original LLM query,
providing a richer prompt. We present an evaluation of CBR-RAG, and examine
different representations (i.e. general and domain-specific embeddings) and
methods of comparison (i.e. inter, intra and hybrid similarity) on the task of
legal question-answering. Our results indicate that the context provided by
CBR's case reuse enforces similarity between relevant components of the
questions and the evidence base leading to significant improvements in the
quality of generated answers.",Nirmalie Wiratunga
2024-04-10T11:03:17Z,http://arxiv.org/abs/2404.06910v2,"Superposition Prompting: Improving and Accelerating Retrieval-Augmented
  Generation","Despite the successes of large language models (LLMs), they exhibit
significant drawbacks, particularly when processing long contexts. Their
inference cost scales quadratically with respect to sequence length, making it
expensive for deployment in some real-world text processing applications, such
as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the
""distraction phenomenon"", where irrelevant context in the prompt degrades
output quality. To address these drawbacks, we propose a novel RAG prompting
methodology, *superposition prompting*, which can be directly applied to
pre-trained transformer-based LLMs *without the need for fine-tuning*. At a
high level, superposition prompting allows the LLM to process input documents
in parallel *prompt paths*, discarding paths once they are deemed irrelevant.
We demonstrate the capability of our method to simultaneously enhance time
efficiency across a variety of question-answering benchmarks using multiple
pre-trained LLMs. Furthermore, our technique significantly improves accuracy
when the retrieved context is large relative the context the model was trained
on. For example, our approach facilitates a 93x reduction in compute time while
*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the
MPT-7B instruction-tuned model over naive RAG.",Thomas Merth
2024-03-22T17:13:46Z,http://arxiv.org/abs/2404.07220v2,"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy
  with Semantic Search and Hybrid Query-Based Retrievers","Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a
private knowledge base of documents with Large Language Models (LLM) to build
Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes
increasingly challenging as the corpus of documents scales up, with Retrievers
playing an outsized role in the overall RAG accuracy by extracting the most
relevant document from the corpus to provide context to the LLM. In this paper,
we propose the 'Blended RAG' method of leveraging semantic search techniques,
such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid
query strategies. Our study achieves better retrieval results and sets new
benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID
datasets. We further extend such a 'Blended Retriever' to the RAG system to
demonstrate far superior results on Generative Q\&A datasets like SQUAD, even
surpassing fine-tuning performance.",Kunal Sawarkar
2024-04-12T01:42:09Z,http://arxiv.org/abs/2404.08189v1,"Reducing hallucination in structured outputs via Retrieval-Augmented
  Generation","A common and fundamental limitation of Generative AI (GenAI) is its
propensity to hallucinate. While large language models (LLM) have taken the
world by storm, without eliminating or at least reducing hallucinations,
real-world GenAI systems may face challenges in user adoption. In the process
of deploying an enterprise application that produces workflows based on natural
language requirements, we devised a system leveraging Retrieval Augmented
Generation (RAG) to greatly improve the quality of the structured output that
represents such workflows. Thanks to our implementation of RAG, our proposed
system significantly reduces hallucinations in the output and improves the
generalization of our LLM in out-of-domain settings. In addition, we show that
using a small, well-trained retriever encoder can reduce the size of the
accompanying LLM, thereby making deployments of LLM-based systems less
resource-intensive.",Patrice Béchard
2024-04-19T00:48:30Z,http://arxiv.org/abs/2404.12560v1,"Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for
  Text-to-SQL","The current state-of-the-art (SOTA) for automated text-to-SQL still falls
well short of expert human performance as measured by execution accuracy (EX)
on the BIRD-SQL benchmark. The most accurate methods are also slow and
expensive. To advance the SOTA for text-to-SQL while reducing cost and
improving speed, we explore the combination of low-cost fine tuning, novel
methods for diverse retrieval-augmented generation (RAG) and new input and
output formats that help large language models (LLMs) achieve higher EX. We
introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record
for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher
performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI,
but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the
next-best model using OpenAI, which instead uses the more expensive GPT-4.
Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by
over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push
EX higher.",Dayton G. Thorpe
2024-04-19T13:27:38Z,http://arxiv.org/abs/2404.12879v1,"Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented
  Generation","While Retrieval-Augmented Generation (RAG) plays a crucial role in the
application of Large Language Models (LLMs), existing retrieval methods in
knowledge-dense domains like law and medicine still suffer from a lack of
multi-perspective views, which are essential for improving interpretability and
reliability. Previous research on multi-view retrieval often focused solely on
different semantic forms of queries, neglecting the expression of specific
domain knowledge perspectives. This paper introduces a novel multi-view RAG
framework, MVRAG, tailored for knowledge-dense domains that utilizes
intention-aware query rewriting from multiple domain viewpoints to enhance
retrieval precision, thereby improving the effectiveness of the final
inference. Experiments conducted on legal and medical case retrieval
demonstrate significant improvements in recall and precision rates with our
framework. Our multi-perspective retrieval approach unleashes the potential of
multi-view information enhancing RAG tasks, accelerating the further
application of LLMs in knowledge-intensive fields.",Guanhua Chen
2024-04-21T21:22:28Z,http://arxiv.org/abs/2404.13781v1,Evaluating Retrieval Quality in Retrieval-Augmented Generation,"Evaluating retrieval-augmented generation (RAG) presents challenges,
particularly for retrieval models within these systems. Traditional end-to-end
evaluation methods are computationally expensive. Furthermore, evaluation of
the retrieval model's performance based on query-document relevance labels
shows a small correlation with the RAG system's downstream performance. We
propose a novel evaluation approach, eRAG, where each document in the retrieval
list is individually utilized by the large language model within the RAG
system. The output generated for each document is then evaluated based on the
downstream task ground truth labels. In this manner, the downstream performance
for each document serves as its relevance label. We employ various downstream
task metrics to obtain document-level annotations and aggregate them using
set-based or ranking metrics. Extensive experiments on a wide range of datasets
demonstrate that eRAG achieves a higher correlation with downstream RAG
performance compared to baseline methods, with improvements in Kendall's $\tau$
correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant
computational advantages, improving runtime and consuming up to 50 times less
GPU memory than end-to-end evaluation.",Alireza Salemi
2024-05-05T05:42:33Z,http://arxiv.org/abs/2405.02816v1,"Stochastic RAG: End-to-End Retrieval-Augmented Generation through
  Expected Utility Maximization","This paper introduces Stochastic RAG--a novel approach for end-to-end
optimization of retrieval-augmented generation (RAG) models that relaxes the
simplifying assumptions of marginalization and document independence, made in
most prior work. Stochastic RAG casts the retrieval process in RAG as a
stochastic sampling without replacement process. Through this formulation, we
employ straight-through Gumbel-top-k that provides a differentiable
approximation for sampling without replacement and enables effective end-to-end
optimization for RAG. We conduct extensive experiments on seven diverse
datasets on a wide range of tasks, from open-domain question answering to fact
verification to slot-filling for relation extraction and to dialogue systems.
By applying this optimization method to a recent and effective RAG model, we
advance state-of-the-art results on six out of seven datasets.",Hamed Zamani
2024-05-13T02:33:25Z,http://arxiv.org/abs/2405.07437v2,Evaluation of Retrieval-Augmented Generation: A Survey,"Retrieval-Augmented Generation (RAG) has recently gained traction in natural
language processing. Numerous studies and real-world applications are
leveraging its ability to enhance generative models through external
information retrieval. Evaluating these RAG systems, however, poses unique
challenges due to their hybrid structure and reliance on dynamic knowledge
sources. To better understand these challenges, we conduct A Unified Evaluation
Process of RAG (Auepora) and aim to provide a comprehensive overview of the
evaluation and benchmarks of RAG systems. Specifically, we examine and compare
several quantifiable metrics of the Retrieval and Generation components, such
as relevance, accuracy, and faithfulness, within the current RAG benchmarks,
encompassing the possible output and ground truth pairs. We then analyze the
various datasets and metrics, discuss the limitations of current benchmarks,
and suggest potential directions to advance the field of RAG benchmarks.",Hao Yu
2024-05-13T17:44:05Z,http://arxiv.org/abs/2405.07963v2,"PyZoBot: A Platform for Conversational Information Extraction and
  Synthesis from Curated Zotero Reference Libraries through Advanced
  Retrieval-Augmented Generation","The exponential growth of scientific literature has resulted in information
overload, challenging researchers to effectively synthesize relevant
publications. This paper explores the integration of traditional reference
management software with advanced computational techniques, including Large
Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an
AI-driven platform developed in Python, incorporating Zoteros reference
management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge
extraction and synthesis from extensive human-curated scientific literature
databases. It demonstrates proficiency in handling complex natural language
queries, integrating data from multiple sources, and meticulously presenting
references to uphold research integrity and facilitate further exploration. By
leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot
offers an effective solution to manage information overload and keep pace with
rapid scientific advancements. The development of such AI-enhanced tools
promises significant improvements in research efficiency and effectiveness
across various disciplines.",Suad Alshammari
2024-05-13T19:05:42Z,http://arxiv.org/abs/2405.08120v1,"From Questions to Insightful Answers: Building an Informed Chatbot for
  University Resources","This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot
system built using Retrieval Augmented Generation (RAG) pipelines to enhance
the user experience and access to information within academic settings.The
objective of BARKPLUG V.2 is to provide information to users about various
campus resources, including academic departments, programs, campus facilities,
and student resources at a university setting in an interactive fashion. Our
system leverages university data as an external data corpus and ingests it into
our RAG pipelines for domain-specific question-answering tasks. We evaluate the
effectiveness of our system in generating accurate and pertinent responses for
Mississippi State University, as a case study, using quantitative measures,
employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS).
Furthermore, we evaluate the usability of this system via subjective
satisfaction surveys using the System Usability Scale (SUS). Our system
demonstrates impressive quantitative performance, with a mean RAGAS score of
0.96, and experience, as validated by usability assessments.",Subash Neupane
2024-05-21T07:35:21Z,http://arxiv.org/abs/2405.13084v2,"The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented
  Generation (FutureDial-RAG)","Recently, increasing research interests have focused on retrieval augmented
generation (RAG) to mitigate hallucination for large language models (LLMs).
Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which
aims at promoting the study of RAG for dialog systems. The challenge builds
upon the MobileCS2 dataset, a real-life customer service datasets with nearly
3000 high-quality dialogs containing annotations for knowledge base query and
corresponding results. Over the dataset, we define two tasks, track 1 for
knowledge retrieval and track 2 for response generation, which are core
research questions in dialog systems with RAG. We build baseline systems for
the two tracks and design metrics to measure whether the systems can perform
accurate retrieval and generate informative and coherent response. The baseline
results show that it is very challenging to perform well on the two tasks,
which encourages the participating teams and the community to study how to make
better use of RAG for real-life dialog systems.",Yucheng Cai
2024-05-21T20:03:40Z,http://arxiv.org/abs/2405.13179v4,"RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented
  Generation and Readability Control for Layman Summarization of Biomedical
  Texts","This paper introduces the RAG-RLRC-LaySum framework, designed to make complex
biomedical research understandable to laymen through advanced Natural Language
Processing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,
enhanced by a reranking method, utilizes multiple knowledge sources to ensure
the precision and pertinence of lay summaries. Additionally, our Reinforcement
Learning for Readability Control (RLRC) strategy improves readability, making
scientific content comprehensible to non-specialists. Evaluations using the
publicly accessible PLOS and eLife datasets show that our methods surpass Plain
Gemini model, demonstrating a 20% increase in readability scores, a 15%
improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual
accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific
knowledge, enhancing public engagement with biomedical discoveries.",Yuelyu Ji
2024-05-22T12:12:40Z,http://arxiv.org/abs/2405.13576v1,"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation
  Research","With the advent of Large Language Models (LLMs), the potential of Retrieval
Augmented Generation (RAG) techniques have garnered considerable research
attention. Numerous novel algorithms and models have been introduced to enhance
various aspects of RAG systems. However, the absence of a standardized
framework for implementation, coupled with the inherently intricate RAG
process, makes it challenging and time-consuming for researchers to compare and
evaluate these approaches in a consistent environment. Existing RAG toolkits
like LangChain and LlamaIndex, while available, are often heavy and unwieldy,
failing to meet the personalized needs of researchers. In response to this
challenge, we propose FlashRAG, an efficient and modular open-source toolkit
designed to assist researchers in reproducing existing RAG methods and in
developing their own RAG algorithms within a unified framework. Our toolkit
implements 12 advanced RAG methods and has gathered and organized 32 benchmark
datasets. Our toolkit has various features, including customizable modular
framework, rich collection of pre-implemented RAG works, comprehensive
datasets, efficient auxiliary pre-processing scripts, and extensive and
standard evaluation metrics. Our toolkit and resources are available at
https://github.com/RUC-NLPIR/FlashRAG.",Jiajie Jin
2024-05-24T11:05:45Z,http://arxiv.org/abs/2405.15436v1,"Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented
  Knowledge Graphs and Vector Database for Accreditation Reporting Assistance","In higher education, accreditation is a quality assurance process, where an
institution demonstrates a commitment to delivering high quality programs and
services to their students. For business schools nationally and internationally
the Association to Advance Collegiate Schools of Business (AACSB) accreditation
is the gold standard. For a business school to receive and subsequently
maintain accreditation, the school must undertake a rigorous, time consuming
reporting and peer review process, to demonstrate alignment with the AACSB
Standards. For this project we create a hybrid context retrieval augmented
generation pipeline that can assist in the documentation alignment and
reporting process necessary for accreditation. We implement both a vector
database and knowledge graph, as knowledge stores containing both institutional
data and AACSB Standard data. The output of the pipeline can be used by
institution stakeholders to build their accreditation report, dually grounded
by the context from the knowledge stores. To develop our knowledge graphs we
utilized both a manual construction process as well as an LLM Augmented
Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework
and observed optimal performance on answer relevancy and answer correctness
metrics.",Candace Edwards
2024-05-26T04:03:13Z,http://arxiv.org/abs/2405.16420v1,"M-RAG: Reinforcing Large Language Model Performance through
  Retrieval-Augmented Generation with Multiple Partitions","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant memories from an external database. However, existing RAG
methods typically organize all memories in a whole database, potentially
limiting focus on crucial memories and introducing noise. In this paper, we
introduce a multiple partition paradigm for RAG (called M-RAG), where each
database partition serves as a basic unit for RAG execution. Based on this
paradigm, we propose a novel framework that leverages LLMs with Multi-Agent
Reinforcement Learning to optimize different language generation tasks
explicitly. Through comprehensive experiments conducted on seven datasets,
spanning three language generation tasks and involving three distinct language
model architectures, we confirm that M-RAG consistently outperforms various
baseline methods, achieving improvements of 11%, 8%, and 12% for text
summarization, machine translation, and dialogue generation, respectively.",Zheng Wang
2024-05-27T23:39:17Z,http://arxiv.org/abs/2405.17706v1,"Video Enriched Retrieval Augmented Generation Using Aligned Video
  Captions","In this work, we propose the use of ""aligned visual captions"" as a mechanism
for integrating information contained within videos into retrieval augmented
generation (RAG) based chat assistant systems. These captions are able to
describe the visual and audio content of videos in a large corpus while having
the advantage of being in a textual format that is both easy to reason about &
incorporate into large language model (LLM) prompts, but also typically require
less multimedia content to be inserted into the multimodal LLM context window,
where typical configurations can aggressively fill up the context window by
sampling video frames from the source video. Furthermore, visual captions can
be adapted to specific use cases by prompting the original foundational model /
captioner for particular visual details or fine tuning. In hopes of helping
advancing progress in this area, we curate a dataset and describe automatic
evaluation procedures on common RAG tasks.",Kevin Dela Rosa
2024-05-29T20:56:52Z,http://arxiv.org/abs/2405.19519v1,"Two-layer retrieval augmented generation framework for low-resource
  medical question-answering: proof of concept using Reddit data","Retrieval augmented generation (RAG) provides the capability to constrain
generative model outputs, and mitigate the possibility of hallucination, by
providing relevant in-context text. The number of tokens a generative large
language model (LLM) can incorporate as context is finite, thus limiting the
volume of knowledge from which to generate an answer. We propose a two-layer
RAG framework for query-focused answer generation and evaluate a
proof-of-concept for this framework in the context of query-focused summary
generation from social media forums, focusing on emerging drug-related
information. The evaluations demonstrate the effectiveness of the two-layer
framework in resource constrained settings to enable researchers in obtaining
near real-time data from users.",Sudeshna Das
2024-06-01T14:45:03Z,http://arxiv.org/abs/2406.00456v1,"Mix-of-Granularity: Optimize the Chunking Granularity for
  Retrieval-Augmented Generation","Integrating information from different reference data sources is a major
challenge for Retrieval-Augmented Generation (RAG) systems because each
knowledge source adopts a unique data structure and follows different
conventions. Retrieving from multiple knowledge sources with one fixed strategy
usually leads to under-exploitation of information. To mitigate this drawback,
inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that
dynamically determines the optimal granularity of a knowledge database based on
input queries using a router. The router is efficiently trained with a newly
proposed loss function employing soft labels. We further extend MoG to
Mix-of-Granularity-Graph (MoGG), where reference documents are pre-processed
into graphs, enabling the retrieval of relevant information from distantly
situated chunks. Extensive experiments demonstrate that both MoG and MoGG
effectively predict optimal granularity levels, significantly enhancing the
performance of the RAG system in downstream tasks. The code of both MoG and
MoGG will be made public.",Zijie Zhong
2024-06-04T12:43:23Z,http://arxiv.org/abs/2406.02266v1,"Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning
  Compressor","Despite the prevalence of retrieval-augmented language models (RALMs), the
seamless integration of these models with retrieval mechanisms to enhance
performance in document-based tasks remains challenging. While some
post-retrieval processing Retrieval-Augmented Generation (RAG) methods have
achieved success, most still lack the ability to distinguish pertinent from
extraneous information, leading to potential inconsistencies and reduced
precision in the generated output, which subsequently affects the truthfulness
of the language model's responses. To address these limitations, this work
proposes a novel two-stage consistency learning approach for retrieved
information compression in retrieval-augmented language models to enhance
performance. By incorporating consistency learning, the aim is to generate
summaries that maintain coherence and alignment with the intended semantic
representations of a teacher model while improving faithfulness to the original
retrieved documents. The proposed method is empirically validated across
multiple datasets, demonstrating notable enhancements in precision and
efficiency for question-answering tasks. It outperforms existing baselines and
showcases the synergistic effects of combining contrastive and consistency
learning paradigms within the retrieval-augmented generation framework.",Chuankai Xu
2024-06-06T03:17:44Z,http://arxiv.org/abs/2406.03714v1,"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis
  with Context-Aware Contrastive Language-Audio Pretraining","Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker
using only a short speech prompt. They leverage a strong in-context ability to
mimic the speech prompts, including speaker style, prosody, and emotion.
Therefore, the selection of a speech prompt greatly influences the generated
speech, akin to the importance of a prompt in large language models (LLMs).
However, current prompt-based TTS models choose the speech prompt manually or
simply at random. Hence, in this paper, we adapt retrieval augmented generation
(RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we
additionally consider contextual information during the retrieval process and
present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model
to extract context-aware, style-related features. The objective and subjective
evaluations demonstrate that our proposed RAG method outperforms baselines, and
our CA-CLAP achieves better results than text-only retrieval methods.",Jinlong Xue
2024-05-31T23:30:52Z,http://arxiv.org/abs/2406.04369v1,RAG Does Not Work for Enterprises,"Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of
large language model outputs by incorporating knowledge retrieval. However,
implementing RAG in enterprises poses challenges around data security,
accuracy, scalability, and integration. This paper explores the unique
requirements for enterprise RAG, surveys current approaches and limitations,
and discusses potential advances in semantic search, hybrid queries, and
optimized retrieval. It proposes an evaluation framework to validate enterprise
RAG solutions, including quantitative testing, qualitative analysis, ablation
studies, and industry case studies. This framework aims to help demonstrate the
ability of purpose-built RAG architectures to deliver accuracy and relevance
improvements with enterprise-grade security, compliance and integration. The
paper concludes with implications for enterprise deployments, limitations, and
future research directions. Close collaboration between researchers and
industry partners may accelerate progress in developing and deploying
retrieval-augmented generation technology.",Tilmann Bruckhaus
2024-06-09T14:11:19Z,http://arxiv.org/abs/2406.05794v3,"RE-RAG: Improving Open-Domain QA Performance and Interpretability with
  Relevance Estimator in Retrieval-Augmented Generation","The Retrieval Augmented Generation (RAG) framework utilizes a combination of
parametric knowledge and external knowledge to demonstrate state-of-the-art
performance on open-domain question answering tasks. However, the RAG framework
suffers from performance degradation when the query is accompanied by
irrelevant contexts. In this work, we propose the RE-RAG framework, which
introduces a relevance estimator (RE) that not only provides relative relevance
between contexts as previous rerankers did, but also provides confidence, which
can be used to classify whether given context is useful for answering the given
question. We propose a weakly supervised method for training the RE simply
utilizing question-answer data without any labels for correct contexts. We show
that RE trained with a small generator (sLM) can not only improve the sLM
fine-tuned together with RE but also improve previously unreferenced large
language models (LLMs). Furthermore, we investigate new decoding strategies
that utilize the proposed confidence measured by RE such as choosing to let the
user know that it is ""unanswerable"" to answer the question given the retrieved
contexts or choosing to rely on LLM's parametric knowledge rather than
unrelated contexts.",Kiseung Kim
2024-06-09T17:55:55Z,http://arxiv.org/abs/2406.05870v2,"Machine Against the RAG: Jamming Retrieval-Augmented Generation with
  Blocker Documents","Retrieval-augmented generation (RAG) systems respond to queries by retrieving
relevant documents from a knowledge database, then generating an answer by
applying an LLM to the retrieved documents. We demonstrate that RAG systems
that operate on databases with untrusted content are vulnerable to a new class
of denial-of-service attacks we call jamming. An adversary can add a single
``blocker'' document to the database that will be retrieved in response to a
specific query and result in the RAG system not answering this query -
ostensibly because it lacks the information or because the answer is unsafe.
  We describe and measure the efficacy of several methods for generating
blocker documents, including a new method based on black-box optimization. This
method (1) does not rely on instruction injection, (2) does not require the
adversary to know the embedding or LLM used by the target RAG system, and (3)
does not use an auxiliary LLM to generate blocker documents.
  We evaluate jamming attacks on several LLMs and embeddings and demonstrate
that the existing safety metrics for LLMs do not capture their vulnerability to
jamming. We then discuss defenses against blocker documents.",Avital Shafran
2024-06-11T08:35:23Z,http://arxiv.org/abs/2406.07053v1,"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation
  and LLMs","Large Language Models (LLMs) have immense potential to transform the
telecommunications industry. They could help professionals understand complex
standards, generate code, and accelerate development. However, traditional LLMs
struggle with the precision and source verification essential for telecom work.
To address this, specialized LLM-based solutions tailored to telecommunication
standards are needed. Retrieval-augmented generation (RAG) offers a way to
create precise, fact-based answers. This paper proposes TelecomRAG, a framework
for a Telecommunication Standards Assistant that provides accurate, detailed,
and verifiable responses. Our implementation, using a knowledge base built from
3GPP Release 16 and Release 18 specification documents, demonstrates how this
assistant surpasses generic LLMs, offering superior accuracy, technical depth,
and verifiability, and thus significant value to the telecommunications field.",Girma M. Yilma
2024-06-12T01:19:36Z,http://arxiv.org/abs/2406.07796v2,"Battling Botpoop using GenAI for Higher Education: A Study of a
  Retrieval Augmented Generation Chatbots Impact on Learning","Generative artificial intelligence (GenAI) and large language models (LLMs)
have simultaneously opened new avenues for enhancing human learning and
increased the prevalence of poor-quality information in student response -
termed Botpoop. This study introduces Professor Leodar, a custom-built,
Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to
enhance educational while reducing Botpoop. Deployed at Nanyang Technological
University, Singapore, Professor Leodar offers a glimpse into the future of
AI-assisted learning, offering personalized guidance, 24/7 availability, and
contextually relevant information. Through a mixed-methods approach, we examine
the impact of Professor Leodar on learning, engagement, and exam preparedness,
with 97.1% of participants reporting positive experiences. These findings help
define possible roles of AI in education and highlight the potential of custom
GenAI chatbots. Our combination of chatbot development, in-class deployment and
outcomes study offers a benchmark for GenAI educational tools and is a stepping
stone for redefining the interplay between AI and human learning.",Maung Thway
2024-06-12T22:05:51Z,http://arxiv.org/abs/2406.09459v1,Ad Auctions for LLMs via Retrieval Augmented Generation,"In the field of computational advertising, the integration of ads into the
outputs of large language models (LLMs) presents an opportunity to support
these services without compromising content integrity. This paper introduces
novel auction mechanisms for ad allocation and pricing within the textual
outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a
segment auction where an ad is probabilistically retrieved for each discourse
segment (paragraph, section, or entire output) according to its bid and
relevance, following the RAG framework, and priced according to competing bids.
We show that our auction maximizes logarithmic social welfare, a new notion of
welfare that balances allocation efficiency and fairness, and we characterize
the associated incentive-compatible pricing rule. These results are extended to
multi-ad allocation per segment. An empirical evaluation validates the
feasibility and effectiveness of our approach over several ad auction
scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more
flexibility to allocate ads.",MohammadTaghi Hajiaghayi
2024-06-10T08:23:52Z,http://arxiv.org/abs/2406.10251v3,"The Impact of Quantization on Retrieval-Augmented Generation: An
  Analysis of Small LLMs","Post-training quantization reduces the computational demand of Large Language
Models (LLMs) but can weaken some of their capabilities. Since LLM abilities
emerge with scale, smaller LLMs are more sensitive to quantization. In this
paper, we explore how quantization affects smaller LLMs' ability to perform
retrieval-augmented generation (RAG), specifically in longer contexts. We chose
personalization for evaluation because it is a challenging domain to perform
using RAG as it requires long-context reasoning over multiple documents. We
compare the original FP16 and the quantized INT4 performance of multiple 7B and
8B LLMs on two tasks while progressively increasing the number of retrieved
documents to test how quantized models fare against longer contexts. To better
understand the effect of retrieval, we evaluate three retrieval models in our
experiments. Our findings reveal that if a 7B LLM performs the task well,
quantization does not impair its performance and long-context reasoning
capabilities. We conclude that it is possible to utilize RAG with quantized
smaller LLMs.",Mert Yazan
2024-06-17T12:23:32Z,http://arxiv.org/abs/2406.11460v1,"TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) offers an effective approach for
addressing question answering (QA) tasks. However, the imperfections of the
retrievers in RAG models often result in the retrieval of irrelevant
information, which could introduce noises and degrade the performance,
especially when handling multi-hop questions that require multiple steps of
reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose
TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series
of logically connected knowledge triples, to identify and integrate supporting
evidence from the retrieved documents for answering questions. Specifically,
TRACE employs a KG Generator to create a knowledge graph (KG) from the
retrieved documents, and then uses an Autoregressive Reasoning Chain
Constructor to build reasoning chains. Experimental results on three multi-hop
QA datasets show that TRACE achieves an average performance improvement of up
to 14.03% compared to using all the retrieved documents. Moreover, the results
indicate that using reasoning chains as context, rather than the entire
documents, is often sufficient to correctly answer questions.",Jinyuan Fang
2024-06-17T17:59:35Z,http://arxiv.org/abs/2406.11830v1,Language Modeling with Editable External Knowledge,"When the world changes, so does the text that humans write about it. How do
we build language models that can be easily updated to reflect these changes?
One popular approach is retrieval-augmented generation, in which new documents
are inserted into a knowledge base and retrieved during prediction for
downstream tasks. Most prior work on these systems have focused on improving
behavior during prediction through better retrieval or reasoning. This paper
introduces ERASE, which instead improves model behavior when new documents are
acquired, by incrementally deleting or rewriting other entries in the knowledge
base each time a document is added. In two new benchmark datasets evaluating
models' ability to answer questions about a stream of news articles or
conversations, ERASE improves accuracy relative to conventional
retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)
absolute. Code and data are available at https://github.com/belindal/ERASE",Belinda Z. Li
2024-06-18T09:25:35Z,http://arxiv.org/abs/2406.12430v1,"PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large
  Language Models as Decision Makers","In this paper, we conduct a study to utilize LLMs as a solution for decision
making that requires complex data analysis. We define Decision QA as the task
of answering the best decision, $d_{best}$, for a decision-making question $Q$,
business rules $R$ and a database $D$. Since there is no benchmark that can
examine Decision QA, we propose Decision QA benchmark, DQA. It has two
scenarios, Locating and Building, constructed from two video games (Europa
Universalis IV and Victoria 3) that have almost the same goal as Decision QA.
To address Decision QA effectively, we also propose a new RAG technique called
the iterative plan-then-retrieval augmented generation (PlanRAG). Our
PlanRAG-based LM generates the plan for decision making as the first step, and
the retriever generates the queries for data analysis as the second step. The
proposed method outperforms the state-of-the-art iterative RAG method by 15.8%
in the Locating scenario and by 7.4% in the Building scenario, respectively. We
release our code and benchmark at https://github.com/myeon9h/PlanRAG.",Myeonghwa Lee
2024-06-18T20:51:34Z,http://arxiv.org/abs/2406.13050v1,Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation,"Despite their impressive capabilities, large language models (LLMs) often
face challenges such as temporal misalignment and generating hallucinatory
content. Enhancing LLMs with retrieval mechanisms to fetch relevant information
from external sources offers a promising solution. Inspired by the proverb
""Think twice before you act,"" we propose a dual-angle evaluated
retrieval-augmented generation framework \textit{Think-then-Act}. Unlike
previous approaches that indiscriminately rewrite queries or perform retrieval
regardless of necessity, or generate temporary responses before deciding on
additional retrieval, which increases model generation costs, our framework
employs a two-phase process: (i) assessing the input query for clarity and
completeness to determine if rewriting is necessary; and (ii) evaluating the
model's capability to answer the query and deciding if additional retrieval is
needed. Experimental results on five datasets show that the
\textit{Think-then-Act} framework significantly improves performance. Our
framework demonstrates notable improvements in accuracy and efficiency compared
to existing baselines and performs well in both English and non-English
contexts. Ablation studies validate the optimal model confidence threshold,
highlighting the resource optimization benefits of our approach.",Yige Shen
2024-06-20T22:53:09Z,http://arxiv.org/abs/2406.14773v1,"Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)
  via Pure Synthetic Data","Retrieval-augmented generation (RAG) enhances the outputs of language models
by integrating relevant information retrieved from external knowledge sources.
However, when the retrieval process involves private data, RAG systems may face
severe privacy risks, potentially leading to the leakage of sensitive
information. To address this issue, we propose using synthetic data as a
privacy-preserving alternative for the retrieval data. We propose SAGE, a novel
two-stage synthetic data generation paradigm. In the stage-1, we employ an
attribute-based extraction and generation approach to preserve key contextual
information from the original data. In the stage-2, we further enhance the
privacy properties of the synthetic data through an agent-based iterative
refinement process. Extensive experiments demonstrate that using our synthetic
data as the retrieval context achieves comparable performance to using the
original data while substantially reducing privacy risks. Our work takes the
first step towards investigating the possibility of generating high-utility and
privacy-preserving synthetic data for RAG, opening up new opportunities for the
safe application of RAG systems in various domains.",Shenglai Zeng
2024-06-21T06:26:38Z,http://arxiv.org/abs/2406.14891v2,"Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop
  Question Answering","Multi-Hop Question Answering (MHQA) tasks present a significant challenge for
large language models (LLMs) due to the intensive knowledge required. Current
solutions, like Retrieval-Augmented Generation, typically retrieve potential
documents from an external corpus to read an answer. However, the performance
of this retrieve-then-read paradigm is constrained by the retriever and the
inevitable noise in the retrieved documents. To mitigate these challenges, we
introduce a novel generate-then-ground (GenGround) framework, synergizing the
parametric knowledge of LLMs and external documents to solve a multi-hop
question. GenGround empowers LLMs to alternate two phases until the final
answer is derived: (1) formulate a simpler, single-hop question and directly
generate the answer; (2) ground the question-answer pair in retrieved
documents, amending any wrong predictions in the answer. We also propose an
instructional grounding distillation method to generalize our method into
smaller models. Extensive experiments conducted on four datasets illustrate the
superiority of our method.",Zhengliang Shi
2024-06-21T14:29:39Z,http://arxiv.org/abs/2406.15187v2,"UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world
  Document Analysis","The use of Retrieval-Augmented Generation (RAG) has improved Large Language
Models (LLMs) in collaborating with external data, yet significant challenges
exist in real-world scenarios. In areas such as academic literature and finance
question answering, data are often found in raw text and tables in HTML or PDF
formats, which can be lengthy and highly unstructured. In this paper, we
introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that
involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We
revisit popular LLM- and RAG-based solutions for document analysis and evaluate
the design choices and answer qualities across multiple document domains and
diverse query types. Our evaluation yields interesting findings and highlights
the importance of data parsing and retrieval. We hope our benchmark can shed
light and better serve real-world document analysis applications. The benchmark
suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",Yulong Hui
2024-06-23T17:18:19Z,http://arxiv.org/abs/2406.16167v1,"FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy
  in Large Language Models","We present a novel extension to Retrieval Augmented Generation with the goal
of mitigating factual inaccuracies in the output of large language models.
Specifically, our method draws on the cognitive linguistic theory of frame
semantics for the indexing and retrieval of factual information relevant to
helping large language models answer queries. We conduct experiments to
demonstrate the effectiveness of this method both in terms of retrieval
effectiveness and in terms of the relevance of the frames and frame relations
automatically generated. Our results show that this novel mechanism of Frame
Semantic-based retrieval, designed to improve Retrieval Augmented Generation
(FS-RAG), is effective and offers potential for providing data-driven insights
into frame semantics theory. We provide open access to our program code and
prompts.",Harish Tayyar Madabushi
2024-06-26T04:49:41Z,http://arxiv.org/abs/2406.18064v3,"Evaluating Quality of Answers for Retrieval-Augmented Generation: A
  Strong LLM Is All You Need","We present a comprehensive study of answer quality evaluation in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
""thumbs-up"" or ""thumbs-down"" gesture commonly used in chat applications. This
approach suits factual business contexts where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.",Yang Wang
2024-06-26T07:02:49Z,http://arxiv.org/abs/2406.18114v2,"Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode
  and Effects Analysis","Failure mode and effects analysis (FMEA) is a critical tool for mitigating
potential failures, particular during ramp-up phases of new products. However,
its effectiveness is often limited by the missing reasoning capabilities of the
FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for fine-tuning on custom datasets for
reasoning within FMEA contexts. However, LLMs face challenges in tasks that
require factual knowledge, a gap that retrieval-augmented generation (RAG)
approaches aim to fill. RAG retrieves information from a non-parametric data
store and uses a language model to generate responses. Building on this idea,
we propose to advance the non-parametric data store with a knowledge graph
(KG). By enhancing the RAG framework with a KG, our objective is to leverage
analytical and semantic question-answering capabilities on FMEA data. This
paper contributes by presenting a new ontology for FMEA observations, an
algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced
RAG framework. Our approach is validated through a human study and we measure
the performance of the context retrieval recall and precision.",Lukas Bahr
2024-06-27T15:18:21Z,http://arxiv.org/abs/2406.19251v1,"AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for
  Retrieval-Augmented Generation","Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.",Jia Fu
2024-06-26T05:36:23Z,http://arxiv.org/abs/2406.19417v1,"""Glue pizza and eat rocks"" -- Exploiting Vulnerabilities in
  Retrieval-Augmented Generative Models","Retrieval-Augmented Generative (RAG) models enhance Large Language Models
(LLMs) by integrating external knowledge bases, improving their performance in
applications like fact-checking and information searching. In this paper, we
demonstrate a security threat where adversaries can exploit the openness of
these knowledge bases by injecting deceptive content into the retrieval
database, intentionally changing the model's behavior. This threat is critical
as it mirrors real-world usage scenarios where RAG systems interact with
publicly accessible knowledge bases, such as web scrapings and user-contributed
data pools. To be more realistic, we target a realistic setting where the
adversary has no knowledge of users' queries, knowledge base data, and the LLM
parameters. We demonstrate that it is possible to exploit the model
successfully through crafted content uploads with access to the retriever. Our
findings emphasize an urgent need for security measures in the design and
deployment of RAG systems to prevent potential manipulation and ensure the
integrity of machine-generated content.",Zhen Tan
2024-06-27T19:20:09Z,http://arxiv.org/abs/2406.19493v2,"Development and Evaluation of a Retrieval-Augmented Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems","Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.",Anubhab Majumder
2024-07-01T09:09:27Z,http://arxiv.org/abs/2407.01102v1,BERGEN: A Benchmarking Library for Retrieval-Augmented Generation,"Retrieval-Augmented Generation allows to enhance Large Language Models with
external knowledge. In response to the recent popularity of generative LLMs,
many RAG approaches have been proposed, which involve an intricate number of
different configurations such as evaluation datasets, collections, metrics,
retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in
comparing approaches and understanding the impact of each component in the
pipeline. In this work, we study best practices that lay the groundwork for a
systematic evaluation of RAG and present BERGEN, an end-to-end library for
reproducible research standardizing RAG experiments. In an extensive study
focusing on QA, we benchmark different state-of-the-art retrievers, rerankers,
and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our
open-source library BERGEN is available under
\url{https://github.com/naver/bergen}.",David Rau
2024-07-01T10:26:19Z,http://arxiv.org/abs/2407.01158v1,"Learning to Explore and Select for Coverage-Conditioned
  Retrieval-Augmented Generation","Interactions with billion-scale large language models typically yield
long-form responses due to their extensive parametric capacities, along with
retrieval-augmented features. While detailed responses provide insightful
viewpoint of a specific subject, they frequently generate redundant and less
engaging content that does not meet user interests. In this work, we focus on
the role of query outlining (i.e., selected sequence of queries) in scenarios
that users request a specific range of information, namely coverage-conditioned
($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets
of information-seeking queries decomposed with various perspectives on certain
topics. By utilizing QTree, we train QPlanner, a 7B language model generating
customized query outlines that follow coverage-conditioned queries. We analyze
the effectiveness of generated outlines through automatic and human evaluation,
targeting on retrieval-augmented generation (RAG). Moreover, the experimental
results demonstrate that QPlanner with alignment training can further provide
outlines satisfying diverse user interests. Our resources are available at
https://github.com/youngerous/qtree.",Takyoung Kim
2024-07-01T12:06:34Z,http://arxiv.org/abs/2407.01219v1,Searching for Best Practices in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) techniques have proven to be effective
in integrating up-to-date information, mitigating hallucinations, and enhancing
response quality, particularly in specialized domains. While many RAG
approaches have been proposed to enhance large language models through
query-dependent retrievals, these approaches still suffer from their complex
implementation and prolonged response times. Typically, a RAG workflow involves
multiple processing steps, each of which can be executed in various ways. Here,
we investigate existing RAG approaches and their potential combinations to
identify optimal RAG practices. Through extensive experiments, we suggest
several strategies for deploying RAG that balance both performance and
efficiency. Moreover, we demonstrate that multimodal retrieval techniques can
significantly enhance question-answering capabilities about visual inputs and
accelerate the generation of multimodal content using a ""retrieval as
generation"" strategy.",Xiaohua Wang
2024-07-01T16:56:50Z,http://arxiv.org/abs/2407.01463v1,Retrieval-augmented generation in multilingual settings,"Retrieval-augmented generation (RAG) has recently emerged as a promising
solution for incorporating up-to-date or domain-specific knowledge into large
language models (LLMs) and improving LLM factuality, but is predominantly
studied in English-only settings. In this work, we consider RAG in the
multilingual setting (mRAG), i.e. with user queries and the datastore in 13
languages, and investigate which components and with which adjustments are
needed to build a well-performing mRAG pipeline, that can be used as a strong
baseline in future works. Our findings highlight that despite the availability
of high-quality off-the-shelf multilingual retrievers and generators,
task-specific prompt engineering is needed to enable generation in user
languages. Moreover, current evaluation metrics need adjustments for
multilingual setting, to account for variations in spelling named entities. The
main limitations to be addressed in future works include frequent
code-switching in non-Latin alphabet languages, occasional fluency errors,
wrong reading of the provided documents, or irrelevant retrieval. We release
the code for the resulting mRAG baseline pipeline at
https://github.com/naver/bergen.",Nadezhda Chirkova
2024-07-02T17:59:17Z,http://arxiv.org/abs/2407.02485v1,"RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in
  LLMs","Large language models (LLMs) typically utilize the top-k contexts from a
retriever in retrieval-augmented generation (RAG). In this work, we propose a
novel instruction fine-tuning framework RankRAG, which instruction-tunes a
single LLM for the dual purpose of context ranking and answer generation in
RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding
a small fraction of ranking data into the training blend, and outperform
existing expert ranking models, including the same LLM exclusively fine-tuned
on a large amount of ranking data. For generation, we compare our model with
many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and
ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG
benchmarks. Specifically, our Llama3-RankRAG significantly outperforms
Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In
addition, it also performs comparably to GPT-4 on five RAG benchmarks in the
biomedical domain without instruction fine-tuning on biomedical data,
demonstrating its superb capability for generalization to new domains.",Yue Yu
2024-07-03T15:55:14Z,http://arxiv.org/abs/2407.03227v2,"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and
  Schema Pruning","We focus on Text-to-SQL semantic parsing from the perspective of
retrieval-augmented generation. Motivated by challenges related to the size of
commercial database schemata and the deployability of business intelligence
solutions, we propose $\text{ASTReS}$ that dynamically retrieves input database
information and uses abstract syntax trees to select few-shot examples for
in-context learning.
  Furthermore, we investigate the extent to which an in-parallel semantic
parser can be leveraged for generating approximated versions of the expected
SQL queries, to support our retrieval. We take this approach to the extreme--we
adapt a model consisting of less than $500$M parameters, to act as an extremely
efficient approximator, enhancing it with the ability to process schemata in a
parallelised manner. We apply $\text{ASTReS}$ to monolingual and cross-lingual
benchmarks for semantic parsing, showing improvements over state-of-the-art
baselines. Comprehensive experiments highlight the contribution of modules
involved in this retrieval-augmented generation setting, revealing interesting
directions for future work.",Zhili Shen
2024-07-04T06:26:51Z,http://arxiv.org/abs/2407.07913v1,"CaseGPT: a case reasoning framework based on language models and
  retrieval-augmented generation","This paper presents CaseGPT, an innovative approach that combines Large
Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to
enhance case-based reasoning in the healthcare and legal sectors. The system
addresses the challenges of traditional database queries by enabling fuzzy
searches based on imprecise descriptions, thereby improving data searchability
and usability. CaseGPT not only retrieves relevant case data but also generates
insightful suggestions and recommendations based on patterns discerned from
existing case data. This functionality proves especially valuable for tasks
such as medical diagnostics, legal precedent research, and case strategy
formulation. The paper includes an in-depth discussion of the system's
methodology, its performance in both medical and legal domains, and its
potential for future applications. Our experiments demonstrate that CaseGPT
significantly outperforms traditional keyword-based and simple LLM-based
systems in terms of precision, recall, and efficiency.",Rui Yang
2024-07-11T08:24:16Z,http://arxiv.org/abs/2407.08275v1,"Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval
  Augmented Generation Systems","The choice of embedding model is a crucial step in the design of Retrieval
Augmented Generation (RAG) systems. Given the sheer volume of available
options, identifying clusters of similar models streamlines this model
selection process. Relying solely on benchmark performance scores only allows
for a weak assessment of model similarity. Thus, in this study, we evaluate the
similarity of embedding models within the context of RAG systems. Our
assessment is two-fold: We use Centered Kernel Alignment to compare embeddings
on a pair-wise level. Additionally, as it is especially pertinent to RAG
systems, we evaluate the similarity of retrieval results between these models
using Jaccard and rank similarity. We compare different families of embedding
models, including proprietary ones, across five datasets from the popular
Benchmark Information Retrieval (BEIR). Through our experiments we identify
clusters of models corresponding to model families, but interestingly, also
some inter-family clusters. Furthermore, our analysis of top-k retrieval
similarity reveals high-variance at low k values. We also identify possible
open-source alternatives to proprietary models, with Mistral exhibiting the
highest similarity to OpenAI models.",Laura Caspari
2024-07-16T23:50:07Z,http://arxiv.org/abs/2407.12216v2,"Mindful-RAG: A Study of Points of Failure in Retrieval Augmented
  Generation","Large Language Models (LLMs) are proficient at generating coherent and
contextually relevant text but face challenges when addressing
knowledge-intensive queries in domain-specific and factual question-answering
tasks. Retrieval-augmented generation (RAG) systems mitigate this by
incorporating external knowledge sources, such as structured knowledge graphs
(KGs). However, LLMs often struggle to produce accurate answers despite access
to KG-extracted information containing necessary facts. Our study investigates
this dilemma by analyzing error patterns in existing KG-based RAG methods and
identifying eight critical failure points. We observed that these errors
predominantly occur due to insufficient focus on discerning the question's
intent and adequately gathering relevant context from the knowledge graph
facts. Drawing on this analysis, we propose the Mindful-RAG approach, a
framework designed for intent-based and contextually aligned knowledge
retrieval. This method explicitly targets the identified failures and offers
improvements in the correctness and relevance of responses provided by LLMs,
representing a significant step forward from existing methods.",Garima Agrawal
2024-07-18T06:06:53Z,http://arxiv.org/abs/2407.13193v2,Retrieval-Augmented Generation for Natural Language Processing: A Survey,"Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG training, including RAG with/without datastore update. Then, we
introduce the application of RAG in representative natural language processing
tasks and industrial scenarios. Finally, this paper discusses the future
directions and challenges of RAG for promoting its development.",Shangyu Wu
2024-07-22T03:44:27Z,http://arxiv.org/abs/2407.15353v2,"Customized Retrieval Augmented Generation and Benchmarking for EDA Tool
  Documentation QA","Retrieval augmented generation (RAG) enhances the accuracy and reliability of
generative AI models by sourcing factual information from external databases,
which is extensively employed in document-grounded question-answering (QA)
tasks. Off-the-shelf RAG flows are well pretrained on general-purpose
documents, yet they encounter significant challenges when being applied to
knowledge-intensive vertical domains, such as electronic design automation
(EDA). This paper addresses such issue by proposing a customized RAG framework
along with three domain-specific techniques for EDA tool documentation QA,
including a contrastive learning scheme for text embedding model fine-tuning, a
reranker distilled from proprietary LLM, and a generative LLM fine-tuned with
high-quality domain corpus. Furthermore, we have developed and released a
documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced
RTL-to-GDSII design platform. Experimental results demonstrate that our
proposed RAG flow and techniques have achieved superior performance on ORD-QA
as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA
benchmark and the training dataset for our customized RAG flow are open-source
at https://github.com/lesliepy99/RAG-EDA.",Yuan Pu
2024-07-22T11:55:14Z,http://arxiv.org/abs/2407.15569v2,"An Empirical Study of Retrieval Augmented Generation with
  Chain-of-Thought","Since the launch of ChatGPT at the end of 2022, generative dialogue models
represented by ChatGPT have quickly become essential tools in daily life. As
user expectations increase, enhancing the capability of generative dialogue
models to solve complex problems has become a focal point of current research.
This paper delves into the effectiveness of the RAFT (Retrieval Augmented
Fine-Tuning) method in improving the performance of Generative dialogue models.
RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and
retrieval augmented generation (RAG), which significantly enhanced the model's
information extraction and logical reasoning abilities. We evaluated the RAFT
method across multiple datasets and analysed its performance in various
reasoning tasks, including long-form QA and short-form QA tasks, tasks in both
Chinese and English, and supportive and comparison reasoning tasks. Notably, it
addresses the gaps in previous research regarding long-form QA tasks and
Chinese datasets. Moreover, we also evaluate the benefit of the
chain-of-thought (CoT) in the RAFT method. This work offers valuable insights
for studies focused on enhancing the performance of generative dialogue models.",Yuetong Zhao
2024-07-22T15:53:27Z,http://arxiv.org/abs/2407.15748v1,"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval
  Augmented Generation","In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the
first specialised AI chatbot for cybersecurity. MoRSE aims to provide
comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG
(Retrieval Augmented Generation) systems designed to retrieve and organize
information from multidimensional cybersecurity contexts. MoRSE differs from
traditional RAGs by using parallel retrievers that work together to retrieve
semantically related information in different formats and structures. Unlike
traditional Large Language Models (LLMs) that rely on Parametric Knowledge
Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases
in response to user queries. Subsequently, MoRSE uses this information to
generate accurate answers. In addition, MoRSE benefits from real-time updates
to its knowledge bases, enabling continuous knowledge enrichment without
retraining. We have evaluated the effectiveness of MoRSE against other
state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific
questions. The experimental evaluation has shown that the improvement in terms
of relevance and correctness of the answer is more than 10\% compared to known
solutions such as GPT-4 and Mixtral 7x8.",Marco Simoni
2024-07-23T20:51:52Z,http://arxiv.org/abs/2407.16833v2,"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach","Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.",Zhuowan Li
2024-07-26T20:42:40Z,http://arxiv.org/abs/2407.19075v1,"ESAC (EQ-SANS Assisting Chatbot): Application of Large Language Models
  and Retrieval-Augmented Generation for Enhanced User Experience at EQ-SANS","Neutron scattering experiments have played vital roles in exploring materials
properties in the past decades. While user interfaces have been improved over
time, neutron scattering experiments still require specific knowledge or
training by an expert due to the complexity of such advanced instrumentation
and the limited number of experiments each person may perform each year. This
paper introduces an innovative chatbot application that leverages Large
Language Models(LLM) and Retrieval-Augmented Generation (RAG) technologies to
significantly enhance the user experience at the EQ-SANS, a small-angle neutron
scattering instrument at the Spallation Neutron Source of Oak Ridge National
Laboratory. Through a user-centric design approach, the EQ-SANS Assisting
Chatbot (ESAC) serves as an interactive reference for users, thereby
facilitating the use of the instrument by visiting scientists. By bridging the
gap between the users of EQ-SANS and the control systems required to perform
their experiments, the ESAC sets a new standard for interactive learning and
support for the scientific community using large-scale scientific facilities.",Changwoo Do
2024-07-31T16:04:03Z,http://arxiv.org/abs/2407.21712v1,Adaptive Retrieval-Augmented Generation for Conversational Systems,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.",Xi Wang
2024-08-02T08:03:38Z,http://arxiv.org/abs/2408.01084v2,"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for
  Handling Noisy Contexts","When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge the gap between
external knowledge and the LLMs' parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLMs
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.",Youna Kim
2024-08-05T15:16:24Z,http://arxiv.org/abs/2408.02545v1,"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation","Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.",Daniel Fleischer
2024-08-09T12:26:57Z,http://arxiv.org/abs/2408.05026v1,"Retrieval-augmented code completion for local projects using large
  language models","The use of large language models (LLMs) is becoming increasingly widespread
among software developers. However, privacy and computational requirements are
problematic with commercial solutions and the use of LLMs. In this work, we
focus on using LLMs with around 160 million parameters that are suitable for
local execution and augmentation with retrieval from local projects. We train
two models based on the transformer architecture, the generative model GPT-2
and the retrieval-adapted RETRO model, on open-source Python files, and
empirically evaluate and compare them, confirming the benefits of vector
embedding based retrieval. Further, we improve our models' performance with
In-context retrieval-augmented generation, which retrieves code snippets based
on the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented
generation on larger models and conclude that, despite its simplicity, the
approach is more suitable than using the RETRO architecture. We highlight the
key role of proper tokenization in achieving the full potential of LLMs in code
completion.",Marko Hostnik
2024-08-14T10:03:28Z,http://arxiv.org/abs/2408.07425v1,Exploring Retrieval Augmented Generation in Arabic,"Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful
technique in natural language processing, combining the strengths of
retrieval-based and generation-based models to enhance text generation tasks.
However, the application of RAG in Arabic, a language with unique
characteristics and resource constraints, remains underexplored. This paper
presents a comprehensive case study on the implementation and evaluation of RAG
for Arabic text. The work focuses on exploring various semantic embedding
models in the retrieval stage and several LLMs in the generation stage, in
order to investigate what works and what doesn't in the context of Arabic. The
work also touches upon the issue of variations between document dialect and
query dialect in the retrieval stage. Results show that existing semantic
embedding models and LLMs can be effectively employed to build Arabic RAG
pipelines.",Samhaa R. El-Beltagy
2024-08-15T10:20:54Z,http://arxiv.org/abs/2408.08067v2,"RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented
  Generation","Despite Retrieval-Augmented Generation (RAG) showing promising capability in
leveraging external knowledge, a comprehensive evaluation of RAG systems is
still challenging due to the modular nature of RAG, evaluation of long-form
responses and reliability of measurements. In this paper, we propose a
fine-grained evaluation framework, RAGChecker, that incorporates a suite of
diagnostic metrics for both the retrieval and generation modules. Meta
evaluation verifies that RAGChecker has significantly better correlations with
human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8
RAG systems and conduct an in-depth analysis of their performance, revealing
insightful patterns and trade-offs in the design choices of RAG architectures.
The metrics of RAGChecker can guide researchers and practitioners in developing
more effective RAG systems. This work has been open sourced at
https://github.com/amazon-science/RAGChecker.",Dongyu Ru
2024-08-21T17:43:11Z,http://arxiv.org/abs/2408.11800v2,"WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy
  Domain","In the rapidly evolving landscape of Natural Language Processing (NLP) and
text generation, the emergence of Retrieval Augmented Generation (RAG) presents
a promising avenue for improving the quality and reliability of generated text
by leveraging information retrieved from user specified database. Benchmarking
is essential to evaluate and compare the performance of the different RAG
configurations in terms of retriever and generator, providing insights into
their effectiveness, scalability, and suitability for the specific domain and
applications. In this paper, we present a comprehensive framework to generate a
domain relevant RAG benchmark. Our framework is based on automatic
question-answer generation with Human (domain experts)-AI Large Language Model
(LLM) teaming. As a case study, we demonstrate the framework by introducing
WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises
of multiple scientific documents/reports related to environmental impact of
wind energy projects. Our framework systematically evaluates RAG performance
using diverse metrics and multiple question types with varying complexity
level. We also demonstrate the performance of different models on our
benchmark.",Rounak Meyur
2024-08-20T09:29:31Z,http://arxiv.org/abs/2408.11875v1,"Hierarchical Retrieval-Augmented Generation Model with Rethink for
  Multi-hop Question Answering","Multi-hop Question Answering (QA) necessitates complex reasoning by
integrating multiple pieces of information to resolve intricate questions.
However, existing QA systems encounter challenges such as outdated information,
context window length limitations, and an accuracy-quantity trade-off. To
address these issues, we propose a novel framework, the Hierarchical
Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising
Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We
introduce a new hierarchical retrieval strategy that incorporates both sparse
retrieval at the document level and dense retrieval at the chunk level,
effectively integrating their strengths. Additionally, we propose a
single-candidate retrieval method to mitigate the limitations of
multi-candidate retrieval. We also construct two new corpora, Indexed
Wikicorpus and Profile Wikicorpus, to address the issues of outdated and
insufficient knowledge.
  Our experimental results on four datasets demonstrate that HiRAG outperforms
state-of-the-art models across most metrics, and our Indexed Wikicorpus is
effective. The code for HiRAG is available at
https://github.com/2282588541a/HiRAG",Xiaoming Zhang
2024-08-18T11:47:55Z,http://arxiv.org/abs/2408.14484v1,Agentic Retrieval-Augmented Generation for Time Series Analysis,"Time series modeling is crucial for many applications, however, it faces
challenges such as complex spatio-temporal dependencies and distribution shifts
in learning from historical context to predict task-specific outcomes. To
address these challenges, we propose a novel approach using an agentic
Retrieval-Augmented Generation (RAG) framework for time series analysis. The
framework leverages a hierarchical, multi-agent architecture where the master
agent orchestrates specialized sub-agents and delegates the end-user request to
the relevant sub-agent. The sub-agents utilize smaller, pre-trained language
models (SLMs) customized for specific time series tasks through fine-tuning
using instruction tuning and direct preference optimization, and retrieve
relevant prompts from a shared repository of prompt pools containing distilled
knowledge about historical patterns and trends to improve predictions on new
data. Our proposed modular, multi-agent RAG approach offers flexibility and
achieves state-of-the-art performance across major time series tasks by
tackling complex challenges more effectively than task-specific customized
methods across benchmark datasets.",Chidaksh Ravuru
2024-08-28T04:44:43Z,http://arxiv.org/abs/2408.15533v2,"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via
  Layer-wise Relevance Propagation","Retrieval-Augmented Generation (RAG) has become a primary technique for
mitigating hallucinations in large language models (LLMs). However, incomplete
knowledge extraction and insufficient understanding can still mislead LLMs to
produce irrelevant or even contradictory responses, which means hallucinations
persist in RAG. In this paper, we propose LRP4RAG, a method based on the
Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations
in RAG. Specifically, we first utilize LRP to compute the relevance between the
input and output of the RAG generator. We then apply further extraction and
resampling to the relevance matrix. The processed relevance data are input into
multiple classifiers to determine whether the output contains hallucinations.
To the best of our knowledge, this is the first time that LRP has been used for
detecting RAG hallucinations, and extensive experiments demonstrate that
LRP4RAG outperforms existing baselines.",Haichuan Hu
2024-09-03T07:17:41Z,http://arxiv.org/abs/2409.01666v1,In Defense of RAG in the Era of Long-Context Language Models,"Overcoming the limited context limitations in early-generation LLMs,
retrieval-augmented generation (RAG) has been a reliable solution for
context-based answer generation in the past. Recently, the emergence of
long-context LLMs allows the models to incorporate much longer text sequences,
making RAG less attractive. Recent studies show that long-context LLMs
significantly outperform RAG in long-context applications. Unlike the existing
works favoring the long-context LLM over RAG, we argue that the extremely long
context in LLMs suffers from a diminished focus on relevant information and
leads to potential degradation in answer quality. This paper revisits the RAG
in long-context answer generation. We propose an order-preserve
retrieval-augmented generation (OP-RAG) mechanism, which significantly improves
the performance of RAG for long-context question-answer applications. With
OP-RAG, as the number of retrieved chunks increases, the answer quality
initially rises, and then declines, forming an inverted U-shaped curve. There
exist sweet points where OP-RAG could achieve higher answer quality with much
less tokens than long-context LLM taking the whole context as input. Extensive
experiments on public benchmark demonstrate the superiority of our OP-RAG.",Tan Yu
2024-09-03T13:05:38Z,http://arxiv.org/abs/2409.01864v1,"The Role of Large Language Models in Musicology: Are We Ready to Trust
  the Machines?","In this work, we explore the use and reliability of Large Language Models
(LLMs) in musicology. From a discussion with experts and students, we assess
the current acceptance and concerns regarding this, nowadays ubiquitous,
technology. We aim to go one step further, proposing a semi-automatic method to
create an initial benchmark using retrieval-augmented generation models and
multiple-choice question generation, validated by human experts. Our evaluation
on 400 human-validated questions shows that current vanilla LLMs are less
reliable than retrieval augmented generation from music dictionaries. This
paper suggests that the potential of LLMs in musicology requires musicology
driven research that can specialized LLMs by including accurate and reliable
domain knowledge.",Pedro Ramoneda
2024-09-05T01:58:29Z,http://arxiv.org/abs/2409.03171v2,"MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented
  Generation Question Answering","In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.",Mitchell DeHaven
2024-09-13T07:28:47Z,http://arxiv.org/abs/2409.08597v1,"LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented
  Generation","Recent advancements in integrating speech information into large language
models (LLMs) have significantly improved automatic speech recognition (ASR)
accuracy. However, existing methods often constrained by the capabilities of
the speech encoders under varied acoustic conditions, such as accents. To
address this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)
paradigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech
datastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy
via LLM in-context learning (ICL) capabilities. Experiments on Mandarin and
various Chinese dialect datasets demonstrate significant improvements in ASR
accuracy compared to existing methods, validating the effectiveness of our
approach, especially in handling accent variations.",Shaojun Li
2024-08-29T16:11:20Z,http://arxiv.org/abs/2409.09046v1,"HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications","While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.",Rishi Kalra
2024-09-18T17:03:30Z,http://arxiv.org/abs/2409.12140v2,MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion,"We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos are available at: https://motion-rag.github.io/",Sai Shashank Kalakonda
2024-09-19T05:14:55Z,http://arxiv.org/abs/2409.12468v2,"Familiarity-Aware Evidence Compression for Retrieval-Augmented
  Generation","Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.",Dongwon Jung
2024-09-19T16:23:42Z,http://arxiv.org/abs/2409.12880v1,"Enhancing E-commerce Product Title Translation with Retrieval-Augmented
  Generation and Large Language Models","E-commerce stores enable multilingual product discovery which require
accurate product title translation. Multilingual large language models (LLMs)
have shown promising capacity to perform machine translation tasks, and it can
also enhance and translate product titles cross-lingually in one step. However,
product title translation often requires more than just language conversion
because titles are short, lack context, and contain specialized terminology.
This study proposes a retrieval-augmented generation (RAG) approach that
leverages existing bilingual product information in e-commerce by retrieving
similar bilingual examples and incorporating them as few-shot prompts to
enhance LLM-based product title translation. Experiment results show that our
proposed RAG approach improve product title translation quality with chrF score
gains of up to 15.3% for language pairs where the LLM has limited proficiency.",Bryan Zhang
2024-09-19T23:38:59Z,http://arxiv.org/abs/2409.13122v2,"RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal
  Reinforcement and Retrieval-Augmented Generation","In real-world software engineering tasks, solving a problem often requires
understanding and modifying multiple functions, classes, and files across a
large codebase. Therefore, on the repository level, it is crucial to extract
the relevant information to achieve accurate code completion effectively.
Existing code completion tools have achieved some success, but they struggle to
optimize the retrieval and generation process dynamically. In this paper, we
propose RepoGenReflex, a generic, dynamic, effective framework to address this
challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with
Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal
results for repository-level code completion. RepoGenReflex uses Reflector to
give directional feedback to the next loop. RepoGenReflex chooses the optimal
results stored in the Experience cache based on the RAG-VRL loop. To validate
the framework's generalization ability, we propose a new benchmark RepoGenEval,
which consists of the latest, high-quality real-world repositories in line
completion scenarios. Our experiments demonstrate that RepoGenReflex achieves
significant improvements after optimizing the Reflector component, resulting in
enhanced accuracy and relevance of code completions. Additionally,
RepoGenReflex consistently demonstrates superior performance and effectiveness
across standard code completion tasks, highlighting the robustness and
adaptability of our framework.",Jicheng Wang
2024-09-20T10:36:49Z,http://arxiv.org/abs/2409.13385v2,"Contextual Compression in Retrieval-Augmented Generation for Large
  Language Models: A Survey","Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.",Sourav Verma
2024-09-21T17:41:46Z,http://arxiv.org/abs/2409.14206v1,"AI Assistants for Spaceflight Procedures: Combining Generative
  Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge
  Graphs With Augmented Reality Cues","This paper describes the capabilities and potential of the intelligent
personal assistant (IPA) CORE (Checklist Organizer for Research and
Exploration), designed to support astronauts during procedures onboard the
International Space Station (ISS), the Lunar Gateway station, and beyond. We
reflect on the importance of a reliable and flexible assistant capable of
offline operation and highlight the usefulness of audiovisual interaction using
augmented reality elements to intuitively display checklist information. We
argue that current approaches to the design of IPAs in space operations fall
short of meeting these criteria. Therefore, we propose CORE as an assistant
that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for
a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements
to ensure an intuitive understanding of procedure steps, reliability, offline
availability, and flexibility in terms of response style and procedure updates.",Oliver Bensch
2024-09-24T03:25:36Z,http://arxiv.org/abs/2409.15699v1,"Lighter And Better: Towards Flexible Context Adaptation For Retrieval
  Augmented Generation","The existing Retrieval-Augmented Generation (RAG) systems face significant
challenges in terms of cost and effectiveness. On one hand, they need to encode
the lengthy retrieved contexts before responding to the input tasks, which
imposes substantial computational overhead. On the other hand, directly using
generic Large Language Models (LLMs) often leads to sub-optimal answers, while
task-specific fine-tuning may compromise the LLMs' general capabilities. To
address these challenges, we introduce a novel approach called FlexRAG
(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts
are compressed into compact embeddings before being encoded by the LLMs.
Simultaneously, these compressed embeddings are optimized to enhance downstream
RAG performance. A key feature of FlexRAG is its flexibility, which enables
effective support for diverse compression ratios and selective preservation of
important contexts. Thanks to these technical designs, FlexRAG achieves
superior generation quality while significantly reducing running costs.
Comprehensive experiments on various question-answering datasets validate our
approach as a cost-effective and flexible solution for RAG systems.",Zheng Liu
2024-09-24T05:39:53Z,http://arxiv.org/abs/2409.15763v2,"IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through
  Semantic Comprehension in Retrieval-Augmented Generation Scenarios","In Retrieval-Augmented Generation (RAG) tasks using Large Language Models
(LLMs), the quality of retrieved information is critical to the final output.
This paper introduces the IRSC benchmark for evaluating the performance of
embedding models in multilingual RAG tasks. The benchmark encompasses five
retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,
keyword retrieval, and summary retrieval. Our research addresses the current
lack of comprehensive testing and effective comparison methods for embedding
models in RAG scenarios. We introduced new metrics: the Similarity of Semantic
Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),
and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our
contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and
3) insights into the cross-lingual limitations of embedding models. The IRSC
benchmark aims to enhance the understanding and development of accurate
retrieval systems in RAG tasks. All code and datasets are available at:
https://github.com/Jasaxion/IRSC_Benchmark",Hai Lin
2024-09-24T14:52:14Z,http://arxiv.org/abs/2409.16146v2,"Controlling Risk of Retrieval-augmented Generation: A Counterfactual
  Prompting Framework","Retrieval-augmented generation (RAG) has emerged as a popular solution to
mitigate the hallucination issues of large language models. However, existing
studies on RAG seldom address the issue of predictive uncertainty, i.e., how
likely it is that a RAG model's prediction is incorrect, resulting in
uncontrollable risks in real-world applications. In this work, we emphasize the
importance of risk control, ensuring that RAG models proactively refuse to
answer questions with low confidence. Our research identifies two critical
latent factors affecting RAG's confidence in its predictions: the quality of
the retrieved results and the manner in which these results are utilized. To
guide RAG models in assessing their own confidence based on these two latent
factors, we develop a counterfactual prompting framework that induces the
models to alter these factors and analyzes the effect on their answers. We also
introduce a benchmarking procedure to collect answers with the option to
abstain, facilitating a series of experiments. For evaluation, we introduce
several risk-related metrics and the experimental results demonstrate the
effectiveness of our approach. Our code and benchmark dataset are available at
https://github.com/ict-bigdatalab/RC-RAG.",Lu Chen
2024-09-26T16:12:33Z,http://arxiv.org/abs/2409.18003v1,"Enhancing Tourism Recommender Systems for Sustainable City Trips Using
  Retrieval-Augmented Generation","Tourism Recommender Systems (TRS) have traditionally focused on providing
personalized travel suggestions, often prioritizing user preferences without
considering broader sustainability goals. Integrating sustainability into TRS
has become essential with the increasing need to balance environmental impact,
local community interests, and visitor satisfaction. This paper proposes a
novel approach to enhancing TRS for sustainable city trips using Large Language
Models (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We
enhance the traditional RAG system by incorporating a sustainability metric
based on a city's popularity and seasonal demand during the prompt augmentation
phase. This modification, called Sustainability Augmented Reranking (SAR),
ensures the system's recommendations align with sustainability goals.
Evaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and
Mistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently
matches or outperforms the baseline (without SAR) across most metrics,
highlighting the benefits of incorporating sustainability into TRS.",Ashmi Banerjee
2024-09-28T16:22:53Z,http://arxiv.org/abs/2409.19401v1,"Crafting Personalized Agents through Retrieval-Augmented Generation on
  Editable Memory Graphs","In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.",Zheng Wang
2024-09-29T22:04:26Z,http://arxiv.org/abs/2409.19804v1,"Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in
  Retrieval-Augmented Generation Systems","RAG (Retrieval-Augmented Generation) have recently gained significant
attention for their enhanced ability to integrate external knowledge sources in
open-domain question answering (QA) tasks. However, it remains unclear how
these models address fairness concerns, particularly with respect to sensitive
attributes such as gender, geographic location, and other demographic factors.
First, as language models evolve to prioritize utility, like improving exact
match accuracy, fairness may have been largely overlooked. Second, RAG methods
are complex pipelines, making it hard to identify and address biases, as each
component is optimized for different goals. In this paper, we aim to
empirically evaluate fairness in several RAG methods. We propose a fairness
evaluation framework tailored to RAG methods, using scenario-based questions
and analyzing disparities across demographic attributes. The experimental
results indicate that, despite recent advances in utility-driven optimization,
fairness issues persist in both the retrieval and generation stages,
highlighting the need for more targeted fairness interventions within RAG
pipelines. We will release our dataset and code upon acceptance of the paper.",Xuyang Wu
2024-09-30T08:26:53Z,http://arxiv.org/abs/2409.20075v1,"BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the
  E-commerce Domain","Retrieval Augmented Generation (RAG) system is important in domains such as
e-commerce, which has many long-tail entities and frequently updated
information. Most existing works adopt separate modules for retrieval and
generation, which may be suboptimal since the retrieval task and the generation
task cannot benefit from each other to improve performance. We propose a novel
Backbone Shared RAG framework (BSharedRAG). It first uses a domain-specific
corpus to continually pre-train a base model as a domain-specific backbone
model and then trains two plug-and-play Low-Rank Adaptation (LoRA) modules
based on the shared backbone to minimize retrieval and generation losses
respectively. Experimental results indicate that our proposed BSharedRAG
outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in
retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation.
Our codes, models, and dataset are available at https://bsharedrag.github.io.",Kaisi Guan
2024-09-12T23:29:33Z,http://arxiv.org/abs/2410.00004v1,"Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy
  Similarity Searches and Domain Shift Generalization","The retrieval augmented generation (RAG) system such as Retro has been shown
to improve language modeling capabilities and reduce toxicity and
hallucinations by retrieving from a database of non-parametric memory
containing trillions of entries. We introduce Retro-li that shows retrieval can
also help using a small-scale database, but it demands more accurate and better
neighbors when searching in a smaller hence sparser non-parametric memory. This
can be met by using a proper semantic similarity search. We further propose
adding a regularization to the non-parametric memory for the first time: it
significantly reduces perplexity when the neighbor search operations are noisy
during inference, and it improves generalization when a domain shift occurs. We
also show that Retro-li's non-parametric memory can potentially be implemented
on analog in-memory computing hardware, exhibiting O(1) search time while
causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our
code is available at:
https://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",Gentiana Rashiti
2024-10-01T16:48:13Z,http://arxiv.org/abs/2410.00857v1,"Quantifying reliance on external information over parametric knowledge
  during Retrieval Augmented Generation (RAG) using mechanistic analysis","Retrieval Augmented Generation (RAG) is a widely used approach for leveraging
external context in several natural language applications such as question
answering and information retrieval. Yet, the exact nature in which a Language
Model (LM) leverages this non-parametric memory or retrieved context isn't
clearly understood. This paper mechanistically examines the RAG pipeline to
highlight that LMs demonstrate a ""shortcut'' effect and have a strong bias
towards utilizing the retrieved context to answer questions, while relying
minimally on model priors. We propose (a) Causal Mediation Analysis; for
proving that parametric memory is minimally utilized when answering a question
and (b) Attention Contributions and Knockouts for showing the last token
residual stream do not get enriched from the subject token in the question, but
gets enriched from tokens of RAG-context. We find this pronounced ""shortcut''
behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi)",Reshmi Ghosh
2024-10-03T17:39:38Z,http://arxiv.org/abs/2410.02719v1,"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling
  for Retrieval-Augmented Generation","We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.",Zixuan Li
2024-10-05T17:11:37Z,http://arxiv.org/abs/2410.04231v1,"Metadata-based Data Exploration with Retrieval-Augmented Generation for
  Large Language Models","Developing the capacity to effectively search for requisite datasets is an
urgent requirement to assist data users in identifying relevant datasets
considering the very limited available metadata. For this challenge, the
utilization of third-party data is emerging as a valuable source for
improvement. Our research introduces a new architecture for data exploration
which employs a form of Retrieval-Augmented Generation (RAG) to enhance
metadata-based data discovery. The system integrates large language models
(LLMs) with external vector databases to identify semantic relationships among
diverse types of datasets. The proposed framework offers a new method for
evaluating semantic similarity among heterogeneous data sources and for
improving data exploration. Our study includes experimental results on four
critical tasks: 1) recommending similar datasets, 2) suggesting combinable
datasets, 3) estimating tags, and 4) predicting variables. Our results
demonstrate that RAG can enhance the selection of relevant datasets,
particularly from different categories, when compared to conventional metadata
approaches. However, performance varied across tasks and models, which confirms
the significance of selecting appropriate techniques based on specific use
cases. The findings suggest that this approach holds promise for addressing
challenges in data exploration and discovery, although further refinement is
necessary for estimation tasks.",Teruaki Hayashi
2024-10-08T08:00:12Z,http://arxiv.org/abs/2410.05779v2,LightRAG: Simple and Fast Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG.",Zirui Guo
2024-10-08T08:34:54Z,http://arxiv.org/abs/2410.05801v1,"Retrieving, Rethinking and Revising: The Chain-of-Verification Can
  Improve Retrieval Augmented Generation","Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language
Models (LLMs) by incorporating extensive knowledge retrieved from external
sources. However, such approach encounters some challenges: Firstly, the
original queries may not be suitable for precise retrieval, resulting in
erroneous contextual knowledge; Secondly, the language model can easily
generate inconsistent answer with external references due to their knowledge
boundary limitation. To address these issues, we propose the
chain-of-verification (CoV-RAG) to enhance the external retrieval correctness
and internal generation consistency. Specifically, we integrate the
verification module into the RAG, engaging in scoring, judgment, and rewriting.
To correct external retrieval errors, CoV-RAG retrieves new knowledge using a
revised query. To correct internal generation errors, we unify QA and
verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our
comprehensive experiments across various LLMs demonstrate the effectiveness and
adaptability compared with other strong baselines. Especially, our CoV-RAG can
significantly surpass the state-of-the-art baselines using different LLM
backbones.",Bolei He
2024-10-10T03:51:58Z,http://arxiv.org/abs/2410.07589v1,"No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in
  LLMs, Even for Vigilant Users","Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness
and cost-efficiency in mitigating hallucinations and enhancing the
domain-specific generation capabilities of large language models (LLMs).
However, is this effectiveness and cost-efficiency truly a free lunch? In this
study, we comprehensively investigate the fairness costs associated with RAG by
proposing a practical three-level threat model from the perspective of user
awareness of fairness. Specifically, varying levels of user fairness awareness
result in different degrees of fairness censorship on the external dataset. We
examine the fairness implications of RAG using uncensored, partially censored,
and fully censored datasets. Our experiments demonstrate that fairness
alignment can be easily undermined through RAG without the need for fine-tuning
or retraining. Even with fully censored and supposedly unbiased external
datasets, RAG can lead to biased outputs. Our findings underscore the
limitations of current alignment methods in the context of RAG-based LLMs and
highlight the urgent need for new strategies to ensure fairness. We propose
potential mitigations and call for further research to develop robust fairness
safeguards in RAG-based LLMs.",Mengxuan Hu
2024-10-10T03:52:54Z,http://arxiv.org/abs/2410.07590v1,"TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed
  KV Caches for Chunked Text","Current Retrieval-Augmented Generation (RAG) systems concatenate and process
numerous retrieved document chunks for prefill which requires a large volume of
computation, therefore leading to significant latency in time-to-first-token
(TTFT). To reduce the computation overhead as well as TTFT, we introduce
TurboRAG, a novel RAG system that redesigns the inference paradigm of the
current RAG system by first pre-computing and storing the key-value (KV) caches
of documents offline, and then directly retrieving the saved KV cache for
prefill. Hence, online computation of KV caches is eliminated during inference.
In addition, we provide a number of insights into the mask matrix and
positional embedding mechanisms, plus fine-tune a pretrained language model to
maintain model accuracy of TurboRAG. Our approach is applicable to most
existing large language models and their applications without any requirement
in modification of models and inference systems. Experimental results across a
suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x
compared to the conventional RAG systems (on an average of 8.6x), but reserving
comparable performance to the standard RAG systems.",Songshuo Lu
2024-10-12T19:24:18Z,http://arxiv.org/abs/2410.09623v1,"Quebec Automobile Insurance Question-Answering With Retrieval-Augmented
  Generation","Large Language Models (LLMs) perform outstandingly in various downstream
tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has
been shown to improve performance for legal question answering (Nuruzzaman and
Hussain, 2020; Louis et al., 2024). However, there are limited applications in
insurance questions-answering, a specific type of legal document. This paper
introduces two corpora: the Quebec Automobile Insurance Expertise Reference
Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance
Questions. Our study leverages both corpora to automatically and manually
assess a GPT4-o, a state-of-the-art LLM, to answer Quebec automobile insurance
questions. Our results demonstrate that, on average, using our expertise
reference corpus generates better responses on both automatic and manual
evaluation metrics. However, they also highlight that LLM QA is unreliable
enough for mass utilization in critical areas. Indeed, our results show that
between 5% to 13% of answered questions include a false statement that could
lead to customer misunderstanding.",David Beauchemin
2024-10-15T06:26:24Z,http://arxiv.org/abs/2410.11315v1,"SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented
  Generation","Recent studies in Retrieval-Augmented Generation (RAG) have investigated
extracting evidence from retrieved passages to reduce computational costs and
enhance the final RAG performance, yet it remains challenging. Existing methods
heavily rely on heuristic-based augmentation, encountering several issues: (1)
Poor generalization due to hand-crafted context filtering; (2) Semantics
deficiency due to rule-based context chunking; (3) Skewed length due to
sentence-wise filter learning. To address these issues, we propose a
model-based evidence extraction learning framework, SEER, optimizing a vanilla
model as an evidence extractor with desired properties through self-aligned
learning. Extensive experiments show that our method largely improves the final
RAG performance, enhances the faithfulness, helpfulness, and conciseness of the
extracted evidence, and reduces the evidence length by 9.25 times. The code
will be available at https://github.com/HITsz-TMG/SEER.",Xinping Zhao
2024-10-15T09:02:09Z,http://arxiv.org/abs/2410.11414v1,"ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via
  Mechanistic Interpretability","Retrieval-Augmented Generation (RAG) models are designed to incorporate
external knowledge, reducing hallucinations caused by insufficient parametric
(internal) knowledge. However, even with accurate and relevant retrieved
content, RAG models can still produce hallucinations by generating outputs that
conflict with the retrieved information. Detecting such hallucinations requires
disentangling how Large Language Models (LLMs) utilize external and parametric
knowledge. Current detection methods often focus on one of these mechanisms or
without decoupling their intertwined effects, making accurate detection
difficult. In this paper, we investigate the internal mechanisms behind
hallucinations in RAG scenarios. We discover hallucinations occur when the
Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual
stream, while Copying Heads fail to effectively retain or integrate external
knowledge from retrieved content. Based on these findings, we propose ReDeEP, a
novel method that detects hallucinations by decoupling LLM's utilization of
external context and parametric knowledge. Our experiments show that ReDeEP
significantly improves RAG hallucination detection accuracy. Additionally, we
introduce AARF, which mitigates hallucinations by modulating the contributions
of Knowledge FFNs and Copying Heads.",Zhongxiang Sun
2024-10-01T03:54:45Z,http://arxiv.org/abs/2410.12812v1,"Optimizing and Evaluating Enterprise Retrieval-Augmented Generation
  (RAG): A Content Design Perspective","Retrieval-augmented generation (RAG) is a popular technique for using large
language models (LLMs) to build customer-support, question-answering solutions.
In this paper, we share our team's practical experience building and
maintaining enterprise-scale RAG solutions that answer users' questions about
our software based on product documentation. Our experience has not always
matched the most common patterns in the RAG literature. This paper focuses on
solution strategies that are modular and model-agnostic. For example, our
experience over the past few years - using different search methods and LLMs,
and many knowledge base collections - has been that simple changes to the way
we create knowledge base content can have a huge impact on our RAG solutions'
success. In this paper, we also discuss how we monitor and evaluate results.
Common RAG benchmark evaluation techniques have not been useful for evaluating
responses to novel user questions, so we have found a flexible, ""human in the
lead"" approach is required.",Sarah Packowski
2024-10-03T22:29:47Z,http://arxiv.org/abs/2410.12837v1,"A Comprehensive Survey of Retrieval-Augmented Generation (RAG):
  Evolution, Current Landscape and Future Directions","This paper presents a comprehensive study of Retrieval-Augmented Generation
(RAG), tracing its evolution from foundational concepts to the current state of
the art. RAG combines retrieval mechanisms with generative language models to
enhance the accuracy of outputs, addressing key limitations of LLMs. The study
explores the basic architecture of RAG, focusing on how retrieval and
generation are integrated to handle knowledge-intensive tasks. A detailed
review of the significant technological advancements in RAG is provided,
including key innovations in retrieval-augmented language models and
applications across various domains such as question-answering, summarization,
and knowledge-based tasks. Recent research breakthroughs are discussed,
highlighting novel methods for improving retrieval efficiency. Furthermore, the
paper examines ongoing challenges such as scalability, bias, and ethical
concerns in deployment. Future research directions are proposed, focusing on
improving the robustness of RAG models, expanding the scope of application of
RAG models, and addressing societal implications. This survey aims to serve as
a foundational resource for researchers and practitioners in understanding the
potential of RAG and its trajectory in natural language processing.",Shailja Gupta
2024-10-17T03:38:54Z,http://arxiv.org/abs/2410.13192v2,"Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented
  Generation with Large Language Models","The integration of documents generated by LLMs themselves (Self-Docs)
alongside retrieved documents has emerged as a promising strategy for
retrieval-augmented generation systems. However, previous research primarily
focuses on optimizing the use of Self-Docs, with their inherent properties
remaining underexplored. To bridge this gap, we first investigate the overall
effectiveness of Self-Docs, identifying key factors that shape their
contribution to RAG performance (RQ1). Building on these insights, we develop a
taxonomy grounded in Systemic Functional Linguistics to compare the influence
of various Self-Docs categories (RQ2) and explore strategies for combining them
with external sources (RQ3). Our findings reveal which types of Self-Docs are
most beneficial and offer practical guidelines for leveraging them to achieve
significant improvements in knowledge-intensive question answering tasks.",Jiatao Li
2024-10-17T06:30:55Z,http://arxiv.org/abs/2410.13258v1,"A Systematic Investigation of Knowledge Retrieval and Selection for
  Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has emerged as a powerful method for
enhancing natural language generation by integrating external knowledge into a
model's output. While prior work has demonstrated the importance of improving
knowledge retrieval for boosting generation quality, the role of knowledge
selection remains less clear. In this paper, we perform a comprehensive
analysis of how knowledge retrieval and selection influence downstream
generation performance in RAG systems. By simulating different retrieval and
selection conditions through a controlled mixture of gold and distractor
knowledge, we assess the impact of these factors on generation outcomes. Our
findings indicate that the downstream generator model's capability, as well as
the complexity of the task and dataset, significantly influence the impact of
knowledge retrieval and selection on the overall RAG system performance. In
typical scenarios, improving the knowledge recall score is key to enhancing
generation outcomes, with the knowledge selector providing a limited additional
benefit when a strong generator model is used on clear, well-defined tasks. For
weaker generator models or more ambiguous tasks and datasets, the knowledge F1
score becomes a critical factor, and the knowledge selector plays a more
prominent role in improving overall performance.",Xiangci Li
2024-10-17T07:46:49Z,http://arxiv.org/abs/2410.13293v2,"SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation","Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations. Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM). Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a ""reasoning score"" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more
structured problem-solving process potentially providing educational benefits
for students.",Prakhar Dixit
2024-10-18T04:17:49Z,http://arxiv.org/abs/2410.14167v1,"Optimizing Retrieval-Augmented Generation with Elasticsearch for
  Enhanced Question-Answering Systems","This study aims to improve the accuracy and quality of large-scale language
models (LLMs) in answering questions by integrating Elasticsearch into the
Retrieval Augmented Generation (RAG) framework. The experiment uses the
Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and
compares the performance of different retrieval methods, including traditional
methods based on keyword matching or semantic similarity calculation, BM25-RAG
and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that
ES-RAG not only has obvious advantages in retrieval efficiency but also
performs well in key indicators such as accuracy, which is 0.51 percentage
points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search
capabilities and rich configuration options enable the entire
question-answering system to better handle complex queries and provide more
flexible and efficient responses based on the diverse needs of users. Future
research directions can further explore how to optimize the interaction
mechanism between Elasticsearch and LLM, such as introducing higher-level
semantic understanding and context-awareness capabilities, to achieve a more
intelligent and humanized question-answering experience.",Jiajing Chen
2024-10-18T14:02:34Z,http://arxiv.org/abs/2410.14479v1,"Backdoored Retrievers for Prompt Injection Attacks on Retrieval
  Augmented Generation of Large Language Models","Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating coherent text but remain limited by the static nature of their
training data. Retrieval Augmented Generation (RAG) addresses this issue by
combining LLMs with up-to-date information retrieval, but also expand the
attack surface of the system. This paper investigates prompt injection attacks
on RAG, focusing on malicious objectives beyond misinformation, such as
inserting harmful links, promoting unauthorized services, and initiating
denial-of-service behaviors. We build upon existing corpus poisoning techniques
and propose a novel backdoor attack aimed at the fine-tuning process of the
dense retriever component. Our experiments reveal that corpus poisoning can
achieve significant attack success rates through the injection of a small
number of compromised documents into the retriever corpus. In contrast,
backdoor attacks demonstrate even higher success rates but necessitate a more
complex setup, as the victim must fine-tune the retriever using the attacker
poisoned dataset.",Cody Clop
2024-10-21T12:21:49Z,http://arxiv.org/abs/2410.15944v1,"Developing Retrieval Augmented Generation (RAG) based LLM Systems from
  PDFs: An Experience Report","This paper presents an experience report on the development of Retrieval
Augmented Generation (RAG) systems using PDF documents as the primary data
source. The RAG architecture combines generative capabilities of Large Language
Models (LLMs) with the precision of information retrieval. This approach has
the potential to redefine how we interact with and augment both structured and
unstructured knowledge in generative models to enhance transparency, accuracy,
and contextuality of responses. The paper details the end-to-end pipeline, from
data collection, preprocessing, to retrieval indexing and response generation,
highlighting technical challenges and practical solutions. We aim to offer
insights to researchers and practitioners developing similar systems using two
distinct approaches: OpenAI's Assistant API with GPT Series and Llama's
open-source models. The practical implications of this research lie in
enhancing the reliability of generative AI systems in various sectors where
domain-specific knowledge and real-time information retrieval is important. The
Python code used in this work is also available at:
https://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.",Ayman Asad Khan
2024-10-23T15:24:16Z,http://arxiv.org/abs/2410.17952v1,"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large
  Language Models to Specialized Domains","Retrieval-augmented generation (RAG) enhances the question-answering (QA)
abilities of large language models (LLMs) by integrating external knowledge.
However, adapting general-purpose RAG systems to specialized fields such as
science and medicine poses unique challenges due to distribution shifts and
limited access to domain-specific data. To tackle this, we propose SimRAG, a
self-training approach that equips the LLM with joint capabilities of question
answering and question generation for domain adaptation. Our method first
fine-tunes the LLM on instruction-following, question-answering, and
search-related data. Then, it prompts the same LLM to generate diverse
domain-relevant questions from unlabeled corpora, with an additional filtering
strategy to retain high-quality synthetic examples. By leveraging these
synthetic examples, the LLM can improve their performance on domain-specific
RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three
domains, demonstrate that SimRAG outperforms baselines by 1.2\%--8.6\%.",Ran Xu
2024-10-24T00:49:46Z,http://arxiv.org/abs/2410.18344v1,"Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned
  and Retrieval-Augmented Generation Models","This paper introduces a novel approach to enhancing closed-domain Question
Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley
National Laboratory (LBL) Science Information Technology (ScienceIT) domain.
Utilizing a rich dataset derived from the ScienceIT documentation, our study
embarks on a detailed comparison of two fine-tuned large language models and
five retrieval-augmented generation (RAG) models. Through data processing
techniques, we transform the documentation into structured
context-question-answer triples, leveraging the latest Large Language Models
(AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for
data-driven insights. Additionally, we introduce the Aggregated Knowledge Model
(AKM), which synthesizes responses from the seven models mentioned above using
K-means clustering to select the most representative answers. The evaluation of
these models across multiple metrics offers a comprehensive look into their
effectiveness and suitability for the LBL ScienceIT environment. The results
demonstrate the potential benefits of integrating fine-tuning and
retrieval-augmented strategies, highlighting significant performance
improvements achieved with the AKM. The insights gained from this study can be
applied to develop specialized QA systems tailored to specific domains.",Fengchen Liu
2024-10-28T05:35:04Z,http://arxiv.org/abs/2410.20753v1,Plan$\times$RAG: Planning-guided Retrieval Augmented Generation,"We introduce Planning-guided Retrieval Augmented Generation
(Plan$\times$RAG), a novel framework that augments the
\emph{retrieve-then-reason} paradigm of existing RAG frameworks to
\emph{plan-then-retrieve}. Plan$\times$RAG formulates a reasoning plan as a
directed acyclic graph (DAG), decomposing queries into interrelated atomic
sub-queries. Answer generation follows the DAG structure, allowing significant
gains in efficiency through parallelized retrieval and generation. While
state-of-the-art RAG solutions require extensive data generation and
fine-tuning of language models (LMs), Plan$\times$RAG incorporates frozen LMs
as plug-and-play experts to generate high-quality answers. Compared to existing
RAG solutions, Plan$\times$RAG demonstrates significant improvements in
reducing hallucinations and bolstering attribution due to its structured
sub-query decomposition. Overall, Plan$\times$RAG offers a new perspective on
integrating external knowledge in LMs while ensuring attribution by design,
contributing towards more reliable LM-based systems.",Prakhar Verma
2024-10-28T08:32:09Z,http://arxiv.org/abs/2410.20833v1,"LLMs are Biased Evaluators But Not Biased for Retrieval Augmented
  Generation","Recent studies have demonstrated that large language models (LLMs) exhibit
significant biases in evaluation tasks, particularly in preferentially rating
and favoring self-generated content. However, the extent to which this bias
manifests in fact-oriented tasks, especially within retrieval-augmented
generation (RAG) frameworks-where keyword extraction and factual accuracy take
precedence over stylistic elements-remains unclear. Our study addresses this
knowledge gap by simulating two critical phases of the RAG framework. In the
first phase, we access the suitability of human-authored versus model-generated
passages, emulating the pointwise reranking process. The second phase involves
conducting pairwise reading comprehension tests to simulate the generation
process. Contrary to previous findings indicating a self-preference in rating
tasks, our results reveal no significant self-preference effect in RAG
frameworks. Instead, we observe that factual accuracy significantly influences
LLMs' output, even in the absence of prior knowledge. Our research contributes
to the ongoing discourse on LLM biases and their implications for RAG-based
system, offering insights that may inform the development of more robust and
unbiased LLM systems.",Yen-Shan Chen
2024-10-30T12:09:29Z,http://arxiv.org/abs/2410.22954v1,Retrieval-Augmented Generation with Estimation of Source Reliability,"Retrieval-augmented generation (RAG) addresses key limitations of large
language models (LLMs), such as hallucinations and outdated knowledge, by
incorporating external databases. These databases typically consult multiple
sources to encompass up-to-date and various information. However, standard RAG
methods often overlook the heterogeneous source reliability in the multi-source
database and retrieve documents solely based on relevance, making them prone to
propagating misinformation. To address this, we propose Reliability-Aware RAG
(RA-RAG) which estimates the reliability of multiple sources and incorporates
this information into both retrieval and aggregation processes. Specifically,
it iteratively estimates source reliability and true answers for a set of
queries with no labelling. Then, it selectively retrieves relevant documents
from a few of reliable sources and aggregates them using weighted majority
voting, where the selective retrieval ensures scalability while not
compromising the performance. We also introduce a benchmark designed to reflect
real-world scenarios with heterogeneous source reliability and demonstrate the
effectiveness of RA-RAG compared to a set of baselines.",Jeongyeon Hwang
2024-10-30T13:29:36Z,http://arxiv.org/abs/2410.23000v2,"Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented
  Generation with Key Point Recall","Retrieval-augmented generation (RAG) is a promising approach to address the
limitations of fixed knowledge in large language models (LLMs). However,
current benchmarks for evaluating RAG systems suffer from two key deficiencies:
(1) they fail to adequately measure LLMs' capability in handling long-context
retrieval due to a lack of datasets that reflect the characteristics of
retrieved documents, and (2) they lack a comprehensive evaluation method for
assessing LLMs' ability to generate long-form responses that effectively
exploits retrieved information. To address these shortcomings, we introduce the
Long$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG
comprises 280 questions spanning 10 domains and across 8 question categories,
each associated with 5 retrieved documents with an average length of 2,444
words. KPR evaluates the extent to which LLMs incorporate key points extracted
from the retrieved documents into their generated responses, providing a more
nuanced assessment of their ability to exploit retrieved information.",Zehan Qi
2024-10-30T15:06:32Z,http://arxiv.org/abs/2410.23090v1,"CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation
  Generation","Retrieval-Augmented Generation (RAG) has become a powerful paradigm for
enhancing large language models (LLMs) through external knowledge retrieval.
Despite its widespread attention, existing academic research predominantly
focuses on single-turn RAG, leaving a significant gap in addressing the
complexities of multi-turn conversations found in real-world applications. To
bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess
RAG systems in realistic multi-turn conversational settings. CORAL includes
diverse information-seeking conversations automatically derived from Wikipedia
and tackles key challenges such as open-domain coverage, knowledge intensity,
free-form responses, and topic shifts. It supports three core tasks of
conversational RAG: passage retrieval, response generation, and citation
labeling. We propose a unified framework to standardize various conversational
RAG methods and conduct a comprehensive evaluation of these methods on CORAL,
demonstrating substantial opportunities for improving existing approaches.",Yiruo Cheng
2024-10-31T13:05:39Z,http://arxiv.org/abs/2410.23902v1,"Responsible Retrieval Augmented Generation for Climate Decision Making
  from Documents","Climate decision making is constrained by the complexity and inaccessibility
of key information within lengthy, technical, and multi-lingual documents.
Generative AI technologies offer a promising route for improving the
accessibility of information contained within these documents, but suffer from
limitations. These include (1) a tendency to hallucinate or mis-represent
information, (2) difficulty in steering or guaranteeing properties of generated
output, and (3) reduced performance in specific technical domains. To address
these challenges, we introduce a novel evaluation framework with
domain-specific dimensions tailored for climate-related documents. We then
apply this framework to evaluate Retrieval-Augmented Generation (RAG)
approaches and assess retrieval- and generation-quality within a prototype tool
that answers questions about individual climate law and policy documents. In
addition, we publish a human-annotated dataset and scalable automated
evaluation tools, with the aim of facilitating broader adoption and robust
assessment of these systems in the climate domain. Our findings highlight the
key components of responsible deployment of RAG to enhance decision-making,
while also providing insights into user experience (UX) considerations for
safely deploying such systems to build trust with users in high-risk domains.",Matyas Juhasz
2024-11-01T15:50:58Z,http://arxiv.org/abs/2411.00689v1,"Towards Multi-Source Retrieval-Augmented Generation via Synergizing
  Reasoning and Preference-Driven Retrieval","Retrieval-Augmented Generation (RAG) has emerged as a reliable external
knowledge augmentation technique to mitigate hallucination issues and
parameterized knowledge limitations in Large Language Models (LLMs). Existing
Adaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval
sources due to their inability to select the right source at the right time. To
address this, we propose a multi-source ARAG framework, termed MSPR, which
synergizes reasoning and preference-driven retrieval to adaptive decide ""when
and what to retrieve"" and ""which retrieval source to use"". To better adapt to
retrieval sources of differing characteristics, we also employ retrieval action
adjustment and answer feedback strategy. They enable our framework to fully
explore the high-quality primary source while supplementing it with secondary
sources at the right time. Extensive and multi-dimensional experiments
conducted on three datasets demonstrate the superiority and effectiveness of
MSPR.",Qingfei Zhao
2024-11-04T02:30:05Z,http://arxiv.org/abs/2411.01751v1,RAGViz: Diagnose and Visualize Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",Tevin Wang
2024-11-04T21:12:08Z,http://arxiv.org/abs/2411.02617v1,"TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support for Network","The telecommunications industry's rapid evolution demands intelligent systems
capable of managing complex networks and adapting to emerging technologies.
While large language models (LLMs) show promise in addressing these challenges,
their deployment in telecom environments faces significant constraints due to
edge device limitations and inconsistent documentation. To bridge this gap, we
present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)
system built on the Phi-2 small language model (SLM). To improve context
retrieval, TeleOracle employs a two-stage retriever that incorporates semantic
chunking and hybrid keyword and semantic search. Additionally, we expand the
context window during inference to enhance the model's performance on
open-ended queries. We also employ low-rank adaption for efficient fine-tuning.
A thorough analysis of the model's performance indicates that our RAG framework
is effective in aligning Phi-2 to the telecom domain in a downstream question
and answer (QnA) task, achieving a 30% improvement in accuracy over the base
Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our
model not only performs on par with the much larger LLMs but also achieves a
higher faithfulness score, indicating higher adherence to the retrieved
context.",Nouf Alabbasi
2024-11-05T06:11:17Z,http://arxiv.org/abs/2411.02832v2,PersianRAG: A Retrieval-Augmented Generation System for Persian Language,"Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.",Hossein Hosseini
2024-11-06T15:32:28Z,http://arxiv.org/abs/2411.05844v1,"LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration","GraphRAG addresses significant challenges in Retrieval-Augmented Generation
(RAG) by leveraging graphs with embedded knowledge to enhance the reasoning
capabilities of Large Language Models (LLMs). Despite its promising potential,
the GraphRAG community currently lacks a unified framework for fine-grained
decomposition of the graph-based knowledge retrieval process. Furthermore,
there is no systematic categorization or evaluation of existing solutions
within the retrieval process. In this paper, we present LEGO-GraphRAG, a
modular framework that decomposes the retrieval process of GraphRAG into three
interconnected modules: subgraph-extraction, path-filtering, and
path-refinement. We systematically summarize and classify the algorithms and
neural network (NN) models relevant to each module, providing a clearer
understanding of the design space for GraphRAG instances. Additionally, we
identify key design factors, such as Graph Coupling and Computational Cost,
that influence the effectiveness of GraphRAG implementations. Through extensive
empirical studies, we construct high-quality GraphRAG instances using a
representative selection of solutions and analyze their impact on retrieval and
reasoning performance. Our findings offer critical insights into optimizing
GraphRAG instance design, ultimately contributing to the advancement of more
accurate and contextually relevant LLM applications.",Yukun Cao
2024-11-09T17:38:01Z,http://arxiv.org/abs/2411.06237v2,"Leveraging Retrieval-Augmented Generation for Persian University
  Knowledge Retrieval","This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.",Arshia Hemmat
2024-11-11T09:03:52Z,http://arxiv.org/abs/2411.06805v1,"AssistRAG: Boosting the Potential of Large Language Models with an
  Intelligent Information Assistant","The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as ""hallucination"". Initial retrieval-augmented
generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.",Yujia Zhou
2024-11-13T08:43:37Z,http://arxiv.org/abs/2411.08438v1,"Towards Optimizing a Retrieval Augmented Generation using Large Language
  Model on Academic Data","Given the growing trend of many organizations integrating Retrieval Augmented
Generation (RAG) into their operations, we assess RAG on domain-specific data
and test state-of-the-art models across various optimization techniques. We
incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble
Retriever, and In-Context-Learning, to enhance the functionality and
performance in the academic domain. We focus on data retrieval, specifically
targeting various study programs at a large technical university. We
additionally introduce a novel evaluation approach, the RAG Confusion Matrix
designed to assess the effectiveness of various configurations within the RAG
framework. By exploring the integration of both open-source (e.g., Llama2,
Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer
valuable insights into the application and optimization of RAG frameworks in
domain-specific contexts. Our experiments show a significant performance
increase when including multi-query in the retrieval phase.",Anum Afzal
2024-10-28T06:41:05Z,http://arxiv.org/abs/2411.08891v1,Calibrated Decision-Making through LLM-Assisted Retrieval,"Recently, large language models (LLMs) have been increasingly used to support
various decision-making tasks, assisting humans in making informed decisions.
However, when LLMs confidently provide incorrect information, it can lead
humans to make suboptimal decisions. To prevent LLMs from generating incorrect
information on topics they are unsure of and to improve the accuracy of
generated content, prior works have proposed Retrieval Augmented Generation
(RAG), where external documents are referenced to generate responses. However,
traditional RAG methods focus only on retrieving documents most relevant to the
input query, without specifically aiming to ensure that the human user's
decisions are well-calibrated. To address this limitation, we propose a novel
retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),
which ensures that decisions informed by the retrieved documents are
well-calibrated. Then we empirically validate that CalibRAG improves
calibration performance as well as accuracy, compared to other baselines across
various datasets.",Chaeyun Jang
2024-11-07T22:11:51Z,http://arxiv.org/abs/2411.11895v1,Deploying Large Language Models With Retrieval Augmented Generation,"Knowing that the generative capabilities of large language models (LLM) are
sometimes hampered by tendencies to hallucinate or create non-factual
responses, researchers have increasingly focused on methods to ground generated
outputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a
key approach for integrating knowledge from data sources outside of the LLM's
training set, including proprietary and up-to-date information. While many
research papers explore various RAG strategies, their true efficacy is tested
in real-world applications with actual data. The journey from conceiving an
idea to actualizing it in the real world is a lengthy process. We present
insights from the development and field-testing of a pilot project that
integrates LLMs with RAG for information retrieval. Additionally, we examine
the impacts on the information value chain, encompassing people, processes, and
technology. Our aim is to identify the opportunities and challenges of
implementing this emerging technology, particularly within the context of
behavioral research in the information systems (IS) field. The contributions of
this work include the development of best practices and recommendations for
adopting this promising technology while ensuring compliance with industry
regulations through a proposed AI governance model.",Sonal Prabhune
2024-11-20T20:10:43Z,http://arxiv.org/abs/2411.13691v1,"Retrieval-Augmented Generation for Domain-Specific Question Answering: A
  Case Study on Pittsburgh and CMU","We designed a Retrieval-Augmented Generation (RAG) system to provide large
language models with relevant documents for answering domain-specific questions
about Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800
subpages using a greedy scraping strategy and employed a hybrid annotation
process, combining manual and Mistral-generated question-answer pairs,
achieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework
integrates BM25 and FAISS retrievers, enhanced with a reranker for improved
document retrieval accuracy. Experimental results show that the RAG system
significantly outperforms a non-RAG baseline, particularly in time-sensitive
and complex queries, with an F1 score improvement from 5.45% to 42.21% and
recall of 56.18%. This study demonstrates the potential of RAG systems in
enhancing answer precision and relevance, while identifying areas for further
optimization in document retrieval and model training.",Haojia Sun
2024-11-21T01:00:25Z,http://arxiv.org/abs/2411.13773v1,FastRAG: Retrieval Augmented Generation for Semi-structured Data,"Efficiently processing and interpreting network data is critical for the
operation of increasingly complex networks. Recent advances in Large Language
Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved
data processing in network management. However, existing RAG methods like
VectorRAG and GraphRAG struggle with the complexity and implicit nature of
semi-structured technical data, leading to inefficiencies in time, cost, and
retrieval. This paper introduces FastRAG, a novel RAG approach designed for
semi-structured data. FastRAG employs schema learning and script learning to
extract and structure data without needing to submit entire data sources to an
LLM. It integrates text search with knowledge graph (KG) querying to improve
accuracy in retrieving context-rich information. Evaluation results demonstrate
that FastRAG provides accurate question answering, while improving up to 90% in
time and 85% in cost compared to GraphRAG.",Amar Abane
2024-11-21T20:39:13Z,http://arxiv.org/abs/2411.14572v1,"Towards Knowledge Checking in Retrieval-augmented Generation: A
  Representation Perspective","Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing
the performance of Large Language Models (LLMs). However, these systems face
challenges in effectively integrating external knowledge with the LLM's
internal knowledge, often leading to issues with misleading or unhelpful
information. This work aims to provide a systematic study on knowledge checking
in RAG systems. We conduct a comprehensive analysis of LLM representation
behaviors and demonstrate the significance of using representations in
knowledge checking. Motivated by the findings, we further develop
representation-based classifiers for knowledge filtering. We show substantial
improvements in RAG performance, even when dealing with noisy knowledge
databases. Our study provides new insights into leveraging LLM representations
for enhancing the reliability and effectiveness of RAG systems.",Shenglai Zeng
2024-11-23T09:56:21Z,http://arxiv.org/abs/2411.16732v1,"Multi-Reranker: Maximizing performance of retrieval-augmented generation
  in the FinanceRAG challenge","As Large Language Models (LLMs) increasingly address domain-specific
problems, their application in the financial sector has expanded rapidly. Tasks
that are both highly valuable and time-consuming, such as analyzing financial
statements, disclosures, and related documents, are now being effectively
tackled using LLMs. This paper details the development of a high-performance,
finance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF
'24 FinanceRAG competition. We optimized performance through ablation studies
on query expansion and corpus refinement during the pre-retrieval phase. To
enhance retrieval accuracy, we employed multiple reranker models. Notably, we
introduced an efficient method for managing long context sizes during the
generation phase, significantly improving response quality without sacrificing
performance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our
key contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced
retrieval algorithm, and (3) a novel approach for long-context management. This
work demonstrates the potential of LLMs in effectively processing and analyzing
complex financial data to generate accurate and valuable insights. The source
code and further details are available at https://github.com/cv-lee/FinanceRAG.",Joohyun Lee
2024-12-02T14:55:02Z,http://arxiv.org/abs/2412.01572v3,"MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation
  through Question Complexity","Retrieval Augmented Generation (RAG) has proven to be highly effective in
boosting the generative performance of language model in knowledge-intensive
tasks. However, existing RAG framework either indiscriminately perform
retrieval or rely on rigid single-class classifiers to select retrieval
methods, leading to inefficiencies and suboptimal performance across queries of
varying complexity. To address these challenges, we propose a reinforcement
learning-based framework that dynamically selects the most suitable retrieval
strategy based on query complexity. % our solution Our approach leverages a
multi-armed bandit algorithm, which treats each retrieval method as a distinct
``arm'' and adapts the selection process by balancing exploration and
exploitation. Additionally, we introduce a dynamic reward function that
balances accuracy and efficiency, penalizing methods that require more
retrieval steps, even if they lead to a correct result. Our method achieves new
state of the art results on multiple single-hop and multi-hop datasets while
reducing retrieval costs. Our code are available at
https://github.com/FUTUREEEEEE/MBA .",Xiaqiang Tang
2024-12-03T16:52:06Z,http://arxiv.org/abs/2412.02563v1,Semantic Tokens in Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) architectures have recently garnered
significant attention for their ability to improve truth grounding and
coherence in natural language processing tasks. However, the reliability of RAG
systems in producing accurate answers diminishes as the volume of data they
access increases. Even with smaller datasets, these systems occasionally fail
to address simple queries. This issue arises from their dependence on
state-of-the-art large language models (LLMs), which can introduce uncertainty
into the system's outputs. In this work, I propose a novel Comparative RAG
system that introduces an evaluator module to bridge the gap between
probabilistic RAG systems and deterministically verifiable responses. The
evaluator compares external recommendations with the retrieved document chunks,
adding a decision-making layer that enhances the system's reliability. This
approach ensures that the chunks retrieved are both semantically relevant and
logically consistent with deterministic insights, thereby improving the
accuracy and overall efficiency of RAG systems. This framework paves the way
for more reliable and scalable question-answering applications in domains
requiring high precision and verifiability.",Joel Suro
2024-12-05T07:23:14Z,http://arxiv.org/abs/2412.03933v1,"Exploring AI Text Generation, Retrieval-Augmented Generation, and
  Detection Technologies: a Comprehensive Overview","The rapid development of Artificial Intelligence (AI) has led to the creation
of powerful text generation models, such as large language models (LLMs), which
are widely used for diverse applications. However, concerns surrounding
AI-generated content, including issues of originality, bias, misinformation,
and accountability, have become increasingly prominent. This paper offers a
comprehensive overview of AI text generators (AITGs), focusing on their
evolution, capabilities, and ethical implications. This paper also introduces
Retrieval-Augmented Generation (RAG), a recent approach that improves the
contextual relevance and accuracy of text generation by integrating dynamic
information retrieval. RAG addresses key limitations of traditional models,
including their reliance on static knowledge and potential inaccuracies in
handling real-world data. Additionally, the paper reviews detection tools that
help differentiate AI-generated text from human-written content and discusses
the ethical challenges these technologies pose. The paper explores future
directions for improving detection accuracy, supporting ethical AI development,
and increasing accessibility. The paper contributes to a more responsible and
reliable use of AI in content creation through these discussions.",Fnu Neha
2024-12-06T01:20:16Z,http://arxiv.org/abs/2412.04697v1,"Privacy-Preserving Retrieval Augmented Generation with Differential
  Privacy","With the recent remarkable advancement of large language models (LLMs), there
has been a growing interest in utilizing them in the domains with highly
sensitive data that lies outside their training data. For this purpose,
retrieval augmented generation (RAG) is particularly effective -- it assists
LLMs by directly providing relevant information from the external knowledge
sources. However, without extra privacy safeguards, RAG outputs risk leaking
sensitive information from the external data source. In this work, we explore
RAG under differential privacy (DP), a formal guarantee of data privacy. The
main challenge with differentially private RAG is how to generate long accurate
answers within a moderate privacy budget. We address this by proposing an
algorithm that smartly spends privacy budget only for the tokens that require
the sensitive information and uses the non-private LLM for other tokens. Our
extensive empirical evaluations reveal that our algorithm outperforms the
non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$
across different models and datasets.",Tatsuki Koga
2024-12-11T16:32:41Z,http://arxiv.org/abs/2412.08519v1,"Bridging Relevance and Reasoning: Rationale Distillation in
  Retrieval-Augmented Generation","The reranker and generator are two critical components in the
Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking
relevant documents and generating responses. However, due to differences in
pre-training data and objectives, there is an inevitable gap between the
documents ranked as relevant by the reranker and those required by the
generator to support answering the query. To address this gap, we propose
RADIO, a novel and practical preference alignment framework with RAtionale
DIstillatiOn. Specifically, We first propose a rationale extraction method that
leverages the reasoning capabilities of Large Language Models (LLMs) to extract
the rationales necessary for answering the query. Subsequently, a
rationale-based alignment process is designed to rerank the documents based on
the extracted rationales, and fine-tune the reranker to align the preferences.
We conduct extensive experiments on two tasks across three datasets to
demonstrate the effectiveness of our approach compared to baseline methods. Our
code is released online to ease reproduction.",Pengyue Jia
2024-12-12T06:38:40Z,http://arxiv.org/abs/2412.08985v1,"Assessing the Robustness of Retrieval-Augmented Generation Systems in
  K-12 Educational Question Answering with Knowledge Discrepancies","Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable
potential as question answering systems in the K-12 Education domain, where
knowledge is typically queried within the restricted scope of authoritative
textbooks. However, the discrepancy between textbooks and the parametric
knowledge in Large Language Models (LLMs) could undermine the effectiveness of
RAG systems. To systematically investigate the robustness of RAG systems under
such knowledge discrepancies, we present EduKDQA, a question answering dataset
that simulates knowledge discrepancies in real applications by applying
hypothetical knowledge updates in answers and source documents. EduKDQA
includes 3,005 questions covering five subjects, under a comprehensive question
typology from the perspective of context utilization and knowledge integration.
We conducted extensive experiments on retrieval and question answering
performance. We find that most RAG systems suffer from a substantial
performance drop in question answering with knowledge discrepancies, while
questions that require integration of contextual knowledge and parametric
knowledge pose a challenge to LLMs.",Tianshi Zheng
2024-12-13T14:11:26Z,http://arxiv.org/abs/2412.10151v1,"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval
  Augmented Generation","We propose the VLR-Bench, a visual question answering (VQA) benchmark for
evaluating vision language models (VLMs) based on retrieval augmented
generation (RAG). Unlike existing evaluation datasets for external
knowledge-based VQA, the proposed VLR-Bench includes five input passages. This
allows testing of the ability to determine which passage is useful for
answering a given query, a capability lacking in previous research. In this
context, we constructed a dataset of 32,000 automatically generated
instruction-following examples, which we denote as VLR-IF. This dataset is
specifically designed to enhance the RAG capabilities of VLMs by enabling them
to learn how to generate appropriate answers based on input passages. We
evaluated the validity of the proposed benchmark and training data and verified
its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3
model. The proposed VLR-Bench and VLR-IF datasets are publicly available
online.",Hyeonseok Lim
2024-12-14T06:24:55Z,http://arxiv.org/abs/2412.10704v1,"VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal
  Retrieval-Augmented Generation","Understanding information from a collection of multiple documents,
particularly those with visually rich elements, is important for
document-grounded question answering. This paper introduces VisDoMBench, the
first comprehensive benchmark designed to evaluate QA systems in multi-document
settings with rich multimodal content, including tables, charts, and
presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval
Augmented Generation (RAG) approach that simultaneously utilizes visual and
textual RAG, combining robust visual retrieval capabilities with sophisticated
linguistic reasoning. VisDoMRAG employs a multi-step reasoning process
encompassing evidence curation and chain-of-thought reasoning for concurrent
textual and visual RAG pipelines. A key novelty of VisDoMRAG is its
consistency-constrained modality fusion mechanism, which aligns the reasoning
processes across modalities at inference time to produce a coherent final
answer. This leads to enhanced accuracy in scenarios where critical information
is distributed across modalities and improved answer verifiability through
implicit context attribution. Through extensive experiments involving
open-source and proprietary large language models, we benchmark
state-of-the-art document QA methods on VisDoMBench. Extensive results show
that VisDoMRAG outperforms unimodal and long-context LLM baselines for
end-to-end multimodal document QA by 12-20%.",Manan Suri
2024-12-16T15:12:53Z,http://arxiv.org/abs/2412.11854v1,"Towards Understanding Systems Trade-offs in Retrieval-Augmented
  Generation Model Inference","The rapid increase in the number of parameters in large language models
(LLMs) has significantly increased the cost involved in fine-tuning and
retraining LLMs, a necessity for keeping models up to date and improving
accuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to
improving the capabilities and accuracy of LLMs without the necessity of
retraining. Although RAG eliminates the need for continuous retraining to
update model data, it incurs a trade-off in the form of slower model inference
times. Resultingly, the use of RAG in enhancing the accuracy and capabilities
of LLMs often involves diverse performance implications and trade-offs based on
its design. In an effort to begin tackling and mitigating the performance
penalties associated with RAG from a systems perspective, this paper introduces
a detailed taxonomy and characterization of the different elements within the
RAG ecosystem for LLMs that explore trade-offs within latency, throughput, and
memory. Our study reveals underlying inefficiencies in RAG for systems
deployment, that can result in TTFT latencies that are twice as long and
unoptimized datastores that consume terabytes of storage.",Michael Shen
2024-12-19T07:01:25Z,http://arxiv.org/abs/2412.14581v1,"CORD: Balancing COnsistency and Rank Distillation for Robust
  Retrieval-Augmented Generation","With the adoption of retrieval-augmented generation (RAG), large language
models (LLMs) are expected to ground their generation to the retrieved
contexts. Yet, this is hindered by position bias of LLMs, failing to evenly
attend to all contexts. Previous work has addressed this by synthesizing
contexts with perturbed positions of gold segment, creating a
position-diversified train set. We extend this intuition to propose consistency
regularization with augmentation and distillation. First, we augment each
training instance with its position perturbation to encourage consistent
predictions, regardless of ordering. We also distill behaviors of this pair,
although it can be counterproductive in certain RAG scenarios where the given
order from the retriever is crucial for generation quality. We thus propose
CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples
noise-controlled perturbations from an interpolation space, ensuring both
consistency and respect for the rank prior. Empirical results show this balance
enables CORD to outperform consistently in diverse RAG benchmarks.",Youngwon Lee
2024-12-19T14:37:11Z,http://arxiv.org/abs/2412.14905v1,"Dehallucinating Parallel Context Extension for Retrieval-Augmented
  Generation","Large language models (LLMs) are susceptible to generating hallucinated
information, despite the integration of retrieval-augmented generation (RAG).
Parallel context extension (PCE) is a line of research attempting to
effectively integrating parallel (unordered) contexts, while it still suffers
from hallucinations when adapted to RAG scenarios. In this paper, we propose
DePaC (Dehallucinating Parallel Context Extension), which alleviates the
hallucination problem with context-aware negative training and
information-calibrated aggregation. DePaC is designed to alleviate two types of
in-context hallucination: fact fabrication (i.e., LLMs present claims that are
not supported by the contexts) and fact omission (i.e., LLMs fail to present
claims that can be supported by the contexts). Specifically, (1) for fact
fabrication, we apply the context-aware negative training that fine-tunes the
LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to
answer when contexts are not related to questions; (2) for fact omission, we
propose the information-calibrated aggregation which prioritizes context
windows with higher information increment from their contexts. The experimental
results on nine RAG tasks demonstrate that DePaC significantly alleviates the
two types of hallucination and consistently achieves better performances on
these tasks.",Zexiong Ma
2024-12-21T14:27:38Z,http://arxiv.org/abs/2412.16643v1,"TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented
  Generation","Although the rise of large language models (LLMs) has introduced new
opportunities for time series forecasting, existing LLM-based solutions require
excessive training and exhibit limited transferability. In view of these
challenges, we propose TimeRAG, a framework that incorporates
Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which
constructs a time series knowledge base from historical sequences, retrieves
reference sequences from the knowledge base that exhibit similar patterns to
the query sequence measured by Dynamic Time Warping (DTW), and combines these
reference sequences and the prediction query as a textual prompt to the time
series forecasting LLM. Experiments on datasets from various domains show that
the integration of RAG improved the prediction accuracy of the original model
by 2.97% on average.",Silin Yang
2024-12-21T16:59:00Z,http://arxiv.org/abs/2412.16701v1,"AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed
  articles","Recent advancements in generative AI have flourished the development of
highly adept Large Language Models (LLMs) that integrate diverse data types to
empower decision-making. Among these, Multimodal Retrieval-Augmented Generation
(RAG) applications are promising for their capability to combine the strengths
of information retrieval and generative models, enhancing their utility across
various domains, including biomedical research. This paper introduces
AlzheimerRAG, a Multimodal RAG pipeline tool for biomedical research use cases,
primarily focusing on Alzheimer's disease from PubMed articles. Our pipeline
incorporates multimodal fusion techniques to integrate textual and visual data
processing by efficiently indexing and accessing vast amounts of biomedical
literature. Preliminary experimental results against benchmarks, such as BioASQ
and PubMedQA, have returned improved results in information retrieval and
synthesis of domain-specific information. We also demonstrate a case study with
our RAG pipeline across different Alzheimer's clinical scenarios. We infer that
AlzheimerRAG can generate responses with accuracy non-inferior to humans and
with low rates of hallucination. Overall, a reduction in cognitive task load is
observed, which allows researchers to gain multimodal insights, improving
understanding and treatment of Alzheimer's disease.",Aritra Kumar Lahiri
2024-12-21T17:31:52Z,http://arxiv.org/abs/2412.16708v1,"Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under
  Adversarial Poisoning Attacks","Retrieval-Augmented Generation (RAG) systems have emerged as a promising
solution to mitigate LLM hallucinations and enhance their performance in
knowledge-intensive domains. However, these systems are vulnerable to
adversarial poisoning attacks, where malicious passages injected into retrieval
databases can mislead the model into generating factually incorrect outputs. In
this paper, we investigate both the retrieval and the generation components of
RAG systems to understand how to enhance their robustness against such attacks.
From the retrieval perspective, we analyze why and how the adversarial contexts
are retrieved and assess how the quality of the retrieved passages impacts
downstream generation. From a generation perspective, we evaluate whether LLMs'
advanced critical thinking and internal knowledge capabilities can be leveraged
to mitigate the impact of adversarial contexts, i.e., using skeptical prompting
as a self-defense mechanism. Our experiments and findings provide actionable
insights into designing safer and more resilient retrieval-augmented
frameworks, paving the way for their reliable deployment in real-world
applications.",Jinyan Su
2024-12-24T13:45:22Z,http://arxiv.org/abs/2412.18431v1,GeAR: Graph-enhanced Agent for Retrieval-augmented Generation,"Retrieval-augmented generation systems rely on effective document retrieval
capabilities. By design, conventional sparse or dense retrievers face
challenges in multi-hop retrieval scenarios. In this paper, we present GeAR,
which advances RAG performance through two key innovations: (i) graph
expansion, which enhances any conventional base retriever, such as BM25, and
(ii) an agent framework that incorporates graph expansion. Our evaluation
demonstrates GeAR's superior retrieval performance on three multi-hop question
answering datasets. Additionally, our system achieves state-of-the-art results
with improvements exceeding 10% on the challenging MuSiQue dataset, while
requiring fewer tokens and iterations compared to other multi-step retrieval
systems.",Zhili Shen
2023-12-18T03:19:31Z,http://arxiv.org/abs/2312.10904v2,"Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)","Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.",Sabrina Toro
2024-09-23T21:42:47Z,http://arxiv.org/abs/2409.15566v1,GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation,"The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.",Brendan Hogan Rappazzo
2022-02-02T16:18:41Z,http://arxiv.org/abs/2202.01110v2,A Survey on Retrieval-Augmented Text Generation,"Recently, retrieval-augmented text generation attracted increasing attention
of the computational linguistics community. Compared with conventional
generation models, retrieval-augmented text generation has remarkable
advantages and particularly has achieved state-of-the-art performance in many
NLP tasks. This paper aims to conduct a survey about retrieval-augmented text
generation. It firstly highlights the generic paradigm of retrieval-augmented
generation, and then it reviews notable approaches according to different tasks
including dialogue response generation, machine translation, and other
generation tasks. Finally, it points out some important directions on top of
recent methods to facilitate future research.",Huayang Li
2021-06-22T03:17:59Z,http://arxiv.org/abs/2106.11517v1,"Fine-tune the Entire RAG Architecture (including DPR retriever) for
  Question-Answering","In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.",Shamane Siriwardhana
2023-11-17T08:21:56Z,http://arxiv.org/abs/2311.10384v2,Retrieval Augmented Generation of Symbolic Music with LLMs,"We explore the use of large language models (LLMs) for music generation using
a retrieval system to select relevant examples. We find promising initial
results for music generation in a dialogue with the user, especially
considering the ease with which such a system can be implemented. The code is
available online.",Nicolas Jonason
2024-03-18T02:01:58Z,http://arxiv.org/abs/2403.11413v1,"Dynamic Contexts for Generating Suggestion Questions in RAG Based
  Conversational Systems","When interacting with Retrieval-Augmented Generation (RAG)-based
conversational agents, the users must carefully craft their queries to be
understood correctly. Yet, understanding the system's capabilities can be
challenging for the users, leading to ambiguous questions that necessitate
further clarification. This work aims to bridge the gap by developing a
suggestion question generator. To generate suggestion questions, our approach
involves utilizing dynamic context, which includes both dynamic few-shot
examples and dynamically retrieved contexts. Through experiments, we show that
the dynamic contexts approach can generate better suggestion questions as
compared to other prompting approaches.",Anuja Tayal
2024-05-13T09:17:19Z,http://arxiv.org/abs/2405.13008v1,Control Token with Dense Passage Retrieval,"This study addresses the hallucination problem in large language models
(LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a
technique that involves embedding relevant information in the prompt to obtain
accurate answers. However, RAG also faced inherent issues in retrieving correct
information. To address this, we employed the Dense Passage Retrieval(DPR)
(Karpukhin et al., 2020) model for fetching domain-specific documents related
to user queries. Despite this, the DPR model still lacked accuracy in document
retrieval. We enhanced the DPR model by incorporating control tokens, achieving
significantly superior performance over the standard DPR model, with a 13%
improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",Juhwan Lee
2024-05-23T19:23:40Z,http://arxiv.org/abs/2405.15007v1,RE-Adapt: Reverse Engineered Adaptation of Large Language Models,"We introduce RE-Adapt, an approach to fine-tuning large language models on
new domains without degrading any pre-existing instruction-tuning. We reverse
engineer an adapter which isolates what an instruction-tuned model has learned
beyond its corresponding pretrained base model. Importantly, this requires no
additional data or training. We can then fine-tune the base model on a new
domain and readapt it to instruction following with the reverse engineered
adapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other
methods of fine-tuning, across multiple popular LLMs and datasets, even when
the models are used in conjunction with retrieval-augmented generation.",William Fleshman
2024-06-10T09:29:08Z,http://arxiv.org/abs/2406.06124v1,"Enhancing Long-Term Memory using Hierarchical Aggregate Tree for
  Retrieval Augmented Generation","Large language models have limited context capacity, hindering reasoning over
long conversations. We propose the Hierarchical Aggregate Tree memory structure
to recursively aggregate relevant dialogue context through conditional tree
traversals. HAT encapsulates information from children nodes, enabling broad
coverage with depth control. We formulate finding best context as optimal tree
traversal. Experiments show HAT improves dialog coherence and summary quality
over baseline contexts, demonstrating the techniques effectiveness for multi
turn reasoning without exponential parameter growth. This memory augmentation
enables more consistent, grounded longform conversations from LLMs",Aadharsh Aadhithya A
2024-07-17T05:50:32Z,http://arxiv.org/abs/2407.12325v1,Optimizing Query Generation for Enhanced Document Retrieval in RAG,"Large Language Models (LLMs) excel in various language tasks but they often
generate incorrect information, a phenomenon known as ""hallucinations"".
Retrieval-Augmented Generation (RAG) aims to mitigate this by using document
retrieval for accurate responses. However, RAG still faces hallucinations due
to vague queries. This study aims to improve RAG by optimizing query generation
with a query-document alignment score, refining queries using LLMs for better
precision and efficiency of document retrieval. Experiments have shown that our
approach improves document retrieval, resulting in an average accuracy gain of
1.6%.",Hamin Koo
2024-07-19T12:28:22Z,http://arxiv.org/abs/2407.14246v2,Unipa-GPT: Large Language Models for university-oriented QA in Italian,"This paper illustrates the architecture and training of Unipa-GPT, a chatbot
relying on a Large Language Model, developed for assisting students in choosing
a bachelor/master degree course at the University of Palermo. Unipa-GPT relies
on gpt-3.5-turbo, it was presented in the context of the European Researchers'
Night (SHARPER night). In our experiments we adopted both the Retrieval
Augmented Generation (RAG) approach and fine-tuning to develop the system. The
whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned
systems are compared, and a brief discussion on their performance is reported.
Further comparison with other Large Language Models and the experimental
results during the SHARPER night are illustrated.",Irene Siragusa
2024-07-23T15:17:11Z,http://arxiv.org/abs/2407.16565v1,"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases
  Generation with Small Language Models","Recent surge in the accessibility of large language models (LLMs) to the
general population can lead to untrackable use of such models for
medical-related recommendations. Language generation via LLMs models has two
key problems: firstly, they are prone to hallucination and therefore, for any
medical purpose they require scientific and factual grounding; secondly, LLMs
pose tremendous challenge to computational resources due to their gigantic
model size. In this work, we introduce pRAGe, a pipeline for Retrieval
Augmented Generation and evaluation of medical paraphrases generation using
Small Language Models (SLM). We study the effectiveness of SLMs and the impact
of external knowledge base for medical paraphrase generation in French.",Ioana Buhnila
2024-08-05T20:21:54Z,http://arxiv.org/abs/2408.02811v1,Development of REGAI: Rubric Enabled Generative Artificial Intelligence,"This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology.",Zach Johnson
2024-08-19T22:01:45Z,http://arxiv.org/abs/2408.10435v1,Enhanced document retrieval with topic embeddings,"Document retrieval systems have experienced a revitalized interest with the
advent of retrieval-augmented generation (RAG). RAG architecture offers a lower
hallucination rate than LLM-only applications. However, the accuracy of the
retrieval mechanism is known to be a bottleneck in the efficiency of these
applications. A particular case of subpar retrieval performance is observed in
situations where multiple documents from several different but related topics
are in the corpus. We have devised a new vectorization method that takes into
account the topic information of the document. The paper introduces this new
method for text vectorization and evaluates it in the context of RAG.
Furthermore, we discuss the challenge of evaluating RAG systems, which pertains
to the case at hand.",Kavsar Huseynova
2024-08-27T16:09:56Z,http://arxiv.org/abs/2408.15171v1,"Measuring text summarization factuality using atomic facts entailment
  metrics in the context of retrieval augmented generation","The use of large language models (LLMs) has significantly increased since the
introduction of ChatGPT in 2022, demonstrating their value across various
applications. However, a major challenge for enterprise and commercial adoption
of LLMs is their tendency to generate inaccurate information, a phenomenon
known as ""hallucination."" This project proposes a method for estimating the
factuality of a summary generated by LLMs when compared to a source text. Our
approach utilizes Naive Bayes classification to assess the accuracy of the
content produced.",N. E. Kriman
2024-10-03T19:05:47Z,http://arxiv.org/abs/2410.02914v1,Streamlining Conformal Information Retrieval via Score Refinement,"Information retrieval (IR) methods, like retrieval augmented generation, are
fundamental to modern applications but often lack statistical guarantees.
Conformal prediction addresses this by retrieving sets guaranteed to include
relevant information, yet existing approaches produce large-sized sets,
incurring high computational costs and slow response times. In this work, we
introduce a score refinement method that applies a simple monotone
transformation to retrieval scores, leading to significantly smaller conformal
sets while maintaining their statistical guarantees. Experiments on various
BEIR benchmarks validate the effectiveness of our approach in producing compact
sets containing relevant information.",Yotam Intrator
2024-10-08T14:09:12Z,http://arxiv.org/abs/2410.06062v3,"LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge Graphs","We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.",Vincent Emonet
2024-10-04T16:46:07Z,http://arxiv.org/abs/2410.09077v1,"A Large Language Model-based Framework for Semi-Structured Tender
  Document Retrieval-Augmented Generation","The drafting of documents in the procurement field has progressively become
more complex and diverse, driven by the need to meet legal requirements, adapt
to technological advancements, and address stakeholder demands. While large
language models (LLMs) show potential in document generation, most LLMs lack
specialized knowledge in procurement. To address this gap, we use
retrieval-augmented techniques to achieve professional document generation,
ensuring accuracy and relevance in procurement documentation.",Yilong Zhao
2024-10-15T14:42:18Z,http://arxiv.org/abs/2410.11655v1,Retrieval Augmented Spelling Correction for E-Commerce Applications,"The rapid introduction of new brand names into everyday language poses a
unique challenge for e-commerce spelling correction services, which must
distinguish genuine misspellings from novel brand names that use unconventional
spelling. We seek to address this challenge via Retrieval Augmented Generation
(RAG). On this approach, product names are retrieved from a catalog and
incorporated into the context used by a large language model (LLM) that has
been fine-tuned to do contextual spelling correction. Through quantitative
evaluation and qualitative error analyses, we find improvements in spelling
correction utilizing the RAG framework beyond a stand-alone LLM. We also
demonstrate the value of additional finetuning of the LLM to incorporate
retrieved context.",Xuan Guo
2024-10-16T21:53:48Z,http://arxiv.org/abs/2410.13070v1,Is Semantic Chunking Worth the Computational Cost?,"Recent advances in Retrieval-Augmented Generation (RAG) systems have
popularized semantic chunking, which aims to improve retrieval performance by
dividing documents into semantically coherent segments. Despite its growing
adoption, the actual benefits over simpler fixed-size chunking, where documents
are split into consecutive, fixed-size segments, remain unclear. This study
systematically evaluates the effectiveness of semantic chunking using three
common retrieval-related tasks: document retrieval, evidence retrieval, and
retrieval-based answer generation. The results show that the computational
costs associated with semantic chunking are not justified by consistent
performance gains. These findings challenge the previous assumptions about
semantic chunking and highlight the need for more efficient chunking strategies
in RAG systems.",Renyi Qu
2024-10-01T20:29:14Z,http://arxiv.org/abs/2410.13865v1,Classifying Peace in Global Media Using RAG and Intergroup Reciprocity,"This paper presents a novel approach to identifying insights of peace in
global media using a Retrieval Augmented Generation (RAG) model and concepts of
Positive and Negative Intergroup Reciprocity (PIR/NIR). By refining the
definitions of PIR and NIR, we offer a more accurate and meaningful analysis of
intergroup relations as represented in media articles. Our methodology provides
insights into the dynamics that contribute to or detract from peace at a
national level.",K. Lian
2024-10-27T00:49:52Z,http://arxiv.org/abs/2410.20301v1,WindTunnel -- A Framework for Community Aware Sampling of Large Corpora,"Conducting comprehensive information retrieval experiments, such as in search
or retrieval augmented generation, often comes with high computational costs.
This is because evaluating a retrieval algorithm requires indexing the entire
corpus, which is significantly larger than the set of (query, result) pairs
under evaluation. This issue is especially pronounced in big data and neural
retrieval, where indexing becomes increasingly time-consuming and complex. In
this paper, we present WindTunnel, a novel framework developed at Yext to
generate representative samples of large corpora, enabling efficient end-to-end
information retrieval experiments. By preserving the community structure of the
dataset, WindTunnel overcomes limitations in current sampling methods,
providing more accurate evaluations.",Michael Iannelli
2024-11-08T19:44:12Z,http://arxiv.org/abs/2411.05934v1,"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for
  Bengali Mathematical Olympiad Problem Solving","We present an innovative approach for solving mathematical problems in
Bengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our
method uses advanced deep learning models, notably the Qwen 2.5 series, with
improvements made through prompt engineering, model quantization, and Tool
Integrated Reasoning (TIR) to handle complex calculations. Initially, we
explored various model architectures, including fine-tuned Mistral and
quantized Qwen models, refining them with translation techniques,
Retrieval-Augmented Generation (RAG), and custom dataset curation. Manual
hyperparameter tuning optimized parameters like temperature and top-p to
enhance model adaptability and accuracy. Removal of RAG and parameter
adjustments further improved robustness. Our approach highlights the potential
of advanced NLP techniques in solving Bengali mathematical problems.",Saad Tahmid
2024-11-11T18:58:46Z,http://arxiv.org/abs/2411.07238v1,OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model,"OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.",Sumeth Yuenyong
2024-11-16T03:06:39Z,http://arxiv.org/abs/2411.12759v1,"A Novel Approach to Eliminating Hallucinations in Large Language
  Model-Assisted Causal Discovery","The increasing use of large language models (LLMs) in causal discovery as a
substitute for human domain experts highlights the need for optimal model
selection. This paper presents the first hallucination survey of popular LLMs
for causal discovery. We show that hallucinations exist when using LLMs in
causal discovery so the choice of LLM is important. We propose using Retrieval
Augmented Generation (RAG) to reduce hallucinations when quality data is
available. Additionally, we introduce a novel method employing multiple LLMs
with an arbiter in a debate to audit edges in causal graphs, achieving a
comparable reduction in hallucinations to RAG.",Grace Sng
2024-12-16T20:33:33Z,http://arxiv.org/abs/2412.15262v1,Advanced ingestion process powered by LLM parsing for RAG system,"Retrieval Augmented Generation (RAG) systems struggle with processing
multimodal documents of varying structural complexity. This paper introduces a
novel multi-strategy parsing approach using LLM-powered OCR to extract content
from diverse document types, including presentations and high text density
files both scanned or not. The methodology employs a node-based extraction
technique that creates relationships between different information types and
generates context-aware metadata. By implementing a Multimodal Assembler Agent
and a flexible embedding strategy, the system enhances document comprehension
and retrieval capabilities. Experimental evaluations across multiple knowledge
bases demonstrate the approach's effectiveness, showing improvements in answer
relevancy and information faithfulness.",Arnau Perez
2024-12-21T16:31:41Z,http://arxiv.org/abs/2412.16689v1,Formal Language Knowledge Corpus for Retrieval Augmented Generation,"The integration of retrieval-augmented techniques with LLMs has shown promise
in improving performance across various domains. However, their utility in
tasks requiring advanced reasoning, such as generating and evaluating
mathematical statements and proofs, remains underexplored. This study explores
the use of Lean, a programming language for writing mathematical proofs, to
populate the knowledge corpus used by RAG systems. We hope for this to lay the
foundation to exploring different methods of using RAGs to improve the
performance of LLMs in advanced logical reasoning tasks.",Majd Zayyad
2021-08-31T15:51:27Z,http://arxiv.org/abs/2108.13934v2,Robust Retrieval Augmented Generation for Zero-shot Slot Filling,"Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.",Michael Glass
2022-10-06T01:21:25Z,http://arxiv.org/abs/2210.02627v1,"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)
  Models for Open Domain Question Answering","Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain
Question Answering (ODQA). RAG has only been trained and explored with a
Wikipedia-based external knowledge base and is not optimized for use in other
specialized domains such as healthcare and news. In this paper, we evaluate the
impact of joint training of the retriever and generator components of RAG for
the task of domain adaptation in ODQA. We propose \textit{RAG-end2end}, an
extension to RAG, that can adapt to a domain-specific knowledge base by
updating all components of the external knowledge base during training. In
addition, we introduce an auxiliary training signal to inject more
domain-specific knowledge. This auxiliary signal forces \textit{RAG-end2end} to
reconstruct a given sentence by accessing the relevant information from the
external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does
joint training of the retriever and generator for the end QA task and domain
adaptation. We evaluate our approach with datasets from three domains:
COVID-19, News, and Conversations, and achieve significant performance
improvements compared to the original RAG model. Our work has been open-sourced
through the Huggingface Transformers library, attesting to our work's
credibility and technical consistency.",Shamane Siriwardhana
2023-07-13T17:25:28Z,http://arxiv.org/abs/2307.06985v10,Retrieval Augmented Generation using Engineering Design Knowledge,"Aiming to support Retrieval Augmented Generation (RAG) in the design process,
we present a method to identify explicit, engineering design facts - {head
entity :: relationship :: tail entity} from patented artefact descriptions.
Given a sentence with a pair of entities (based on noun phrases) marked in a
unique manner, our method extracts the relationship that is explicitly
communicated in the sentence. For this task, we create a dataset of 375,084
examples and fine-tune language models for relation identification (token
classification) and elicitation (sequence-to-sequence). The token
classification approach achieves up to 99.7 % accuracy. Upon applying the
method to a domain of 4,870 fan system patents, we populate a knowledge base of
over 2.93 million facts. Using this knowledge base, we demonstrate how Large
Language Models (LLMs) are guided by explicit facts to synthesise knowledge and
generate technical and cohesive responses when sought out for knowledge
retrieval tasks in the design process.",L. Siddharth
2023-10-04T22:09:28Z,http://arxiv.org/abs/2310.03184v2,"Retrieval-augmented Generation to Improve Math Question-Answering:
  Trade-offs Between Groundedness and Human Preference","For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.",Zachary Levonian
2023-10-17T18:18:32Z,http://arxiv.org/abs/2310.11511v1,"Self-RAG: Learning to Retrieve, Generate, and Critique through
  Self-Reflection","Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.",Akari Asai
2023-11-10T07:13:06Z,http://arxiv.org/abs/2311.05903v2,"Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-Prompting for Non-Specialist LLM Users","Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.",Jennifer Dodgson
2023-12-11T15:45:27Z,http://arxiv.org/abs/2312.06457v1,"Large Language Models with Retrieval-Augmented Generation for Zero-Shot
  Disease Phenotyping","Identifying disease phenotypes from electronic health records (EHRs) is
critical for numerous secondary uses. Manually encoding physician knowledge
into rules is particularly challenging for rare diseases due to inadequate EHR
coding, necessitating review of clinical notes. Large language models (LLMs)
offer promise in text understanding but may not efficiently handle real-world
clinical documentation. We propose a zero-shot LLM-based method enriched by
retrieval-augmented generation and MapReduce, which pre-identifies
disease-related text snippets to be used in parallel as queries for the LLM to
establish diagnosis. We show that this method as applied to pulmonary
hypertension (PH), a rare disease characterized by elevated arterial pressures
in the lungs, significantly outperforms physician logic rules ($F_1$ score of
0.62 vs. 0.75). This method has the potential to enhance rare disease cohort
identification, expanding the scope of robust clinical research and care gap
identification.",Will E. Thompson
2023-12-14T14:26:57Z,http://arxiv.org/abs/2312.08976v2,Dynamic Retrieval-Augmented Generation,"Current state-of-the-art large language models are effective in generating
high-quality text and encapsulating a broad spectrum of world knowledge. These
models, however, often hallucinate and lack locally relevant factual data.
Retrieval-augmented approaches were introduced to overcome these problems and
provide more accurate responses. Typically, the retrieved information is simply
appended to the main request, restricting the context window size of the model.
We propose a novel approach for the Dynamic Retrieval-Augmented Generation
(DRAG), based on the entity-augmented generation, which injects compressed
embeddings of the retrieved entities into the generative model. The proposed
pipeline was developed for code-generation tasks, yet can be transferred to
some domains of natural language processing. To train the model, we collect and
publish a new project-level code generation dataset. We use it for the
evaluation along with publicly available datasets. Our approach achieves
several targets: (1) lifting the length limitations of the context window,
saving on the prompt size; (2) allowing huge expansion of the number of
retrieval entities available for the context; (3) alleviating the problem of
misspelling or failing to find relevant entity names. This allows the model to
beat all baselines (except GPT-3.5) with a strong margin.",Anton Shapkin
2023-12-16T14:47:03Z,http://arxiv.org/abs/2312.10466v1,"RIGHT: Retrieval-augmented Generation for Mainstream Hashtag
  Recommendation","Automatic mainstream hashtag recommendation aims to accurately provide users
with concise and popular topical hashtags before publication. Generally,
mainstream hashtag recommendation faces challenges in the comprehensive
difficulty of newly posted tweets in response to new topics, and the accurate
identification of mainstream hashtags beyond semantic correctness. However,
previous retrieval-based methods based on a fixed predefined mainstream hashtag
list excel in producing mainstream hashtags, but fail to understand the
constant flow of up-to-date information. Conversely, generation-based methods
demonstrate a superior ability to comprehend newly posted tweets, but their
capacity is constrained to identifying mainstream hashtags without additional
features. Inspired by the recent success of the retrieval-augmented technique,
in this work, we attempt to adopt this framework to combine the advantages of
both approaches. Meantime, with the help of the generator component, we could
rethink how to further improve the quality of the retriever component at a low
cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag
Recommender (RIGHT), which consists of three components: 1) a retriever seeks
relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances
mainstream identification by introducing global signals; and 3) a generator
incorporates input tweets and selected hashtags to directly generate the
desired hashtags. The experimental results show that our method achieves
significant improvements over state-of-the-art baselines. Moreover, RIGHT can
be easily integrated into large language models, improving the performance of
ChatGPT by more than 10%.",Run-Ze Fan
2023-12-18T07:47:33Z,http://arxiv.org/abs/2312.10997v5,Retrieval-Augmented Generation for Large Language Models: A Survey,"Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.",Yunfan Gao
2023-12-18T17:18:04Z,http://arxiv.org/abs/2312.11361v3,"""Knowing When You Don't Know"": A Multilingual Relevance Assessment
  Dataset for Robust Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.",Nandan Thakur
2023-12-30T16:56:24Z,http://arxiv.org/abs/2401.00280v3,"Advancing TTP Analysis: Harnessing the Power of Large Language Models
  with Retrieval Augmented Generation","Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use
to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK
framework can be challenging for cybersecurity practitioners due to presumed
expertise and complex dependencies. Meanwhile, advancements with Large Language
Models (LLMs) have led to recent surge in studies exploring its uses in
cybersecurity operations. It is, however, unclear how LLMs can be used in an
efficient and proper way to provide accurate responses for critical domains
such as cybersecurity. This leads us to investigate how to better use two types
of LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only
(e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended
purposes (i.e., tactics) of a cyberattack procedure. This work studies and
compares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs.
Retrieval Augmented Generation (RAG) for decoder-only LLMs (without
fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with
relevant contexts for each cyberattack procedure. Our studies show decoder-only
LLMs with RAG achieves better performance than encoder-only models with SFT,
particularly when directly relevant context is extracted by RAG. The
decoder-only results could suffer low `Precision' while achieving high
`Recall'. Our findings further highlight a counter-intuitive observation that
more generic prompts tend to yield better predictions of cyberattack tactics
than those that are more specifically tailored.",Reza Fayyazi
2024-01-21T03:46:00Z,http://arxiv.org/abs/2401.11391v1,"Interactive AI with Retrieval-Augmented Generation for Next Generation
  Networking","With the advance of artificial intelligence (AI), the emergence of Google
Gemini and OpenAI Q* marks the direction towards artificial general
intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has
been introduced, which can interactively understand and respond not only to
human user input but also to dynamic system and network conditions. In this
article, we explore an integration and enhancement of IAI in networking. We
first comprehensively review recent developments and future perspectives of AI
and then introduce the technology and components of IAI. We then explore the
integration of IAI into the next-generation networks, focusing on how implicit
and explicit interactions can enhance network functionality, improve user
experience, and promote efficient network management. Subsequently, we propose
an IAI-enabled network management and optimization framework, which consists of
environment, perception, action, and brain units. We also design the pluggable
large language model (LLM) module and retrieval augmented generation (RAG)
module to build the knowledge base and contextual memory for decision-making in
the brain unit. We demonstrate the effectiveness of the framework through case
studies. Finally, we discuss potential research directions for IAI-based
networks.",Ruichen Zhang
2024-01-27T11:41:48Z,http://arxiv.org/abs/2401.15391v1,"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
  Queries","Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.",Yixuan Tang
2024-02-05T06:52:53Z,http://arxiv.org/abs/2402.02764v1,"List-aware Reranking-Truncation Joint Model for Search and
  Retrieval-augmented Generation","The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.",Shicheng Xu
2024-02-11T12:25:41Z,http://arxiv.org/abs/2402.07179v3,"Prompt Perturbation in Retrieval-Augmented Generation based Large
  Language Models","The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.",Zhibo Hu
2024-02-12T13:13:04Z,http://arxiv.org/abs/2402.07630v3,"G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering","Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}",Xiaoxin He
2024-02-04T20:42:30Z,http://arxiv.org/abs/2402.14594v1,"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.",Zifei FeiFei Han
2024-02-27T19:08:05Z,http://arxiv.org/abs/2402.17840v3,"Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems","Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. We also study multiple
effects of RAG setup on the extractability of data, indicating that following
unexpected instructions to regurgitate data can be an outcome of failure in
effectively utilizing contexts for modern LMs, and further show that such
vulnerability can be greatly mitigated by position bias elimination strategies.
Extending our study to production RAG models GPTs, we design an attack that can
cause datastore leakage with a 100% success rate on 25 randomly selected
customized GPTs with at most 2 queries, and we extract text data verbatim at a
rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words
by prompting the GPTs with only 100 queries generated by themselves.",Zhenting Qi
2024-02-28T08:24:38Z,http://arxiv.org/abs/2402.18150v2,"Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.",Shicheng Xu
2024-03-03T08:07:55Z,http://arxiv.org/abs/2403.01432v5,"Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge","Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting
strong performance across diverse tasks and domains. However, it has been
observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LMs on
low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning
(FT) over synthetic data. This paper explores and evaluates the impact of RAG
and FT on customizing LMs in handling low-frequency entities on question
answering tasks. We conduct extensive experiments on twelve LMs of varying size
and type and different fine tuning, data augmentation, and retrieval models.
Our findings indicate that while FT boosts the performance across entities of
varying popularity, RAG surpasses FT by a large margin particularly for least
popular factual knowledge. Additionally, the success of both RAG and FT
approaches is amplified by improving retrieval and data augmentation
techniques. Fine tuning, while beneficial for small LMs, requires extensive
resources. To address this issue, we propose the new Stimulus RAG approach that
surpasses the effectiveness of fine tuning based approaches, thereby
eliminating the need for the costly data augmentation and fine tuning step for
enriching LMs with less popular factual knowledge. The code is available at
\url{https://github.com/informagi/RAGvsFT}.",Heydar Soudani
2024-03-12T21:06:31Z,http://arxiv.org/abs/2403.09727v1,"Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems","The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.",Robert Lakatos
2024-03-27T18:09:55Z,http://arxiv.org/abs/2403.18920v1,CPR: Retrieval Augmented Generation for Copyright Protection,"Retrieval Augmented Generation (RAG) is emerging as a flexible and robust
technique to adapt models to private users data without training, to handle
credit attribution, and to allow efficient machine unlearning at scale.
However, RAG techniques for image generation may lead to parts of the retrieved
samples being copied in the model's output. To reduce risks of leaking private
information contained in the retrieved set, we introduce Copy-Protected
generation with Retrieval (CPR), a new method for RAG with strong copyright
protection guarantees in a mixed-private setting for diffusion models.CPR
allows to condition the output of diffusion models on a set of retrieved
images, while also guaranteeing that unique identifiable information about
those example is not exposed in the generated outputs. In particular, it does
so by sampling from a mixture of public (safe) distribution and private (user)
distribution by merging their diffusion scores at inference. We prove that CPR
satisfies Near Access Freeness (NAF) which bounds the amount of information an
attacker may be able to extract from the generated images. We provide two
algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously
proposed rejection-sampling-based NAF methods, our methods enable efficient
copyright-protected sampling with a single run of backward diffusion. We show
that our method can be applied to any pre-trained conditional diffusion model,
such as Stable Diffusion or unCLIP. In particular, we empirically show that
applying CPR on top of unCLIP improves quality and text-to-image alignment of
the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit
attribution, copy-right protection, and deterministic, constant time,
unlearning.",Aditya Golatkar
2024-04-18T18:32:30Z,http://arxiv.org/abs/2404.12457v2,RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has shown significant improvements in
various natural language processing tasks by integrating the strengths of large
language models (LLMs) and external knowledge databases. However, RAG
introduces long sequence generation and leads to high computation and memory
costs. We propose RAGCache, a novel multilevel dynamic caching system tailored
for RAG. Our analysis benchmarks current RAG systems, pinpointing the
performance bottleneck (i.e., long sequence due to knowledge injection) and
optimization opportunities (i.e., caching knowledge's intermediate states).
Based on these insights, we design RAGCache, which organizes the intermediate
states of retrieved knowledge in a knowledge tree and caches them in the GPU
and host memory hierarchy. RAGCache proposes a replacement policy that is aware
of LLM inference characteristics and RAG retrieval patterns. It also
dynamically overlaps the retrieval and inference steps to minimize the
end-to-end latency. We implement RAGCache and evaluate it on vLLM, a
state-of-the-art LLM inference system and Faiss, a state-of-the-art vector
database. The experimental results show that RAGCache reduces the time to first
token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to
vLLM integrated with Faiss.",Chao Jin
2024-04-20T14:42:43Z,http://arxiv.org/abs/2404.13397v1,Retrieval-Augmented Generation-based Relation Extraction,"Information Extraction (IE) is a transformative process that converts
unstructured text data into a structured format by employing entity and
relation extraction (RE) methodologies. The identification of the relation
between a pair of entities plays a crucial role within this framework. Despite
the existence of various techniques for relation extraction, their efficacy
heavily relies on access to labeled data and substantial computational
resources. In addressing these challenges, Large Language Models (LLMs) emerge
as promising solutions; however, they might return hallucinating responses due
to their own training data. To overcome these limitations, Retrieved-Augmented
Generation-based Relation Extraction (RAG4RE) in this work is proposed,
offering a pathway to enhance the performance of relation extraction tasks.
  This work evaluated the effectiveness of our RAG4RE approach utilizing
different LLMs. Through the utilization of established benchmarks, such as
TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to
comprehensively evaluate the efficacy of our RAG4RE approach. In particularly,
we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our
investigation. The results of our study demonstrate that our RAG4RE approach
surpasses performance of traditional RE approaches based solely on LLMs,
particularly evident in the TACRED dataset and its variations. Furthermore, our
approach exhibits remarkable performance compared to previous RE methodologies
across both TACRED and TACREV datasets, underscoring its efficacy and potential
for advancing RE tasks in natural language processing.",Sefika Efeoglu
2024-04-22T09:56:59Z,http://arxiv.org/abs/2404.14043v1,"LLMs Know What They Need: Leveraging a Missing Information Guided
  Framework to Empower Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating
outdated knowledge or hallucination by supplying LLMs with updated and relevant
knowledge. However, there are still several difficulties for RAG in
understanding complex multi-hop query and retrieving relevant documents, which
require LLMs to perform reasoning and retrieve step by step. Inspired by
human's reasoning process in which they gradually search for the required
information, it is natural to ask whether the LLMs could notice the missing
information in each reasoning step. In this work, we first experimentally
verified the ability of LLMs to extract information as well as to know the
missing. Based on the above discovery, we propose a Missing Information Guided
Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the
identification of missing information to generate a targeted query that steers
the subsequent knowledge retrieval. Besides, we design a sentence-level
re-ranking filtering approach to filter the irrelevant content out from
document, along with the information extraction capability of LLMs to extract
useful information from cleaned-up documents, which in turn to bolster the
overall efficacy of RAG. Extensive experiments conducted on multiple public
datasets reveal the superiority of the proposed MIGRES method, and analytical
experiments demonstrate the effectiveness of our proposed modules.",Keheng Wang
2024-04-26T23:05:20Z,http://arxiv.org/abs/2404.17723v2,"Retrieval-Augmented Generation with Knowledge Graphs for Customer
  Service Question Answering","In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.",Zhentao Xu
2024-05-07T04:04:53Z,http://arxiv.org/abs/2405.03989v2,"A Method for Parsing and Vectorization of Semi-structured Data used in
  Retrieval Augmented Generation","This paper presents a novel method for parsing and vectorizing
semi-structured data to enhance the functionality of Retrieval-Augmented
Generation (RAG) within Large Language Models (LLMs). We developed a
comprehensive pipeline for converting various data formats into .docx, enabling
efficient parsing and structured data extraction. The core of our methodology
involves the construction of a vector database using Pinecone, which integrates
seamlessly with LLMs to provide accurate, context-specific responses,
particularly in environmental management and wastewater treatment operations.
Through rigorous testing with both English and Chinese texts in diverse
document formats, our results demonstrate a marked improvement in the precision
and reliability of LLMs outputs. The RAG-enhanced models displayed enhanced
ability to generate contextually rich and technically accurate responses,
underscoring the potential of vector knowledge bases in significantly boosting
the performance of LLMs in specialized domains. This research not only
illustrates the effectiveness of our method but also highlights its potential
to revolutionize data processing and analysis in environmental sciences,
setting a precedent for future advancements in AI-driven applications. Our code
is available at https://github.com/linancn/TianGong-AI-Unstructure.git.",Hang Yang
2024-05-07T22:31:50Z,http://arxiv.org/abs/2405.04700v1,"Robust Implementation of Retrieval-Augmented Generation on Edge-based
  Computing-in-Memory Architectures","Large Language Models (LLMs) deployed on edge devices learn through
fine-tuning and updating a certain portion of their parameters. Although such
learning methods can be optimized to reduce resource utilization, the overall
required resources remain a heavy burden on edge devices. Instead,
Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method,
can improve the quality of the LLM-generated content without updating model
parameters. However, the RAG-based LLM may involve repetitive searches on the
profile data in every user-LLM interaction. This search can lead to significant
latency along with the accumulation of user data. Conventional efforts to
decrease latency result in restricting the size of saved user data, thus
reducing the scalability of RAG as user data continuously grows. It remains an
open question: how to free RAG from the constraints of latency and scalability
on edge devices? In this paper, we propose a novel framework to accelerate RAG
via Computing-in-Memory (CiM) architectures. It accelerates matrix
multiplications by performing in-situ computation inside the memory while
avoiding the expensive data transfer between the computing unit and memory. Our
framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive
learning-based training method and noise-aware training, can enable RAG to
efficiently search profile data with CiM. To the best of our knowledge, this is
the first work utilizing CiM to accelerate RAG.",Ruiyang Qin
2024-05-05T18:32:06Z,http://arxiv.org/abs/2405.06681v1,"Leveraging Lecture Content for Improved Feedback: Explorations with
  GPT-4 and Retrieval Augmented Generation","This paper presents the use of Retrieval Augmented Generation (RAG) to
improve the feedback generated by Large Language Models for programming tasks.
For this purpose, corresponding lecture recordings were transcribed and made
available to the Large Language Model GPT-4 as external knowledge source
together with timestamps as metainformation by using RAG. The purpose of this
is to prevent hallucinations and to enforce the use of the technical terms and
phrases from the lecture. In an exercise platform developed to solve
programming problems for an introductory programming lecture, students can
request feedback on their solutions generated by GPT-4. For this task GPT-4
receives the students' code solution, the compiler output, the result of unit
tests and the relevant passages from the lecture notes available through the
use of RAG as additional context. The feedback generated by GPT-4 should guide
students to solve problems independently and link to the lecture content, using
the time stamps of the transcript as meta-information. In this way, the
corresponding lecture videos can be viewed immediately at the corresponding
positions. For the evaluation, students worked with the tool in a workshop and
decided for each feedback whether it should be extended by RAG or not. First
results based on a questionnaire and the collected usage data show that the use
of RAG can improve feedback generation and is preferred by students in some
situations. Due to the slower speed of feedback generation, the benefits are
situation dependent.",Sven Jacobs
2024-05-13T07:56:15Z,http://arxiv.org/abs/2405.07530v1,Prompt-based Code Completion via Multi-Retrieval Augmented Generation,"Automated code completion, aiming at generating subsequent tokens from
unfinished code, has been significantly benefited from recent progress in
pre-trained Large Language Models (LLMs). However, these models often suffer
from coherence issues and hallucinations when dealing with complex code logic
or extrapolating beyond their training data. Existing Retrieval Augmented
Generation (RAG) techniques partially address these issues by retrieving
relevant code with a separate encoding model where the retrieved snippet serves
as contextual reference for code completion. However, their retrieval scope is
subject to a singular perspective defined by the encoding model, which largely
overlooks the complexity and diversity inherent in code semantics. To address
this limitation, we propose ProCC, a code completion framework leveraging
prompt engineering and the contextual multi-armed bandits algorithm to flexibly
incorporate and adapt to multiple perspectives of code. ProCC first employs a
prompt-based multi-retriever system which crafts prompt templates to elicit LLM
knowledge to understand code semantics with multiple retrieval perspectives.
Then, it adopts the adaptive retrieval selection algorithm to incorporate code
similarity into the decision-making process to determine the most suitable
retrieval perspective for the LLM to complete the code. Experimental results
demonstrate that ProCC outperforms state-of-the-art code completion technique
by 8.6% on our collected open-source benchmark suite and 10.1% on the
private-domain benchmark suite collected from a billion-user e-commerce company
in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in
a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned
model.",Hanzhuo Tan
2024-05-15T12:41:20Z,http://arxiv.org/abs/2405.13021v1,"IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning
  Inner Monologues","Although the Retrieval-Augmented Generation (RAG) paradigms can use external
knowledge to enhance and ground the outputs of Large Language Models (LLMs) to
mitigate generative hallucinations and static knowledge base problems, they
still suffer from limited flexibility in adopting Information Retrieval (IR)
systems with varying capabilities, constrained interpretability during the
multi-round retrieval process, and a lack of end-to-end optimization. To
address these challenges, we propose a novel LLM-centric approach, IM-RAG, that
integrates IR systems with LLMs to support multi-round RAG through learning
Inner Monologues (IM, i.e., the human inner voice that narrates one's
thoughts). During the IM process, the LLM serves as the core reasoning model
(i.e., Reasoner) to either propose queries to collect more information via the
Retriever or to provide a final answer based on the conversational context. We
also introduce a Refiner that improves the outputs from the Retriever,
effectively bridging the gap between the Reasoner and IR modules with varying
capabilities and fostering multi-round communications. The entire IM process is
optimized via Reinforcement Learning (RL) where a Progress Tracker is
incorporated to provide mid-step rewards, and the answer prediction is further
separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive
experiments with the HotPotQA dataset, a popular benchmark for retrieval-based,
multi-step question-answering. The results show that our approach achieves
state-of-the-art (SOTA) performance while providing high flexibility in
integrating IR modules as well as strong interpretability exhibited in the
learned inner monologues.",Diji Yang
2024-05-22T07:21:32Z,http://arxiv.org/abs/2405.13401v4,"TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in
  Large Language Models","Large language models (LLMs) have raised concerns about potential security
threats despite performing significantly in Natural Language Processing (NLP).
Backdoor attacks initially verified that LLM is doing substantial harm at all
stages, but the cost and robustness have been criticized. Attacking LLMs is
inherently risky in security review, while prohibitively expensive. Besides,
the continuous iteration of LLMs will degrade the robustness of backdoors. In
this paper, we propose TrojanRAG, which employs a joint backdoor attack in the
Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack
scenarios. Specifically, the adversary constructs elaborate target contexts and
trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized
by contrastive learning, thus constraining the triggering conditions to a
parameter subspace to improve the matching. To improve the recall of the RAG
for the target contexts, we introduce a knowledge graph to construct structured
data to achieve hard matching at a fine-grained level. Moreover, we normalize
the backdoor scenarios in LLMs to analyze the real harm caused by backdoors
from both attackers' and users' perspectives and further verify whether the
context is a favorable tool for jailbreaking models. Extensive experimental
results on truthfulness, language understanding, and harmfulness show that
TrojanRAG exhibits versatility threats while maintaining retrieval capabilities
on normal queries.",Pengzhou Cheng
2024-05-27T08:26:45Z,http://arxiv.org/abs/2405.16933v1,"Empowering Large Language Models to Set up a Knowledge Retrieval Indexer
  via Self-Learning","Retrieval-Augmented Generation (RAG) offers a cost-effective approach to
injecting real-time knowledge into large language models (LLMs). Nevertheless,
constructing and validating high-quality knowledge repositories require
considerable effort. We propose a pre-retrieval framework named Pseudo-Graph
Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students
by providing them with abundant raw reading materials and encouraging them to
engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through
common topics or complementary facts to form a pseudo-graph database. During
the retrieval phase, PG-RAG mimics the human behavior in flipping through
notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates
highly corroborated fact paths to provide a structured and refined sub-graph
assisting LLMs. We validated PG-RAG on three specialized question-answering
datasets. In single-document tasks, PG-RAG significantly outperformed the
current best baseline, KGP-LLaMA, across all key evaluation metrics, with an
average overall performance improvement of 11.6%. Specifically, its BLEU score
increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In
multi-document scenarios, the average metrics of PG-RAG were at least 2.35%
higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed
stable improvements of around 7.55% and 12.75%, respectively. Our code:
https://github.com/IAAR-Shanghai/PGRAG.",Xun Liang
2024-05-27T19:02:18Z,http://arxiv.org/abs/2405.17602v1,Augmenting Textual Generation via Topology Aware Retrieval,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.",Yu Wang
2024-05-29T03:17:16Z,http://arxiv.org/abs/2405.18727v2,CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control,"Retrieval-augmented generation (RAG) has emerged as a promising solution for
mitigating hallucinations of large language models (LLMs) with retrieved
external knowledge. Adaptive RAG enhances this approach by enabling dynamic
retrieval during generation, activating retrieval only when the query exceeds
LLM's internal knowledge. Existing methods primarily focus on detecting LLM's
confidence via statistical uncertainty. Instead, we present the first attempts
to solve adaptive RAG from a representation perspective and develop an inherent
control-based framework, termed \name. Specifically, we extract the features
that represent the honesty and confidence directions of LLM and adopt them to
control LLM behavior and guide retrieval timing decisions. We also design a
simple yet effective query formulation strategy to support adaptive retrieval.
Experiments show that \name is superior to existing adaptive RAG methods on a
diverse set of tasks, the honesty steering can effectively make LLMs more
honest and confidence monitoring is a promising indicator of retrieval
trigger.Our code is available at \url{https://github.com/HSLiu-Initial/CtrlA}.",Huanshuo Liu
2024-05-30T19:46:36Z,http://arxiv.org/abs/2405.20446v2,"Is My Data in Your Retrieval Database? Membership Inference Attacks
  Against Retrieval Augmented Generation","Retrieval Augmented Generation (RAG) systems have shown great promise in
natural language processing. However, their reliance on data stored in a
retrieval database, which may contain proprietary or sensitive information,
introduces new privacy concerns. Specifically, an attacker may be able to infer
whether a certain text passage appears in the retrieval database by observing
the outputs of the RAG system, an attack known as a Membership Inference Attack
(MIA). Despite the significance of this threat, MIAs against RAG systems have
yet remained under-explored. This study addresses this gap by introducing an
efficient and easy-to-use method for conducting MIA against RAG systems. We
demonstrate the effectiveness of our attack using two benchmark datasets and
multiple generative models, showing that the membership of a document in the
retrieval database can be efficiently determined through the creation of an
appropriate prompt in both black-box and gray-box settings. Moreover, we
introduce an initial defense strategy based on adding instructions to the RAG
template, which shows high effectiveness for some datasets and models. Our
findings highlight the importance of implementing security countermeasures in
deployed RAG systems and developing more advanced defenses to protect the
privacy and security of retrieval databases.",Maya Anderson
2024-05-24T16:36:47Z,http://arxiv.org/abs/2406.00029v1,Clustered Retrieved Augmented Generation (CRAG),"Providing external knowledge to Large Language Models (LLMs) is a key point
for using these models in real-world applications for several reasons, such as
incorporating up-to-date content in a real-time manner, providing access to
domain-specific knowledge, and contributing to hallucination prevention. The
vector database-based Retrieval Augmented Generation (RAG) approach has been
widely adopted to this end. Thus, any part of external knowledge can be
retrieved and provided to some LLM as the input context. Despite RAG approach's
success, it still might be unfeasible for some applications, because the
context retrieved can demand a longer context window than the size supported by
LLM. Even when the context retrieved fits into the context window size, the
number of tokens might be expressive and, consequently, impact costs and
processing time, becoming impractical for most applications. To address these,
we propose CRAG, a novel approach able to effectively reduce the number of
prompting tokens without degrading the quality of the response generated
compared to a solution using RAG. Through our experiments, we show that CRAG
can reduce the number of tokens by at least 46\%, achieving more than 90\% in
some cases, compared to RAG. Moreover, the number of tokens with CRAG does not
increase considerably when the number of reviews analyzed is higher, unlike
RAG, where the number of tokens is almost 9x higher when there are 75 reviews
compared to 4 reviews.",Simon Akesson
2024-06-03T02:56:14Z,http://arxiv.org/abs/2406.00944v2,A Theory for Token-Level Harmonization in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.",Shicheng Xu
2024-06-09T05:33:51Z,http://arxiv.org/abs/2406.05654v2,"DomainRAG: A Chinese Benchmark for Evaluating Domain-specific
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) offers a promising solution to address
various limitations of Large Language Models (LLMs), such as hallucination and
difficulties in keeping up with real-time updates. This approach is
particularly critical in expert and domain-specific applications where LLMs
struggle to cover expert knowledge. Therefore, evaluating RAG models in such
scenarios is crucial, yet current studies often rely on general knowledge
sources like Wikipedia to assess the models' abilities in solving common-sense
problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific
context, college enrollment. We identified six required abilities for RAG
models, including the ability in conversational RAG, analyzing structural
information, faithfulness to external knowledge, denoising, solving
time-sensitive problems, and understanding multi-document interactions. Each
ability has an associated dataset with shared corpora to evaluate the RAG
models' performance. We evaluated popular LLMs such as Llama, Baichuan,
ChatGLM, and GPT models. Experimental results indicate that existing
closed-book LLMs struggle with domain-specific questions, highlighting the need
for RAG models to solve expert problems. Moreover, there is room for RAG models
to improve their abilities in comprehending conversational history, analyzing
structural information, denoising, processing multi-document interactions, and
faithfulness in expert knowledge. We expect future studies could solve these
problems better.",Shuting Wang
2024-06-17T06:48:31Z,http://arxiv.org/abs/2406.11258v2,"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented
  Generation","Large Language Models (LLMs) have shown great potential in the biomedical
domain with the advancement of retrieval-augmented generation (RAG). However,
existing retrieval-augmented approaches face challenges in addressing diverse
queries and documents, particularly for medical knowledge queries, resulting in
sub-optimal performance. To address these limitations, we propose a novel
plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search
(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree
search, SeRTS boosts the zero-shot performance of retrieving high-quality and
informative results for RAG. We further enhance retrieval performance by
fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the
trajectories collected by SeRTS as feedback. Controlled experiments using the
BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method
significantly improves the performance of the BM25 retriever and surpasses the
strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than
self-reflection. Our proposed method effectively adapts LLMs to document
retrieval tasks, enhancing their ability to retrieve highly relevant documents
for RAG in the context of medical knowledge queries. This work presents a
significant step forward in leveraging LLMs for accurate and comprehensive
biomedical question answering.",Minda Hu
2024-06-18T12:09:02Z,http://arxiv.org/abs/2406.12534v4,Unified Active Retrieval for Retrieval Augmented Generation,"In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and
applying it to every instruction is sub-optimal. Therefore, determining whether
to retrieve is crucial for RAG, which is usually referred to as Active
Retrieval. However, existing active retrieval methods face two challenges: 1.
They usually rely on a single criterion, which struggles with handling various
types of instructions. 2. They depend on specialized and highly differentiated
procedures, and thus combining them makes the RAG system more complicated and
leads to higher response latency. To address these challenges, we propose
Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts
them into plug-and-play classification tasks, which achieves multifaceted
retrieval timing judgements with negligible extra inference cost. We further
introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to
process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR
significantly outperforms existing work on the retrieval timing judgement and
the performance of downstream tasks, which shows the effectiveness of UAR and
its helpfulness to downstream tasks.",Qinyuan Cheng
2024-06-18T12:52:51Z,http://arxiv.org/abs/2406.12566v3,"RichRAG: Crafting Rich Responses for Multi-faceted Queries in
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) effectively addresses issues of static
knowledge and hallucination in large language models. Existing studies mostly
focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with
diverse sub-intents, for which they desire rich and long-form answers covering
multiple relevant aspects. To tackle this important yet underexplored problem,
we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect
explorer to identify potential sub-aspects of input questions, a multi-faceted
retriever to build a candidate pool of diverse external documents related to
these sub-aspects, and a generative list-wise ranker, which is a key module to
provide the top-k most valuable documents for the final generator. These ranked
documents sufficiently cover various query aspects and are aware of the
generator's preferences, hence incentivizing it to produce rich and
comprehensive responses for users. The training of our ranker involves a
supervised fine-tuning stage to ensure the basic coverage of documents, and a
reinforcement learning stage to align downstream LLM's preferences to the
ranking of documents. Experimental results on two publicly available datasets
prove that our framework effectively and efficiently provides comprehensive and
satisfying responses to users.",Shuting Wang
2024-06-19T09:14:41Z,http://arxiv.org/abs/2406.13372v2,"Thread: A Logic-Based Data Organization Paradigm for How-To Question
  Answering with Retrieval Augmented Generation","Recent advances in retrieval-augmented generation have significantly improved
the performance of question-answering systems, particularly on factoid '5Ws'
questions. However, these systems still face substantial challenges when
addressing '1H' questions, specifically how-to questions, which are integral to
decision-making processes and require dynamic, step-by-step answers. The key
limitation lies in the prevalent data organization paradigm, chunk, which
divides documents into fixed-size segments, and disrupts the logical coherence
and connections within the context. To overcome this, in this paper, we propose
Thread, a novel data organization paradigm aimed at enabling current systems to
handle how-to questions more effectively. Specifically, we introduce a new
knowledge granularity, termed 'logic unit', where documents are transformed
into more structured and loosely interconnected logic units with large language
models. Extensive experiments conducted across both open-domain and industrial
settings demonstrate that Thread outperforms existing paradigms significantly,
improving the success rate of handling how-to questions by 21% to 33%.
Moreover, Thread exhibits high adaptability in processing various document
formats, drastically reducing the candidate quantity in the knowledge base and
minimizing the required information to one-fourth compared with chunk,
optimizing both efficiency and effectiveness.",Kaikai An
2024-06-19T15:25:29Z,http://arxiv.org/abs/2406.13629v2,"InstructRAG: Instructing Retrieval-Augmented Generation via
  Self-Synthesized Rationales","Retrieval-augmented generation (RAG) has shown promising potential to enhance
the accuracy and factuality of language models (LMs). However, imperfect
retrievers or noisy corpora can introduce misleading or even erroneous
information to the retrieved contents, posing a significant challenge to the
generation quality. Existing RAG methods typically address this challenge by
directly predicting final answers despite potentially noisy inputs, resulting
in an implicit denoising process that is difficult to interpret and verify. On
the other hand, the acquisition of explicit denoising supervision is often
costly, involving significant human efforts. In this work, we propose
InstructRAG, where LMs explicitly learn the denoising process through
self-synthesized rationales -- First, we instruct the LM to explain how the
ground-truth answer is derived from retrieved documents. Then, these rationales
can be used either as demonstrations for in-context learning of explicit
denoising or as supervised fine-tuning data to train the model. Compared to
standard RAG approaches, InstructRAG requires no additional supervision, allows
for easier verification of the predicted answers, and effectively improves
generation accuracy. Experiments show InstructRAG consistently outperforms
existing RAG methods in both training-free and trainable scenarios, achieving a
relative improvement of 8.3% over the best baseline method on average across
five knowledge-intensive benchmarks. Extensive analysis indicates that
InstructRAG scales well with increased numbers of retrieved documents and
consistently exhibits robust denoising ability even in out-of-domain datasets,
demonstrating strong generalizability.",Zhepei Wei
2024-06-19T16:10:26Z,http://arxiv.org/abs/2406.13663v4,"Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation","Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.",Jirui Qi
2024-06-19T19:06:36Z,http://arxiv.org/abs/2406.13779v1,"FoRAG: Factuality-optimized Retrieval Augmented Generation for
  Web-enhanced Long-form Question Answering","Retrieval Augmented Generation (RAG) has become prevalent in
question-answering (QA) tasks due to its ability of utilizing search engine to
enhance the quality of long-form question-answering (LFQA). Despite the
emergence of various open source methods and web-enhanced commercial systems
such as Bing Chat, two critical problems remain unsolved, i.e., the lack of
factuality and clear logic in the generated long-form answers. In this paper,
we remedy these issues via a systematic study on answer generation in
web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced
generator to achieve clear logic in the generation of multifaceted answers and
construct two datasets accordingly. Then we propose a factuality optimization
method based on a carefully designed doubly fine-grained RLHF framework, which
contains automatic evaluation and reward modeling in different levels of
granularity. Our generic framework comprises conventional fine-grained RLHF
methods as special cases. Extensive experiments verify the superiority of our
proposed \textit{Factuality-optimized RAG (FoRAG)} method on both English and
Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat,
the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly
used metrics (i.e., coherence, helpfulness, and factuality), while the number
of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets
and models are made publicly available for better reproducibility:
https://huggingface.co/forag.",Tianchi Cai
2024-06-20T10:04:09Z,http://arxiv.org/abs/2406.14162v3,"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation","Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.",Jingwei Ni
2024-06-20T23:20:34Z,http://arxiv.org/abs/2406.14783v2,Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework,"Challenges in the automated evaluation of Retrieval-Augmented Generation
(RAG) Question-Answering (QA) systems include hallucination problems in
domain-specific knowledge and the lack of gold standard benchmarks for company
internal tasks. This results in difficulties in evaluating RAG variations, like
RAG-Fusion (RAGF), in the context of a product QA task at Infineon
Technologies. To solve these problems, we propose a comprehensive evaluation
framework, which leverages Large Language Models (LLMs) to generate large
datasets of synthetic queries based on real user queries and in-domain
documents, uses LLM-as-a-judge to rate retrieved documents and answers,
evaluates the quality of answers, and ranks different variants of
Retrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based
competition. LLM-as-a-judge rating of a random sample of synthetic queries
shows a moderate, positive correlation with domain expert scoring in relevance,
accuracy, completeness, and precision. While RAGF outperformed RAG in Elo
score, a significance analysis against expert annotations also shows that RAGF
significantly outperforms RAG in completeness, but underperforms in precision.
In addition, Infineon's RAGF assistant demonstrated slightly higher performance
in document relevance based on MRR@5 scores. We find that RAGElo positively
aligns with the preferences of human annotators, though due caution is still
required. Finally, RAGF's approach leads to more complete answers based on
expert annotations and better answers overall based on RAGElo's evaluation
criteria.",Zackary Rackauckas
2024-06-24T17:37:52Z,http://arxiv.org/abs/2406.16828v1,"Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024
  Retrieval-Augmented Generation Track","Did you try out the new Bing Search? Or maybe you fiddled around with Google
AI~Overviews? These might sound familiar because the modern-day search stack
has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language
models (LLMs) to provide a well-informed, attributed, concise summary in
contrast to the traditional search paradigm that relies on displaying a ranked
list of documents. Therefore, given these recent advancements, it is crucial to
have an arena to build, test, visualize, and systematically evaluate RAG-based
search systems. With this in mind, we propose the TREC 2024 RAG Track to foster
innovation in evaluating RAG systems. In our work, we lay out the steps we've
made towards making this track a reality -- we describe the details of our
reusable framework, Ragnar\""ok, explain the curation of the new MS MARCO V2.1
collection choice, release the development topics for the track, and
standardize the I/O definitions which assist the end user. Next, using
Ragnar\""ok, we identify and provide key industrial baselines such as OpenAI's
GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface
for an interactive arena allowing benchmarking pairwise RAG systems by
crowdsourcing. We open-source our Ragnar\""ok framework and baselines to achieve
a unified standard for future RAG systems.",Ronak Pradeep
2024-06-26T07:21:02Z,http://arxiv.org/abs/2406.18122v1,Poisoned LangChain: Jailbreak LLMs by LangChain,"With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.",Ziqiu Wang
2024-06-26T18:26:53Z,http://arxiv.org/abs/2406.18676v2,"Understand What LLM Needs: Dual Preference Alignment for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) has demonstrated effectiveness in
mitigating the hallucination problem of large language models (LLMs). However,
the difficulty of aligning the retriever with the diverse LLMs' knowledge
preferences inevitably poses an inevitable challenge in developing a reliable
RAG system. To address this issue, we propose DPA-RAG, a universal framework
designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction
pipline and incorporate five novel query augmentation strategies to alleviate
preference data scarcity. Based on preference data, DPA-RAG accomplishes both
external and internal preference alignment: 1) It jointly integrate pair-wise,
point-wise, and contrastive preference alignment abilities into the reranker,
achieving external preference alignment among RAG components. 2) It further
introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),
enabling LLMs to implicitly capture knowledge aligned with their reasoning
preferences, achieving LLMs' internal alignment. Experimental results across
four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all
baselines and seamlessly integrates both black-box and open-sourced LLM
readers. Further qualitative analysis and discussions also provide empirical
guidance for achieving reliable RAG systems. Our code is publicly available at
https://github.com/dongguanting/DPA-RAG.",Guanting Dong
2024-06-27T14:58:38Z,http://arxiv.org/abs/2406.19234v2,"Generating Is Believing: Membership Inference Attacks against
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that
mitigates issues such as hallucinations and knowledge staleness in Large
Language Models (LLMs) by retrieving relevant knowledge from an external
database to assist in content generation. Existing research has demonstrated
potential privacy risks associated with the LLMs of RAG. However, the privacy
risks posed by the integration of an external database, which often contains
sensitive data such as medical records or personal identities, have remained
largely unexplored. In this paper, we aim to bridge this gap by focusing on
membership privacy of RAG's external database, with the aim of determining
whether a given sample is part of the RAG's database. Our basic idea is that if
a sample is in the external database, it will exhibit a high degree of semantic
similarity to the text generated by the RAG system. We present S$^2$MIA, a
\underline{M}embership \underline{I}nference \underline{A}ttack that utilizes
the \underline{S}emantic \underline{S}imilarity between a given sample and the
content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate
the potential to breach the membership privacy of the RAG database. Extensive
experiment results demonstrate that S$^2$MIA can achieve a strong inference
performance compared with five existing MIAs, and is able to escape from the
protection of three representative defenses.",Yuying Li
2024-07-01T08:35:04Z,http://arxiv.org/abs/2407.01080v2,"Face4RAG: Factual Consistency Evaluation for Retrieval Augmented
  Generation in Chinese","The prevailing issue of factual inconsistency errors in conventional
Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency
Evaluation (FCE). Despite the various FCE methods proposed earlier, these
methods are evaluated on datasets generated by specific Large Language Models
(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE
methods perform on other LLMs with different error distributions or even unseen
error types, as these methods may fail to detect the error types generated by
other LLMs. To fill this gap, in this paper, we propose the first comprehensive
FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our
benchmark consists of a synthetic dataset built upon a carefully designed
typology for factuality inconsistency error and a real-world dataset
constructed from six commonly used LLMs, enabling evaluation of FCE methods on
specific error types or real-world error distributions. On the proposed
benchmark, we discover the failure of existing FCE methods to detect the
logical fallacy, which refers to a mismatch of logic structures between the
answer and the retrieved reference. To fix this issue, we further propose a new
method called \emph{L-Face4RAG} with two novel designs of logic-preserving
answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG
substantially outperforms previous methods for factual inconsistency detection
on a wide range of tasks, notably beyond the RAG task from which it is
originally motivated. Both the benchmark and our proposed method are publicly
available.\footnote{\url{https://huggingface.co/datasets/yq27/Face4RAG}\label{link_face4rag}}",Yunqi Xu
2024-07-04T04:30:04Z,http://arxiv.org/abs/2407.03627v5,"DSLR: Document Refinement with Sentence-Level Re-ranking and
  Reconstruction to Enhance Retrieval-Augmented Generation","Recent advancements in Large Language Models (LLMs) have significantly
improved their performance across various Natural Language Processing (NLP)
tasks. However, LLMs still struggle with generating non-factual responses due
to limitations in their parametric memory. Retrieval-Augmented Generation (RAG)
systems address this issue by incorporating external knowledge with a retrieval
module. Despite their successes, however, current RAG systems face challenges
with retrieval failures and the limited ability of LLMs to filter out
irrelevant information. Therefore, in this work, we propose DSLR (Document
Refinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised
framework that decomposes retrieved documents into sentences, filters out
irrelevant sentences, and reconstructs them again into coherent passages. We
experimentally validate DSLR on multiple open-domain QA datasets and the
results demonstrate that DSLR significantly enhances the RAG performance over
conventional fixed-size passage. Furthermore, our DSLR enhances performance in
specific, yet realistic scenarios without the need for additional training,
providing an effective and efficient solution for refining retrieved documents
in RAG systems.",Taeho Hwang
2024-07-11T06:50:19Z,http://arxiv.org/abs/2407.08223v1,"Speculative RAG: Enhancing Retrieval Augmented Generation through
  Drafting","Retrieval augmented generation (RAG) combines the generative abilities of
large language models (LLMs) with external knowledge sources to provide more
accurate and up-to-date responses. Recent RAG advancements focus on improving
retrieval outcomes through iterative LLM refinement or self-critique
capabilities acquired through additional instruction tuning of LLMs. In this
work, we introduce Speculative RAG - a framework that leverages a larger
generalist LM to efficiently verify multiple RAG drafts produced in parallel by
a smaller, distilled specialist LM. Each draft is generated from a distinct
subset of retrieved documents, offering diverse perspectives on the evidence
while reducing input token counts per draft. This approach enhances
comprehension of each subset and mitigates potential position bias over long
context. Our method accelerates RAG by delegating drafting to the smaller
specialist LM, with the larger generalist LM performing a single verification
pass over the drafts. Extensive experiments demonstrate that Speculative RAG
achieves state-of-the-art performance with reduced latency on TriviaQA,
MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy
by up to 12.97% while reducing latency by 51% compared to conventional RAG
systems on PubHealth.",Zilong Wang
2024-06-25T20:23:15Z,http://arxiv.org/abs/2407.11005v1,"RAGBench: Explainable Benchmark for Retrieval-Augmented Generation
  Systems","Retrieval-Augmented Generation (RAG) has become a standard architectural
pattern for incorporating domain-specific knowledge into user-facing chat
applications powered by Large Language Models (LLMs). RAG systems are
characterized by (1) a document retriever that queries a domain-specific corpus
for context information relevant to an input query, and (2) an LLM that
generates a response based on the provided query and context. However,
comprehensive evaluation of RAG systems remains a challenge due to the lack of
unified evaluation criteria and annotated datasets. In response, we introduce
RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k
examples. It covers five unique industry-specific domains and various RAG task
types. RAGBench examples are sourced from industry corpora such as user
manuals, making it particularly relevant for industry applications. Further, we
formalize the TRACe evaluation framework: a set of explainable and actionable
RAG evaluation metrics applicable across all RAG domains. We release the
labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.
RAGBench explainable labels facilitate holistic evaluation of RAG systems,
enabling actionable feedback for continuous improvement of production
applications. Thorough extensive benchmarking, we find that LLM-based RAG
evaluation methods struggle to compete with a finetuned RoBERTa model on the
RAG evaluation task. We identify areas where existing approaches fall short and
propose the adoption of RAGBench with TRACe towards advancing the state of RAG
evaluation systems.",Robert Friel
2024-07-16T18:09:21Z,http://arxiv.org/abs/2407.12101v1,Better RAG using Relevant Information Gain,"A common way to extend the memory of large language models (LLMs) is by
retrieval augmented generation (RAG), which inserts text retrieved from a
larger memory into an LLM's context window. However, the context window is
typically limited to several thousand tokens, which limits the number of
retrieved passages that can inform a model's response. For this reason, it's
important to avoid occupying context window space with redundant information by
ensuring a degree of diversity among retrieved passages. At the same time, the
information should also be relevant to the current task. Most prior methods
that encourage diversity among retrieved results, such as Maximal Marginal
Relevance (MMR), do so by incorporating an objective that explicitly trades off
diversity and relevance. We propose a novel simple optimization metric based on
relevant information gain, a probabilistic measure of the total information
relevant to a query for a set of retrieved results. By optimizing this metric,
diversity organically emerges from our system. When used as a drop-in
replacement for the retrieval component of a RAG system, this method yields
state-of-the-art performance on question answering tasks from the Retrieval
Augmented Generation Benchmark (RGB), outperforming existing metrics that
directly optimize for relevance and diversity.",Marc Pickett
2024-07-17T07:44:18Z,http://arxiv.org/abs/2407.12888v1,"Explainable Biomedical Hypothesis Generation via Retrieval Augmented
  Generation enabled Large Language Models","The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.",Alexander R. Pelletier
2024-07-18T17:55:55Z,http://arxiv.org/abs/2407.13757v1,"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation
  of Large Language Models","Retrieval-Augmented Generation (RAG) is applied to solve hallucination
problems and real-time constraints of large language models, but it also
induces vulnerabilities against retrieval corruption attacks. Existing research
mainly explores the unreliability of RAG in white-box and closed-domain QA
tasks. In this paper, we aim to reveal the vulnerabilities of
Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks
for opinion manipulation. We explore the impact of such attacks on user
cognition and decision-making, providing new insight to enhance the reliability
and security of RAG models. We manipulate the ranking results of the retrieval
model in RAG with instruction and use these results as data to train a
surrogate model. By employing adversarial retrieval attack methods to the
surrogate model, black-box transfer attacks on RAG are further realized.
Experiments conducted on opinion datasets across multiple topics show that the
proposed attack strategy can significantly alter the opinion polarity of the
content generated by RAG. This demonstrates the model's vulnerability and, more
importantly, reveals the potential negative impact on user cognition and
decision-making, making it easier to mislead users into accepting incorrect or
biased information.",Zhuo Chen
2024-07-20T10:46:42Z,http://arxiv.org/abs/2407.14838v1,"Retrieval Augmented Generation Integrated Large Language Models in Smart
  Contract Vulnerability Detection","The rapid growth of Decentralized Finance (DeFi) has been accompanied by
substantial financial losses due to smart contract vulnerabilities,
underscoring the critical need for effective security auditing. With attacks
becoming more frequent, the necessity and demand for auditing services has
escalated. This especially creates a financial burden for independent
developers and small businesses, who often have limited available funding for
these services. Our study builds upon existing frameworks by integrating
Retrieval-Augmented Generation (RAG) with large language models (LLMs),
specifically employing GPT-4-1106 for its 128k token context window. We
construct a vector store of 830 known vulnerable contracts, leveraging Pinecone
for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and
LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a
binary answer for vulnerability detection. We first test 52 smart contracts 40
times each against a provided vulnerability type, verifying the replicability
and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7%
success rate in guided detection of vulnerabilities. Second, we challenge the
model under a ""blind"" audit setup, without the vulnerability type provided in
the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates
the general vulnerability detection capabilities without hinted context
assistance. Under these conditions, a 60.71% success rate was observed. While
the results are promising, we still emphasize the need for human auditing at
this time. We provide this study as a proof of concept for a cost-effective
smart contract auditing process, moving towards democratic access to security.",Jeffy Yu
2024-06-18T14:23:54Z,http://arxiv.org/abs/2407.16896v1,"Free to play: UN Trade and Development's experience with developing its
  own open-source Retrieval Augmented Generation Large Language Model
  application","Generative artificial intelligence (AI), and in particular Large Language
Models (LLMs), have exploded in popularity and attention since the release to
the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in
November of 2022. Due to the power of these general purpose models and their
ability to communicate in natural language, they can be useful in a range of
domains, including the work of official statistics and international
organizations. However, with such a novel and seemingly complex technology, it
can feel as if generative AI is something that happens to an organization,
something that can be talked about but not understood, that can be commented on
but not contributed to. Additionally, the costs of adoption and operation of
proprietary solutions can be both uncertain and high, a barrier for often
cost-constrained international organizations. In the face of these challenges,
United Nations Trade and Development (UNCTAD), through its Global Crisis
Response Group (GCRG), has explored and developed its own open-source Retrieval
Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more
useful for the organization's domain and work. Developing in-house solutions
comes with pros and cons, with pros including cost, flexibility, and fostering
institutional knowledge. Cons include time and skill investments and gaps and
application polish and power. The three libraries developed to produce the app,
nlp_pipeline for document processing and statistical analysis, local_rag_llm
for running a local RAG LLM, and streamlit_rag for the user interface, are
publicly available on PyPI and GitHub with Dockerfiles. A fourth library,
local_llm_finetune, is also available for fine-tuning existing LLMs which can
then be used in the application.",Daniel Hopp
2024-07-29T00:41:48Z,http://arxiv.org/abs/2407.19619v1,"Enhancing Code Translation in Language Models with Few-Shot Learning via
  Retrieval-Augmented Generation","The advent of large language models (LLMs) has significantly advanced the
field of code translation, enabling automated translation between programming
languages. However, these models often struggle with complex translation tasks
due to inadequate contextual understanding. This paper introduces a novel
approach that enhances code translation through Few-Shot Learning, augmented
with retrieval-based techniques. By leveraging a repository of existing code
translations, we dynamically retrieve the most relevant examples to guide the
model in translating new code segments. Our method, based on
Retrieval-Augmented Generation (RAG), substantially improves translation
quality by providing contextual examples from which the model can learn in
real-time. We selected RAG over traditional fine-tuning methods due to its
ability to utilize existing codebases or a locally stored corpus of code, which
allows for dynamic adaptation to diverse translation tasks without extensive
retraining. Extensive experiments on diverse datasets with open LLM models such
as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code
Instruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5
Turbo and GPT-4o, demonstrate our approach's superiority over traditional
zero-shot methods, especially in translating between Fortran and CPP. We also
explored varying numbers of shots i.e. examples provided during inference,
specifically 1, 2, and 3 shots and different embedding models for RAG,
including Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and
effectiveness of our approach.",Manish Bhattarai
2024-07-31T08:43:17Z,http://arxiv.org/abs/2407.21439v2,"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities. However, a significant drawback of MLLMs is their reliance on
static training data, leading to outdated information and limited contextual
awareness. This static nature hampers their ability to provide accurate and
up-to-date responses, particularly in dynamic or rapidly evolving contexts.
Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)
offers a promising solution, the system would inevitably encounter the
multi-granularity noisy correspondence (MNC) problem, which hinders accurate
retrieval and generation. In this work, we propose RagVL, a novel framework
with knowledge-enhanced reranking and noise-injected training, to address these
limitations. We instruction-tune the MLLM with a simple yet effective
instruction template to induce its ranking ability and serve it as a reranker
to precisely filter the top-k retrieved images. For generation, we inject
visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments on the subsets of two datasets
that require retrieving and reasoning over images to answer a given query
verify the effectiveness of our method. Code and models are available at
https://github.com/IDEA-FinAI/RagVL.",Zhanpeng Chen
2024-07-31T09:16:33Z,http://arxiv.org/abs/2407.21459v1,"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government
  Financial Data and Regulations to Enhance Decision Making","Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.",Gilang Fajar Febrian
2024-08-05T22:34:28Z,http://arxiv.org/abs/2408.02854v3,"Wiping out the limitations of Large Language Models -- A Taxonomy for
  Retrieval Augmented Generation","Current research on RAGs is distributed across various disciplines, and since
the technology is evolving very quickly, its unit of analysis is mostly on
technological innovations, rather than applications in business contexts. Thus,
in this research, we aim to create a taxonomy to conceptualize a comprehensive
overview of the constituting characteristics that define RAG applications,
facilitating the adoption of this technology in the IS community. To the best
of our knowledge, no RAG application taxonomies have been developed so far. We
describe our methodology for developing the taxonomy, which includes the
criteria for selecting papers, an explanation of our rationale for employing a
Large Language Model (LLM)-supported approach to extract and identify initial
characteristics, and a concise overview of our systematic process for
conceptualizing the taxonomy. Our systematic taxonomy development process
includes four iterative phases designed to refine and enhance our understanding
and presentation of RAG's core dimensions. We have developed a total of five
meta-dimensions and sixteen dimensions to comprehensively capture the concept
of Retrieval-Augmented Generation (RAG) applications. When discussing our
findings, we also detail the specific research areas and pose key research
questions to guide future information system researchers as they explore the
emerging topics of RAG systems.",Mahei Manhai Li
2024-08-08T03:11:12Z,http://arxiv.org/abs/2408.04187v2,"Medical Graph RAG: Towards Safe Medical Large Language Model via Graph
  Retrieval-Augmented Generation","We introduce a novel graph-based Retrieval-Augmented Generation (RAG)
framework specifically designed for the medical domain, called
\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)
capabilities for generating evidence-based medical responses, thereby improving
safety and reliability when handling private medical data. Graph-based RAG
(GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong
potential for gaining holistic insights from long-form documents. However, its
standard implementation is overly complex for general use and lacks the ability
to generate evidence-based responses, limiting its effectiveness in the medical
field. To extend the capabilities of GraphRAG to the medical domain, we propose
unique Triple Graph Construction and U-Retrieval techniques over it. In our
graph construction, we create a triple-linked structure that connects user
documents to credible medical sources and controlled vocabularies. In the
retrieval process, we propose U-Retrieval which combines Top-down Precise
Retrieval with Bottom-up Response Refinement to balance global context
awareness with precise indexing. These effort enable both source information
retrieval and comprehensive response generation. Our approach is validated on 9
medical Q\&A benchmarks, 2 health fact-checking benchmarks, and one collected
dataset testing long-form generation. The results show that MedGraphRAG
consistently outperforms state-of-the-art models across all benchmarks, while
also ensuring that responses include credible source documentation and
definitions. Our code is released at:
https://github.com/MedicineToken/Medical-Graph-RAG.",Junde Wu
2024-08-09T09:07:48Z,http://arxiv.org/abs/2408.04948v1,"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction","Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain",Bhaskarjit Sarmah
2024-08-14T15:19:16Z,http://arxiv.org/abs/2408.07611v2,"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs","Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.",Weijian Xie
2024-08-12T08:54:32Z,http://arxiv.org/abs/2408.08901v1,Bayesian inference to improve quality of Retrieval Augmented Generation,"Retrieval Augmented Generation or RAG is the most popular pattern for modern
Large Language Model or LLM applications. RAG involves taking a user query and
finding relevant paragraphs of context in a large corpus typically captured in
a vector database. Once the first level of search happens over a vector
database, the top n chunks of relevant text are included directly in the
context and sent as prompt to the LLM. Problem with this approach is that
quality of text chunks depends on effectiveness of search. There is no strong
post processing after search to determine if the chunk does hold enough
information to include in prompt. Also many times there may be chunks that have
conflicting information on the same subject and the model has no prior
experience which chunk to prioritize to make a decision. Often times, this
leads to the model providing a statement that there are conflicting statements,
and it cannot produce an answer. In this research we propose a Bayesian
approach to verify the quality of text chunks from the search results. Bayes
theorem tries to relate conditional probabilities of the hypothesis with
evidence and prior probabilities. We propose that, finding likelihood of text
chunks to give a quality answer and using prior probability of quality of text
chunks can help us improve overall quality of the responses from RAG systems.
We can use the LLM itself to get a likelihood of relevance of a context
paragraph. For prior probability of the text chunk, we use the page number in
the documents parsed. Assumption is that that paragraphs in earlier pages have
a better probability of being findings and more relevant to generalizing an
answer.",Dattaraj Rao
2024-08-15T12:20:24Z,http://arxiv.org/abs/2408.08921v2,Graph Retrieval-Augmented Generation: A Survey,"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.",Boci Peng
2024-08-19T06:05:24Z,http://arxiv.org/abs/2408.09713v2,"Carbon Footprint Accounting Driven by Large Language Models and
  Retrieval-augmented Generation","Carbon footprint accounting is crucial for quantifying greenhouse gas
emissions and achieving carbon neutrality.The dynamic nature of processes,
accounting rules, carbon-related policies, and energy supply structures
necessitates real-time updates of CFA. Traditional life cycle assessment
methods rely heavily on human expertise, making near-real-time updates
challenging. This paper introduces a novel approach integrating large language
models (LLMs) with retrieval-augmented generation technology to enhance the
real-time, professional, and economical aspects of carbon footprint information
retrieval and analysis. By leveraging LLMs' logical and language understanding
abilities and RAG's efficient retrieval capabilities, the proposed method
LLMs-RAG-CFA can retrieve more relevant professional information to assist
LLMs, enhancing the model's generative abilities. This method offers broad
professional coverage, efficient real-time carbon footprint information
acquisition and accounting, and cost-effective automation without frequent
LLMs' parameter updates. Experimental results across five industries(primary
aluminum, lithium battery, photovoltaic, new energy vehicles, and
transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional
methods and other LLMs, achieving higher information retrieval rates and
significantly lower information deviations and carbon footprint accounting
deviations. The economically viable design utilizes RAG technology to balance
real-time updates with cost-effectiveness, providing an efficient, reliable,
and cost-saving solution for real-time carbon emission management, thereby
enhancing environmental sustainability practices.",Haijin Wang
2024-08-19T18:30:18Z,http://arxiv.org/abs/2408.10343v1,"LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the
  Legal Domain","Retrieval-Augmented Generation (RAG) systems are showing promising potential,
and are becoming increasingly relevant in AI-powered legal applications.
Existing benchmarks, such as LegalBench, assess the generative capabilities of
Large Language Models (LLMs) in the legal domain, but there is a critical gap
in evaluating the retrieval component of RAG systems. To address this, we
introduce LegalBench-RAG, the first benchmark specifically designed to evaluate
the retrieval step of RAG pipelines within the legal space. LegalBench-RAG
emphasizes precise retrieval by focusing on extracting minimal, highly relevant
text segments from legal documents. These highly relevant snippets are
preferred over retrieving document IDs, or large sequences of imprecise chunks,
both of which can exceed context window limitations. Long context windows cost
more to process, induce higher latency, and lead LLMs to forget or hallucinate
information. Additionally, precise results allow LLMs to generate citations for
the end user. The LegalBench-RAG benchmark is constructed by retracing the
context used in LegalBench queries back to their original locations within the
legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus
of over 79M characters, entirely human-annotated by legal experts. We also
introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and
experimentation. By providing a dedicated benchmark for legal retrieval,
LegalBench-RAG serves as a critical tool for companies and researchers focused
on enhancing the accuracy and performance of RAG systems in the legal domain.
The LegalBench-RAG dataset is publicly available at
https://github.com/zeroentropy-cc/legalbenchrag.",Nicholas Pipitone
2024-08-21T07:20:48Z,http://arxiv.org/abs/2408.11381v2,"RAGLAB: A Modular and Research-Oriented Unified Framework for
  Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate human-level capabilities in
dialogue, reasoning, and knowledge retention. However, even the most advanced
LLMs face challenges such as hallucinations and real-time updating of their
knowledge. Current research addresses this bottleneck by equipping LLMs with
external knowledge, a technique known as Retrieval Augmented Generation (RAG).
However, two key issues constrained the development of RAG. First, there is a
growing lack of comprehensive and fair comparisons between novel RAG
algorithms. Second, open-source tools such as LlamaIndex and LangChain employ
high-level abstractions, which results in a lack of transparency and limits the
ability to develop novel algorithms and evaluation metrics. To close this gap,
we introduce RAGLAB, a modular and research-oriented open-source library.
RAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem
for investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair
comparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers
can efficiently compare the performance of various algorithms and develop novel
algorithms.",Xuanwang Zhang
2024-08-21T17:00:05Z,http://arxiv.org/abs/2408.11775v1,"Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support: For 3GPP Standards","Recent studies show that large language models (LLMs) struggle with technical
standards in telecommunications. We propose a fine-tuned retrieval-augmented
generation (RAG) system based on the Phi-2 small language model (SLM) to serve
as an oracle for communication networks. Our developed system leverages
forward-looking semantic chunking to adaptively determine parsing breakpoints
based on embedding similarity, enabling effective processing of diverse
document formats. To handle the challenge of multiple similar contexts in
technical standards, we employ a re-ranking algorithm to prioritize the most
relevant retrieved chunks. Recognizing the limitations of Phi-2's small context
window, we implement a recent technique, namely SelfExtend, to expand the
context window during inference, which not only boosts the performance but also
can accommodate a wider range of user queries and design requirements from
customers to specialized technicians. For fine-tuning, we utilize the low-rank
adaptation (LoRA) technique to enhance computational efficiency during training
and enable effective fine-tuning on small datasets. Our comprehensive
experiments demonstrate substantial improvements over existing
question-answering approaches in the telecom domain, achieving performance that
exceeds larger language models such as GPT-4 (which is about 880 times larger
in size). This work presents a novel approach to leveraging SLMs for
communication networks, offering a balance of efficiency and performance. This
work can serve as a foundation towards agentic language models for networks.",Omar Erak
2024-08-21T17:25:45Z,http://arxiv.org/abs/2408.11793v2,"Leveraging Chemistry Foundation Models to Facilitate Structure Focused
  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and
  Materials Design","Molecular property prediction and generative design via deep learning models
has been the subject of intense research given its potential to accelerate
development of new, high-performance materials. More recently, these workflows
have been significantly augmented with the advent of large language models
(LLMs) and systems of autonomous agents capable of utilizing pre-trained models
to make predictions in the context of more complex research tasks. While
effective, there is still room for substantial improvement within agentic
systems on the retrieval of salient information for material design tasks.
Within this context, alternative uses of predictive deep learning models, such
as leveraging their latent representations to facilitate cross-modal retrieval
augmented generation within agentic systems for task-specific materials design,
has remained unexplored. Herein, we demonstrate that large, pre-trained
chemistry foundation models can serve as a basis for enabling
structure-focused, semantic chemistry information retrieval for both
small-molecules, complex polymeric materials, and reactions. Additionally, we
show the use of chemistry foundation models in conjunction with multi-modal
models such as OpenCLIP facilitate unprecedented queries and information
retrieval across multiple characterization data domains. Finally, we
demonstrate the integration of these models within multi-agent systems to
facilitate structure and topological-based natural language queries and
information retrieval for different research tasks.",Nathaniel H. Park
2024-08-18T11:52:24Z,http://arxiv.org/abs/2408.13273v1,"Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach
  for Temporal Knowledge Graph Forecasting","Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.",Geethan Sannidhi
2024-08-26T09:23:35Z,http://arxiv.org/abs/2408.14523v1,Retrieval Augmented Generation for Dynamic Graph Modeling,"Dynamic graph modeling is crucial for analyzing evolving patterns in various
applications. Existing approaches often integrate graph neural networks with
temporal modules or redefine dynamic graph modeling as a generative sequence
task. However, these methods typically rely on isolated historical contexts of
the target nodes from a narrow perspective, neglecting occurrences of similar
patterns or relevant cases associated with other nodes. In this work, we
introduce the Retrieval-Augmented Generation for Dynamic Graph Modeling
(RAG4DyG) framework, which leverages guidance from contextually and temporally
analogous examples to broaden the perspective of each node. This approach
presents two critical challenges: (1) How to identify and retrieve high-quality
demonstrations that are contextually and temporally analogous to dynamic graph
samples? (2) How can these demonstrations be effectively integrated to improve
dynamic graph modeling? To address these challenges, we propose RAG4DyG, which
enriches the understanding of historical contexts by retrieving and learning
from contextually and temporally pertinent demonstrations. Specifically, we
employ a time- and context-aware contrastive learning module to identify and
retrieve relevant cases for each query sequence. Moreover, we design a graph
fusion strategy to integrate the retrieved cases, thereby augmenting the
inherent historical contexts for improved prediction. Extensive experiments on
real-world datasets across different domains demonstrate the effectiveness of
RAG4DyG for dynamic graph modeling.",Yuxia Wu
2024-09-09T13:20:31Z,http://arxiv.org/abs/2409.05591v2,"MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge
  Discovery","Retrieval-Augmented Generation (RAG) leverages retrieval tools to access
external databases, thereby enhancing the generation quality of large language
models (LLMs) through optimized context. However, the existing retrieval
methods are constrained inherently, as they can only perform relevance matching
between explicitly stated queries and well-formed knowledge, but unable to
handle tasks involving ambiguous information needs or unstructured knowledge.
Consequently, existing RAG systems are primarily effective for straightforward
question-answering tasks. In this work, we propose MemoRAG, a novel
retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG
adopts a dual-system architecture. On the one hand, it employs a light but
long-range LLM to form the global memory of database. Once a task is presented,
it generates draft answers, cluing the retrieval tools to locate useful
information within the database. On the other hand, it leverages an expensive
but expressive LLM, which generates the ultimate answer based on the retrieved
information. Building on this general framework, we further optimize MemoRAG's
performance by enhancing its cluing mechanism and memorization capacity. In our
experiment, MemoRAG achieves superior performance across a variety of
evaluation tasks, including both complex ones where conventional RAG fails and
straightforward ones where RAG is commonly applied.",Hongjin Qian
2024-09-16T09:06:44Z,http://arxiv.org/abs/2409.10102v1,Trustworthiness in Retrieval-Augmented Generation Systems: A Survey,"Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.",Yujia Zhou
2024-09-17T13:44:42Z,http://arxiv.org/abs/2409.11190v2,"SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer","We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.",Anmol Gautam
2024-09-17T15:29:34Z,http://arxiv.org/abs/2409.11279v1,"P-RAG: Progressive Retrieval Augmented Generation For Planning on
  Embodied Everyday Task","Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.",Weiye Xu
2024-09-17T23:10:04Z,http://arxiv.org/abs/2409.11598v2,"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented
  Generation","Many language models now enhance their responses with retrieval capabilities,
leading to the widespread adoption of retrieval-augmented generation (RAG)
systems. However, despite retrieval being a core component of RAG, much of the
research in this area overlooks the extensive body of work on fair ranking,
neglecting the importance of considering all stakeholders involved. This paper
presents the first systematic evaluation of RAG systems integrated with fair
rankings. We focus specifically on measuring the fair exposure of each relevant
item across the rankings utilized by RAG systems (i.e., item-side fairness),
aiming to promote equitable growth for relevant item providers. To gain a deep
understanding of the relationship between item-fairness, ranking quality, and
generation quality in the context of RAG, we analyze nine different RAG systems
that incorporate fair rankings across seven distinct datasets. Our findings
indicate that RAG systems with fair rankings can maintain a high level of
generation quality and, in many cases, even outperform traditional RAG systems,
despite the general trend of a tradeoff between ensuring fairness and
maintaining system-effectiveness. We believe our insights lay the groundwork
for responsible and equitable RAG systems and open new avenues for future
research. We publicly release our codebase and dataset at
https://github.com/kimdanny/Fair-RAG.",To Eun Kim
2024-09-19T17:52:07Z,http://arxiv.org/abs/2409.12941v2,"Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented
  Generation","Large Language Models (LLMs) have demonstrated significant performance
improvements across various cognitive tasks. An emerging application is using
LLMs to enhance retrieval-augmented generation (RAG) capabilities. These
systems require LLMs to understand user queries, retrieve relevant information,
and synthesize coherent and accurate responses. Given the increasing real-world
deployment of such systems, comprehensive evaluation becomes crucial. To this
end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),
a high-quality evaluation dataset designed to test LLMs' ability to provide
factual responses, assess retrieval capabilities, and evaluate the reasoning
required to generate final answers. While previous work has provided datasets
and benchmarks to evaluate these abilities in isolation, FRAMES offers a
unified framework that provides a clearer picture of LLM performance in
end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions
that require the integration of information from multiple sources. We present
baseline results demonstrating that even state-of-the-art LLMs struggle with
this task, achieving 0.40 accuracy with no retrieval. The accuracy is
significantly improved with our proposed multi-step retrieval pipeline,
achieving an accuracy of 0.66 (>50% improvement). We hope our work will help
bridge evaluation gaps and assist in developing more robust and capable RAG
systems.",Satyapriya Krishna
2024-09-03T03:31:37Z,http://arxiv.org/abs/2409.13694v2,"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study","Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach for mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of externeal knowledge source. However, in
real-world applications, most situations involve diverse knowledge from various
sources, yet this area has been less explored. The main dilemma is the lack of
a suitable dataset containing multiple knowledge sources and pre-exploration of
the associated issues. To address these challenges, we standardize a benchmark
dataset that combines structured and unstructured knowledge across diverse and
complementary domains. Based on this dataset, we further develop a
plug-and-play RAG framework, PruningRAG, whose main characteristic is to employ
multi-granularity pruning strategies for optimizing the integration of relevant
information and minimizing misleading context. Building upon the standardized
dataset and PruningRAG, we also report a series of experimental results, as
well as insightful findings. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of
advancing future research in the RAG community.",Shuo Yu
2024-09-23T14:51:22Z,http://arxiv.org/abs/2409.15076v1,"Enhancing Scientific Reproducibility Through Automated BioCompute Object
  Creation Using Retrieval-Augmented Generation from Publications","The exponential growth in computational power and accessibility has
transformed the complexity and scale of bioinformatics research, necessitating
standardized documentation for transparency, reproducibility, and regulatory
compliance. The IEEE BioCompute Object (BCO) standard addresses this need but
faces adoption challenges due to the overhead of creating compliant
documentation, especially for legacy research. This paper presents a novel
approach to automate the creation of BCOs from scientific papers using
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We
describe the development of the BCO assistant tool that leverages RAG to
extract relevant information from source papers and associated code
repositories, addressing key challenges such as LLM hallucination and
long-context understanding. The implementation incorporates optimized retrieval
processes, including a two-pass retrieval with re-ranking, and employs
carefully engineered prompts for each BCO domain. We discuss the tool's
architecture, extensibility, and evaluation methods, including automated and
manual assessment approaches. The BCO assistant demonstrates the potential to
significantly reduce the time and effort required for retroactive documentation
of bioinformatics research while maintaining compliance with the standard. This
approach opens avenues for AI-assisted scientific documentation and knowledge
extraction from publications thereby enhancing scientific reproducibility. The
BCO assistant tool and documentation is available at
https://biocompute-objects.github.io/bco-rag/.",Sean Kim
2024-09-23T17:56:08Z,http://arxiv.org/abs/2409.15260v1,"Generative AI Is Not Ready for Clinical Use in Patient Education for
  Lower Back Pain Patients, Even With Retrieval-Augmented Generation","Low back pain (LBP) is a leading cause of disability globally. Following the
onset of LBP and subsequent treatment, adequate patient education is crucial
for improving functionality and long-term outcomes. Despite advancements in
patient education strategies, significant gaps persist in delivering
personalized, evidence-based information to patients with LBP. Recent
advancements in large language models (LLMs) and generative artificial
intelligence (GenAI) have demonstrated the potential to enhance patient
education. However, their application and efficacy in delivering educational
content to patients with LBP remain underexplored and warrant further
investigation. In this study, we introduce a novel approach utilizing LLMs with
Retrieval-Augmented Generation (RAG) and few-shot learning to generate tailored
educational materials for patients with LBP. Physical therapists manually
evaluated our model responses for redundancy, accuracy, and completeness using
a Likert scale. In addition, the readability of the generated education
materials is assessed using the Flesch Reading Ease score. The findings
demonstrate that RAG-based LLMs outperform traditional LLMs, providing more
accurate, complete, and readable patient education materials with less
redundancy. Having said that, our analysis reveals that the generated materials
are not yet ready for use in clinical practice. This study underscores the
potential of AI-driven models utilizing RAG to improve patient education for
LBP; however, significant challenges remain in ensuring the clinical relevance
and granularity of content generated by these models.",Yi-Fei Zhao
2024-09-12T02:43:40Z,http://arxiv.org/abs/2409.17275v1,"On the Vulnerability of Applying Retrieval-Augmented Generation within
  Knowledge-Intensive Application Domains","Retrieval-Augmented Generation (RAG) has been empirically shown to enhance
the performance of large language models (LLMs) in knowledge-intensive domains
such as healthcare, finance, and legal contexts. Given a query, RAG retrieves
relevant documents from a corpus and integrates them into the LLMs' generation
process. In this study, we investigate the adversarial robustness of RAG,
focusing specifically on examining the retrieval system. First, across 225
different setup combinations of corpus, retriever, query, and targeted
information, we show that retrieval systems are vulnerable to universal
poisoning attacks in medical Q\&A. In such attacks, adversaries generate
poisoned documents containing a broad spectrum of targeted information, such as
personally identifiable information. When these poisoned documents are inserted
into a corpus, they can be accurately retrieved by any users, as long as
attacker-specified queries are used. To understand this vulnerability, we
discovered that the deviation from the query's embedding to that of the
poisoned document tends to follow a pattern in which the high similarity
between the poisoned document and the query is retained, thereby enabling
precise retrieval. Based on these findings, we develop a new detection-based
defense to ensure the safe use of RAG. Through extensive experiments spanning
various Q\&A domains, we observed that our proposed method consistently
achieves excellent detection rates in nearly all cases.",Xun Xian
2024-09-29T15:40:54Z,http://arxiv.org/abs/2409.19745v2,"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Generation with Zero Inference Overhead","Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.",Tao Tan
2024-10-01T04:20:14Z,http://arxiv.org/abs/2410.00387v1,"Boosting the Capabilities of Compact Models in Low-Data Contexts with
  Large Language Models and Retrieval-Augmented Generation","The data and compute requirements of current language modeling technology
pose challenges for the processing and analysis of low-resource languages.
Declarative linguistic knowledge has the potential to partially bridge this
data scarcity gap by providing models with useful inductive bias in the form of
language-specific rules. In this paper, we propose a retrieval augmented
generation (RAG) framework backed by a large language model (LLM) to correct
the output of a smaller model for the linguistic task of morphological
glossing. We leverage linguistic information to make up for the lack of data
and trainable parameters, while allowing for inputs from written descriptive
grammars interpreted and distilled through an LLM.
  The results demonstrate that significant leaps in performance and efficiency
are possible with the right combination of: a) linguistic inputs in the form of
grammars, b) the interpretive power of LLMs, and c) the trainability of smaller
token classification networks. We show that a compact, RAG-supported model is
highly effective in data-scarce settings, achieving a new state-of-the-art for
this task and our target languages. Our work also offers documentary linguists
a more reliable and more usable tool for morphological glossing by providing
well-reasoned explanations and confidence scores for each output.",Bhargav Shandilya
2024-10-02T01:59:07Z,http://arxiv.org/abs/2410.01171v1,"BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented
  Generation","Large language models excel at creative generation but continue to struggle
with the issues of hallucination and bias. While retrieval-augmented generation
(RAG) provides a framework for grounding LLMs' responses in accurate and
up-to-date information, it still raises the question of bias: which sources
should be selected for inclusion in the context? And how should their
importance be weighted? In this paper, we study the challenge of cross-lingual
RAG and present a dataset to investigate the robustness of existing systems at
answering queries about geopolitical disputes, which exist at the intersection
of linguistic, cultural, and political boundaries. Our dataset is sourced from
Wikipedia pages containing information relevant to the given queries and we
investigate the impact of including additional context, as well as the
composition of this context in terms of language and source, on an LLM's
response. Our results show that existing RAG systems continue to be challenged
by cross-lingual use cases and suffer from a lack of consistency when they are
provided with competing information in multiple languages. We present case
studies to illustrate these issues and outline steps for future research to
address these challenges. We make our dataset and code publicly available at
https://github.com/manestay/bordIRlines.",Bryan Li
2024-10-04T14:21:27Z,http://arxiv.org/abs/2410.03461v1,"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding
  Verification in Retrieval Augmented Generation","While retrieval augmented generation (RAG) has been shown to enhance
factuality of large language model (LLM) outputs, LLMs still suffer from
hallucination, generating incorrect or irrelevant information. One common
detection strategy involves prompting the LLM again to assess whether its
response is grounded in the retrieved evidence, but this approach is costly.
Alternatively, lightweight natural language inference (NLI) models for
efficient grounding verification can be used at inference time. While existing
pre-trained NLI models offer potential solutions, their performance remains
subpar compared to larger models on realistic RAG inputs. RAG inputs are more
complex than most datasets used for training NLI models and have
characteristics specific to the underlying knowledge base, requiring adaptation
of the NLI models to a specific target domain. Additionally, the lack of
labeled instances in the target domain makes supervised domain adaptation,
e.g., through fine-tuning, infeasible. To address these challenges, we
introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework
enables unsupervised domain adaptation through synthetic data generation.
Unlike previous methods that rely on handcrafted filtering and augmentation
strategies, Auto-GDA employs an iterative process to continuously improve the
quality of generated samples using weak labels from less efficient teacher
models and discrete optimization to select the most promising augmented
samples. Experimental results demonstrate the effectiveness of our approach,
with models fine-tuned on synthetic data using Auto-GDA often surpassing the
performance of the teacher model and reaching the performance level of LLMs at
10 % of their computational cost.",Tobias Leemann
2024-10-06T03:42:15Z,http://arxiv.org/abs/2410.04343v1,Inference Scaling for Long-Context Retrieval Augmented Generation,"The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring strategies beyond simply increasing the quantity of
knowledge. We focus on two inference scaling strategies: in-context learning
and iterative prompting. These strategies provide additional flexibility to
scale test-time computation (e.g., by increasing retrieved documents or
generation steps), thereby enhancing LLMs' ability to effectively acquire and
utilize contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",Zhenrui Yue
2024-10-10T19:14:55Z,http://arxiv.org/abs/2410.08320v1,"Do You Know What You Are Talking About? Characterizing Query-Knowledge
  Relevance For Reliable Retrieval Augmented Generation","Language models (LMs) are known to suffer from hallucinations and
misinformation. Retrieval augmented generation (RAG) that retrieves verifiable
information from an external knowledge corpus to complement the parametric
knowledge in LMs provides a tangible solution to these problems. However, the
generation quality of RAG is highly dependent on the relevance between a user's
query and the retrieved documents. Inaccurate responses may be generated when
the query is outside of the scope of knowledge represented in the external
knowledge corpus or if the information in the corpus is out-of-date. In this
work, we establish a statistical framework that assesses how well a query can
be answered by an RAG system by capturing the relevance of knowledge. We
introduce an online testing procedure that employs goodness-of-fit (GoF) tests
to inspect the relevance of each user query to detect out-of-knowledge queries
with low knowledge relevance. Additionally, we develop an offline testing
framework that examines a collection of user queries, aiming to detect
significant shifts in the query distribution which indicates the knowledge
corpus is no longer sufficiently capable of supporting the interests of the
users. We demonstrate the capabilities of these strategies through a systematic
evaluation on eight question-answering (QA) datasets, the results of which
indicate that the new testing framework is an efficient solution to enhance the
reliability of existing RAG systems.",Zhuohang Li
2024-10-11T14:03:29Z,http://arxiv.org/abs/2410.08821v1,"Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors
and hallucinated outputs generated by Large Language Models (LLMs) in
open-domain question-answering tasks (OpenQA) via introducing external
knowledge. For complex QA, however, existing RAG methods use LLMs to actively
predict retrieval timing and directly use the retrieved information for
generation, regardless of whether the retrieval timing accurately reflects the
actual information needs, or sufficiently considers prior retrieved knowledge,
which may result in insufficient information gathering and interaction,
yielding low-quality answers. To address these, we propose a generic RAG
approach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA
tasks, which includes the iterative information collector, adaptive memory
reviewer, and task-oriented generator, while following a new
Retriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an
overarching view of knowledge growth, iteratively gathering new information in
the form of notes and updating them into the existing optimal knowledge
structure, enhancing high-quality knowledge interactions. In addition, we
employ an adaptive, note-based stop-exploration strategy to decide ""what to
retrieve and when to stop"" to encourage sufficient knowledge exploration. We
conduct extensive experiments on five complex QA datasets, and the results
demonstrate the superiority and effectiveness of our method and its components.
The code and data are at https://github.com/thunlp/Adaptive-Note.",Ruobing Wang
2024-10-08T05:13:27Z,http://arxiv.org/abs/2410.09090v1,"Automating Bibliometric Analysis with Sentence Transformers and
  Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and
  Contextual Search for Customized Literature Characterization for High-Impact
  Urban Research","Bibliometric analysis is essential for understanding research trends, scope,
and impact in urban science, especially in high-impact journals, such Nature
Portfolios. However, traditional methods, relying on keyword searches and basic
NLP techniques, often fail to uncover valuable insights not explicitly stated
in article titles or keywords. These approaches are unable to perform semantic
searches and contextual understanding, limiting their effectiveness in
classifying topics and characterizing studies. In this paper, we address these
limitations by leveraging Generative AI models, specifically transformers and
Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric
analysis. We developed a technical workflow that integrates a vector database,
Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and
Large Language Models (LLMs) to enable contextual search, topic ranking, and
characterization of research using customized prompt templates. A pilot study
analyzing 223 urban science-related articles published in Nature Communications
over the past decade highlights the effectiveness of our approach in generating
insightful summary statistics on the quality, scope, and characteristics of
papers in high-impact journals. This study introduces a new paradigm for
enhancing bibliometric analysis and knowledge retrieval in urban research,
positioning an AI agent as a powerful tool for advancing research evaluation
and understanding.",Haowen Xu
2024-10-12T16:30:51Z,http://arxiv.org/abs/2410.09584v1,"Toward General Instruction-Following Alignment for Retrieval-Augmented
  Generation","Following natural instructions is crucial for the effective application of
Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in
Large Language Models (LLMs), research on assessing and improving
instruction-following (IF) alignment within the RAG domain remains limited. To
address this issue, we propose VIF-RAG, the first automated, scalable, and
verifiable synthetic pipeline for instruction-following alignment in RAG
systems. We start by manually crafting a minimal set of atomic instructions
(<100) and developing combination rules to synthesize and verify complex
instructions for a seed set. We then use supervised models for instruction
rewriting while simultaneously generating code to automate the verification of
instruction quality via a Python executor. Finally, we integrate these
instructions with extensive RAG and general data samples, scaling up to a
high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further
bridge the gap in instruction-following auto-evaluation for RAG systems, we
introduce FollowRAG Benchmark, which includes approximately 3K test samples,
covering 22 categories of general instruction constraints and four
knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG
can seamlessly integrate with different RAG benchmarks. Using FollowRAG and
eight widely-used IF and foundational abilities benchmarks for LLMs, we
demonstrate that VIF-RAG markedly enhances LLM performance across a broad range
of general instruction constraints while effectively leveraging its
capabilities in RAG scenarios. Further analysis offers practical insights for
achieving IF alignment in RAG systems. Our code and datasets are released at
https://FollowRAG.github.io.",Guanting Dong
2024-10-13T02:34:47Z,http://arxiv.org/abs/2410.09699v1,"Honest AI: Fine-Tuning ""Small"" Language Models to Say ""I Don't Know"",
  and Reducing Hallucination in RAG","Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune ""small"" language models to say ""I don't know"" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.",Xinxi Chen
2024-10-14T09:17:43Z,http://arxiv.org/abs/2410.10315v2,"EasyRAG: Efficient Retrieval-Augmented Generation Framework for
  Automated Network Operations","This paper presents EasyRAG, a simple, lightweight, and efficient
retrieval-augmented generation framework for automated network operations. Our
framework has three advantages. The first is accurate question answering. We
designed a straightforward RAG scheme based on (1) a specific data processing
workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker
for reranking (4) LLM answer generation and optimization. This approach
achieved first place in the GLM4 track in the preliminary round and second
place in the GLM4 track in the semifinals. The second is simple deployment. Our
method primarily consists of BM25 retrieval and BGE-reranker reranking,
requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy,
and highly scalable; we provide a flexible code library with various search and
generation strategies, facilitating custom process implementation. The last one
is efficient inference. We designed an efficient inference acceleration scheme
for the entire coarse ranking, reranking, and generation process that
significantly reduces the inference latency of RAG while maintaining a good
level of accuracy; each acceleration scheme can be plug-and-play into any
component of the RAG process, consistently enhancing the efficiency of the RAG
system. Our code and data are released at
\url{https://github.com/BUAADreamer/EasyRAG}.",Zhangchi Feng
2024-10-14T15:04:18Z,http://arxiv.org/abs/2410.10594v1,"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
  Documents","Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 25--39\%
end-to-end performance gain over traditional text-based RAG pipeline. Further
analysis reveals that VisRAG is effective in utilizing training data and
demonstrates strong generalization capability, positioning it as a promising
solution for RAG on multi-modality documents. Our code and data are available
at https://github.com/openbmb/visrag .",Shi Yu
2024-10-08T12:42:42Z,http://arxiv.org/abs/2410.10869v1,"Application of NotebookLM, a Large Language Model with
  Retrieval-Augmented Generation, for Lung Cancer Staging","Purpose: In radiology, large language models (LLMs), including ChatGPT, have
recently gained attention, and their utility is being rapidly evaluated.
However, concerns have emerged regarding their reliability in clinical
applications due to limitations such as hallucinations and insufficient
referencing. To address these issues, we focus on the latest technology,
retrieval-augmented generation (RAG), which enables LLMs to reference reliable
external knowledge (REK). Specifically, this study examines the utility and
reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for
staging lung cancer.
  Materials and methods: We summarized the current lung cancer staging
guideline in Japan and provided this as REK to NotebookLM. We then tasked
NotebookLM with staging 100 fictional lung cancer cases based on CT findings
and evaluated its accuracy. For comparison, we performed the same task using a
gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.
  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer
staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the
REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in
searching reference locations within the REK.
  Conclusion: NotebookLM successfully performed lung cancer staging by
utilizing the REK, demonstrating superior performance compared to GPT-4o.
Additionally, it provided highly accurate reference locations within the REK,
allowing radiologists to efficiently evaluate the reliability of NotebookLM's
responses and detect possible hallucinations. Overall, this study highlights
the potential of NotebookLM, a RAG-LLM, in image diagnosis.",Ryota Tozuka
2024-10-14T18:34:29Z,http://arxiv.org/abs/2410.11001v1,"Graph of Records: Boosting Retrieval Augmented Generation for
  Long-context Summarization with Graphs","Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose \textit{graph of
records} (\textbf{GoR}), which leverages historical responses generated by LLMs
to enhance RAG for long-context global summarization. Inspired by the
\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR further features a \textit{graph neural network} and an elaborately
designed \textit{BERTScore}-based objective for self-supervised model training,
enabling seamless supervision signal backpropagation between reference
summaries and node embeddings. We comprehensively compare GoR with 12 baselines
across four long-context summarization datasets, and the results indicate that
our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\%
improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP
dataset). Extensive experiments further demonstrate the effectiveness of GoR.
Code is available at https://github.com/ulab-uiuc/GoR",Haozhen Zhang
2024-10-16T05:20:32Z,http://arxiv.org/abs/2410.12248v1,"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for
  Retrieval-Augmented Generation with Enhanced Data Diversity","Retrieval-Augmented Generation (RAG) aims to enhance large language models
(LLMs) to generate more accurate and reliable answers with the help of the
retrieved context from external knowledge sources, thereby reducing the
incidence of hallucinations. Despite the advancements, evaluating these systems
remains a crucial research area due to the following issues: (1) Limited data
diversity: The insufficient diversity of knowledge sources and query types
constrains the applicability of RAG systems; (2) Obscure problems location:
Existing evaluation methods have difficulty in locating the stage of the RAG
pipeline where problems occur; (3) Unstable retrieval evaluation: These methods
often fail to effectively assess retrieval performance, particularly when the
chunking strategy changes. To tackle these challenges, we propose a
Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough
evaluation across the entire RAG pipeline, including chunking, retrieval,
reranking, and generation. To effectively evaluate the first three phases, we
introduce multi-granularity keywords, including coarse-grained and fine-grained
keywords, to assess the retrieved context instead of relying on the annotation
of golden chunks. Moreover, we release a holistic benchmark dataset tailored
for diverse data scenarios covering a wide range of document formats and query
types. We demonstrate the utility of the CoFE-RAG framework by conducting
experiments to evaluate each stage of RAG systems. Our evaluation method
provides unique insights into the effectiveness of RAG systems in handling
diverse data scenarios, offering a more nuanced understanding of their
capabilities and limitations.",Jintao Liu
2024-10-17T12:53:29Z,http://arxiv.org/abs/2410.13509v1,"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable
  Data Rewards","Retrieval-Augmented Generation (RAG) has proven its effectiveness in
mitigating hallucinations in Large Language Models (LLMs) by retrieving
knowledge from external resources. To adapt LLMs for RAG pipelines, current
approaches use instruction tuning to optimize LLMs, improving their ability to
utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses
on equipping LLMs to handle diverse RAG tasks using different instructions.
However, it trains RAG modules to overfit training signals and overlooks the
varying data preferences among agents within the RAG system. In this paper, we
propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG
systems by aligning data preferences between different RAG modules. DDR works
by collecting the rewards to optimize each agent with a rollout method. This
method prompts agents to sample some potential responses as perturbations,
evaluates the impact of these perturbations on the whole RAG system, and
subsequently optimizes the agent to produce outputs that improve the
performance of the RAG system. Our experiments on various knowledge-intensive
tasks demonstrate that DDR significantly outperforms the SFT method,
particularly for LLMs with smaller-scale parameters that depend more on the
retrieved knowledge. Additionally, DDR exhibits a stronger capability to align
the data preference between RAG modules. The DDR method makes generation module
more effective in extracting key information from documents and mitigating
conflicts between parametric memory and external knowledge. All codes are
available at https://github.com/OpenMatch/RAG-DDR.",Xinze Li
2024-10-17T16:18:49Z,http://arxiv.org/abs/2410.13716v1,"MIRAGE-Bench: Automatic Multilingual Benchmark Arena for
  Retrieval-Augmented Generation Systems","Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different
heuristic-based metrics for evaluation, but these require human preferences as
ground truth for reference. In contrast, arena-based benchmarks, where two
models compete each other, require an expensive Large Language Model (LLM) as a
judge for a reliable evaluation. We present an easy and efficient technique to
get the best of both worlds. The idea is to train a learning to rank model as a
""surrogate"" judge using RAG-based evaluation heuristics as input, to produce a
synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a
standardized arena-based multilingual RAG benchmark for 18 diverse languages on
Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and
extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG
extensively coupling both heuristic features and LLM as a judge evaluator. In
our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high
correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned
using heuristic features with pairwise evaluations and between GPT-4o as a
teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We
observe proprietary and large open-source LLMs currently dominate in
multilingual RAG. MIRAGE-Bench is available at:
https://github.com/vectara/mirage-bench.",Nandan Thakur
2024-10-18T22:07:36Z,http://arxiv.org/abs/2410.14881v2,"Class-RAG: Real-Time Content Moderation with Retrieval Augmented
  Generation","Robust content moderation classifiers are essential for the safety of
Generative AI systems. In this task, differences between safe and unsafe inputs
are often extremely subtle, making it difficult for classifiers (and indeed,
even humans) to properly distinguish violating vs. benign samples without
context or explanation. Scaling risk discovery and mitigation through
continuous model fine-tuning is also slow, challenging and costly, preventing
developers from being able to respond quickly and effectively to emergent
harms. We propose a Classification approach employing Retrieval-Augmented
Generation (Class-RAG). Class-RAG extends the capability of its base LLM
through access to a retrieval library which can be dynamically updated to
enable semantic hotfixing for immediate, flexible risk mitigation. Compared to
model fine-tuning, Class-RAG demonstrates flexibility and transparency in
decision-making, outperforms on classification and is more robust against
adversarial attack, as evidenced by empirical studies. Our findings also
suggest that Class-RAG performance scales with retrieval library size,
indicating that increasing the library size is a viable and low-cost approach
to improve content moderation.",Jianfa Chen
2024-10-20T03:51:01Z,http://arxiv.org/abs/2410.15267v1,"When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep
  Secret or Forget Knowledge?","The deployment of large language models (LLMs) like ChatGPT and Gemini has
shown their powerful natural language generation capabilities. However, these
models can inadvertently learn and retain sensitive information and harmful
content during training, raising significant ethical and legal concerns. To
address these issues, machine unlearning has been introduced as a potential
solution. While existing unlearning methods take into account the specific
characteristics of LLMs, they often suffer from high computational demands,
limited applicability, or the risk of catastrophic forgetting. To address these
limitations, we propose a lightweight unlearning framework based on
Retrieval-Augmented Generation (RAG) technology. By modifying the external
knowledge base of RAG, we simulate the effects of forgetting without directly
interacting with the unlearned LLM. We approach the construction of unlearned
knowledge as a constrained optimization problem, deriving two key components
that underpin the effectiveness of RAG-based unlearning. This RAG-based
approach is particularly effective for closed-source LLMs, where existing
unlearning methods often fail. We evaluate our framework through extensive
experiments on both open-source and closed-source models, including ChatGPT,
Gemini, Llama-2-7b-chat-hf, and PaLM 2. The results demonstrate that our
approach meets five key unlearning criteria: effectiveness, universality,
harmlessness, simplicity, and robustness. Meanwhile, this approach can extend
to multimodal large language models and LLM-based agents.",Shang Wang
2024-10-21T01:36:08Z,http://arxiv.org/abs/2410.15572v1,"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka
  Chatbots: Design Insights and User Perceptions","In an era where cultural preservation is increasingly intertwined with
technological innovation, this study introduces a groundbreaking approach to
promoting and safeguarding the rich heritage of Taiwanese Hakka culture through
the development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.
Traditional large language models (LLMs), while powerful, often fall short in
delivering accurate and contextually rich responses, particularly in culturally
specific domains. By integrating external databases with generative AI models,
RAG technology bridges this gap, empowering chatbots to not only provide
precise answers but also resonate deeply with the cultural nuances that are
crucial for authentic interactions. This study delves into the intricate
process of augmenting the chatbot's knowledge base with targeted cultural data,
specifically curated to reflect the unique aspects of Hakka traditions,
language, and practices. Through dynamic information retrieval, the
RAG-enhanced chatbot becomes a versatile tool capable of handling complex
inquiries that demand an in-depth understanding of Hakka cultural context. This
is particularly significant in an age where digital platforms often dilute
cultural identities, making the role of culturally aware AI systems more
critical than ever. System usability studies conducted as part of our research
reveal a marked improvement in both user satisfaction and engagement,
highlighting the chatbot's effectiveness in fostering a deeper connection with
Hakka culture. The feedback underscores the potential of RAG technology to not
only enhance user experience but also to serve as a vital instrument in the
broader mission of ethnic mainstreaming and cultural celebration.",Chen-Chi Chang
2024-10-23T11:32:46Z,http://arxiv.org/abs/2410.17783v1,"Leveraging the Domain Adaptation of Retrieval Augmented Generation
  Models for Question Answering and Reducing Hallucination","While ongoing advancements in Large Language Models have demonstrated
remarkable success across various NLP tasks, Retrieval Augmented Generation
Model stands out to be highly effective on downstream applications like
Question Answering. Recently, RAG-end2end model further optimized the
architecture and achieved notable performance improvements on domain
adaptation. However, the effectiveness of these RAG-based architectures remains
relatively unexplored when fine-tuned on specialized domains such as customer
service for building a reliable conversational AI system. Furthermore, a
critical challenge persists in reducing the occurrence of hallucinations while
maintaining high domain-specific accuracy. In this paper, we investigated the
performance of diverse RAG and RAG-like architectures through domain adaptation
and evaluated their ability to generate accurate and relevant response grounded
in the contextual knowledge base. To facilitate the evaluation of the models,
we constructed a novel dataset HotelConvQA, sourced from wide range of
hotel-related conversations and fine-tuned all the models on our domain
specific dataset. We also addressed a critical research gap on determining the
impact of domain adaptation on reducing hallucinations across different RAG
architectures, an aspect that was not properly measured in prior work. Our
evaluation shows positive results in all metrics by employing domain
adaptation, demonstrating strong performance on QA tasks and providing insights
into their efficacy in reducing hallucinations. Our findings clearly indicate
that domain adaptation not only enhances the models' performance on QA tasks
but also significantly reduces hallucination across all evaluated RAG
architectures.",Salman Rakin
2024-10-23T17:24:58Z,http://arxiv.org/abs/2410.18050v2,"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for
  Long-Context Question Answering","Long-Context Question Answering (LCQA), a challenging task, aims to reason
over long-context documents to yield accurate answers to questions. Existing
long-context Large Language Models (LLMs) for LCQA often struggle with the
""lost in the middle"" issue. Retrieval-Augmented Generation (RAG) mitigates this
issue by providing external factual evidence. However, its chunking strategy
disrupts the global long-context information, and its low-quality retrieval in
long contexts hinders LLMs from identifying effective factual details due to
substantial noise. To this end, we propose LongRAG, a general,
dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance
RAG's understanding of complex long-context knowledge (i.e., global information
and factual details). We design LongRAG as a plug-and-play paradigm,
facilitating adaptation to various domains and LLMs. Extensive experiments on
three multi-hop datasets demonstrate that LongRAG significantly outperforms
long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG
(up by 17.25%). Furthermore, we conduct quantitative ablation studies and
multi-dimensional analyses, highlighting the effectiveness of the system's
components and fine-tuning strategies. Data and code are available at
https://github.com/QingFei1/LongRAG.",Qingfei Zhao
2024-10-26T10:43:39Z,http://arxiv.org/abs/2410.20142v1,"Mask-based Membership Inference Attacks for Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) has been an effective approach to
mitigate hallucinations in large language models (LLMs) by incorporating
up-to-date and domain-specific knowledge. Recently, there has been a trend of
storing up-to-date or copyrighted data in RAG knowledge databases instead of
using it for LLM training. This practice has raised concerns about Membership
Inference Attacks (MIAs), which aim to detect if a specific target document is
stored in the RAG system's knowledge database so as to protect the rights of
data producers. While research has focused on enhancing the trustworthiness of
RAG systems, existing MIAs for RAG systems remain largely insufficient.
Previous work either relies solely on the RAG system's judgment or is easily
influenced by other documents or the LLM's internal knowledge, which is
unreliable and lacks explainability. To address these limitations, we propose a
Mask-Based Membership Inference Attacks (MBA) framework. Our framework first
employs a masking algorithm that effectively masks a certain number of words in
the target document. The masked text is then used to prompt the RAG system, and
the RAG system is required to predict the mask values. If the target document
appears in the knowledge database, the masked text will retrieve the complete
target document as context, allowing for accurate mask prediction. Finally, we
adopt a simple yet effective threshold-based method to infer the membership of
target document by analyzing the accuracy of mask prediction. Our mask-based
approach is more document-specific, making the RAG system's generation less
susceptible to distractions from other documents or the LLM's internal
knowledge. Extensive experiments demonstrate the effectiveness of our approach
compared to existing baseline models.",Mingrui Liu
2024-10-27T21:12:12Z,http://arxiv.org/abs/2410.20598v2,"R^3AG: First Workshop on Refined and Reliable Retrieval Augmented
  Generation","Retrieval-augmented generation (RAG) has gained wide attention as the key
component to improve generative models with external knowledge augmentation
from information retrieval. It has shown great prominence in enhancing the
functionality and performance of large language model (LLM)-based applications.
However, with the comprehensive application of RAG, more and more problems and
limitations have been identified, thus urgently requiring further fundamental
exploration to improve current RAG frameworks. This workshop aims to explore in
depth how to conduct refined and reliable RAG for downstream AI tasks.
  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024
to call for participants to re-examine and formulate the basic principles and
practical implementation of refined and reliable RAG. The workshop serves as a
platform for both academia and industry researchers to conduct discussions,
share insights, and foster research to build the next generation of RAG
systems. Participants will engage in discussions and presentations focusing on
fundamental challenges, cutting-edge research, and potential pathways to
improve RAG. At the end of the workshop, we aim to have a clearer understanding
of how to improve the reliability and applicability of RAG with more robust
information retrieval and language generation.",Zihan Wang
2024-10-28T04:39:32Z,http://arxiv.org/abs/2410.20724v2,"Simple is Effective: The Roles of Graphs and Large Language Models in
  Knowledge-Graph-Based Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.",Mufei Li
2024-10-29T11:53:19Z,http://arxiv.org/abs/2410.21970v1,"Not All Languages are Equal: Insights into Multilingual
  Retrieval-Augmented Generation","RALMs (Retrieval-Augmented Language Models) broaden their knowledge scope by
incorporating external textual resources. However, the multilingual nature of
global knowledge necessitates RALMs to handle diverse languages, a topic that
has received limited research focus. In this work, we propose
\textit{Futurepedia}, a carefully crafted benchmark containing parallel texts
across eight representative languages. We evaluate six multilingual RALMs using
our benchmark to explore the challenges of multilingual RALMs. Experimental
results reveal linguistic inequalities: 1) high-resource languages stand out in
Monolingual Knowledge Extraction; 2) Indo-European languages lead RALMs to
provide answers directly from documents, alleviating the challenge of
expressing answers across languages; 3) English benefits from RALMs' selection
bias and speaks louder in multilingual knowledge selection. Based on these
findings, we offer advice for improving multilingual Retrieval Augmented
Generation. For monolingual knowledge extraction, careful attention must be
paid to cascading errors from translating low-resource languages into
high-resource ones. In cross-lingual knowledge transfer, encouraging RALMs to
provide answers within documents in different languages can improve transfer
performance. For multilingual knowledge selection, incorporating more
non-English documents and repositioning English documents can help mitigate
RALMs' selection bias. Through comprehensive experiments, we underscore the
complexities inherent in multilingual RALMs and offer valuable insights for
future research.",Suhang Wu
2024-11-01T01:40:23Z,http://arxiv.org/abs/2411.00300v1,"Rationale-Guided Retrieval Augmented Generation for Medical Question
  Answering","Large language models (LLM) hold significant potential for applications in
biomedicine, but they struggle with hallucinations and outdated knowledge.
While retrieval-augmented generation (RAG) is generally employed to address
these issues, it also has its own set of challenges: (1) LLMs are vulnerable to
irrelevant or incorrect context, (2) medical queries are often not
well-targeted for helpful information, and (3) retrievers are prone to bias
toward the specific source corpus they were trained on. In this study, we
present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the
reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key
innovations: a small filtering model trained on perplexity-based labels of
rationales, which selectively augments informative snippets of documents while
filtering out distractors; LLM-generated rationales as queries to improve the
utility of retrieved snippets; a structure designed to retrieve snippets evenly
from a comprehensive set of four biomedical corpora, effectively mitigating
retriever bias. Our experiments demonstrate that RAG$^2$ improves the
state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\%, and
it outperforms the previous best medical RAG model by up to 5.6\% across three
medical question-answering benchmarks. Our code is available at
https://github.com/dmis-lab/RAG2.",Jiwoong Sohn
2024-11-03T22:27:40Z,http://arxiv.org/abs/2411.01705v1,Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.",Yuefeng Peng
2024-11-05T09:27:21Z,http://arxiv.org/abs/2411.02937v3,"Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA
  Dataset and Self-adaptive Planning Agent","Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the ""hallucination"" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of ""dynamic""
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.",Yangning Li
2024-11-06T14:42:39Z,http://arxiv.org/abs/2411.03957v1,"Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.",Yuhang Liu
2024-11-07T19:50:28Z,http://arxiv.org/abs/2411.05141v1,"Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with
  Retrieval-Augmented Generation","Current leading Text-To-Audio (TTA) generation models suffer from degraded
performance on zero-shot and few-shot settings. It is often challenging to
generate high-quality audio for audio events that are unseen or uncommon in the
training set. Inspired by the success of Retrieval-Augmented Generation (RAG)
in Large Language Model (LLM)-based knowledge-intensive tasks, we extend the
TTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,
a novel retrieval-augmented TTA approach based on Audiobox, a conditional
flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution
which generates audio conditioned on text, we augmented the conditioning input
with retrieved audio samples that provide additional acoustic information to
generate the target audio. Our retrieval method does not require the external
database to have labeled audio, offering more practical use cases. To evaluate
our proposed method, we curated test sets in zero-shot and few-shot settings.
Our empirical results show that the proposed model can effectively leverage the
retrieved audio samples and significantly improve zero-shot and few-shot TTA
performance, with large margins on multiple evaluation metrics, while
maintaining the ability to generate semantically aligned audio for the
in-domain setting. In addition, we investigate the effect of different
retrieval methods and data sources.",Mu Yang
2024-11-09T02:13:14Z,http://arxiv.org/abs/2411.06037v2,Sufficient Context: A New Lens on Retrieval Augmented Generation Systems,"Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a way to classify instances that have enough information to answer
the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the
context is sufficient, but often output incorrect answers instead of abstaining
when the context is not. On the other hand, open-source LLMs (Llama, Mistral,
Gemma) hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2-10% for Gemini, GPT,
and Gemma.",Hailey Joren
2024-11-12T13:14:09Z,http://arxiv.org/abs/2411.07773v1,Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.",Tianyu Liu
2024-11-21T13:18:03Z,http://arxiv.org/abs/2411.14110v1,"RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented
  Generation Applications with Agent-based Attacks","While large language models (LLMs) have achieved notable success in
generative tasks, they still face limitations, such as lacking up-to-date
knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)
enhances LLM performance by integrating external knowledge bases, providing
additional context which significantly improves accuracy and knowledge
coverage. However, building these external knowledge bases often requires
substantial resources and may involve sensitive information. In this paper, we
propose an agent-based automated privacy attack called RAG-Thief, which can
extract a scalable amount of private data from the private database used in RAG
applications. We conduct a systematic study on the privacy risks associated
with RAG applications, revealing that the vulnerability of LLMs makes the
private knowledge bases suffer significant privacy risks. Unlike previous
manual attacks which rely on traditional prompt injection techniques, RAG-Thief
starts with an initial adversarial query and learns from model responses,
progressively generating new queries to extract as many chunks from the
knowledge base as possible. Experimental results show that our RAG-Thief can
extract over 70% information from the private knowledge bases within customized
RAG applications deployed on local machines and real-world platforms, including
OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy
vulnerabilities in current RAG applications and underscore the pressing need
for stronger safeguards.",Changyue Jiang
2024-11-29T03:01:05Z,http://arxiv.org/abs/2411.19443v1,"Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language
  Models","Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.",Tian Yu
2024-12-03T17:23:47Z,http://arxiv.org/abs/2412.02592v1,"OCR Hinders RAG: Evaluating the Cascading Impact of OCR on
  Retrieval-Augmented Generation","Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge to reduce hallucinations and incorporate
up-to-date information without retraining. As an essential part of RAG,
external knowledge bases are commonly built by extracting structured data from
unstructured PDF documents using Optical Character Recognition (OCR). However,
given the imperfect prediction of OCR and the inherent non-uniform
representation of structured data, knowledge bases inevitably contain various
OCR noises. In this paper, we introduce OHRBench, the first benchmark for
understanding the cascading impact of OCR on RAG systems. OHRBench includes 350
carefully selected unstructured PDF documents from six real-world RAG
application domains, along with Q&As derived from multimodal elements in
documents, challenging existing OCR solutions used for RAG To better understand
OCR's impact on RAG systems, we identify two primary types of OCR noise:
Semantic Noise and Formatting Noise and apply perturbation to generate a set of
structured data with varying degrees of each OCR noise. Using OHRBench, we
first conduct a comprehensive evaluation of current OCR solutions and reveal
that none is competent for constructing high-quality knowledge bases for RAG
systems. We then systematically evaluate the impact of these two noise types
and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the
potential of employing Vision-Language Models (VLMs) without OCR in RAG
systems. Code: https://github.com/opendatalab/OHR-Bench",Junyuan Zhang
2024-12-06T16:22:32Z,http://arxiv.org/abs/2412.05159v1,"Enhancing Cross-Language Code Translation via Task-Specific Embedding
  Alignment in Retrieval-Augmented Generation","We introduce a novel method to enhance cross-language code translation from
Fortran to C++ by integrating task-specific embedding alignment into a
Retrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval
approaches that utilize generic embeddings agnostic to the downstream task, our
strategy aligns the retrieval model directly with the objective of maximizing
translation quality, as quantified by the CodeBLEU metric. This alignment
ensures that the embeddings are semantically and syntactically meaningful for
the specific code translation task. Our methodology involves constructing a
dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and
generating their corresponding C++ translations using the LLaMA 3.1-8B language
model. We compute pairwise CodeBLEU scores between the generated translations
and ground truth examples to capture fine-grained similarities. These scores
serve as supervision signals in a contrastive learning framework, where we
optimize the embedding model to retrieve Fortran-C++ pairs that are most
beneficial for improving the language model's translation performance. By
integrating these CodeBLEU-optimized embeddings into the RAG framework, our
approach significantly enhances both retrieval accuracy and code generation
quality over methods employing generic embeddings. On the HPC Fortran2C++
dataset, our method elevates the average CodeBLEU score from 0.64 to 0.73,
achieving a 14% relative improvement. On the Numerical Recipes dataset, we
observe an increase from 0.52 to 0.60, marking a 15% relative improvement.
Importantly, these gains are realized without any fine-tuning of the language
model, underscoring the efficiency and practicality of our approach.",Manish Bhattarai
2024-12-10T04:55:57Z,http://arxiv.org/abs/2412.07189v1,"When Graph Meets Retrieval Augmented Generation for Wireless Networks: A
  Tutorial and Case Study","The rapid development of next-generation networking technologies underscores
their transformative role in revolutionizing modern communication systems,
enabling faster, more reliable, and highly interconnected solutions. However,
such development has also brought challenges to network optimizations. Thanks
to the emergence of Large Language Models (LLMs) in recent years, tools
including Retrieval Augmented Generation (RAG) have been developed and applied
in various fields including networking, and have shown their effectiveness.
Taking one step further, the integration of knowledge graphs into RAG
frameworks further enhanced the performance of RAG in networking applications
such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing
more contextually relevant responses through more accurate retrieval of related
network information. This paper introduces the RAG framework that integrates
knowledge graphs in its database and explores such framework's application in
networking. We begin by exploring RAG's applications in networking and the
limitations of conventional RAG and present the advantages that knowledge
graphs' structured knowledge representation brings to the retrieval and
generation processes. Next, we propose a detailed GraphRAG-based framework for
networking, including a step-by-step tutorial on its construction. Our
evaluation through a case study on channel gain prediction demonstrates
GraphRAG's enhanced capability in generating accurate, contextually rich
responses, surpassing traditional RAG models. Finally, we discuss key future
directions for applying knowledge-graphs-empowered RAG frameworks in
networking, including robust updates, mitigation of hallucination, and enhanced
security measures for networking applications.",Yang Xiong
2024-12-10T15:56:03Z,http://arxiv.org/abs/2412.07618v2,"Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced
  Retrieval-Augmented Generation on Knowledge Graphs","Despite the superior performance of Large language models on many NLP tasks,
they still face significant limitations in memorizing extensive world
knowledge. Recent studies have demonstrated that leveraging the
Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs
that encapsulate extensive factual data in a structured format, robustly
enhances the reasoning capabilities of LLMs. However, deploying such systems in
real-world scenarios presents challenges: the continuous evolution of
non-stationary environments may lead to performance degradation and user
satisfaction requires a careful balance of performance and responsiveness. To
address these challenges, we introduce a Multi-objective Multi-Armed Bandit
enhanced RAG framework, supported by multiple retrieval methods with diverse
capabilities under rich and evolving retrieval contexts in practice. Within
this framework, each retrieval method is treated as a distinct ``arm''. The
system utilizes real-time user feedback to adapt to dynamic environments, by
selecting the appropriate retrieval method based on input queries and the
historical multi-objective performance of each arm. Extensive experiments
conducted on two benchmark KGQA datasets demonstrate that our method
significantly outperforms baseline methods in non-stationary settings while
achieving state-of-the-art performance in stationary environments. Code and
data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",Xiaqiang Tang
2024-12-16T17:32:38Z,http://arxiv.org/abs/2412.12006v2,"Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A
  Novel Weighted Retrieval-Augmented Generation Paradigm","Technical troubleshooting in enterprise environments often involves
navigating diverse, heterogeneous data sources to resolve complex issues
effectively. This paper presents a novel agentic AI solution built on a
Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise
technical troubleshooting. By dynamically weighting retrieval sources such as
product manuals, internal knowledge bases, FAQs, and troubleshooting guides
based on query context, the framework prioritizes the most relevant data. For
instance, it gives precedence to product manuals for SKU-specific queries while
incorporating general FAQs for broader issues. The system employs FAISS for
efficient dense vector search, coupled with a dynamic aggregation mechanism to
seamlessly integrate results from multiple sources. A Llama-based
self-evaluator ensures the contextual accuracy and confidence of the generated
responses before delivering them. This iterative cycle of retrieval and
validation enhances precision, diversity, and reliability in response
generation. Preliminary evaluations on large enterprise datasets demonstrate
the framework's efficacy in improving troubleshooting accuracy, reducing
resolution times, and adapting to varied technical challenges. Future research
aims to enhance the framework by integrating advanced conversational AI
capabilities, enabling more interactive and intuitive troubleshooting
experiences. Efforts will also focus on refining the dynamic weighting
mechanism through reinforcement learning to further optimize the relevance and
precision of retrieved information. By incorporating these advancements, the
proposed framework is poised to evolve into a comprehensive, autonomous AI
solution, redefining technical service workflows across enterprise settings.",Rajat Khanda
2024-12-17T05:38:27Z,http://arxiv.org/abs/2412.12559v2,"EXIT: Context-Aware Extractive Compression for Enhancing
  Retrieval-Augmented Generation","We introduce EXIT, an extractive context compression framework that enhances
both the effectiveness and efficiency of retrieval-augmented generation (RAG)
in question answering (QA). Current RAG systems often struggle when retrieval
models fail to rank the most relevant documents, leading to the inclusion of
more context at the expense of latency and accuracy. While abstractive
compression methods can drastically reduce token counts, their token-by-token
generation process significantly increases end-to-end latency. Conversely,
existing extractive methods reduce latency but rely on independent,
non-adaptive sentence selection, failing to fully utilize contextual
information. EXIT addresses these limitations by classifying sentences from
retrieved documents - while preserving their contextual dependencies - enabling
parallelizable, context-aware extraction that adapts to query complexity and
retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks
show that EXIT consistently surpasses existing compression methods and even
uncompressed baselines in QA accuracy, while also delivering substantial
reductions in inference time and token count. By improving both effectiveness
and efficiency, EXIT provides a promising direction for developing scalable,
high-quality QA solutions in RAG pipelines. Our code is available at
https://github.com/ThisIsHwang/EXIT",Taeho Hwang
2024-12-19T04:18:51Z,http://arxiv.org/abs/2412.14510v1,PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization,"The emergence of Retrieval-augmented generation (RAG) has alleviated the
issues of outdated and hallucinatory content in the generation of large
language models (LLMs), yet it still reveals numerous limitations. When a
general-purpose LLM serves as the RAG generator, it often suffers from
inadequate response informativeness, response robustness, and citation quality.
Past approaches to tackle these limitations, either by incorporating additional
steps beyond generating responses or optimizing the generator through
supervised fine-tuning (SFT), still failed to align with the RAG requirement
thoroughly. Consequently, optimizing the RAG generator from multiple preference
perspectives while maintaining its end-to-end LLM form remains a challenge. To
bridge this gap, we propose Multiple Perspective Preference Alignment for
Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator
of RAG systems to align with RAG requirements comprehensively. Specifically, we
construct high-quality instruction fine-tuning data and multi-perspective
preference data by sampling varied quality responses from the generator across
different prompt documents quality scenarios. Subsequently, we optimize the
generator using SFT and Direct Preference Optimization (DPO). Extensive
experiments conducted on four question-answer datasets across three LLMs
demonstrate that PA-RAG can significantly enhance the performance of RAG
generators. Our code and datasets are available at
https://github.com/wujwyi/PA-RAG.",Jiayi Wu
2024-12-12T01:21:03Z,http://arxiv.org/abs/2412.15235v1,"OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large
  Language Models","This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented
Generation method designed to enhance LLM-generated responses by anchoring
retrieval processes in domain-specific ontologies. While LLMs are widely used
for tasks like question answering and search, they struggle to adapt to
specialized knowledge, such as industrial workflows or knowledge work, without
expensive fine-tuning or sub-optimal retrieval methods. Existing
retrieval-augmented models, such as RAG, offer improvements but fail to account
for structured domain knowledge, leading to suboptimal context generation.
Ontologies, which conceptually organize domain knowledge by defining entities
and their interrelationships, offer a structured representation to address this
gap. OG-RAG constructs a hypergraph representation of domain documents, where
each hyperedge encapsulates clusters of factual knowledge grounded using
domain-specific ontology. An optimization algorithm then retrieves the minimal
set of hyperedges that constructs a precise, conceptually grounded context for
the LLM. This method enables efficient retrieval while preserving the complex
relationships between entities. OG-RAG applies to domains where fact-based
reasoning is essential, particularly in tasks that require workflows or
decision-making steps to follow predefined rules and procedures. These include
industrial workflows in healthcare, legal, and agricultural sectors, as well as
knowledge-driven tasks such as news journalism, investigative research,
consulting and more. Our evaluations demonstrate that OG-RAG increases the
recall of accurate facts by 55% and improves response correctness by 40% across
four different LLMs. Additionally, OG-RAG enables 30% faster attribution of
responses to context and boosts fact-based reasoning accuracy by 27% compared
to baseline methods.",Kartik Sharma
2024-12-14T06:47:56Z,http://arxiv.org/abs/2412.15246v1,Accelerating Retrieval-Augmented Generation,"An evolving solution to address hallucination and enhance accuracy in large
language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves
augmenting LLMs with information retrieved from an external knowledge source,
such as the web. This paper profiles several RAG execution pipelines and
demystifies the complex interplay between their retrieval and generation
phases. We demonstrate that while exact retrieval schemes are expensive, they
can reduce inference time compared to approximate retrieval variants because an
exact retrieval model can send a smaller but more accurate list of documents to
the generative model while maintaining the same end-to-end accuracy. This
observation motivates the acceleration of the exact nearest neighbor search for
RAG.
  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL
device that implements a scale-out near-memory acceleration architecture with a
novel cache-coherent interface between the host CPU and near-memory
accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a
512GB vector database compared with executing the search on Intel Sapphire
Rapids CPUs. This higher search performance translates to 1.7-26.3x lower
end-to-end inference time for representative RAG applications. IKS is
inherently a memory expander; its internal DRAM can be disaggregated and used
for other applications running on the server to prevent DRAM, which is the most
expensive component in today's servers, from being stranded.",Derrick Quinn
2024-12-20T03:37:07Z,http://arxiv.org/abs/2412.15529v2,"XRAG: eXamining the Core -- Benchmarking Foundational Components in
  Advanced Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent
data with the generative capabilities of Large Language Models (LLMs), ensuring
that the generated output is not only contextually relevant but also accurate
and current. We introduce XRAG, an open-source, modular codebase that
facilitates exhaustive evaluation of the performance of foundational components
of advanced RAG modules. These components are systematically categorized into
four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We
systematically analyse them across reconfigured datasets, providing a
comprehensive benchmark for their effectiveness. As the complexity of RAG
systems continues to escalate, we underscore the critical need to identify
potential failure points in RAG systems. We formulate a suite of experimental
methodologies and diagnostic testing protocols to dissect the failure points
inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed
at bolstering the overall performance of these modules. Our work thoroughly
evaluates the performance of advanced core components in RAG systems, providing
insights into optimizations for prevalent failure points.",Qianren Mao
2024-12-20T19:49:12Z,http://arxiv.org/abs/2412.16311v1,"HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational
  Knowledge Bases","Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as ""hybrid"" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.",Meng-Chieh Lee
2024-12-21T06:16:04Z,http://arxiv.org/abs/2412.16500v1,"Speech Retrieval-Augmented Generation without Automatic Speech
  Recognition","One common approach for question answering over speech data is to first
transcribe speech using automatic speech recognition (ASR) and then employ
text-based retrieval-augmented generation (RAG) on the transcriptions. While
this cascaded pipeline has proven effective in many practical settings, ASR
errors can propagate to the retrieval and generation steps. To overcome this
limitation, we introduce SpeechRAG, a novel framework designed for
open-question answering over spoken data. Our proposed approach fine-tunes a
pre-trained speech encoder into a speech adapter fed into a frozen large
language model (LLM)--based retrieval model. By aligning the embedding spaces
of text and speech, our speech retriever directly retrieves audio passages from
text-based queries, leveraging the retrieval capacity of the frozen text
retriever. Our retrieval experiments on spoken question answering datasets show
that direct speech retrieval does not degrade over the text-based baseline, and
outperforms the cascaded systems using ASR. For generation, we use a speech
language model (SLM) as a generator, conditioned on audio passages rather than
transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded
text-based models when there is high WER in the transcripts.",Do June Min
2024-12-22T14:16:38Z,http://arxiv.org/abs/2412.17031v1,"A Reality Check on Context Utilisation for Retrieval-Augmented
  Generation","Retrieval-augmented generation (RAG) helps address the limitations of the
parametric knowledge embedded within a language model (LM). However,
investigations of how LMs utilise retrieved information of varying complexity
in real-world scenarios have been limited to synthetic contexts. We introduce
DRUID (Dataset of Retrieved Unreliable, Insufficient and
Difficult-to-understand contexts) with real-world queries and contexts manually
annotated for stance. The dataset is based on the prototypical task of
automated claim verification, for which automated retrieval of real-world
evidence is crucial. We compare DRUID to synthetic datasets (CounterFact,
ConflictQA) and find that artificial datasets often fail to represent the
complex and diverse real-world context settings. We show that synthetic
datasets exaggerate context characteristics rare in real retrieved data, which
leads to inflated context utilisation results, as measured by our novel ACU
score. Moreover, while previous work has mainly focused on singleton context
characteristics to explain context utilisation, correlations between singleton
context properties and ACU on DRUID are surprisingly small compared to other
properties related to context source. Overall, our work underscores the need
for real-world aligned context utilisation studies to represent and improve
performance in real-world RAG settings.",Lovisa Hagström
2020-05-22T21:34:34Z,http://arxiv.org/abs/2005.11401v4,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when fine-tuned on
downstream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their
performance lags behind task-specific architectures. Additionally, providing
provenance for their decisions and updating their world knowledge remain open
research problems. Pre-trained models with a differentiable access mechanism to
explicit non-parametric memory can overcome this issue, but have so far been
only investigated for extractive downstream tasks. We explore a general-purpose
fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
combine pre-trained parametric and non-parametric memory for language
generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index
of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
formulations, one which conditions on the same retrieved passages across the
whole generated sequence, the other can use different passages per token. We
fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
tasks and set the state-of-the-art on three open domain QA tasks, outperforming
parametric seq2seq models and task-specific retrieve-and-extract architectures.
For language generation tasks, we find that RAG models generate more specific,
diverse and factual language than a state-of-the-art parametric-only seq2seq
baseline.",Patrick Lewis
2024-01-24T06:50:20Z,http://arxiv.org/abs/2401.13256v3,"UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for
  Personalized Dialogue Systems","Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.",Hongru Wang
2024-01-30T14:25:32Z,http://arxiv.org/abs/2401.17043v3,"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
  Generation of Large Language Models","Retrieval-Augmented Generation (RAG) is a technique that enhances the
capabilities of large language models (LLMs) by incorporating external
knowledge sources. This method addresses common LLM limitations, including
outdated information and the tendency to produce inaccurate ""hallucinated""
content. However, the evaluation of RAG systems is challenging, as existing
benchmarks are limited in scope and diversity. Most of the current benchmarks
predominantly assess question-answering applications, overlooking the broader
spectrum of situations where RAG could prove advantageous. Moreover, they only
evaluate the performance of the LLM component of the RAG pipeline in the
experiments, and neglect the influence of the retrieval component and the
external knowledge database. To address these issues, this paper constructs a
large-scale and more comprehensive benchmark, and evaluates all the components
of RAG systems in various RAG application scenarios. Specifically, we have
categorized the range of RAG applications into four distinct types-Create,
Read, Update, and Delete (CRUD), each representing a unique use case. ""Create""
refers to scenarios requiring the generation of original, varied content.
""Read"" involves responding to intricate questions in knowledge-intensive
situations. ""Update"" focuses on revising and rectifying inaccuracies or
inconsistencies in pre-existing texts. ""Delete"" pertains to the task of
summarizing extensive texts into more concise forms. For each of these CRUD
categories, we have developed comprehensive datasets to evaluate the
performance of RAG systems. We also analyze the effects of various components
of the RAG system, such as the retriever, the context length, the knowledge
base construction, and the LLM. Finally, we provide useful insights for
optimizing the RAG technology for different scenarios.",Yuanjie Lyu
2024-02-01T00:07:23Z,http://arxiv.org/abs/2402.00247v2,Towards AI-Assisted Synthesis of Verified Dafny Methods,"Large language models show great promise in many domains, including
programming. A promise is easy to make but hard to keep, and language models
often fail to keep their promises, generating erroneous code. A promising
avenue to keep models honest is to incorporate formal verification: generating
programs' specifications as well as code so that the code can be proved correct
with respect to the specifications. Unfortunately, existing large language
models show a severe lack of proficiency in verified programming.
  In this paper, we demonstrate how to improve two pretrained models'
proficiency in the Dafny verification-aware language. Using 178 problems from
the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to
synthesize Dafny methods. We use three different types of prompts: a direct
Contextless prompt; a Signature prompt that includes a method signature and
test cases, and a Chain of Thought (CoT) prompt that decomposes the problem
into steps and includes retrieval augmentation generated example problems and
solutions. Our results show that GPT-4 performs better than PaLM-2 on these
tasks and that both models perform best with the retrieval augmentation
generated CoT prompt. GPT-4 was able to generate verified, human-evaluated,
Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the
problems with the Contextless prompt, and even fewer (10%) for the Signature
prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP
problems, 50 that we wrote manually, and 103 synthesized by GPT-4.
  Our results demonstrate that the benefits of formal program verification are
now within reach of code generating large language models...",Md Rakib Hossain Misu
2024-01-29T06:49:53Z,http://arxiv.org/abs/2402.01733v1,"Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report","Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.",YuHe Ke
2024-02-12T14:53:28Z,http://arxiv.org/abs/2402.07688v2,"CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation
  for Evaluating LLMs in Cybersecurity Knowledge","Large Language Models (LLMs) are increasingly used across various domains,
from software development to cyber threat intelligence. Understanding all the
different fields of cybersecurity, which includes topics such as cryptography,
reverse engineering, and risk assessment, poses a challenge even for human
experts. To accurately test the general knowledge of LLMs in cybersecurity, the
research community needs a diverse, accurate, and up-to-date dataset. To
address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000,
and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets
comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing
GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents,
including NIST standards, research papers, publicly accessible books, RFCs, and
other publications in the cybersecurity domain, to generate questions, each
with four possible answers. The results underwent several rounds of error
checking and refinement. Human experts invested over 200 hours validating the
questions and solutions to ensure their accuracy and relevance, and to filter
out any questions unrelated to cybersecurity. We have evaluated and compared 25
state-of-the-art LLM models on the CyberMetric datasets. In addition to our
primary goal of evaluating LLMs, we involved 30 human participants to solve
CyberMetric-80 in a closed-book scenario. The results can serve as a reference
for comparing the general cybersecurity knowledge of humans and LLMs. The
findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct,
Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs.
Additionally, the top LLMs were more accurate than humans on CyberMetric-80,
although highly experienced human experts still outperformed small models such
as Llama-3-8B, Phi-2 or Gemma-7b.",Norbert Tihanyi
2024-02-12T18:28:36Z,http://arxiv.org/abs/2402.07867v3,"PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented
  Generation of Large Language Models","Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate these limitations. The key idea of RAG is to ground the answer
generation of an LLM on external knowledge retrieved from a knowledge database.
Existing studies mainly focus on improving the accuracy or efficiency of RAG,
leaving its security largely unexplored. We aim to bridge the gap in this work.
We find that the knowledge database in a RAG system introduces a new and
practical attack surface. Based on this attack surface, we propose PoisonedRAG,
the first knowledge corruption attack to RAG, where an attacker could inject a
few malicious texts into the knowledge database of a RAG system to induce an
LLM to generate an attacker-chosen target answer for an attacker-chosen target
question. We formulate knowledge corruption attacks as an optimization problem,
whose solution is a set of malicious texts. Depending on the background
knowledge (e.g., black-box and white-box settings) of an attacker on a RAG
system, we propose two solutions to solve the optimization problem,
respectively. Our results show PoisonedRAG could achieve a 90% attack success
rate when injecting five malicious texts for each target question into a
knowledge database with millions of texts. We also evaluate several defenses
and our results show they are insufficient to defend against PoisonedRAG,
highlighting the need for new defenses.",Wei Zou
2024-02-29T18:59:01Z,http://arxiv.org/abs/2402.19473v6,Retrieval-Augmented Generation for AI-Generated Content: A Survey,"Advancements in model algorithms, the growth of foundational models, and
access to high-quality datasets have propelled the evolution of Artificial
Intelligence Generated Content (AIGC). Despite its notable successes, AIGC
still faces hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and inference costs.
Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to
address such challenges. In particular, RAG introduces the information
retrieval process, which enhances the generation process by retrieving relevant
objects from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing efforts that
integrate RAG technique into AIGC scenarios. We first classify RAG foundations
according to how the retriever augments the generator, distilling the
fundamental abstractions of the augmentation methodologies for various
retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",Penghao Zhao
2024-06-03T02:25:33Z,http://arxiv.org/abs/2406.00083v2,"BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of
  Large Language Models","Large Language Models (LLMs) are constrained by outdated information and a
tendency to generate incorrect data, commonly referred to as ""hallucinations.""
Retrieval-Augmented Generation (RAG) addresses these limitations by combining
the strengths of retrieval-based methods and generative models. This approach
involves retrieving relevant information from a large, up-to-date dataset and
using it to enhance the generation process, leading to more accurate and
contextually appropriate responses. Despite its benefits, RAG introduces a new
attack surface for LLMs, particularly because RAG databases are often sourced
from public data, such as the web. In this paper, we propose \TrojRAG{} to
identify the vulnerabilities and attacks on retrieval parts (RAG database) and
their indirect attacks on generative parts (LLMs). Specifically, we identify
that poisoning several customized content passages could achieve a retrieval
backdoor, where the retrieval works well for clean queries but always returns
customized poisoned adversarial queries. Triggers and poisoned passages can be
highly customized to implement various attacks. For example, a trigger could be
a semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial
passages can be tailored to different contents, not only linked to the triggers
but also used to indirectly attack generative LLMs without modifying them.
These attacks can include denial-of-service attacks on RAG and semantic
steering attacks on LLM generations conditioned by the triggers. Our
experiments demonstrate that by just poisoning 10 adversarial passages can
induce 98.2\% success rate to retrieve the adversarial passages. Then, these
passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\%
or increase the rate of negative responses from 0.22\% to 72\% for targeted
queries.",Jiaqi Xue
2024-06-11T19:20:27Z,http://arxiv.org/abs/2406.10273v5,"Beyond Words: On Large Language Models Actionability in Mission-Critical
  Risk Analysis","Context. Risk analysis assesses potential risks in specific scenarios. Risk
analysis principles are context-less; the same methodology can be applied to a
risk connected to health and information technology security. Risk analysis
requires a vast knowledge of national and international regulations and
standards and is time and effort-intensive. A large language model can quickly
summarize information in less time than a human and can be fine-tuned to
specific tasks.
  Aim. Our empirical study aims to investigate the effectiveness of
Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our
knowledge, no prior study has explored its capabilities in risk analysis.
  Method. We manually curated 193 unique scenarios leading to 1283
representative samples from over 50 mission-critical analyses archived by the
industrial context team in the last five years. We compared the base GPT-3.5
and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned
counterparts. We employ two human experts as competitors of the models and
three other human experts to review the models and the former human experts'
analysis. The reviewers analyzed 5,000 scenario analyses.
  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs
are quicker and more actionable. Moreover, our findings show that RAG-assisted
LLMs have the lowest hallucination rates, effectively uncovering hidden risks
and complementing human expertise. Thus, the choice of model depends on
specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and
base models for comprehensiveness and actionability. Therefore, experts can
leverage LLMs as an effective complementing companion in risk analysis within a
condensed timeframe. They can also save costs by averting unnecessary expenses
associated with implementing unwarranted countermeasures.",Matteo Esposito
2024-07-10T17:20:59Z,http://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key
applications to enhance employee productivity. Retrieval Augmented Generation
(RAG), Large Language Models (LLMs), and orchestration frameworks like
Langchain and Llamaindex are crucial for building these chatbots. However,
creating effective enterprise chatbots is challenging and requires meticulous
RAG pipeline engineering. This includes fine-tuning embeddings and LLMs,
extracting documents from vector databases, rephrasing queries, reranking
results, designing prompts, honoring document access controls, providing
concise responses, including references, safeguarding personal information, and
building orchestration agents. We present a framework for building RAG-based
chatbots based on our experience with three NVIDIA chatbots: for IT/HR
benefits, financial earnings, and general content. Our contributions are
three-fold: introducing the FACTS framework (Freshness, Architectures, Cost,
Testing, Security), presenting fifteen RAG pipeline control points, and
providing empirical results on accuracy-latency tradeoffs between large and
small LLMs. To the best of our knowledge, this is the first paper of its kind
that provides a holistic view of the factors as well as solutions for building
secure enterprise-grade chatbots.""",Rama Akkiraju
2024-07-15T15:20:40Z,http://arxiv.org/abs/2407.10805v6,"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has improved large language models
(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.
However, current RAG methods often fall short of ensuring the depth and
completeness of retrieved information, which is necessary for complex reasoning
tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG
framework that iteratively retrieves information from both unstructured and
structured knowledge sources in a tight-coupling manner. Specifically, ToG-2
leverages knowledge graphs (KGs) to link documents via entities, facilitating
deep and knowledge-guided context retrieval. Simultaneously, it utilizes
documents as entity contexts to achieve precise and efficient graph retrieval.
ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate answers. We
conduct a series of well-designed experiments to highlight the following
advantages of ToG-2: 1) ToG-2 tightly couples the processes of context
retrieval and graph retrieval, deepening context retrieval via the KG while
enabling reliable graph retrieval based on contexts; 2) it achieves deep and
faithful reasoning in LLMs through an iterative knowledge retrieval process of
collaboration between contexts and the KG; and 3) ToG-2 is training-free and
plug-and-play compatible with various LLMs. Extensive experiments demonstrate
that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7
knowledge-intensive datasets with GPT-3.5, and can elevate the performance of
smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
The source code is available on https://github.com/IDEA-FinAI/ToG-2.",Shengjie Ma
2024-07-22T13:29:56Z,http://arxiv.org/abs/2407.15621v1,"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in
  Radiology Using Dynamic Retrieval Augmented Generation","Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.",Soroosh Tayebi Arasteh
2024-08-01T17:18:17Z,http://arxiv.org/abs/2408.00727v3,"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions","The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a conventional RAG system
and they will be further used to guide the query generation in the next
iteration. Our experiments show the improved performance of various LLMs
brought by i-MedRAG compared with conventional RAG on complex questions from
clinical vignettes in the United States Medical Licensing Examination (USMLE),
as well as various knowledge tests in the Massive Multitask Language
Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all
existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an
accuracy of 69.68% on the MedQA dataset. In addition, we characterize the
scaling properties of i-MedRAG with different iterations of follow-up queries
and different numbers of queries per iteration. Our case studies show that
i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing
an in-depth analysis of medical questions. To the best of our knowledge, this
is the first-of-its-kind study on incorporating follow-up queries into medical
RAG. The implementation of i-MedRAG is available at
https://github.com/Teddy-XiongGZ/MedRAG.",Guangzhi Xiong
2024-08-03T22:14:13Z,http://arxiv.org/abs/2408.01869v1,"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented
  Generation for Pharmacovigilance","In the era of Large Language Models (LLMs), given their remarkable text
understanding and generation abilities, there is an unprecedented opportunity
to develop new, LLM-based methods for trustworthy medical knowledge synthesis,
extraction and summarization. This paper focuses on the problem of
Pharmacovigilance (PhV), where the significance and challenges lie in
identifying Adverse Drug Events (ADEs) from diverse text sources, such as
medical literature, clinical notes, and drug labels. Unfortunately, this task
is hindered by factors including variations in the terminologies of drugs and
outcomes, and ADE descriptions often being buried in large amounts of narrative
text. We present MALADE, the first effective collaborative multi-agent system
powered by LLM with Retrieval Augmented Generation for ADE extraction from drug
label data. This technique involves augmenting a query to an LLM with relevant
information extracted from text resources, and instructing the LLM to compose a
response consistent with the augmented data. MALADE is a general LLM-agnostic
architecture, and its unique capabilities are: (1) leveraging a variety of
external sources, such as medical literature, drug labels, and FDA tools (e.g.,
OpenFDA drug information API), (2) extracting drug-outcome association in a
structured format along with the strength of the association, and (3) providing
explanations for established associations. Instantiated with GPT-4 Turbo or
GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area
Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our
implementation leverages the Langroid multi-agent LLM framework and can be
found at https://github.com/jihyechoi77/malade.",Jihye Choi
2024-08-14T13:22:14Z,http://arxiv.org/abs/2408.07542v1,"New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson
  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation","Introduction: Poor educational quality in Secondary Schools is still regarded
as one of the major struggles in 21st century Uganda - especially in rural
areas. Research identifies several problems, including low quality or absent
teacher lesson planning. As the government pushes towards the implementation of
a new curriculum, exiting lesson plans become obsolete and the problem is
worsened. Using a Retrieval Augmented Generation approach, we developed a
prototype that generates customized lesson plans based on the
government-accredited textbooks. This helps teachers create lesson plans more
efficiently and with better quality, ensuring they are fully aligned the new
curriculum and the competence-based learning approach.
  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,
and LangChain Framework - and thereafter made available on a public website.
Vector stores were trained for three new curriculum textbooks (ICT,
Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were
generated following a pseudo-random generation protocol, based on the suggested
periods in the textbooks. The lesson plans were analyzed regarding their
technical quality by three independent raters following the Lesson Plan
Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically
designed for East Africa and competence-based curriculums.
  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average
quality of between 75 and 80%, corresponding to ""very good lesson plan"". None
of the lesson plans scored below 65%, although one lesson plan could be argued
to have been missing the topic. In conclusion, the quality of the generated
lesson plans is at least comparable, if not better, than those created by
humans, as demonstrated in a study in Rwanda, whereby no lesson plan even
reached the benchmark of 50%.",Simon Kloker
2024-08-31T16:14:42Z,http://arxiv.org/abs/2409.00494v2,"GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:
  Opportunities and Challenges for Integrating Large Language Models (LLMs) and
  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems","Leveraging recent advances in generative AI, multi-agent systems are
increasingly being developed to enhance the functionality and efficiency of
smart city applications. This paper explores the transformative potential of
large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)
technologies in Intelligent Transportation Systems (ITS), paving the way for
innovative solutions to address critical challenges in urban mobility. We begin
by providing a comprehensive overview of the current state-of-the-art in
mobility data, ITS, and Connected Vehicles (CV) applications. Building on this
review, we discuss the rationale behind RAG and examine the opportunities for
integrating these Generative AI (GenAI) technologies into the smart mobility
sector. We propose a conceptual framework aimed at developing multi-agent
systems capable of intelligently and conversationally delivering smart mobility
services to urban commuters, transportation operators, and decision-makers. Our
approach seeks to foster an autonomous and intelligent approach that (a)
promotes science-based advisory to reduce traffic congestion, accidents, and
carbon emissions at multiple scales, (b) facilitates public education and
engagement in participatory mobility management, and (c) automates specialized
transportation management tasks and the development of critical ITS platforms,
such as data analytics and interpretation, knowledge representation, and
traffic simulations. By integrating LLM and RAG, our approach seeks to overcome
the limitations of traditional rule-based multi-agent systems, which rely on
fixed knowledge bases and limited reasoning capabilities. This integration
paves the way for a more scalable, intuitive, and automated multi-agent
paradigm, driving advancements in ITS and urban mobility.",Haowen Xu
2024-09-15T15:21:45Z,http://arxiv.org/abs/2409.10576v2,"Language Models and Retrieval Augmented Generation for Automated
  Structured Data Extraction from Diagnostic Reports","Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.",Mohamed Sobhi Jabal
2024-09-20T21:06:00Z,http://arxiv.org/abs/2409.13902v1,"Enhancing Large Language Models with Domain-specific Retrieval Augment
  Generation: A Case Study on Long-form Consumer Health Question Answering in
  Ophthalmology","Despite the potential of Large Language Models (LLMs) in medicine, they may
generate responses lacking supporting evidence or based on hallucinated
evidence. While Retrieval Augment Generation (RAG) is popular to address this
issue, few studies implemented and evaluated RAG in downstream domain-specific
applications. We developed a RAG pipeline with 70,000 ophthalmology-specific
documents that retrieve relevant documents to augment LLMs during inference
time. In a case study on long-form consumer health questions, we systematically
evaluated the responses including over 500 references of LLMs with and without
RAG on 100 questions with 10 healthcare professionals. The evaluation focuses
on factuality of evidence, selection and ranking of evidence, attribution of
evidence, and answer accuracy and completeness. LLMs without RAG provided 252
references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor
errors, and 20.6% were correct. In contrast, LLMs with RAG significantly
improved accuracy (54.5% being correct) and reduced error rates (18.8% with
minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents
retrieved by RAG were selected as the top references in the LLM response, with
an average ranking of 4.9. The use of RAG also improved evidence attribution
(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight
decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47
to 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited
hallucinated and erroneous evidence in the responses, raising concerns for
downstream applications in the medical domain. RAG substantially reduced the
proportion of such evidence but encountered challenges.",Aidan Gilson
2024-09-23T11:20:20Z,http://arxiv.org/abs/2409.14924v1,"Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey
  on How to Make your LLMs use External Data More Wisely","Large language models (LLMs) augmented with external data have demonstrated
remarkable capabilities in completing real-world tasks. Techniques for
integrating external data into LLMs, such as Retrieval-Augmented Generation
(RAG) and fine-tuning, are gaining increasing attention and widespread
application. Nonetheless, the effective deployment of data-augmented LLMs
across various specialized fields presents substantial challenges. These
challenges encompass a wide range of issues, from retrieving relevant data and
accurately interpreting user intent to fully harnessing the reasoning
capabilities of LLMs for complex tasks. We believe that there is no
one-size-fits-all solution for data-augmented LLM applications. In practice,
underperformance often arises from a failure to correctly identify the core
focus of a task or because the task inherently requires a blend of multiple
capabilities that must be disentangled for better resolution. In this survey,
we propose a RAG task categorization method, classifying user queries into four
levels based on the type of external data required and primary focus of the
task: explicit fact queries, implicit fact queries, interpretable rationale
queries, and hidden rationale queries. We define these levels of queries,
provide relevant datasets, and summarize the key challenges and most effective
techniques for addressing these challenges. Finally, we discuss three main
forms of integrating external data into LLMs: context, small model, and
fine-tuning, highlighting their respective strengths, limitations, and the
types of problems they are suited to solve. This work aims to help readers
thoroughly understand and decompose the data requirements and key bottlenecks
in building LLM applications, offering solutions to the different challenges
and serving as a guide to systematically developing such applications.",Siyun Zhao
2024-10-03T17:40:55Z,http://arxiv.org/abs/2410.02721v1,"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization","Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.",Ryan C. Barron
2024-10-12T10:21:00Z,http://arxiv.org/abs/2410.09472v1,"DRCap: Decoding CLAP Latents with Retrieval-augmented Generation for
  Zero-shot Audio Captioning","While automated audio captioning (AAC) has made notable progress, traditional
fully supervised AAC models still face two critical challenges: the need for
expensive audio-text pair data for training and performance degradation when
transferring across domains. To overcome these limitations, we present DRCap, a
data-efficient and flexible zero-shot audio captioning system that requires
text-only data for training and can quickly adapt to new domains without
additional fine-tuning. DRCap integrates a contrastive language-audio
pre-training (CLAP) model and a large-language model (LLM) as its backbone.
During training, the model predicts the ground-truth caption with a fixed text
encoder from CLAP, whereas, during inference, the text encoder is replaced with
the audio encoder to generate captions for audio clips in a zero-shot manner.
To mitigate the modality gap of the CLAP model, we use both the projection
strategy from the encoder side and the retrieval-augmented generation strategy
from the decoder side. Specifically, audio embeddings are first projected onto
a text embedding support to absorb extensive semantic information within the
joint multi-modal space of CLAP. At the same time, similar captions retrieved
from a datastore are fed as prompts to instruct the LLM, incorporating external
knowledge to take full advantage of its strong generative capability.
Conditioned on both the projected CLAP embedding and the retrieved similar
captions, the model is able to produce a more accurate and semantically rich
textual description. By tailoring the text embedding support and the caption
datastore to the target domain, DRCap acquires a robust ability to adapt to new
domains in a training-free manner. Experimental results demonstrate that DRCap
outperforms all other zero-shot models in in-domain scenarios and achieves
state-of-the-art performance in cross-domain scenarios.",Xiquan Li
2024-11-29T16:09:43Z,http://arxiv.org/abs/2411.19804v1,"Advanced System Integration: Analyzing OpenAPI Chunking for
  Retrieval-Augmented Generation","Integrating multiple (sub-)systems is essential to create advanced
Information Systems (ISs). Difficulties mainly arise when integrating dynamic
environments across the IS lifecycle. A traditional approach is a registry that
provides the API documentation of the systems' endpoints. Large Language Models
(LLMs) have shown to be capable of automatically creating system integrations
(e.g., as service composition) based on this documentation but require concise
input due to input token limitations, especially regarding comprehensive API
descriptions. Currently, it is unknown how best to preprocess these API
descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented
Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing,
of OpenAPIs to reduce the input token length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints and retrieves details on
demand. We evaluate RAG for endpoint discovery using the RestBench benchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval recall, precision, and F1 score. Then, we assess the
Discovery Agent using the same test set. With our prototype, we demonstrate how
to successfully employ RAG for endpoint discovery to reduce the token count.
While revealing high values for recall, precision, and F1, further research is
necessary to retrieve all requisite endpoints. Our experiments show that for
preprocessing, LLM-based and format-specific approaches outperform na\""ive
chunking methods. Relying on an agent further enhances these results as the
agent splits the tasks into multiple fine granular subtasks, improving the
overall RAG performance in the token count, precision, and F1 score.",Robin D. Pesl
2024-12-18T20:18:03Z,http://arxiv.org/abs/2412.14304v1,"Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing
  LLM Ophthalmological QA in LMICs","Current ophthalmology clinical workflows are plagued by over-referrals, long
waits, and complex and heterogeneous medical records. Large language models
(LLMs) present a promising solution to automate various procedures such as
triaging, preliminary tests like visual acuity assessment, and report
summaries. However, LLMs have demonstrated significantly varied performance
across different languages in natural language question-answering tasks,
potentially exacerbating healthcare disparities in Low and Middle-Income
Countries (LMICs). This study introduces the first multilingual
ophthalmological question-answering benchmark with manually curated questions
parallel across languages, allowing for direct cross-lingual comparisons. Our
evaluation of 6 popular LLMs across 7 different languages reveals substantial
bias across different languages, highlighting risks for clinical deployment of
LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought
or Retrieval-augmented generation (RAG) by themselves fall short of closing
this performance gap, often failing to improve performance across all languages
and lacking specificity for the medical domain. To address this issue, We
propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time
de-biasing method leveraging retrieval augmented generation and
self-verification. Our approach not only improves performance across all
languages but also significantly reduces the multilingual bias gap,
facilitating equitable LLM application across the globe.",David Restrepo
2021-09-08T16:15:50Z,http://arxiv.org/abs/2109.03754v2,"Memory and Knowledge Augmented Language Models for Inferring Salience in
  Long-Form Stories","Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.",David Wilmot
2021-09-15T12:58:51Z,http://arxiv.org/abs/2109.07263v2,End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs,"We propose a novel problem within end-to-end learning of task-oriented
dialogs (TOD), in which the dialog system mimics a troubleshooting agent who
helps a user by diagnosing their problem (e.g., car not starting). Such dialogs
are grounded in domain-specific flowcharts, which the agent is supposed to
follow during the conversation. Our task exposes novel technical challenges for
neural TOD, such as grounding an utterance to the flowchart without explicit
annotation, referring to additional manual pages when user asks a clarification
question, and ability to follow unseen flowcharts at test time. We release a
dataset (FloDial) consisting of 2,738 dialogs grounded on 12 different
troubleshooting flowcharts. We also design a neural model, FloNet, which uses a
retrieval-augmented generation architecture to train the dialog agent. Our
experiments find that FloNet can do zero-shot transfer to unseen flowcharts,
and sets a strong baseline for future research.",Dinesh Raghu
2021-12-16T08:18:47Z,http://arxiv.org/abs/2112.08688v2,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,"Retrieval-augmented generation models have shown state-of-the-art performance
across many knowledge-intensive NLP tasks such as open question answering and
fact verification. These models are trained to generate the final output given
the retrieved passages, which can be irrelevant to the original query, leading
to learning spurious cues or answer memorization. This work introduces a method
to incorporate the evidentiality of passages -- whether a passage contains
correct evidence to support the output -- into training the generator. We
introduce a multi-task learning framework to jointly generate the final output
and predict the evidentiality of each passage, leveraging a new task-agnostic
method to obtain silver evidentiality labels for supervision. Our experiments
on five datasets across three knowledge-intensive tasks show that our new
evidentiality-guided generator significantly outperforms its direct counterpart
with the same-size model and advances the state of the art on FaVIQ-Ambig. We
attribute these improvements to both the auxiliary multi-task learning and
silver evidentiality mining techniques.",Akari Asai
2022-03-30T23:30:16Z,http://arxiv.org/abs/2203.16714v1,End-to-End Table Question Answering via Retrieval-Augmented Generation,"Most existing end-to-end Table Question Answering (Table QA) models consist
of a two-stage framework with a retriever to select relevant table candidates
from a corpus and a reader to locate the correct answers from table candidates.
Even though the accuracy of the reader models is significantly improved with
the recent transformer-based approaches, the overall performance of such
frameworks still suffers from the poor accuracy of using traditional
information retrieval techniques as retrievers. To alleviate this problem, we
introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense
vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence
model to generate answer tokens. Given any natural language question, T-RAG
utilizes a unified pipeline to automatically search through a table corpus to
directly locate the correct answer from the table cells. We apply T-RAG to
recent open-domain Table QA benchmarks and demonstrate that the fine-tuned
T-RAG model is able to achieve state-of-the-art performance in both the
end-to-end Table QA and the table retrieval tasks.",Feifei Pan
2022-04-08T10:36:21Z,http://arxiv.org/abs/2204.03985v2,KGI: An Integrated Framework for Knowledge Intensive Language Tasks,"In this paper, we present a system to showcase the capabilities of the latest
state-of-the-art retrieval augmented generation models trained on
knowledge-intensive language tasks, such as slot filling, open domain question
answering, dialogue, and fact-checking. Moreover, given a user query, we show
how the output from these different models can be combined to cross-examine the
outputs of each other. Particularly, we show how accuracy in dialogue can be
improved using the question answering model. We are also releasing all models
used in the demo as a contribution of this paper. A short video demonstrating
the system is available at https://ibm.box.com/v/emnlp2022-demo.",Md Faisal Mahbub Chowdhury
2022-07-07T00:57:02Z,http://arxiv.org/abs/2207.03030v1,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,"This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.",Sebastian Hofstätter
2022-09-28T17:54:55Z,http://arxiv.org/abs/2209.14290v1,FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation,"Retrieval-augmented generation models offer many benefits over standalone
language models: besides a textual answer to a given query they provide
provenance items retrieved from an updateable knowledge base. However, they are
also more complex systems and need to handle long inputs. In this work, we
introduce FiD-Light to strongly increase the efficiency of the state-of-the-art
retrieval-augmented FiD model, while maintaining the same level of
effectiveness. Our FiD-Light model constrains the information flow from the
encoder (which encodes passages separately) to the decoder (using concatenated
encoded representations). Furthermore, we adapt FiD-Light with re-ranking
capabilities through textual source pointers, to improve the top-ranked
provenance precision. Our experiments on a diverse set of seven knowledge
intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier
between query latency and effectiveness. FiD-Light with source pointing sets
substantial new state-of-the-art results on six KILT tasks for combined text
generation and provenance retrieval evaluation, while maintaining reasonable
efficiency.",Sebastian Hofstätter
2022-10-10T17:45:38Z,http://arxiv.org/abs/2210.04873v2,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",Tanay Dixit
2023-05-02T15:33:01Z,http://arxiv.org/abs/2305.01526v1,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset","In this paper, we release a largest ever medical Question Answering (QA)
dataset with 26 million QA pairs. We benchmark many existing approaches in our
dataset in terms of both retrieval and generation. Experimental results show
that the existing models perform far lower than expected and the released
dataset is still challenging in the pre-trained language model era. Moreover,
we also experimentally show the benefit of the proposed dataset in many
aspects: (i) trained models for other QA datasets in a zero-shot fashion; and
(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)
improving existing pre-trained language models by using the QA pairs as a
pre-training corpus in continued training manner. We believe that this dataset
will not only contribute to medical research but also facilitate both the
patients and clinical doctors. See
\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",Jianquan Li
2023-05-05T16:28:03Z,http://arxiv.org/abs/2305.03660v1,"Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT
  models","We propose Retrieval Augmented Generation (RAG) as an approach for automated
radiology report writing that leverages multimodally aligned embeddings from a
contrastively pretrained vision language model for retrieval of relevant
candidate radiology text for an input radiology image and a general domain
generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for
report generation using the relevant radiology text retrieved. This approach
keeps hallucinated generations under check and provides capabilities to
generate report content in the format we desire leveraging the instruction
following capabilities of these generative models. Our approach achieves better
clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score
of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different
clinical settings as it allows to augment the automated radiology report
generation process with content relevant for that setting while also having the
ability to inject user intents and requirements in the prompts as part of the
report generation process to modulate the content and format of the generated
reports as applicable for that clinical setting.",Mercy Ranjit
2023-05-10T22:36:27Z,http://arxiv.org/abs/2305.06488v4,A Platform for the Biomedical Application of Large Language Models,"Current-generation Large Language Models (LLMs) have stirred enormous
interest in recent months, yielding great potential for accessibility and
automation, while simultaneously posing significant challenges and risk of
misuse. To facilitate interfacing with LLMs in the biomedical space, while at
the same time safeguarding their functionalities through sensible constraints,
we propose a dedicated, open-source framework: BioChatter. Based on open-source
software packages, we synergise the many functionalities that are currently
developing around LLMs, such as knowledge integration / retrieval-augmented
generation, model chaining, and benchmarking, resulting in an easy-to-use and
inclusive framework for application in many use cases of biomedicine. We focus
on robust and user-friendly implementation, including ways to deploy
privacy-preserving local open-source LLMs. We demonstrate use cases via two
multi-purpose web apps (https://chat.biocypher.org), and provide documentation,
support, and an open community.",Sebastian Lobentanzer
2023-05-24T16:57:04Z,http://arxiv.org/abs/2305.15344v1,"Learning Answer Generation using Supervision from Automatic Question
  Answering Evaluators","Recent studies show that sentence-level extractive QA, i.e., based on Answer
Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA)
models, which generate answers using the top-k answer sentences ranked by AS2
models (a la retrieval-augmented generation style). In this paper, we propose a
novel training paradigm for GenQA using supervision from automatic QA
evaluation models (GAVA). Specifically, we propose three strategies to transfer
knowledge from these QA evaluation models to a GenQA model: (i) augmenting
training data with answers generated by the GenQA model and labelled by GAVA
(either statically, before training, or (ii) dynamically, at every training
epoch); and (iii) using the GAVA score for weighting the generator loss during
the learning of the GenQA model. We evaluate our proposed methods on two
academic and one industrial dataset, obtaining a significant improvement in
answering accuracy over the previous state of the art.",Matteo Gabburo
2023-05-30T08:36:45Z,http://arxiv.org/abs/2305.18846v1,"Knowledge Graph-Augmented Language Models for Knowledge-Grounded
  Dialogue Generation","Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",Minki Kang
2023-06-12T03:54:04Z,http://arxiv.org/abs/2306.06851v2,"UniPoll: A Unified Social Media Poll Generation Framework via
  Multi-Objective Optimization","Social media platforms are vital for expressing opinions and understanding
public sentiment, yet many analytical tools overlook passive users who mainly
consume content without engaging actively. To address this, we introduce
UniPoll, an advanced framework designed to automatically generate polls from
social media posts using sophisticated natural language generation (NLG)
techniques. Unlike traditional methods that struggle with social media's
informal and context-sensitive nature, UniPoll leverages enriched contexts from
user comments and employs multi-objective optimization to enhance poll
relevance and engagement. To tackle the inherently noisy nature of social media
data, UniPoll incorporates Retrieval-Augmented Generation (RAG) and synthetic
data generation, ensuring robust performance across real-world scenarios. The
framework surpasses existing models, including T5, ChatGLM3, and GPT-3.5, in
generating coherent and contextually appropriate question-answer pairs.
Evaluated on the Chinese WeiboPolls dataset and the newly introduced English
RedditPolls dataset, UniPoll demonstrates superior cross-lingual and
cross-platform capabilities, making it a potent tool to boost user engagement
and create a more inclusive environment for interaction.",Yixia Li
2023-07-07T02:42:06Z,http://arxiv.org/abs/2307.04642v2,"TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal
  Prediction","When applied to open-domain question answering, large language models (LLMs)
frequently generate incorrect responses based on made-up facts, which are
called $\textit{hallucinations}$. Retrieval augmented generation (RAG) is a
promising strategy to avoid hallucinations, but it does not provide guarantees
on its correctness. To address this challenge, we propose the Trustworthy
Retrieval Augmented Question Answering, or $\textit{TRAQ}$, which provides the
first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal
prediction, a statistical technique for constructing prediction sets that are
guaranteed to contain the semantically correct response with high probability.
Additionally, TRAQ leverages Bayesian optimization to minimize the size of the
constructed sets. In an extensive experimental evaluation, we demonstrate that
TRAQ provides the desired correctness guarantee while reducing prediction set
size by 16.2% on average compared to an ablation. The implementation is
available at $\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.",Shuo Li
2023-08-24T05:26:54Z,http://arxiv.org/abs/2308.12574v2,"Modeling Uncertainty and Using Post-fusion as Fallback Improves
  Retrieval Augmented Generation with LLMs","The integration of retrieved passages and large language models (LLMs), such
as ChatGPTs, has significantly contributed to improving open-domain question
answering. However, there is still a lack of exploration regarding the optimal
approach for incorporating retrieved passages into the answer generation
process. This paper aims to fill this gap by investigating different methods of
combining retrieved passages with LLMs to enhance answer generation. We begin
by examining the limitations of a commonly-used concatenation approach.
Surprisingly, this approach often results in generating ""unknown"" outputs, even
when the correct document is among the top-k retrieved passages. To address
this issue, we explore four alternative strategies for integrating the
retrieved passages with the LLMs. These strategies include two single-round
methods that utilize chain-of-thought reasoning and two multi-round strategies
that incorporate feedback loops. Through comprehensive analyses and
experiments, we provide insightful observations on how to effectively leverage
retrieved passages to enhance the answer generation capability of LLMs.",Ye Liu
2023-09-27T06:33:29Z,http://arxiv.org/abs/2309.15427v2,Graph Neural Prompting with Large Language Models,"Large language models (LLMs) have shown remarkable generalization capability
with exceptional performance in various language modeling tasks. However, they
still exhibit inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored utilizing knowledge graphs
(KGs) to enhance language modeling via joint training and customized model
architectures, applying this to LLMs is problematic owing to their large number
of parameters and high computational cost. Therefore, how to enhance
pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented
generation, remains an open question. In this work, we propose Graph Neural
Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in
learning beneficial knowledge from KGs. GNP encompasses various designs,
including a standard graph neural network encoder, a cross-modality pooling
module, a domain projector, and a self-supervised link prediction objective.
Extensive experiments on multiple datasets demonstrate the superiority of GNP
on both commonsense and biomedical reasoning tasks across different LLM sizes
and settings. Code is available at https://github.com/meettyj/GNP.",Yijun Tian
2023-09-28T05:19:06Z,http://arxiv.org/abs/2310.01427v1,Attention Sorting Combats Recency Bias In Long Context Language Models,"Current language models often fail to incorporate long contexts efficiently
during generation. We show that a major contributor to this issue are attention
priors that are likely learned during pre-training: relevant information
located earlier in context is attended to less on average. Yet even when models
fail to use the information from a relevant document in their response, they
still pay preferential attention to that document compared to an irrelevant
document at the same position. We leverage this fact to introduce ``attention
sorting'': perform one step of decoding, sort documents by the attention they
receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves
performance of long context models. Our findings highlight some challenges in
using off-the-shelf language models for retrieval augmented generation.",Alexander Peysakhovich
2023-10-11T15:19:50Z,http://arxiv.org/abs/2310.07581v2,"Qlarify: Recursively Expandable Abstracts for Directed Information
  Retrieval over Scientific Papers","Navigating the vast scientific literature often starts with browsing a
paper's abstract. However, when a reader seeks additional information, not
present in the abstract, they face a costly cognitive chasm during their dive
into the full text. To bridge this gap, we introduce recursively expandable
abstracts, a novel interaction paradigm that dynamically expands abstracts by
progressively incorporating additional information from the papers' full text.
This lightweight interaction allows scholars to specify their information needs
by quickly brushing over the abstract or selecting AI-suggested expandable
entities. Relevant information is synthesized using a retrieval-augmented
generation approach, presented as a fluid, threaded expansion of the abstract,
and made efficiently verifiable via attribution to relevant source-passages in
the paper. Through a series of user studies, we demonstrate the utility of
recursively expandable abstracts and identify future opportunities to support
low-effort and just-in-time exploration of long-form information contexts
through LLM-powered interactions.",Raymond Fok
2023-10-16T18:45:38Z,http://arxiv.org/abs/2310.10760v1,"Towards reducing hallucination in extracting information from financial
  reports using Large Language Models","For a financial analyst, the question and answer (Q\&A) segment of the
company financial report is a crucial piece of information for various analysis
and investment decisions. However, extracting valuable insights from the Q\&A
section has posed considerable challenges as the conventional methods such as
detailed reading and note-taking lack scalability and are susceptible to human
errors, and Optical Character Recognition (OCR) and similar techniques
encounter difficulties in accurately processing unstructured transcript text,
often missing subtle linguistic nuances that drive investor decisions. Here, we
demonstrate the utilization of Large Language Models (LLMs) to efficiently and
rapidly extract information from earnings report transcripts while ensuring
high accuracy transforming the extraction process as well as reducing
hallucination by combining retrieval-augmented generation technique as well as
metadata. We evaluate the outcomes of various LLMs with and without using our
proposed approach based on various objective metrics for evaluating Q\&A
systems, and empirically demonstrate superiority of our method.",Bhaskarjit Sarmah
2023-10-23T11:33:41Z,http://arxiv.org/abs/2310.15205v2,"DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple
  Experts Fine-tuning","We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.",Wei Chen
2023-10-22T14:45:14Z,http://arxiv.org/abs/2310.18344v1,Chainpoll: A high efficacy method for LLM hallucination detection,"Large language models (LLMs) have experienced notable advancements in
generating coherent and contextually relevant responses. However,
hallucinations - incorrect or unfounded claims - are still prevalent, prompting
the creation of automated metrics to detect these in LLM outputs. Our
contributions include: introducing ChainPoll, an innovative hallucination
detection method that excels compared to its counterparts, and unveiling
RealHall, a refined collection of benchmark datasets to assess hallucination
detection metrics from recent studies. While creating RealHall, we assessed
tasks and datasets from previous hallucination detection studies and observed
that many are not suitable for the potent LLMs currently in use. Overcoming
this, we opted for four datasets challenging for modern LLMs and pertinent to
real-world scenarios. Using RealHall, we conducted a comprehensive comparison
of ChainPoll with numerous hallucination metrics from recent studies. Our
findings indicate that ChainPoll outperforms in all RealHall benchmarks,
achieving an overall AUROC of 0.781. This surpasses the next best theoretical
method by 11% and exceeds industry standards by over 23%. Additionally,
ChainPoll is cost-effective and offers greater transparency than other metrics.
We introduce two novel metrics to assess LLM hallucinations: Adherence and
Correctness. Adherence is relevant to Retrieval Augmented Generation workflows,
evaluating an LLM's analytical capabilities within given documents and
contexts. In contrast, Correctness identifies logical and reasoning errors.",Robert Friel
2023-11-07T19:27:28Z,http://arxiv.org/abs/2311.04310v1,"KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI
  Integration using Retrieval-Augmented Generation","Academic researchers face challenges keeping up with exponentially growing
published findings in their field. Performing comprehensive literature reviews
to synthesize knowledge is time-consuming and labor-intensive using manual
approaches. Recent advances in artificial intelligence provide promising
solutions, yet many require coding expertise, limiting accessibility.
KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the
KNIME visual programming platform to automate literature review tasks for users
with no coding experience. By leveraging KNIME's intuitive graphical interface,
researchers can create workflows to search their Zotero libraries and utilize
OpenAI models to extract key information without coding. Users simply provide
API keys and configure settings through a user-friendly interface in a locally
stored copy of the workflow. KNIMEZoBot then allows asking natural language
questions via a chatbot and retrieves relevant passages from papers to generate
synthesized answers. This system has significant potential to expedite
literature reviews for researchers unfamiliar with coding by automating
retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot
demonstrates how thoughtfully designed AI tools can expand accessibility and
accelerate knowledge building across diverse research domains.",Suad Alshammari
2023-11-09T06:45:04Z,http://arxiv.org/abs/2311.05169v1,"Large Language Models and Prompt Engineering for Biomedical Query
  Focused Multi-Document Summarisation","This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",Diego Mollá
2023-11-09T10:40:04Z,http://arxiv.org/abs/2311.05261v1,RAGLog: Log Anomaly Detection using Retrieval Augmented Generation,"The ability to detect log anomalies from system logs is a vital activity
needed to ensure cyber resiliency of systems. It is applied for fault
identification or facilitate cyber investigation and digital forensics.
However, as logs belonging to different systems and components differ
significantly, the challenge to perform such analysis is humanly challenging
from the volume, variety and velocity of logs. This is further complicated by
the lack or unavailability of anomalous log entries to develop trained machine
learning or artificial intelligence models for such purposes. In this research
work, we explore the use of a Retrieval Augmented Large Language Model that
leverages a vector database to detect anomalies from logs. We used a Question
and Answer configuration pipeline. To the best of our knowledge, our experiment
which we called RAGLog is a novel one and the experimental results show much
promise.",Jonathan Pan
2023-11-10T15:10:36Z,http://arxiv.org/abs/2311.06102v1,"Making LLMs Worth Every Penny: Resource-Limited Text Classification in
  Banking","Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",Lefteris Loukas
2023-11-14T18:41:54Z,http://arxiv.org/abs/2311.08377v1,Learning to Filter Context for Retrieval-Augmented Generation,"On-the-fly retrieval of relevant knowledge has proven an essential element of
reliable systems for tasks such as open-domain question answering and fact
verification. However, because retrieval systems are not perfect, generation
models are required to generate outputs given partially or entirely irrelevant
passages. This can cause over- or under-reliance on context, and result in
problems in the generated output such as hallucinations. To alleviate these
problems, we propose FILCO, a method that improves the quality of the context
provided to the generator by (1) identifying useful context based on lexical
and information-theoretic approaches, and (2) training context filtering models
that can filter retrieved contexts at test time. We experiment on six
knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our
method outperforms existing approaches on extractive question answering (QA),
complex multi-hop and long-form QA, fact verification, and dialog generation
tasks. FILCO effectively improves the quality of context, whether or not it
supports the canonical output.",Zhiruo Wang
2023-11-15T17:04:56Z,http://arxiv.org/abs/2311.09114v2,"Ever: Mitigating Hallucination in Large Language Models through
  Real-Time Verification and Rectification","Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating fluent text. However, they often encounter the challenge of
generating inaccurate or hallucinated content. This issue is common in both
non-retrieval-based generation and retrieval-augmented generation approaches,
and existing post-hoc rectification methods may not address the accumulated
hallucination errors that may be caused by the ""snowballing"" issue, especially
in reasoning tasks. To tackle these challenges, we introduce a novel approach
called Real-time Verification and Rectification (Ever). Instead of waiting
until the end of the generation process to rectify hallucinations, Ever employs
a real-time, step-wise generation and hallucination rectification strategy. The
primary objective is to detect and rectify hallucinations as they occur during
the text generation process. When compared to both retrieval-based and
non-retrieval-based baselines, Ever demonstrates a significant improvement in
generating trustworthy and factually accurate text across a diverse range of
tasks, including short-form QA, biography generation, and multi-hop reasoning.",Haoqiang Kang
2023-11-21T19:41:46Z,http://arxiv.org/abs/2311.12955v1,"Don't forget private retrieval: distributed private similarity search
  for large language models","While the flexible capabilities of large language models (LLMs) allow them to
answer a range of queries based on existing learned knowledge, information
retrieval to augment generation is an important tool to allow LLMs to answer
questions on information not included in pre-training data. Such private
information is increasingly being generated in a wide array of distributed
contexts by organizations and individuals. Performing such information
retrieval using neural embeddings of queries and documents always leaked
information about queries and database content unless both were stored locally.
We present Private Retrieval Augmented Generation (PRAG), an approach that uses
multi-party computation (MPC) to securely transmit queries to a distributed set
of servers containing a privately constructed database to return top-k and
approximate top-k documents. This is a first-of-its-kind approach to dense
information retrieval that ensures no server observes a client's query or can
see the database content. The approach introduces a novel MPC friendly protocol
for inverted file approximate search (IVF) that allows for fast document search
over distributed and private data in sublinear communication complexity. This
work presents new avenues through which data for use in LLMs can be accessed
and used without needing to centralize or forgo privacy.",Guy Zyskind
2023-11-23T09:58:39Z,http://arxiv.org/abs/2311.13878v1,"Minimizing Factual Inconsistency and Hallucination in Large Language
  Models","Large Language Models (LLMs) are widely used in critical fields such as
healthcare, education, and finance due to their remarkable proficiency in
various language-related tasks. However, LLMs are prone to generating factually
incorrect responses or ""hallucinations,"" which can lead to a loss of
credibility and trust among users. To address this issue, we propose a
multi-stage framework that generates the rationale first, verifies and refines
incorrect ones, and uses them as supporting references to generate the answer.
The generated rationale enhances the transparency of the answer and our
framework provides insights into how the model arrived at this answer, by using
this rationale and the references to the context. In this paper, we demonstrate
its effectiveness in improving the quality of responses to drug-related
inquiries in the life sciences industry. Our framework improves traditional
Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be
14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,
fine-tuning samples based on our framework improves the accuracy of smaller
open-access LLMs by 33-42% and competes with RAG on commercial models.",Muneeswaran I
2023-11-27T05:27:13Z,http://arxiv.org/abs/2311.15548v1,"Deficiency of Large Language Models in Finance: An Empirical Examination
  of Hallucination","The hallucination issue is recognized as a fundamental deficiency of large
language models (LLMs), especially when applied to fields such as finance,
education, and law. Despite the growing concerns, there has been a lack of
empirical investigation. In this paper, we provide an empirical examination of
LLMs' hallucination behaviors in financial tasks. First, we empirically
investigate LLM model's ability of explaining financial concepts and
terminologies. Second, we assess LLM models' capacity of querying historical
stock prices. Third, to alleviate the hallucination issue, we evaluate the
efficacy of four practical methods, including few-shot learning, Decoding by
Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method
and the prompt-based tool learning method for a function to generate a query
command. Finally, our major finding is that off-the-shelf LLMs experience
serious hallucination behaviors in financial tasks. Therefore, there is an
urgent need to call for research efforts in mitigating LLMs' hallucination.",Haoqiang Kang
2023-11-27T19:17:39Z,http://arxiv.org/abs/2311.16267v2,"Novel Preprocessing Technique for Data Embedding in Engineering Code
  Generation Using Large Language Model","We present four main contributions to enhance the performance of Large
Language Models (LLMs) in generating domain-specific code: (i) utilizing
LLM-based data splitting and data renovation techniques to improve the semantic
representation of embeddings' space; (ii) introducing the Chain of Density for
Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text
Renovation (ATR) algorithm for assessing data renovation reliability; (iii)
developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt
technique; and (iv) effectively refactoring existing scripts to generate new
and high-quality scripts with LLMs. By using engineering simulation software
RedHawk-SC as a case study, we demonstrate the effectiveness of our data
pre-processing method for expanding and categorizing scripts. When combined
with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG)
method in retrieving more relevant information, ultimately achieving a 73.33%
""Percentage of Correct Lines"" for code generation problems in MapReduce
applications.",Yu-Chen Lin
2023-11-28T06:18:54Z,http://arxiv.org/abs/2311.16543v3,"RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language
  Models","This paper presents RTLFixer, a novel framework enabling automatic syntax
errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's
promising capabilities, our analysis indicates that approximately 55% of errors
in LLM-generated Verilog are syntax-related, leading to compilation failures.
To tackle this issue, we introduce a novel debugging framework that employs
Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act
as autonomous agents in interactively debugging the code with feedback. This
framework demonstrates exceptional proficiency in resolving syntax errors,
successfully correcting about 98.5% of compilation errors in our debugging
dataset, comprising 212 erroneous implementations derived from the VerilogEval
benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates
in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.",Yun-Da Tsai
2023-12-10T02:29:05Z,http://arxiv.org/abs/2312.05733v1,DevBots can co-design APIs,"DevBots are automated tools that perform various tasks in order to support
software development. They are a growing trend and have been used in
repositories to automate repetitive tasks, as code generators, and as
collaborators in eliciting requirements and defining architectures. In this
study, we analyzed 24 articles to investigate the state of the art of using
DevBots in software development, trying to understand their characteristics,
identify use cases, learn the relationship between DevBots and conversational
software development, and discuss how prompt engineering can enable
collaboration between human developers and bots. Additionally, we identified a
gap to address by applying prompt engineering to collaborative API design
between human designers and DevBots and proposed an experiment to assess what
approach, between using Retrieval Augmented Generation or not, is more
suitable. Our conclusion is that DevBots can collaborate with human API
designers, but the two approaches have advantages and disadvantages.",Vinicius Soares Silva Marques
2023-12-10T16:52:00Z,http://arxiv.org/abs/2312.05934v3,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,"Large language models (LLMs) encapsulate a vast amount of factual information
within their pre-trained weights, as evidenced by their ability to answer
diverse questions across different domains. However, this knowledge is
inherently limited, relying heavily on the characteristics of the training
data. Consequently, using external datasets to incorporate new information or
refine the capabilities of LLMs on previously seen information poses a
significant challenge. In this study, we compare two common approaches:
unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate
both approaches on a variety of knowledge-intensive tasks across different
topics. Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing knowledge
encountered during training and entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information through unsupervised
fine-tuning, and that exposing them to numerous variations of the same fact
during training could alleviate this problem.",Oded Ovadia
2023-12-12T09:34:27Z,http://arxiv.org/abs/2312.07104v2,SGLang: Efficient Execution of Structured Language Model Programs,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",Lianmin Zheng
2023-12-15T16:58:52Z,http://arxiv.org/abs/2312.09948v1,"GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading
  Scholarly Article Searches for Systematic Reviews","Systematic reviews (SRs) - the librarian-assisted literature survey of
scholarly articles takes time and requires significant human resources. Given
the ever-increasing volume of published studies, applying existing computing
and informatics technology can decrease this time and resource burden. Due to
the revolutionary advances in (1) Generative AI such as ChatGPT, and (2)
External knowledge-augmented information extraction efforts such as
Retrieval-Augmented Generation, In this work, we explore the use of techniques
from (1) and (2) for SR. We demonstrate a system that takes user queries,
performs query expansion to obtain enriched context (includes additional terms
and definitions by querying language models and knowledge graphs), and uses
this context to search for articles on scholarly databases to retrieve
articles. We perform qualitative evaluations of our system through comparison
against sentinel (ground truth) articles provided by an in-house librarian. The
demo can be found at: https://youtu.be/zMdP56GJ9mU.",Kaushik Roy
2023-12-21T22:52:44Z,http://arxiv.org/abs/2312.14327v1,"Parameter Efficient Tuning Allows Scalable Personalization of LLMs for
  Text Entry: A Case Study on Abbreviation Expansion","Abbreviation expansion is a strategy used to speed up communication by
limiting the amount of typing and using a language model to suggest expansions.
Here we look at personalizing a Large Language Model's (LLM) suggestions based
on prior conversations to enhance the relevance of predictions, particularly
when the user data is small (~1000 samples). Specifically, we compare
fine-tuning, prompt-tuning, and retrieval augmented generation of expanded text
suggestions for abbreviated inputs. Our case study with a deployed 8B parameter
LLM on a real user living with ALS, and experiments on movie character
personalization indicates that (1) customization may be necessary in some
scenarios and prompt-tuning generalizes well to those, (2) fine-tuning on
in-domain data (with as few as 600 samples) still shows some gains, however (3)
retrieval augmented few-shot selection also outperforms fine-tuning. (4)
Parameter efficient tuning allows for efficient and scalable personalization.
For prompt-tuning, we also find that initializing the learned ""soft-prompts"" to
user relevant concept tokens leads to higher accuracy than random
initialization.",Katrin Tomanek
2024-01-03T02:32:55Z,http://arxiv.org/abs/2401.01511v1,"Enhancing Multilingual Information Retrieval in Mixed Human Resources
  Environments: A RAG Model Implementation for Multicultural Enterprise","The advent of Large Language Models has revolutionized information retrieval,
ushering in a new era of expansive knowledge accessibility. While these models
excel in providing open-world knowledge, effectively extracting answers in
diverse linguistic environments with varying levels of literacy remains a
formidable challenge. Retrieval Augmented Generation (RAG) emerges as a
promising solution, bridging the gap between information availability and
multilingual comprehension. However, deploying RAG models in real-world
scenarios demands careful consideration of various factors. This paper
addresses the critical challenges associated with implementing RAG models in
multicultural environments. We delve into essential considerations, including
data feeding strategies, timely updates, mitigation of hallucinations,
prevention of erroneous responses, and optimization of delivery speed. Our work
involves the integration of a diverse array of tools, meticulously combined to
facilitate the seamless adoption of RAG models across languages and literacy
levels within a multicultural organizational context. Through strategic tweaks
in our approaches, we achieve not only effectiveness but also efficiency,
ensuring the accelerated and accurate delivery of information in a manner that
is tailored to the unique requirements of multilingual and multicultural
settings.",Syed Rameel Ahmad
2024-01-04T16:16:14Z,http://arxiv.org/abs/2401.02333v3,"Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models","The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.",Uday Allu
2024-01-13T02:20:17Z,http://arxiv.org/abs/2401.06954v2,Bridging the Preference Gap between Retrievers and LLMs,"Large Language Models (LLMs) have demonstrated superior results across a wide
range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to
enhance the performance by locating relevant information and placing it into
the context window of the LLM. However, the relationship between retrievers and
LLMs in a RAG is still under-investigated. Most existing work treats the
retriever and the LLM as independent components and leaves a gap between
retrieving human-""friendly"" information and assembling a LLM-""friendly""
context. In this work, we examine a novel bridge mechanism. We validate the
ranking and selection assumptions of retrievers in the context of RAG and
propose a framework that chains together supervised and reinforcement learning
to train a bridge model that optimizes the connection between the retriever and
the LLM. Empirical results demonstrate the effectiveness of our method in both
question-answering and personalized generation tasks.",Zixuan Ke
2024-01-16T02:11:35Z,http://arxiv.org/abs/2401.10286v3,"Code-Based English Models Surprising Performance on Chinese QA Pair
  Extraction Task","In previous studies, code-based models have consistently outperformed
text-based models in reasoning-intensive scenarios. When generating our
knowledge base for Retrieval-Augmented Generation (RAG), we observed that
code-based models also perform exceptionally well in Chinese QA Pair Extraction
task. Further, our experiments and the metrics we designed discovered that
code-based models containing a certain amount of Chinese data achieve even
better performance. Additionally, the capabilities of code-based English models
in specified Chinese tasks offer a distinct perspective for discussion on the
philosophical ""Chinese Room"" thought experiment.",Linghan Zheng
2024-02-02T12:34:09Z,http://arxiv.org/abs/2402.01364v2,Continual Learning for Large Language Models: A Survey,"Large language models (LLMs) are not amenable to frequent re-training, due to
high training costs arising from their massive scale. However, updates are
necessary to endow LLMs with new skills and keep them up-to-date with rapidly
evolving human knowledge. This paper surveys recent works on continual learning
for LLMs. Due to the unique nature of LLMs, we catalog continue learning
techniques in a novel multi-staged categorization scheme, involving continual
pretraining, instruction tuning, and alignment. We contrast continual learning
for LLMs with simpler adaptation methods used in smaller models, as well as
with other enhancement strategies like retrieval-augmented generation and model
editing. Moreover, informed by a discussion of benchmarks and evaluation, we
identify several challenges and future work directions for this crucial task.",Tongtong Wu
2024-01-27T00:18:07Z,http://arxiv.org/abs/2402.01722v1,"Enhancing Large Language Model Performance To Answer Questions and
  Extract Information More Accurately","Large Language Models (LLMs) generate responses to questions; however, their
effectiveness is often hindered by sub-optimal quality of answers and
occasional failures to provide accurate responses to questions. To address
these challenges, a fine-tuning process is employed, involving feedback and
examples to refine models. The objective is to enhance AI models through
continuous feedback loops, utilizing metrics such as cosine similarity, LLM
evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like
GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on
financial datasets, including the FinanceBench and RAG Instruct Benchmark
Tester Dataset, illustrating the necessity of fine-tuning. The results showcase
the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,
providing superior question and answering capabilities. Notably, the
combination of fine-tuning the LLM with a process known as Retrieval Augmented
Generation (RAG) proves to generate responses with improved accuracy.",Liang Zhang
2024-02-01T02:24:15Z,http://arxiv.org/abs/2402.01767v2,HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA,"Retrieval-augmented generation (RAG) has rapidly advanced the language model
field, particularly in question-answering (QA) systems. By integrating external
documents during the response generation phase, RAG significantly enhances the
accuracy and reliability of language models. This method elevates the quality
of responses and reduces the frequency of hallucinations, where the model
generates incorrect or misleading information. However, these methods exhibit
limited retrieval accuracy when faced with numerous indistinguishable
documents, presenting notable challenges in their practical application. In
response to these emerging challenges, we present HiQA, an advanced
multi-document question-answering (MDQA) framework that integrates cascading
metadata into content and a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.",Xinyue Chen
2024-02-05T14:36:51Z,http://arxiv.org/abs/2402.03053v1,"Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for
  Semantic Representations","In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
  For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
  In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the ""Melayu""
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
  These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99",Husein Zolkepli
2024-02-06T18:01:29Z,http://arxiv.org/abs/2402.04206v1,"Explaining Autonomy: Enhancing Human-Robot Interaction through
  Explanation Generation with Large Language Models","This paper introduces a system designed to generate explanations for the
actions performed by an autonomous robot in Human-Robot Interaction (HRI).
Explainability in robotics, encapsulated within the concept of an eXplainable
Autonomous Robot (XAR), is a growing research area. The work described in this
paper aims to take advantage of the capabilities of Large Language Models
(LLMs) in performing natural language processing tasks. This study focuses on
the possibility of generating explanations using such models in combination
with a Retrieval Augmented Generation (RAG) method to interpret data gathered
from the logs of autonomous systems. In addition, this work also presents a
formalization of the proposed explanation system. It has been evaluated through
a navigation test from the European Robotics League (ERL), a Europe-wide social
robotics competition. Regarding the obtained results, a validation
questionnaire has been conducted to measure the quality of the explanations
from the perspective of technical users. The results obtained during the
experiment highlight the potential utility of LLMs in achieving explanatory
capabilities in robots.",David Sobrín-Hidalgo
2024-02-06T21:14:45Z,http://arxiv.org/abs/2402.04411v2,"DFA-RAG: Conversational Semantic Router for Large Language Model with
  Definite Finite Automaton","This paper introduces the retrieval-augmented large language model with
Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the
capabilities of conversational agents using large language models (LLMs).
Traditional LLMs face challenges in generating regulated and compliant
responses in special scenarios with predetermined response guidelines, like
emotional support and customer service. Our framework addresses these
challenges by embedding a Definite Finite Automaton (DFA), learned from
training dialogues, within the LLM. This structured approach acts as a semantic
router which enables the LLM to adhere to a deterministic response pathway. The
routing is achieved by the retrieval-augmentation generation (RAG) strategy,
which carefully selects dialogue examples aligned with the current
conversational context. The advantages of DFA-RAG include an interpretable
structure through human-readable DFA, context-aware retrieval for responses in
conversations, and plug-and-play compatibility with existing LLMs. Extensive
benchmarks validate DFA-RAG's effectiveness, indicating its potential as a
valuable contribution to the conversational agent.",Yiyou Sun
2024-02-06T11:29:44Z,http://arxiv.org/abs/2402.05135v1,"CADReN: Contextual Anchor-Driven Relational Network for Controllable
  Cross-Graphs Node Importance Estimation","Node Importance Estimation (NIE) is crucial for integrating external
information into Large Language Models through Retriever-Augmented Generation.
Traditional methods, focusing on static, single-graph characteristics, lack
adaptability to new graphs and user-specific requirements. CADReN, our proposed
method, addresses these limitations by introducing a Contextual Anchor (CA)
mechanism. This approach enables the network to assess node importance relative
to the CA, considering both structural and semantic features within Knowledge
Graphs (KGs). Extensive experiments show that CADReN achieves better
performance in cross-graph NIE task, with zero-shot prediction ability. CADReN
is also proven to match the performance of previous models on single-graph NIE
task. Additionally, we introduce and opensource two new datasets, RIC200 and
WK1K, specifically designed for cross-graph NIE research, providing a valuable
resource for future developments in this domain.",Zijie Zhong
2024-02-15T12:12:19Z,http://arxiv.org/abs/2402.09906v2,Generative Representational Instruction Tuning,"All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.",Niklas Muennighoff
2024-02-16T19:28:52Z,http://arxiv.org/abs/2402.11035v3,Dense Passage Retrieval: Is it Retrieving?,"Dense passage retrieval (DPR) is the first step in the retrieval augmented
generation (RAG) paradigm for improving the performance of large language
models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of
the embeddings between queries and relevant textual data. A deeper
understanding of DPR fine-tuning will be required to fundamentally unlock the
full potential of this approach. In this work, we explore DPR-trained models
mechanistically by using a combination of probing, layer activation analysis,
and model editing. Our experiments show that DPR training decentralizes how
knowledge is stored in the network, creating multiple access pathways to the
same information. We also uncover a limitation in this training style: the
internal knowledge of the pre-trained model bounds what the retrieval model can
retrieve. These findings suggest a few possible directions for dense retrieval:
(1) expose the DPR training process to more knowledge so more can be
decentralized, (2) inject facts as decentralized representations, (3) model and
incorporate knowledge uncertainty in the retrieval process, and (4) directly
map internal model knowledge to a knowledge base.",Benjamin Reichman
2024-02-19T14:33:24Z,http://arxiv.org/abs/2402.12177v4,Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning,"Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.",Mingtian Zhang
2024-02-21T05:41:34Z,http://arxiv.org/abs/2402.13542v2,"ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling","Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.",Lingxi Zhang
2024-02-21T06:04:53Z,http://arxiv.org/abs/2402.13547v2,"ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through
  Retrieval-Augmented Agents","Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to
leverage external knowledge, enhancing their performance on knowledge-intensive
tasks. However, existing RAG models often treat LLMs as passive recipients of
information, which can lead to interference from noisy retrieved content. In
this paper, we introduce ActiveRAG, a multi-agent framework that mimics human
learning behavior to help LLMs actively engage with and learn from retrieved
evidence. ActiveRAG designs a knowledge assimilation agent to form the
knowledge understanding by associating external knowledge with the parametric
memory of LLMs. Then our model employs the thought accommodation agent to
calibrate the internal thought of LLMs for response refinement. Our experiments
show that ActiveRAG achieves a 10\% improvement over vanilla RAG on various
question-answering benchmarks. Further analysis reveals that ActiveRAG
mitigates the impact of noisy retrievals, alleviates conflicts between external
knowledge and parametric memory and improves the self-consistency of LLMs in
answering the question. All data and codes are available at
https://github.com/OpenMatch/ActiveRAG.",Zhipeng Xu
2024-02-21T08:54:47Z,http://arxiv.org/abs/2402.13625v2,MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning,"Since commonsense information has been recorded significantly less frequently
than its existence, language models pre-trained by text generation have
difficulty to learn sufficient commonsense knowledge. Several studies have
leveraged text retrieval to augment the models' commonsense ability. Unlike
text, images capture commonsense information inherently but little effort has
been paid to effectively utilize them. In this work, we propose a novel
Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and
images to enhance the commonsense ability of language models. Extensive
experiments on the Common-Gen task have demonstrated the efficacy of MORE based
on the pre-trained models of both single and multiple modalities.",Wanqing Cui
2024-02-22T06:21:41Z,http://arxiv.org/abs/2402.14318v1,Assessing generalization capability of text ranking models in Polish,"Retrieval-augmented generation (RAG) is becoming an increasingly popular
technique for integrating internal knowledge bases with large language models.
In a typical RAG pipeline, three models are used, responsible for the
retrieval, reranking, and generation stages. In this article, we focus on the
reranking problem for the Polish language, examining the performance of
rerankers and comparing their results with available retrieval models. We
conduct a comprehensive evaluation of existing models and those trained by us,
utilizing a benchmark of 41 diverse information retrieval tasks for the Polish
language. The results of our experiments show that most models struggle with
out-of-domain generalization. However, a combination of effective optimization
method and a large training dataset allows for building rerankers that are both
compact in size and capable of generalization. The best of our models
establishes a new state-of-the-art for reranking in the Polish language,
outperforming existing models with up to 30 times more parameters.",Sławomir Dadas
2024-02-23T13:02:10Z,http://arxiv.org/abs/2402.15301v2,"Causal Graph Discovery with Retrieval-Augmented Generation based Large
  Language Models","Causal graph recovery is traditionally done using statistical
estimation-based methods or based on individual's knowledge about variables of
interests. They often suffer from data collection biases and limitations of
individuals' knowledge. The advance of large language models (LLMs) provides
opportunities to address these problems. We propose a novel method that
leverages LLMs to deduce causal relationships in general causal graph recovery
tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs
extracted from scientific publication database as well as experiment data about
factors of interest to achieve this goal. Our method gives a prompting strategy
to extract associational relationships among those factors and a mechanism to
perform causality verification for these associations. Comparing to other
LLM-based methods that directly instruct LLMs to do the highly complex causal
reasoning, our method shows clear advantage on causal graph quality on
benchmark datasets. More importantly, as causality among some factors may
change as new research results emerge, our method show sensitivity to new
evidence in the literature and can provide useful information for updating
causal graphs accordingly.",Yuzhe Zhang
2024-02-26T08:59:05Z,http://arxiv.org/abs/2402.16406v1,"From RAGs to riches: Using large language models to write documents for
  clinical trials","Clinical trials require numerous documents to be written -- protocols,
consent forms, clinical study reports and others. Large language models (LLMs)
offer the potential to rapidly generate first versions of these documents,
however there are concerns about the quality of their output Here we report an
evaluation of LLMs in generating parts of one such document, clinical trial
protocols. We find that an offthe-shelf LLM delivers reasonable results,
especially when assessing content relevance and the correct use of terminology.
However, deficiencies remain: specifically clinical thinking and logic, and
appropriate use of references. To improve performance, we used
retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date
information. As a result of using RAG, the writing quality of the LLM improves
substantially, which has implications for the practical useability of LLMs in
clinical trial-related writing.",Nigel Markey
2024-02-26T18:55:15Z,http://arxiv.org/abs/2402.16829v1,"GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning","Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.",Aivin V. Solatorio
2024-02-06T13:19:53Z,http://arxiv.org/abs/2402.16874v1,"Enhancing Retrieval Processes for Language Generation with Augmented
  Queries","In the rapidly changing world of smart technology, searching for documents
has become more challenging due to the rise of advanced language models. These
models sometimes face difficulties, like providing inaccurate information,
commonly known as ""hallucination."" This research focuses on addressing this
issue through Retrieval-Augmented Generation (RAG), a technique that guides
models to give accurate responses based on real facts. To overcome scalability
issues, the study explores connecting user queries with sophisticated language
models such as BERT and Orca2, using an innovative query optimization process.
The study unfolds in three scenarios: first, without RAG, second, without
additional assistance, and finally, with extra help. Choosing the compact yet
efficient Orca2 7B model demonstrates a smart use of computing resources. The
empirical results indicate a significant improvement in the initial language
model's performance under RAG, particularly when assisted with prompts
augmenters. Consistency in document retrieval across different encodings
highlights the effectiveness of using language model-generated queries. The
introduction of UMAP for BERT further simplifies document retrieval while
maintaining strong results.",Julien Pierre Edmond Ghali
2024-02-28T17:38:06Z,http://arxiv.org/abs/2402.18510v4,"RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval","This paper investigates the gap in representation powers of Recurrent Neural
Networks (RNNs) and Transformers in the context of solving algorithmic
problems. We focus on understanding whether RNNs, known for their memory
efficiency in handling long sequences, can match the performance of
Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.
Our theoretical analysis reveals that CoT improves RNNs but is insufficient to
close the gap with Transformers. A key bottleneck lies in the inability of RNNs
to perfectly retrieve information from the context, even with CoT: for several
tasks that explicitly or implicitly require this capability, such as
associative recall and determining if a graph is a tree, we prove that RNNs are
not expressive enough to solve the tasks while Transformers can solve them with
ease. Conversely, we prove that adopting techniques to enhance the in-context
retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)
and adding a single Transformer layer, can elevate RNNs to be capable of
solving all polynomial-time solvable problems with CoT, hence closing the
representation gap with Transformers.",Kaiyue Wen
2024-03-02T12:19:04Z,http://arxiv.org/abs/2403.01193v3,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,"Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.",Philip Feldman
2024-03-06T15:40:30Z,http://arxiv.org/abs/2403.03792v2,"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt
  Injection Attacks","We introduce a new family of prompt injection attacks, termed Neural Exec.
Unlike known attacks that rely on handcrafted strings (e.g., ""Ignore previous
instructions and...""), we show that it is possible to conceptualize the
creation of execution triggers as a differentiable search problem and use
learning-based methods to autonomously generate them.
  Our results demonstrate that a motivated adversary can forge triggers that
are not only drastically more effective than current handcrafted ones but also
exhibit inherent flexibility in shape, properties, and functionality. In this
direction, we show that an attacker can design and generate Neural Execs
capable of persisting through multi-stage preprocessing pipelines, such as in
the case of Retrieval-Augmented Generation (RAG)-based applications. More
critically, our findings show that attackers can produce triggers that deviate
markedly in form and shape from any known attack, sidestepping existing
blacklist-based detection and sanitation approaches.",Dario Pasquini
2024-03-06T17:48:06Z,http://arxiv.org/abs/2403.03888v3,FaaF: Facts as a Function for the evaluation of generated text,"The demand for accurate and efficient verification of information in texts
generated by large language models (LMs) is at an all-time high, but remains
unresolved. Recent efforts have focused on extracting and verifying atomic
facts from these texts via prompting LM evaluators. However, we demonstrate
that this method of prompting is unreliable when faced with incomplete or
inaccurate reference information. We introduce Facts as a Function (FaaF), a
new approach to the fact verification task that leverages the function-calling
capabilities of LMs. FaaF significantly enhances the ability of LMs to identify
unsupported facts in texts, while also improving efficiency and significantly
lowering costs compared to prompt-based methods. Additionally, we propose a
framework for evaluating factual recall in Retrieval Augmented Generation (RAG)
systems, which we employ to compare prompt-based and FaaF methods using various
LMs under challenging conditions.",Vasileios Katranidis
2024-03-07T08:25:46Z,http://arxiv.org/abs/2403.04307v3,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,"Hallucinations pose a significant challenge to the reliability of large
language models (LLMs) in critical domains. Recent benchmarks designed to
assess LLM hallucinations within conventional NLP tasks, such as
knowledge-intensive question answering (QA) and summarization, are insufficient
for capturing the complexities of user-LLM interactions in dynamic, real-world
settings. To address this gap, we introduce HaluEval-Wild, the first benchmark
specifically designed to evaluate LLM hallucinations in the wild. We
meticulously collect challenging (adversarially filtered by Alpaca) user
queries from ShareGPT, an existing real-world user-LLM interaction datasets, to
evaluate the hallucination rates of various LLMs. Upon analyzing the collected
queries, we categorize them into five distinct types, which enables a
fine-grained analysis of the types of hallucinations LLMs exhibit, and
synthesize the reference answers with the powerful GPT-4 model and
retrieval-augmented generation (RAG). Our benchmark offers a novel approach
towards enhancing our comprehension of and improving LLM reliability in
scenarios reflective of real-world interactions. Our benchmark is available at
https://github.com/HaluEval-Wild/HaluEval-Wild.",Zhiying Zhu
2024-03-07T17:13:12Z,http://arxiv.org/abs/2403.04666v2,Telecom Language Models: Must They Be Large?,"The increasing interest in Large Language Models (LLMs) within the
telecommunications sector underscores their potential to revolutionize
operational efficiency. However, the deployment of these sophisticated models
is often hampered by their substantial size and computational demands, raising
concerns about their viability in resource-constrained environments. Addressing
this challenge, recent advancements have seen the emergence of small language
models that surprisingly exhibit performance comparable to their larger
counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a
compact yet powerful model, exemplifies this new wave of efficient small
language models. This paper conducts a comprehensive evaluation of Phi-2's
intrinsic understanding of the telecommunications domain. Recognizing the
scale-related limitations, we enhance Phi-2's capabilities through a
Retrieval-Augmented Generation approach, meticulously integrating an extensive
knowledge base specifically curated with telecom standard specifications. The
enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering
questions about telecom standards with a precision that closely rivals the more
resource-intensive GPT-3.5. The paper further explores the refined capabilities
of Phi-2 in addressing problem-solving scenarios within the telecom sector,
highlighting its potential and limitations.",Nicola Piovesan
2024-03-11T16:01:05Z,http://arxiv.org/abs/2403.06840v2,"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation
  via Iterative Self-Feedback","Large language models (LLMs) demonstrate exceptional performance in numerous
tasks but still heavily rely on knowledge stored in their parameters. Moreover,
updating this knowledge incurs high training costs. Retrieval-augmented
generation (RAG) methods address this issue by integrating external knowledge.
The model can answer questions it couldn't previously by retrieving knowledge
relevant to the query. This approach improves performance in certain scenarios
for specific tasks. However, if irrelevant texts are retrieved, it may impair
model performance. In this paper, we propose Retrieval Augmented Iterative
Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and
processes them in three submodules to enhance the model's problem-solving
capabilities. Experiments show that our method outperforms existing benchmarks,
performing well on models like GPT3.5, Llama2, significantly enhancing factual
reasoning capabilities and reducing hallucinations.",Yanming Liu
2024-03-14T06:17:20Z,http://arxiv.org/abs/2403.09125v5,"Exploring the Capabilities and Limitations of Large Language Models in
  the Electric Energy Sector","Large Language Models (LLMs) as chatbots have drawn remarkable attention
thanks to their versatile capability in natural language processing as well as
in a wide range of tasks. While there has been great enthusiasm towards
adopting such foundational model-based artificial intelligence tools in all
sectors possible, the capabilities and limitations of such LLMs in improving
the operation of the electric energy sector need to be explored, and this
article identifies fruitful directions in this regard. Key future research
directions include data collection systems for fine-tuning LLMs, embedding
power system-specific tools in the LLMs, and retrieval augmented generation
(RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in
safety-critical use cases.",Subir Majumder
2024-03-14T09:45:05Z,http://arxiv.org/abs/2403.09226v2,"Retrieval augmented text-to-SQL generation for epidemiological question
  answering using electronic health records","Electronic health records (EHR) and claims data are rich sources of
real-world data that reflect patient health status and healthcare utilization.
Querying these databases to answer epidemiological questions is challenging due
to the intricacy of medical terminology and the need for complex SQL queries.
Here, we introduce an end-to-end methodology that combines text-to-SQL
generation with retrieval augmented generation (RAG) to answer epidemiological
questions using EHR and claims data. We show that our approach, which
integrates a medical coding step into the text-to-SQL process, significantly
improves the performance over simple prompting. Our findings indicate that
although current language models are not yet sufficiently accurate for
unsupervised use, RAG offers a promising direction for improving their
capabilities, as shown in a realistic industry setting.",Angelo Ziletti
2024-03-15T09:54:04Z,http://arxiv.org/abs/2403.10153v3,"Improving Medical Multi-modal Contrastive Learning with Expert
  Annotations","We introduce eCLIP, an enhanced version of the CLIP model that integrates
expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key
challenges in contrastive multi-modal medical imaging analysis, notably data
scarcity and the ""modality gap"" -- a significant disparity between image and
text embeddings that diminishes the quality of representations and hampers
cross-modal interoperability. eCLIP integrates a heatmap processor and
leverages mixup augmentation to efficiently utilize the scarce expert
annotations, thus boosting the model's learning effectiveness. eCLIP is
designed to be generally applicable to any variant of CLIP without requiring
any modifications of the core architecture. Through detailed evaluations across
several tasks, including zero-shot inference, linear probing, cross-modal
retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using
a frozen Large Language Model, eCLIP showcases consistent improvements in
embedding quality. The outcomes reveal enhanced alignment and uniformity,
affirming eCLIP's capability to harness high-quality annotations for enriched
multi-modal analysis in the medical imaging domain.",Yogesh Kumar
2024-03-15T16:30:14Z,http://arxiv.org/abs/2403.10446v1,"Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A
  Case Study on Domain-Specific Queries in Private Knowledge-Bases","We proposed an end-to-end system design towards utilizing Retrieval Augmented
Generation (RAG) to improve the factual accuracy of Large Language Models
(LLMs) for domain-specific and time-sensitive queries related to private
knowledge-bases. Our system integrates RAG pipeline with upstream datasets
processing and downstream performance evaluation. Addressing the challenge of
LLM hallucinations, we finetune models with a curated dataset which originates
from CMU's extensive resources and annotated with the teacher model. Our
experiments demonstrate the system's effectiveness in generating more accurate
answers to domain-specific and time-sensitive inquiries. The results also
revealed the limitations of fine-tuning LLMs with small-scale and skewed
datasets. This research highlights the potential of RAG systems in augmenting
LLMs with external datasets for improved performance in knowledge-intensive
tasks. Our code and models are available on Github.",Jiarui Li
2024-03-17T23:02:04Z,http://arxiv.org/abs/2403.11366v2,"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning","The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.",Anique Tahir
2024-03-18T12:58:16Z,http://arxiv.org/abs/2403.11747v2,Embedded Named Entity Recognition using Probing Classifiers,"Streaming text generation has become a common way of increasing the
responsiveness of language model powered applications, such as chat assistants.
At the same time, extracting semantic information from generated text is a
useful tool for applications such as automated fact checking or retrieval
augmented generation. Currently, this requires either separate models during
inference, which increases computational cost, or destructive fine-tuning of
the language model. Instead, we propose an approach called EMBER which enables
streaming named entity recognition in decoder-only language models without
fine-tuning them and while incurring minimal additional computational cost at
inference time. Specifically, our experiments show that EMBER maintains high
token generation rates, with only a negligible decrease in speed of around 1%
compared to a 43.64% slowdown measured for a baseline. We make our code and
data available online, including a toolkit for training, testing, and deploying
efficient token classification models optimized for streaming text generation.",Nicholas Popovič
2024-03-03T03:01:14Z,http://arxiv.org/abs/2403.14666v2,SyllabusQA: A Course Logistics Question Answering Dataset,"Automated teaching assistants and chatbots have significant potential to
reduce the workload of human instructors, especially for logistics-related
question answering, which is important to students yet repetitive for
instructors. However, due to privacy concerns, there is a lack of publicly
available datasets. We introduce SyllabusQA, an open-source dataset with 63
real course syllabi covering 36 majors, containing 5,078 open-ended course
logistics-related question-answer pairs that are diverse in both question types
and answer formats. Since many logistics-related questions contain critical
information like the date of an exam, it is important to evaluate the
factuality of answers. We benchmark several strong baselines on this task, from
large language model prompting to retrieval-augmented generation. We introduce
Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of
predicted answers. We find that despite performing close to humans on
traditional metrics of textual similarity, there remains a significant gap
between automated approaches and humans in terms of fact precision.",Nigel Fernandez
2024-03-27T04:20:18Z,http://arxiv.org/abs/2403.18243v1,"Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check","Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.",Linhao Ye
2024-03-29T00:14:46Z,http://arxiv.org/abs/2403.19889v1,Towards a Robust Retrieval-Based Summarization System,"This paper describes an investigation of the robustness of large language
models (LLMs) for retrieval augmented generation (RAG)-based summarization
tasks. While LLMs provide summarization capabilities, their performance in
complex, real-world scenarios remains under-explored. Our first contribution is
LogicSumm, an innovative evaluation framework incorporating realistic scenarios
to assess LLM robustness during RAG-based summarization. Based on limitations
identified by LogiSumm, we then developed SummRAG, a comprehensive system to
create training dialogues and fine-tune a model to enhance robustness within
LogicSumm's scenarios. SummRAG is an example of our goal of defining structured
methods to test the capabilities of an LLM, rather than addressing issues in a
one-off fashion. Experimental results confirm the power of SummRAG, showcasing
improved logical coherence and summarization quality. Data, corresponding model
weights, and Python code are available online.",Shengjie Liu
2024-03-29T03:56:19Z,http://arxiv.org/abs/2403.19964v3,FairRAG: Fair Human Generation via Fair Retrieval Augmentation,"Existing text-to-image generative models reflect or even amplify societal
biases ingrained in their training data. This is especially concerning for
human image generation where models are biased against certain demographic
groups. Existing attempts to rectify this issue are hindered by the inherent
limitations of the pre-trained models and fail to substantially improve
demographic diversity. In this work, we introduce Fair Retrieval Augmented
Generation (FairRAG), a novel framework that conditions pre-trained generative
models on reference images retrieved from an external image database to improve
fairness in human generation. FairRAG enables conditioning through a
lightweight linear module that projects reference images into the textual
space. To enhance fairness, FairRAG applies simple-yet-effective debiasing
strategies, providing images from diverse demographic groups during the
generative process. Extensive experiments demonstrate that FairRAG outperforms
existing methods in terms of demographic diversity, image-text alignment, and
image fidelity while incurring minimal computational overhead during inference.",Robik Shrestha
2024-04-01T10:43:52Z,http://arxiv.org/abs/2404.01037v1,ARAGOG: Advanced RAG Output Grading,"Retrieval-Augmented Generation (RAG) is essential for integrating external
knowledge into Large Language Model (LLM) outputs. While the literature on RAG
is growing, it primarily focuses on systematic reviews and comparisons of new
state-of-the-art (SoTA) techniques against their predecessors, with a gap in
extensive experimental comparisons. This study begins to address this gap by
assessing various RAG methods' impacts on retrieval precision and answer
similarity. We found that Hypothetical Document Embedding (HyDE) and LLM
reranking significantly enhance retrieval precision. However, Maximal Marginal
Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a
baseline Naive RAG system, and Multi-query approaches underperformed. Sentence
Window Retrieval emerged as the most effective for retrieval precision, despite
its variable performance on answer similarity. The study confirms the potential
of the Document Summary Index as a competent retrieval approach. All resources
related to this research are publicly accessible for further investigation
through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We
welcome the community to further this exploratory study in RAG systems.",Matouš Eibich
2024-04-02T15:10:11Z,http://arxiv.org/abs/2404.02022v3,"Improving Retrieval Augmented Open-Domain Question-Answering with
  Vectorized Contexts","In the era of large language models, applying techniques such as Retrieval
Augmented Generation can better address Open-Domain Question-Answering
problems. Due to constraints including model sizes and computing resources, the
length of context is often limited, and it becomes challenging to empower the
model to cover overlong contexts while answering questions from open domains.
This paper proposes a general and convenient method to covering longer contexts
in Open-Domain Question-Answering tasks. It leverages a small encoder language
model that effectively encodes contexts, and the encoding applies
cross-attention with origin inputs. With our method, the origin language models
can cover several times longer contexts while keeping the computing
requirements close to the baseline. Our experiments demonstrate that after
fine-tuning, there is improved performance across two held-in datasets, four
held-out datasets, and also in two In Context Learning settings.",Zhuo Chen
2024-04-02T17:00:11Z,http://arxiv.org/abs/2404.02103v2,"CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions
  for RAG systems","Retrieval Augmented Generation (RAG) has become a popular application for
large language models. It is preferable that successful RAG systems provide
accurate answers that are supported by being grounded in a passage without any
hallucinations. While considerable work is required for building a full RAG
pipeline, being able to benchmark performance is also necessary. We present
ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG
pipeline. ClapNQ includes long answers with grounded gold passages from Natural
Questions (NQ) and a corpus to perform either retrieval, generation, or the
full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full
passage, and cohesive, meaning that the answer is composed fluently, often by
integrating multiple pieces of the passage that are not contiguous. RAG models
must adapt to these properties to be successful at ClapNQ. We present baseline
experiments and analysis for ClapNQ that highlight areas where there is still
significant room for improvement in grounded RAG. CLAPNQ is publicly available
at https://github.com/primeqa/clapnq",Sara Rosenthal
2024-04-02T21:35:54Z,http://arxiv.org/abs/2404.02319v2,"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization","In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .",Tobias Schnabel
2024-04-03T05:31:59Z,http://arxiv.org/abs/2404.02474v1,uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?,"Inspired by human cognition, Jiang et al.(2023c) create a benchmark for
assessing LLMs' lateral thinking-thinking outside the box. Building upon this
benchmark, we investigate how different prompting methods enhance LLMs'
performance on this task to reveal their inherent power for outside-the-box
thinking ability. Through participating in SemEval-2024, task 9, Sentence
Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)
and direct prompting, enhancing with informative descriptions, and employing
contextualizing prompts using a retrieval augmented generation (RAG) pipeline.
Our experiments involve three LLMs including GPT-3.5, GPT-4, and
Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and
options using GPT-4, validated by humans for quality. Findings indicate that
compressed informative prompts enhance performance. Dynamic in-context learning
enhances model performance significantly. Furthermore, fine-tuning Zephyr on
our dataset enhances performance across other commonsense datasets,
underscoring the value of innovative thinking.",Pouya Sadeghi
2024-04-04T00:10:39Z,http://arxiv.org/abs/2404.03122v1,"Towards Standards-Compliant Assistive Technology Product Specifications
  via LLMs","In the rapidly evolving field of assistive technology (AT), ensuring that
products meet national and international standards is essential for user
safety, efficacy, and accessibility. In this vision paper, we introduce
CompliAT, a pioneering framework designed to streamline the compliance process
of AT product specifications with these standards through the innovative use of
Large Language Models (LLMs). CompliAT addresses three critical tasks: checking
terminology consistency, classifying products according to standards, and
tracing key product specifications to standard requirements. We tackle the
challenge of terminology consistency to ensure that the language used in
product specifications aligns with relevant standards, reducing
misunderstandings and non-compliance risks. We propose a novel approach for
product classification, leveraging a retrieval-augmented generation model to
accurately categorize AT products aligning to international standards, despite
the sparse availability of training data. Finally, CompliAT implements a
traceability and compliance mechanism from key product specifications to
standard requirements, ensuring all aspects of an AT product are thoroughly
vetted against the corresponding standards. By semi-automating these processes,
CompliAT aims to significantly reduce the time and effort required for AT
product standards compliance and uphold quality and safety standards. We
outline our planned implementation and evaluation plan for CompliAT.",Chetan Arora
2024-04-05T18:44:54Z,http://arxiv.org/abs/2404.04351v2,"Assisting humans in complex comparisons: automated information
  comparison at scale","Generative Large Language Models enable efficient analytics across knowledge
domains, rivalling human experts in information comparisons. However, the
applications of LLMs for information comparisons face scalability challenges
due to the difficulties in maintaining information across large contexts and
overcoming model token limitations. To address these challenges, we developed
the novel Abstractive Summarization & Criteria-driven Comparison Endpoint
(ASC$^2$End) system to automate information comparison at scale. Our system
employs Semantic Text Similarity comparisons for generating evidence-supported
analyses. We utilize proven data-handling strategies such as abstractive
summarization and retrieval augmented generation to overcome token limitations
and retain relevant information during model inference. Prompts were designed
using zero-shot strategies to contextualize information for improved model
reasoning. We evaluated abstractive summarization using ROUGE scoring and
assessed the generated comparison quality using survey responses. Models
evaluated on the ASC$^2$End system show desirable results providing insights on
the expected performance of the system. ASC$^2$End is a novel system and tool
that enables accurate, automated information comparison at scale across
knowledge domains, overcoming limitations in context length and retrieval.",Truman Yuen
2024-04-06T05:44:53Z,http://arxiv.org/abs/2404.04510v1,"IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe
  Biomedical Natural Language Inference for Clinical Trials","Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.",Shreyasi Mandal
2024-04-08T15:00:36Z,http://arxiv.org/abs/2404.05587v2,"Enhancing Software-Related Information Extraction via Single-Choice
  Question Answering with Large Language Models","This paper describes our participation in the Shared Task on Software
Mentions Disambiguation (SOMD), with a focus on improving relation extraction
in scholarly texts through generative Large Language Models (LLMs) using
single-choice question-answering. The methodology prioritises the use of
in-context learning capabilities of GLMs to extract software-related entities
and their descriptive attributes, such as distributive information. Our
approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for
Named Entity Recognition (NER) and Attributive NER to identify relationships
between extracted software entities, providing a structured solution for
analysing software citations in academic literature. The paper provides a
detailed description of our approach, demonstrating how using GLMs in a
single-choice QA paradigm can greatly enhance IE methodologies. Our
participation in the SOMD shared task highlights the importance of precise
software citation practices and showcases our system's ability to overcome the
challenges of disambiguating and extracting relationships between software
mentions. This sets the groundwork for future research and development in this
field.",Wolfgang Otto
2024-04-09T04:20:27Z,http://arxiv.org/abs/2404.06004v1,"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free
  Information Retrieval","In approximate nearest neighbor search (ANNS) methods based on approximate
proximity graphs, DiskANN achieves good recall-speed balance for large-scale
datasets using both of RAM and storage. Despite it claims to save memory usage
by loading compressed vectors by product quantization (PQ), its memory usage
increases in proportion to the scale of datasets. In this paper, we propose
All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the
compressed vectors to storage. Our method achieves $\sim$10 MB memory usage in
query search even with billion-scale datasets with minor performance
degradation. AiSAQ also reduces the index load time before query search, which
enables the index switch between muitiple billion-scale datasets and
significantly enhances the flexibility of retrieval-augmented generation (RAG).
This method is applicable to all graph-based ANNS algorithms and can be
combined with higher-spec ANNS methods in the future.",Kento Tatsuno
2024-04-09T13:02:22Z,http://arxiv.org/abs/2404.06278v1,"Dimensionality Reduction in Sentence Transformer Vector Databases with
  Fast Fourier Transform","Dimensionality reduction in vector databases is pivotal for streamlining AI
data management, enabling efficient storage, faster computation, and improved
model performance. This paper explores the benefits of reducing vector database
dimensions, with a focus on computational efficiency and overcoming the curse
of dimensionality. We introduce a novel application of Fast Fourier Transform
(FFT) to dimensionality reduction, a method previously underexploited in this
context. By demonstrating its utility across various AI domains, including
Retrieval-Augmented Generation (RAG) models and image processing, this
FFT-based approach promises to improve data retrieval processes and enhance the
efficiency and scalability of AI solutions. The incorporation of FFT may not
only optimize operations in real-time processing and recommendation systems but
also extend to advanced image processing techniques, where dimensionality
reduction can significantly improve performance and analysis efficiency. This
paper advocates for the broader adoption of FFT in vector database management,
marking a significant stride towards addressing the challenges of data volume
and complexity in AI research and applications. Unlike many existing
approaches, we directly handle the embedding vectors produced by the model
after processing a test input.",Vitaly Bulgakov
2024-04-10T02:02:34Z,http://arxiv.org/abs/2404.06680v1,"Onco-Retriever: Generative Classifier for Retrieval of EHR Records in
  Oncology","Retrieving information from EHR systems is essential for answering specific
questions about patient journeys and improving the delivery of clinical care.
Despite this fact, most EHR systems still rely on keyword-based searches. With
the advent of generative large language models (LLMs), retrieving information
can lead to better search and summarization capabilities. Such retrievers can
also feed Retrieval-augmented generation (RAG) pipelines to answer any query.
However, the task of retrieving information from EHR real-world clinical data
contained within EHR systems in order to solve several downstream use cases is
challenging due to the difficulty in creating query-document support pairs. We
provide a blueprint for creating such datasets in an affordable manner using
large language models. Our method results in a retriever that is 30-50 F-1
points better than propriety counterparts such as Ada and Mistral for oncology
data elements. We further compare our model, called Onco-Retriever, against
fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation
on real-world EHR data along with latency analysis of the different models and
provide a path forward for healthcare organizations to build domain-specific
retrievers.",Shashi Kant Gupta
2024-03-23T00:49:40Z,http://arxiv.org/abs/2404.07221v2,"Improving Retrieval for RAG based Question Answering Models on Financial
  Documents","The effectiveness of Large Language Models (LLMs) in generating accurate
responses relies heavily on the quality of input provided, particularly when
employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by
sourcing the most relevant text chunk(s) to base queries upon. Despite the
significant advancements in LLMs' response quality in recent years, users may
still encounter inaccuracies or irrelevant answers; these issues often stem
from suboptimal text chunk retrieval by RAG rather than the inherent
capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine
the RAG process. This paper explores the existing constraints of RAG pipelines
and introduces methodologies for enhancing text retrieval. It delves into
strategies such as sophisticated chunking techniques, query expansion, the
incorporation of metadata annotations, the application of re-ranking
algorithms, and the fine-tuning of embedding algorithms. Implementing these
approaches can substantially improve the retrieval quality, thereby elevating
the overall performance and reliability of LLMs in processing and responding to
queries.",Spurthi Setty
2024-04-10T22:26:26Z,http://arxiv.org/abs/2404.07376v2,LLMs in Biomedicine: A study on clinical Named Entity Recognition,"Large Language Models (LLMs) demonstrate remarkable versatility in various
NLP tasks but encounter distinct challenges in biomedical due to the
complexities of language and data scarcity. This paper investigates LLMs
application in the biomedical domain by exploring strategies to enhance their
performance for the NER task. Our study reveals the importance of meticulously
designed prompts in the biomedical. Strategic selection of in-context examples
yields a marked improvement, offering ~15-20\% increase in F1 score across all
benchmark datasets for biomedical few-shot NER. Additionally, our results
indicate that integrating external biomedical knowledge via prompting
strategies can enhance the proficiency of general-purpose LLMs to meet the
specialized needs of biomedical NER. Leveraging a medical knowledge base, our
proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can
boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at
\url{https://github.com/masoud-monajati/LLM_Bio_NER}",Masoud Monajatipoor
2024-04-13T09:33:00Z,http://arxiv.org/abs/2404.08940v1,Introducing Super RAGs in Mistral 8x7B-v1,"The relentless pursuit of enhancing Large Language Models (LLMs) has led to
the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel
approach designed to elevate the performance of LLMs by integrating external
knowledge sources with minimal structural modifications. This paper presents
the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,
and examines the resultant improvements in accuracy, speed, and user
satisfaction. Our methodology uses a fine-tuned instruct model setup and a
cache tuning fork system, ensuring efficient and relevant data retrieval. The
evaluation, conducted over several epochs, demonstrates significant
enhancements across all metrics. The findings suggest that Super RAGs can
effectively augment LLMs, paving the way for more sophisticated and reliable AI
systems. This research contributes to the field by providing empirical evidence
of the benefits of Super RAGs and offering insights into their potential
applications.",Ayush Thakur
2024-04-16T17:59:10Z,http://arxiv.org/abs/2404.10774v2,MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents,"Recognizing if LLM output can be grounded in evidence is central to many
tasks in NLP: retrieval-augmented generation, summarization, document-grounded
dialogue, and more. Current approaches to this kind of fact-checking are based
on verifying each piece of a model generation against potential evidence using
an LLM. However, this process can be very computationally expensive, requiring
many calls to a model to check a single response. In this work, we show how to
build small fact-checking models that have GPT-4-level performance but for 400x
lower cost. We do this by constructing synthetic training data with GPT-4,
which involves creating realistic yet challenging instances of factual errors
via a structured generation procedure. Training on this data teaches models to
check each fact in the claim and recognize synthesis of information across
sentences. For evaluation, we unify datasets from recent work on fact-checking
and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best
system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable
size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data
synthesis, and models.",Liyan Tang
2024-04-17T01:27:42Z,http://arxiv.org/abs/2404.10981v2,"A Survey on Retrieval-Augmented Text Generation for Large Language
  Models","Retrieval-Augmented Generation (RAG) merges retrieval methods with deep
learning advancements to address the static limitations of large language
models (LLMs) by enabling the dynamic integration of up-to-date external
information. This methodology, focusing primarily on the text domain, provides
a cost-effective solution to the generation of plausible but possibly incorrect
responses by LLMs, thereby enhancing the accuracy and reliability of their
outputs through the use of real-world data. As RAG grows in complexity and
incorporates multiple concepts that can influence its performance, this paper
organizes the RAG paradigm into four categories: pre-retrieval, retrieval,
post-retrieval, and generation, offering a detailed perspective from the
retrieval viewpoint. It outlines RAG's evolution and discusses the field's
progression through the analysis of significant studies. Additionally, the
paper introduces evaluation methods for RAG, addressing the challenges faced
and proposing future research directions. By offering an organized framework
and categorization, the study aims to consolidate existing research on RAG,
clarify its technological underpinnings, and highlight its potential to broaden
the adaptability and applications of LLMs.",Yizheng Huang
2024-04-17T10:00:56Z,http://arxiv.org/abs/2404.11216v2,"Position Engineering: Boosting Large Language Models through Positional
  Information Manipulation","The performance of large language models (LLMs) is significantly influenced
by the quality of the prompts provided. In response, researchers have developed
enormous prompt engineering strategies aimed at modifying the prompt text to
enhance task performance. In this paper, we introduce a novel technique termed
position engineering, which offers a more efficient way to guide large language
models. Unlike prompt engineering, which requires substantial effort to modify
the text provided to LLMs, position engineering merely involves altering the
positional information in the prompt without modifying the text itself. We have
evaluated position engineering in two widely-used LLM scenarios:
retrieval-augmented generation (RAG) and in-context learning (ICL). Our
findings show that position engineering substantially improves upon the
baseline in both cases. Position engineering thus represents a promising new
strategy for exploiting the capabilities of large language models.",Zhiyuan He
2024-04-17T18:13:16Z,http://arxiv.org/abs/2404.11672v1,MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory,"While current large language models (LLMs) demonstrate some capabilities in
knowledge-intensive tasks, they are limited by relying on their parameters as
an implicit storage mechanism. As a result, they struggle with infrequent
knowledge and temporal degradation. In addition, the uninterpretable nature of
parametric memorization makes it challenging to understand and prevent
hallucination. Parametric memory pools and model editing are only partial
solutions. Retrieval Augmented Generation (RAG) $\unicode{x2013}$ though
non-parametric $\unicode{x2013}$ has its own limitations: it lacks structure,
complicates interpretability and makes it hard to effectively manage stored
knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs
by integrating a structured and explicit read-and-write memory module. MemLLM
tackles the aforementioned challenges by enabling dynamic interaction with the
memory and improving the LLM's capabilities in using stored knowledge. Our
experiments indicate that MemLLM enhances the LLM's performance and
interpretability, in language modeling in general and knowledge-intensive tasks
in particular. We see MemLLM as an important step towards making LLMs more
grounded and factual through memory augmentation.",Ali Modarressi
2024-04-18T08:01:20Z,http://arxiv.org/abs/2404.11973v1,"Exploring the landscape of large language models: Foundations,
  techniques, and challenges","In this review paper, we delve into the realm of Large Language Models
(LLMs), covering their foundational principles, diverse applications, and
nuanced training processes. The article sheds light on the mechanics of
in-context learning and a spectrum of fine-tuning approaches, with a special
focus on methods that optimize efficiency in parameter usage. Additionally, it
explores how LLMs can be more closely aligned with human preferences through
innovative reinforcement learning frameworks and other novel methods that
incorporate human feedback. The article also examines the emerging technique of
retrieval augmented generation, integrating external knowledge into LLMs. The
ethical dimensions of LLM deployment are discussed, underscoring the need for
mindful and responsible application. Concluding with a perspective on future
research trajectories, this review offers a succinct yet comprehensive overview
of the current state and emerging trends in the evolving landscape of LLMs,
serving as an insightful guide for both researchers and practitioners in
artificial intelligence.",Milad Moradi
2024-04-18T10:25:42Z,http://arxiv.org/abs/2404.12065v2,"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political
  Fact-Checking using Multimodal Large Language Models","The escalating challenge of misinformation, particularly in political
discourse, requires advanced fact-checking solutions; this is even clearer in
the more complex scenario of multimodal claims. We tackle this issue using a
multimodal large language model in conjunction with retrieval-augmented
generation (RAG), and introduce two novel reasoning techniques: Chain of RAG
(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by
extracting both textual and image content, retrieving external information, and
reasoning subsequent questions to be answered based on prior evidence. We
achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique
by 0.14 points. Human evaluation confirms that the vast majority of our
generated fact-check explanations contain all information from gold standard
data.",M. Abdul Khaliq
2024-04-22T05:46:40Z,http://arxiv.org/abs/2404.13892v2,Retrieval-Augmented Audio Deepfake Detection,"With recent advances in speech synthesis including text-to-speech (TTS) and
voice conversion (VC) systems enabling the generation of ultra-realistic audio
deepfakes, there is growing concern about their potential misuse. However, most
deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a
single model, resulting in performance bottlenecks and transparency issues.
Inspired by retrieval-augmented generation (RAG), we propose a
retrieval-augmented detection (RAD) framework that augments test samples with
similar retrieved samples for enhanced detection. We also extend the
multi-fusion attentive classifier to integrate it with our proposed RAD
framework. Extensive experiments show the superior performance of the proposed
RAD framework over baseline methods, achieving state-of-the-art results on the
ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.
Further sample analysis indicates that the retriever consistently retrieves
samples mostly from the same speaker with acoustic characteristics highly
consistent with the query audio, thereby improving detection performance.",Zuheng Kang
2024-04-23T05:51:45Z,http://arxiv.org/abs/2404.14760v2,Retrieval Augmented Generation for Domain-specific Question Answering,"Question answering (QA) has become an important application in the advanced
development of large language models. General pre-trained large language models
for question-answering are not trained to properly understand the knowledge or
terminology for a specific domain, such as finance, healthcare, education, and
customer service for a product. To better cater to domain-specific
understanding, we build an in-house question-answering system for Adobe
products. We propose a novel framework to compile a large question-answer
database and develop the approach for retrieval-aware finetuning of a Large
Language model. We showcase that fine-tuning the retriever leads to major
improvements in the final generation. Our overall approach reduces
hallucinations during generation while keeping in context the latest retrieval
information for contextual grounding.",Sanat Sharma
2024-04-23T18:00:09Z,http://arxiv.org/abs/2404.15406v2,"Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal
  LLMs","Multimodal LLMs are the natural evolution of LLMs, and enlarge their
capabilities so as to work beyond the pure textual modality. As research is
being carried out to design novel architectures and vision-and-language
adapters, in this paper we concentrate on endowing such models with the
capability of answering questions that require external knowledge. Our
approach, termed Wiki-LLaVA, aims at integrating an external knowledge source
of multimodal documents, which is accessed through a hierarchical retrieval
pipeline. Relevant passages, using this approach, are retrieved from the
external knowledge source and employed as additional context for the LLM,
augmenting the effectiveness and precision of generated dialogues. We conduct
extensive experiments on datasets tailored for visual question answering with
external data and demonstrate the appropriateness of our approach.",Davide Caffagni
2024-04-24T15:58:59Z,http://arxiv.org/abs/2404.15939v3,"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language
  Models for Telecommunications","The application of Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems in the telecommunication domain presents unique
challenges, primarily due to the complex nature of telecom standard documents
and the rapid evolution of the field. The paper introduces Telco-RAG, an
open-source RAG framework designed to handle the specific needs of
telecommunications standards, particularly 3rd Generation Partnership Project
(3GPP) documents. Telco-RAG addresses the critical challenges of implementing a
RAG pipeline on highly technical content, paving the way for applying LLMs in
telecommunications and offering guidelines for RAG implementation in other
technical domains.",Andrei-Laurentiu Bornea
2024-04-24T17:59:36Z,http://arxiv.org/abs/2404.16032v2,"Studying Large Language Model Behaviors Under Context-Memory Conflicts
  With Real Documents","Retrieval-augmented generation (RAG) mitigates many problems of fully
parametric language models, such as temporal degradation, hallucinations, and
lack of grounding. In RAG, the model's knowledge can be updated from documents
provided in context. This leads to cases of conflict between the model's
parametric knowledge and the contextual information, where the model may not
always update its knowledge. Previous work studied context-memory knowledge
conflicts by creating synthetic documents that contradict the model's correct
parametric answers. We present a framework for studying such knowledge
conflicts in a realistic setup. We update incorrect parametric knowledge using
real conflicting documents. This reflects how knowledge conflicts arise in
practice. In this realistic scenario, we find that knowledge updates fail less
often than previously reported. In cases where the models still fail to update
their answers, we find a parametric bias: the incorrect parametric answer
appearing in context makes the knowledge update likelier to fail. These results
suggest that the factual parametric knowledge of LLMs can negatively influence
their reading abilities and behaviors. Our code is available at
https://github.com/kortukov/realistic_knowledge_conflicts/ .",Evgenii Kortukov
2024-04-24T19:30:18Z,http://arxiv.org/abs/2404.16160v2,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,"Large language models (LLMs) have demonstrated impressive generalization
capabilities on specific tasks with human-written instruction data. However,
the limited quantity, diversity, and professional expertise of such instruction
data raise concerns about the performance of LLMs in psychotherapy tasks when
provided with domain-specific instructions. To address this, we firstly propose
Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and
secondly, we use an adaption fine-tuning method and retrieval augmented
generation method to improve pre-trained LLMs. Through quantitative evaluation
of linguistic quality using automatic and human evaluation, we observe that
pre-trained LLMs on Psychotherapy Assistant Instructions outperform
state-of-the-art LLMs response baselines. Our Assistant-Instruction approach
offers a half-annotation method to align pre-trained LLMs with instructions and
provide pre-trained LLMs with more psychotherapy knowledge.",Cheng Kang
2024-04-25T13:10:48Z,http://arxiv.org/abs/2404.16587v1,"Understanding Privacy Risks of Embeddings Induced by Large Language
  Models","Large language models (LLMs) show early signs of artificial general
intelligence but struggle with hallucinations. One promising solution to
mitigate these hallucinations is to store external knowledge as embeddings,
aiding LLMs in retrieval-augmented generation. However, such a solution risks
compromising privacy, as recent studies experimentally showed that the original
text can be partially reconstructed from text embeddings by pre-trained
language models. The significant advantage of LLMs over traditional pre-trained
models may exacerbate these concerns. To this end, we investigate the
effectiveness of reconstructing original knowledge and predicting entity
attributes from these embeddings when LLMs are employed. Empirical findings
indicate that LLMs significantly improve the accuracy of two evaluated tasks
over those from pre-trained models, regardless of whether the texts are
in-distribution or out-of-distribution. This underscores a heightened potential
for LLMs to jeopardize user privacy, highlighting the negative consequences of
their widespread use. We further discuss preliminary strategies to mitigate
this risk.",Zhihao Zhu
2024-04-26T07:11:18Z,http://arxiv.org/abs/2404.17196v1,"Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
  Applications","Presently, with the assistance of advanced LLM application development
frameworks, more and more LLM-powered applications can effortlessly augment the
LLMs' knowledge with external content using the retrieval augmented generation
(RAG) technique. However, these frameworks' designs do not have sufficient
consideration of the risk of external content, thereby allowing attackers to
undermine the applications developed with these frameworks. In this paper, we
reveal a new threat to LLM-powered applications, termed retrieval poisoning,
where attackers can guide the application to yield malicious responses during
the RAG process. Specifically, through the analysis of LLM application
frameworks, attackers can craft documents visually indistinguishable from
benign ones. Despite the documents providing correct information, once they are
used as reference sources for RAG, the application is misled into generating
incorrect responses. Our preliminary experiments indicate that attackers can
mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate
in the real-world application, demonstrating the potential impact of retrieval
poisoning.",Quan Zhang
2024-04-26T11:51:53Z,http://arxiv.org/abs/2404.17347v1,InspectorRAGet: An Introspection Platform for RAG Evaluation,"Large Language Models (LLM) have become a popular approach for implementing
Retrieval Augmented Generation (RAG) systems, and a significant amount of
effort has been spent on building good models and metrics. In spite of
increased recognition of the need for rigorous evaluation of RAG systems, few
tools exist that go beyond the creation of model output and automatic
calculation. We present InspectorRAGet, an introspection platform for RAG
evaluation. InspectorRAGet allows the user to analyze aggregate and
instance-level performance of RAG systems, using both human and algorithmic
metrics as well as annotator quality. InspectorRAGet is suitable for multiple
use cases and is available publicly to the community. The demo video is
available at https://youtu.be/MJhe8QIXcEc",Kshitij Fadnis
2024-04-30T19:51:37Z,http://arxiv.org/abs/2405.00175v1,"Towards a Search Engine for Machines: Unified Ranking for Multiple
  Retrieval-Augmented Large Language Models","This paper introduces uRAG--a framework with a unified retrieval engine that
serves multiple downstream retrieval-augmented generation (RAG) systems. Each
RAG system consumes the retrieval results for a unique purpose, such as
open-domain question answering, fact verification, entity linking, and relation
extraction. We introduce a generic training guideline that standardizes the
communication between the search engine and the downstream RAG systems that
engage in optimizing the retrieval model. This lays the groundwork for us to
build a large-scale experimentation ecosystem consisting of 18 RAG systems that
engage in training and 18 unknown RAG systems that use the uRAG as the new
users of the search engine. Using this experimentation ecosystem, we answer a
number of fundamental research questions that improve our understanding of
promises and challenges in developing search engines for machines.",Alireza Salemi
2024-05-02T15:06:18Z,http://arxiv.org/abs/2405.01359v1,GAIA: A General AI Assistant for Intelligent Accelerator Operations,"Large-scale machines like particle accelerators are usually run by a team of
experienced operators. In case of a particle accelerator, these operators
possess suitable background knowledge on both accelerator physics and the
technology comprising the machine. Due to the complexity of the machine,
particular subsystems of the machine are taken care of by experts, who the
operators can turn to. In this work the reasoning and action (ReAct) prompting
paradigm is used to couple an open-weights large language model (LLM) with a
high-level machine control system framework and other tools, e.g. the
electronic logbook or machine design documentation. By doing so, a multi-expert
retrieval augmented generation (RAG) system is implemented, which assists
operators in knowledge retrieval tasks, interacts with the machine directly if
needed, or writes high level control system scripts. This consolidation of
expert knowledge and machine interaction can simplify and speed up machine
operation tasks for both new and experienced human operators.",Frank Mayet
2024-04-28T14:58:55Z,http://arxiv.org/abs/2405.01585v1,"Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular
  RAG Applications","In recent times Large Language Models have exhibited tremendous capabilities,
especially in the areas of mathematics, code generation and general-purpose
reasoning. However for specialized domains especially in applications that
require parsing and analyzing large chunks of numeric or tabular data even
state-of-the-art (SOTA) models struggle. In this paper, we introduce a new
approach to solving domain-specific tabular data analysis tasks by presenting a
unique RAG workflow that mitigates the scalability issues of existing tabular
LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel
approach to fine-tune embedding models for tabular Retrieval-Augmentation
Generation (RAG) applications. Embedding models form a crucial component in the
RAG workflow and even current SOTA embedding models struggle as they are
predominantly trained on textual datasets and thus underperform in scenarios
involving complex tabular data. The evaluation results showcase that our
approach not only outperforms current SOTA embedding models in this domain but
also does so with a notably smaller and more efficient model structure.",Sujit Khanna
2024-05-08T04:07:38Z,http://arxiv.org/abs/2405.06697v1,Automated Conversion of Static to Dynamic Scheduler via Natural Language,"In this paper, we explore the potential application of Large Language Models
(LLMs) that will automatically model constraints and generate code for dynamic
scheduling problems given an existing static model. Static scheduling problems
are modelled and coded by optimization experts. These models may be easily
obsoleted as the underlying constraints may need to be fine-tuned in order to
reflect changes in the scheduling rules. Furthermore, it may be necessary to
turn a static model into a dynamic one in order to cope with disturbances in
the environment. In this paper, we propose a Retrieval-Augmented Generation
(RAG) based LLM model to automate the process of implementing constraints for
Dynamic Scheduling (RAGDyS), without seeking help from an optimization modeling
expert. Our framework aims to minimize technical complexities related to
mathematical modelling and computational workload for end-users, thereby
allowing end-users to quickly obtain a new schedule close to the original
schedule with changes reflected by natural language constraint descriptions.",Paul Mingzheng Tang
2024-05-15T07:48:10Z,http://arxiv.org/abs/2405.09161v2,"Exploring the Potential of Large Language Models for Automation in
  Technical Customer Service","Purpose: The purpose of this study is to investigate the potential of Large
Language Models (LLMs) in transforming technical customer service (TCS) through
the automation of cognitive tasks. Design/Methodology/Approach: Using a
prototyping approach, the research assesses the feasibility of automating
cognitive tasks in TCS with LLMs, employing real-world technical incident data
from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks
such as translation, summarization, and content generation can be effectively
automated with LLMs like GPT-4, while higher-level tasks such as reasoning
require more advanced technological approaches such as Retrieval-Augmented
Generation (RAG) or finetuning ; furthermore, the study underscores the
significance of data ecosystems in enabling more complex cognitive tasks by
fostering data sharing among various actors involved. Originality/Value: This
study contributes to the emerging theory on LLM potential and technical
feasibility in service management, providing concrete insights for operators of
TCS units and highlighting the need for further research to address limitations
and validate the applicability of LLMs across different domains.",Jochen Wulf
2024-05-16T10:53:31Z,http://arxiv.org/abs/2405.09980v1,FinTextQA: A Dataset for Long-form Financial Question Answering,"Accurate evaluation of financial question answering (QA) systems necessitates
a comprehensive dataset encompassing diverse question types and contexts.
However, current financial QA datasets lack scope diversity and question
complexity. This work introduces FinTextQA, a novel dataset for long-form
question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,
source-attributed QA pairs extracted and selected from finance textbooks and
government agency websites.Moreover, we developed a Retrieval-Augmented
Generation (RAG)-based LFQA system, comprising an embedder, retriever,
reranker, and generator. A multi-faceted evaluation approach, including human
ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the
performance of different LFQA system configurations under heightened noisy
conditions. The results indicate that: (1) Among all compared generators,
Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The
most effective system configuration on our dataset involved setting the
embedder, retriever, reranker, and generator as Ada2, Automated Merged
Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are
less susceptible to noise after the length of contexts reaching a specific
threshold.",Jian Chen
2024-05-20T14:03:05Z,http://arxiv.org/abs/2405.12035v1,KG-RAG: Bridging the Gap Between Knowledge and Creativity,"Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.",Diego Sanmartin
2024-05-20T20:27:00Z,http://arxiv.org/abs/2405.12363v2,Question-Based Retrieval using Atomic Units for Enterprise RAG,"Enterprise retrieval augmented generation (RAG) offers a highly flexible
framework for combining powerful large language models (LLMs) with internal,
possibly temporally changing, documents. In RAG, documents are first chunked.
Relevant chunks are then retrieved for a user query, which are passed as
context to a synthesizer LLM to generate the query response. However, the
retrieval step can limit performance, as incorrect chunks can lead the
synthesizer LLM to generate a false response. This work applies a zero-shot
adaptation of standard dense retrieval steps for more accurate chunk recall.
Specifically, a chunk is first decomposed into atomic statements. A set of
synthetic questions are then generated on these atoms (with the chunk as the
context). Dense retrieval involves finding the closest set of synthetic
questions, and associated chunks, to the user query. It is found that retrieval
with the atoms leads to higher recall than retrieval with chunks. Further
performance gain is observed with retrieval using the synthetic questions
generated over the atoms. Higher recall at the retrieval step enables higher
performance of the enterprise LLM using the RAG pipeline.",Vatsal Raina
2024-05-23T11:00:19Z,http://arxiv.org/abs/2405.14431v1,RaFe: Ranking Feedback Improves Query Rewriting for RAG,"As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)
techniques have evolved, query rewriting has been widely incorporated into the
RAG system for downstream tasks like open-domain QA. Many works have attempted
to utilize small models with reinforcement learning rather than costly LLMs to
improve query rewriting. However, current methods require annotations (e.g.,
labeled relevant documents or downstream answers) or predesigned rewards for
feedback, which lack generalization, and fail to utilize signals tailored for
query rewriting. In this paper, we propose ours, a framework for training query
rewriting models free of annotations. By leveraging a publicly available
reranker, ours~provides feedback aligned well with the rewriting objectives.
Experimental results demonstrate that ours~can obtain better performance than
baselines.",Shengyu Mao
2024-05-23T20:04:54Z,http://arxiv.org/abs/2405.15028v1,AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings,"Ranking is a fundamental and popular problem in search. However, existing
ranking algorithms usually restrict the granularity of ranking to full passages
or require a specific dense index for each desired level of granularity. Such
lack of flexibility in granularity negatively affects many applications that
can benefit from more granular ranking, such as sentence-level ranking for
open-domain question-answering, or proposition-level ranking for attribution.
In this work, we introduce the idea of any-granularity ranking, which leverages
multi-vector embeddings to rank at varying levels of granularity while
maintaining encoding at a single (coarser) level of granularity. We propose a
multi-granular contrastive loss for training multi-vector approaches, and
validate its utility with both sentences and propositions as ranking units.
Finally, we demonstrate the application of proposition-level ranking to
post-hoc citation addition in retrieval-augmented generation, surpassing the
performance of prompt-driven citation generation.",Revanth Gangi Reddy
2024-05-24T13:44:25Z,http://arxiv.org/abs/2405.15556v1,Certifiably Robust RAG against Retrieval Corruption,"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval
corruption attacks: an attacker can inject malicious passages into retrieval
results to induce inaccurate responses. In this paper, we propose RobustRAG as
the first defense framework against retrieval corruption attacks. The key
insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM
responses from each passage in isolation and then securely aggregate these
isolated responses. To instantiate RobustRAG, we design keyword-based and
decoding-based algorithms for securely aggregating unstructured text responses.
Notably, RobustRAG can achieve certifiable robustness: we can formally prove
and certify that, for certain queries, RobustRAG can always return accurate
responses, even when the attacker has full knowledge of our defense and can
arbitrarily inject a small number of malicious passages. We evaluate RobustRAG
on open-domain QA and long-form text generation datasets and demonstrate its
effectiveness and generalizability across various tasks and datasets.",Chong Xiang
2024-05-25T05:45:55Z,http://arxiv.org/abs/2405.16072v4,"SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS
  Design Generation","In this paper, we introduce SynthAI, a new method for the automated creation
of High-Level Synthesis (HLS) designs. SynthAI integrates ReAct agents,
Chain-of-Thought (CoT) prompting, web search technologies, and the
Retrieval-Augmented Generation (RAG) framework within a structured decision
graph. This innovative approach enables the systematic decomposition of complex
hardware design tasks into multiple stages and smaller, manageable modules. As
a result, SynthAI produces synthesizable designs that closely adhere to
user-specified design objectives and functional requirements. We further
validate the capabilities of SynthAI through several case studies, highlighting
its proficiency in generating complex, multi-module logic designs from a single
initial prompt. The SynthAI code is provided via the following repo:
\url{https://github.com/sarashs/FPGA_AGI}",Seyed Arash Sheikholeslam
2024-05-27T11:18:25Z,http://arxiv.org/abs/2405.17053v2,"WirelessLLM: Empowering Large Language Models Towards Wireless
  Intelligence","The rapid evolution of wireless technologies and the growing complexity of
network infrastructures necessitate a paradigm shift in how communication
networks are designed, configured, and managed. Recent advancements in Large
Language Models (LLMs) have sparked interest in their potential to
revolutionize wireless communication systems. However, existing studies on LLMs
for wireless systems are limited to a direct application for telecom language
understanding. To empower LLMs with knowledge and expertise in the wireless
domain, this paper proposes WirelessLLM, a comprehensive framework for adapting
and enhancing LLMs to address the unique challenges and requirements of
wireless communication networks. We first identify three foundational
principles that underpin WirelessLLM: knowledge alignment, knowledge fusion,
and knowledge evolution. Then, we investigate the enabling technologies to
build WirelessLLM, including prompt engineering, retrieval augmented
generation, tool usage, multi-modal pre-training, and domain-specific
fine-tuning. Moreover, we present three case studies to demonstrate the
practical applicability and benefits of WirelessLLM for solving typical
problems in wireless networks. Finally, we conclude this paper by highlighting
key challenges and outlining potential avenues for future research.",Jiawei Shao
2024-05-27T13:16:29Z,http://arxiv.org/abs/2405.17147v1,"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability","The rapid advancement of Large Language Models (LLMs) has significantly
impacted human-computer interaction, epitomized by the release of GPT-4o, which
introduced comprehensive multi-modality capabilities. In this paper, we first
explored the deployment strategies, economic considerations, and sustainability
challenges associated with the state-of-the-art LLMs. More specifically, we
discussed the deployment debate between Retrieval-Augmented Generation (RAG)
and fine-tuning, highlighting their respective advantages and limitations.
After that, we quantitatively analyzed the requirement of xPUs in training and
inference. Additionally, for the tokenomics of LLM services, we examined the
balance between performance and cost from the quality of experience (QoE)'s
perspective of end users. Lastly, we envisioned the future hybrid architecture
of LLM processing and its corresponding sustainability concerns, particularly
in the environmental carbon footprint impact. Through these discussions, we
provided a comprehensive overview of the operational and strategic
considerations essential for the responsible development and deployment of
LLMs.",Haiwei Dong
2024-05-27T18:40:49Z,http://arxiv.org/abs/2405.17587v2,RAGSys: Item-Cold-Start Recommender as RAG System,"Large Language Models (LLM) hold immense promise for real-world applications,
but their generic knowledge often falls short of domain-specific needs.
Fine-tuning, a common approach, can suffer from catastrophic forgetting and
hinder generalizability. In-Context Learning (ICL) offers an alternative, which
can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant
demonstrations for few-shot learning tasks. This paper explores the desired
qualities of a demonstration retrieval system for ICL. We argue that ICL
retrieval in this context resembles item-cold-start recommender systems,
prioritizing discovery and maximizing information gain over strict relevance.
We propose a novel evaluation method that measures the LLM's subsequent
performance on NLP tasks, eliminating the need for subjective diversity scores.
Our findings demonstrate the critical role of diversity and quality bias in
retrieved demonstrations for effective ICL, and highlight the potential of
recommender system techniques in this domain.",Emile Contal
2024-05-28T17:56:46Z,http://arxiv.org/abs/2405.18414v1,Don't Forget to Connect! Improving RAG with Graph-based Reranking,"Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.",Jialin Dong
2024-05-29T01:12:53Z,http://arxiv.org/abs/2405.18682v2,"Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical
  Machine Reading Comprehension","Large language models (LLMs) have shown remarkable performance on many tasks
in different domains. However, their performance in closed-book biomedical
machine reading comprehension (MRC) has not been evaluated in depth. In this
work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We
experiment with different conventional prompting techniques as well as
introduce our own novel prompting method. To solve some of the retrieval
problems inherent to LLMs, we propose a prompting strategy named Implicit
Retrieval Augmented Generation (RAG) that alleviates the need for using vector
databases to retrieve important chunks in traditional RAG setups. Moreover, we
report qualitative assessments on the natural language generation outputs from
our approach. The results show that our new prompting technique is able to get
the best performance in two out of four datasets and ranks second in rest of
them. Experiments show that modern-day LLMs like GPT even in a zero-shot
setting can outperform supervised models, leading to new state-of-the-art
(SoTA) results on two of the benchmarks.",Shubham Vatsal
2024-05-29T15:47:57Z,http://arxiv.org/abs/2405.19207v1,A Multi-Source Retrieval Question Answering Framework Based on RAG,"With the rapid development of large-scale language models,
Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing
RAG paradigms are inevitably influenced by erroneous retrieval information,
thereby reducing the reliability and correctness of generated results.
Therefore, to improve the relevance of retrieval information, this study
proposes a method that replaces traditional retrievers with GPT-3.5, leveraging
its vast corpus knowledge to generate retrieval information. We also propose a
web retrieval based method to implement fine-grained knowledge retrieval,
Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic
partitioning of problem.In order to mitigate the illusion of GPT retrieval and
reduce noise in Web retrieval,we proposes a multi-source retrieval framework,
named MSRAG, which combines GPT retrieval with web retrieval. Experiments on
multiple knowledge-intensive QA datasets demonstrate that the proposed
framework in this study performs better than existing RAG framework in
enhancing the overall efficiency and accuracy of QA systems.",Ridong Wu
2024-05-26T06:45:39Z,http://arxiv.org/abs/2405.19366v2,"ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with
  LLM-Enhanced Cardiological Text","The utilization of deep learning on electrocardiogram (ECG) analysis has
brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.
By leveraging the capabilities of deep learning in semantic understanding,
especially in feature extraction and representation learning, this study
introduces a new multimodal contrastive pretaining framework that aims to
improve the quality and robustness of learned representations of 12-lead ECG
signals. Our framework comprises two key components, including Cardio Query
Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a
retrieval-augmented generation (RAG) pipeline to leverage large language models
(LLMs) and external medical knowledge to generate detailed textual descriptions
of ECGs. The generated text is enriched with information about demographics and
waveform patterns. ESI integrates both contrastive and captioning loss to
pretrain ECG encoders for enhanced representations. We validate our approach
through various downstream tasks, including arrhythmia detection and ECG-based
subject identification. Our experimental results demonstrate substantial
improvements over strong baselines in these tasks. These baselines encompass
supervised and self-supervised learning methods, as well as prior multimodal
pretraining approaches.",Han Yu
2024-05-29T23:11:53Z,http://arxiv.org/abs/2405.19563v1,Unlearning Climate Misinformation in Large Language Models,"Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.",Michael Fore
2024-05-30T03:44:54Z,http://arxiv.org/abs/2405.19670v4,"One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for
  Retrieval-Augmented Large Language Models","Retrieval-augmented generation (RAG) is a promising way to improve large
language models (LLMs) for generating more factual, accurate, and up-to-date
content. Existing methods either optimize prompts to guide LLMs in leveraging
retrieved information or directly fine-tune LLMs to adapt to RAG scenarios.
Although fine-tuning can yield better performance, it often compromises the
LLMs' general generation capabilities by modifying their parameters. This
limitation poses challenges in practical applications, especially when LLMs are
already deployed, as parameter adjustments may affect their original
functionality. To address this, we propose a novel method that involves
learning scalable and pluggable virtual tokens for RAG. By maintaining the
LLMs' original parameters and fine-tuning only the embeddings of these
pluggable tokens, our approach not only enhances LLMs' performance but also
preserves their general generation capabilities. Furthermore, we design several
training strategies to improve the scalability, flexibility, and
generalizability of our method. Comprehensive experiments across 12
question-answering tasks demonstrate the superiority of our approach.",Yutao Zhu
2024-05-27T17:55:36Z,http://arxiv.org/abs/2406.00041v2,"QUB-Cirdan at ""Discharge Me!"": Zero shot discharge letter generation by
  open-source LLM","The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to
reduce the administrative burden on clinicians by automating the creation of
critical sections of patient discharge letters. This paper presents our
approach using the Llama3 8B quantized model to generate the ""Brief Hospital
Course"" and ""Discharge Instructions"" sections. We employ a zero-shot method
combined with Retrieval-Augmented Generation (RAG) to produce concise,
contextually accurate summaries. Our contributions include the development of a
curated template-based approach to ensure reliability and consistency, as well
as the integration of RAG for word count prediction. We also describe several
unsuccessful experiments to provide insights into our pathway for the
competition. Our results demonstrate the effectiveness and efficiency of our
approach, achieving high scores across multiple evaluation metrics.",Rui Guo
2024-06-02T06:48:43Z,http://arxiv.org/abs/2406.00638v1,"COS-Mix: Cosine Similarity and Distance Fusion for Improved Information
  Retrieval","This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented
Generation (RAG) that integrates cosine similarity and cosine distance measures
to improve retrieval performance, particularly for sparse data. The traditional
cosine similarity measure is widely used to capture the similarity between
vectors in high-dimensional spaces. However, it has been shown that this
measure can yield arbitrary results in certain scenarios. To address this
limitation, we incorporate cosine distance measures to provide a complementary
perspective by quantifying the dissimilarity between vectors. Our approach is
experimented on proprietary data, unlike recent publications that have used
open-source datasets. The proposed method demonstrates enhanced retrieval
performance and provides a more comprehensive understanding of the semantic
relationships between documents or items. This hybrid strategy offers a
promising solution for efficiently and accurately retrieving relevant
information in knowledge-intensive applications, leveraging techniques such as
BM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based
retrieval to facilitate efficient information retrieval.",Kush Juvekar
2024-06-03T04:14:21Z,http://arxiv.org/abs/2406.00975v2,"Luna: An Evaluation Foundation Model to Catch Language Model
  Hallucinations with High Accuracy and Low Cost","Retriever Augmented Generation (RAG) systems have become pivotal in enhancing
the capabilities of language models by incorporating external knowledge
retrieval mechanisms. However, a significant challenge in deploying these
systems in industry applications is the detection and mitigation of
hallucinations: instances where the model generates information that is not
grounded in the retrieved context. Addressing this issue is crucial for
ensuring the reliability and accuracy of responses generated by large language
models (LLMs) in diverse industry settings. Current hallucination detection
techniques fail to deliver accuracy, low latency, and low cost simultaneously.
We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination
detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and
commercial evaluation frameworks on the hallucination detection task, with 97%
and 91% reduction in cost and latency, respectively. Luna is lightweight and
generalizes across multiple industry verticals and out-of-domain data, making
it an ideal candidate for industry LLM applications.",Masha Belyi
2024-06-03T06:55:10Z,http://arxiv.org/abs/2406.01045v1,"Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs","Large Language Models (LLMs) demonstrate significant capabilities in
processing natural language data, promising efficient knowledge extraction from
diverse textual sources to enhance situational awareness and support
decision-making. However, concerns arise due to their susceptibility to
hallucination, resulting in contextually inaccurate content. This work focuses
on harnessing LLMs for automated Event Extraction, introducing a new method to
address hallucination by decomposing the task into Event Detection and Event
Argument Extraction. Moreover, the proposed method integrates dynamic
schema-aware augmented retrieval examples into prompts tailored for each
specific inquiry, thereby extending and adapting advanced prompting techniques
such as Retrieval-Augmented Generation. Evaluation findings on prominent event
extraction benchmarks and results from a synthesized benchmark illustrate the
method's superior performance compared to baseline approaches.",Fatemeh Shiri
2024-06-03T12:39:04Z,http://arxiv.org/abs/2406.01273v2,SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper introduces SoccerRAG, an innovative framework designed to
harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. Our evaluations indicate that SoccerRAG
effectively handles complex queries, offering significant improvements over
traditional retrieval systems in terms of accuracy and user engagement. The
results underscore the potential of using RAG and LLMs in sports analytics,
paving the way for future advancements in the accessibility and real-time
processing of sports data.",Aleksander Theo Strand
2024-06-03T12:48:38Z,http://arxiv.org/abs/2406.01280v2,Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper demonstrates SoccerRAG, an innovative framework designed
to harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. We present a novel interactive user interface
(UI) based on the Chainlit framework which wraps around the core functionality,
and enable users to interact with the SoccerRAG framework in a chatbot-like
visual manner.",Aleksander Theo Strand
2024-06-03T20:18:56Z,http://arxiv.org/abs/2406.01768v1,"TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP
  Specifications","Understanding telecom standards involves sorting through numerous technical
documents, such as those produced by the 3rd Generation Partnership Project
(3GPP), which is time-consuming and labor-intensive. While large language
models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive
dataset is crucial for their effective pre-training and fine-tuning. In this
paper, we introduce \textit{TSpec-LLM}, an open-source comprehensive dataset
covering all 3GPP documents from Release 8 to Release 19 (1999--2023). To
evaluate its efficacy, we first select a representative sample of 3GPP
documents, create corresponding technical questions, and assess the baseline
performance of various LLMs. We then incorporate a retrieval-augmented
generation (RAG) framework to enhance LLM capabilities by retrieving relevant
context from the \textit{TSpec-LLM} dataset. Our evaluation shows that using a
naive-RAG framework on \textit{TSpec-LLM} improves the accuracy of GPT-3.5,
Gemini 1.0 Pro, and GPT-4 from 44\%, 46\%, and 51\% to 71\%, 75\%, and 72\%,
respectively.",Rasoul Nikbakht
2024-06-04T16:42:17Z,http://arxiv.org/abs/2406.02472v1,"Analyzing Temporal Complex Events with Large Language Models? A
  Benchmark towards Temporal, Long Context Understanding","The digital landscape is rapidly evolving with an ever-increasing volume of
online news, emphasizing the need for swift and precise analysis of complex
events. We refer to the complex events composed of many news articles over an
extended period as Temporal Complex Event (TCE). This paper proposes a novel
approach using Large Language Models (LLMs) to systematically extract and
analyze the event chain within TCE, characterized by their key points and
timestamps. We establish a benchmark, named TCELongBench, to evaluate the
proficiency of LLMs in handling temporal dynamics and understanding extensive
text. This benchmark encompasses three distinct tasks - reading comprehension,
temporal sequencing, and future event forecasting. In the experiment, we
leverage retrieval-augmented generation (RAG) method and LLMs with long context
window to deal with lengthy news articles of TCE. Our findings indicate that
models with suitable retrievers exhibit comparable performance with those
utilizing long context window.",Zhihan Zhang
2024-06-10T16:46:22Z,http://arxiv.org/abs/2406.06458v1,"Evaluating the Retrieval Component in LLM-Based Question Answering
  Systems","Question answering systems (QA) utilizing Large Language Models (LLMs)
heavily depend on the retrieval component to provide them with domain-specific
information and reduce the risk of generating inaccurate responses or
hallucinations. Although the evaluation of retrievers dates back to the early
research in Information Retrieval, assessing their performance within LLM-based
chatbots remains a challenge.
  This study proposes a straightforward baseline for evaluating retrievers in
Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate
that this evaluation framework provides a better image of how the retriever
performs and is more aligned with the overall performance of the QA system.
Although conventional metrics such as precision, recall, and F1 score may not
fully capture LLMs' capabilities - as they can yield accurate responses despite
imperfect retrievers - our method considers LLMs' strengths to ignore
irrelevant contexts, as well as potential errors and hallucinations in their
responses.",Ashkan Alinejad
2024-06-03T19:40:28Z,http://arxiv.org/abs/2406.06575v1,"Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and
  Abbreviation De-hallucination","Electronic design engineers are challenged to find relevant information
efficiently for a myriad of tasks within design construction, verification and
technology development. Large language models (LLM) have the potential to help
improve productivity by serving as conversational agents that effectively
function as subject-matter experts. In this paper we demonstrate Ask-EDA, a
chat agent designed to serve as a 24x7 expert available to provide guidance to
design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation
(RAG) and abbreviation de-hallucination (ADH) techniques to deliver more
relevant and accurate responses. We curated three evaluation datasets, namely
q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct
aspect: general design question answering, design command handling and
abbreviation resolution. We demonstrated that hybrid RAG offers over a 40%
improvement in Recall on the q2a-100 dataset and over a 60% improvement on the
cmds-100 dataset compared to not using RAG, while ADH yields over a 70%
enhancement in Recall on the abbr-100 dataset. The evaluation results show that
Ask-EDA can effectively respond to design-related inquiries.",Luyao Shi
2024-06-11T13:36:19Z,http://arxiv.org/abs/2406.07257v1,"Scholarly Question Answering using Large Language Models in the
  NFDI4DataScience Gateway","This paper introduces a scholarly Question Answering (QA) system on top of
the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based
(RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a
unified and intuitive interface for querying various scientific databases using
federated search. The RAG-based scholarly QA, powered by a Large Language Model
(LLM), facilitates dynamic interaction with search results, enhancing filtering
capabilities and fostering a conversational engagement with the Gateway search.
The effectiveness of both the Gateway and the scholarly QA system is
demonstrated through experimental analysis.",Hamed Babaei Giglou
2024-06-12T07:55:32Z,http://arxiv.org/abs/2406.07973v2,"Unique Security and Privacy Threats of Large Language Model: A
  Comprehensive Survey","With the rapid development of artificial intelligence, large language models
(LLMs) have made remarkable advancements in natural language processing. These
models are trained on vast datasets to exhibit powerful language understanding
and generation capabilities across various applications, including machine
translation, chatbots, and agents. However, LLMs have revealed a variety of
privacy and security issues throughout their life cycle, drawing significant
academic and industrial attention. Moreover, the risks faced by LLMs differ
significantly from those encountered by traditional language models. Given that
current surveys lack a clear taxonomy of unique threat models across diverse
scenarios, we emphasize the unique privacy and security threats associated with
five specific scenarios: pre-training, fine-tuning, retrieval-augmented
generation systems, deployment, and LLM-based agents. Addressing the
characteristics of each risk, this survey outlines potential threats and
countermeasures. Research on attack and defense situations can offer feasible
research directions, enabling more areas to benefit from LLMs.",Shang Wang
2024-06-12T08:26:30Z,http://arxiv.org/abs/2406.07990v1,"Blowfish: Topological and statistical signatures for quantifying
  ambiguity in semantic search","This works reports evidence for the topological signatures of ambiguity in
sentence embeddings that could be leveraged for ranking and/or explanation
purposes in the context of vector search and Retrieval Augmented Generation
(RAG) systems. We proposed a working definition of ambiguity and designed an
experiment where we have broken down a proprietary dataset into collections of
chunks of varying size - 3, 5, and 10 lines and used the different collections
successively as queries and answers sets. It allowed us to test the signatures
of ambiguity with removal of confounding factors. Our results show that proxy
ambiguous queries (size 10 queries against size 3 documents) display different
distributions of homologies 0 and 1 based features than proxy clear queries
(size 5 queries against size 10 documents). We then discuss those results in
terms increased manifold complexity and/or approximately discontinuous
embedding submanifolds. Finally we propose a strategy to leverage those
findings as a new scoring strategy of semantic similarities.",Thomas Roland Barillot
2024-06-14T08:21:42Z,http://arxiv.org/abs/2406.09818v3,"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from
  Corporate Climate Disclosures","To handle the vast amounts of qualitative data produced in corporate climate
communication, stakeholders increasingly rely on Retrieval Augmented Generation
(RAG) systems. However, a significant gap remains in evaluating domain-specific
information retrieval - the basis for answer generation. To address this
challenge, this work simulates the typical tasks of a sustainability analyst by
examining 30 sustainability reports with 16 detailed climate-related questions.
As a result, we obtain a dataset with over 8.5K unique question-source-answer
pairs labeled by different levels of relevance. Furthermore, we develop a use
case with the dataset to investigate the integration of expert knowledge into
information retrieval with embeddings. Although we show that incorporating
expert knowledge works, we also outline the critical limitations of embeddings
in knowledge-intensive downstream domains like climate change communication.",Tobias Schimanski
2024-06-14T12:41:07Z,http://arxiv.org/abs/2406.09979v2,HIRO: Hierarchical Information Retrieval Optimization,"Retrieval-Augmented Generation (RAG) has revolutionized natural language
processing by dynamically integrating external knowledge into Large Language
Models (LLMs), addressing their limitation of static training datasets. Recent
implementations of RAG leverage hierarchical data structures, which organize
documents at various levels of summarization and information density. This
complexity, however, can cause LLMs to ""choke"" on information overload,
necessitating more sophisticated querying mechanisms. In this context, we
introduce Hierarchical Information Retrieval Optimization (HIRO), a novel
querying approach that employs a Depth-First Search (DFS)-based recursive
similarity score calculation and branch pruning. This method uniquely minimizes
the context delivered to the LLM without informational loss, effectively
managing the challenge of excessive data. HIRO's refined approach is validated
by a 10.85% improvement in performance on the NarrativeQA dataset.",Krish Goel
2024-06-11T02:37:06Z,http://arxiv.org/abs/2406.10263v1,"A Lightweight Framework for Adaptive Retrieval In Code Completion With
  Critique Model","Recent advancements in Retrieval-Augmented Generation have significantly
enhanced code completion at the repository level. Various RAG-based code
completion systems are proposed based on different design choices. For
instance, gaining more effectiveness at the cost of repeating the
retrieval-generation process multiple times. However, the indiscriminate use of
retrieval in current methods reveals issues in both efficiency and
effectiveness, as a considerable portion of retrievals are unnecessary and may
introduce unhelpful or even harmful suggestions to code language models. To
address these challenges, we introduce CARD, a lightweight critique method
designed to provide insights into the necessity of retrievals and select the
optimal answer from multiple predictions. CARD can seamlessly integrate into
any RAG-based code completion system. Our evaluation shows that CARD saves 21%
to 46% times of retrieval for Line completion, 14% to 40% times of retrieval
for API completion, and 6% to 46.5% times of retrieval for function completion
respectively, while improving the accuracy. CARD reduces latency ranging from
16% to 83%. CARD is generalizable to different LMs, retrievers, and programming
languages. It is lightweight with training in few seconds and inference in few
milliseconds.",Wenrui Zhang
2024-06-17T03:29:14Z,http://arxiv.org/abs/2406.11177v1,TIFG: Text-Informed Feature Generation with Large Language Models,"Textual information of data is of vital importance for data mining and
feature engineering. However, existing methods focus on learning the data
structures and overlook the textual information along with the data.
Consequently, they waste this valuable resource and miss out on the deeper data
relationships embedded within the texts. In this paper, we introduce
Text-Informed Feature Generation (TIFG), a novel LLM-based text-informed
feature generation framework. TIFG utilizes the textual information to generate
features by retrieving possible relevant features within external knowledge
with Retrieval Augmented Generation (RAG) technology. In this approach, the
TIFG can generate new explainable features to enrich the feature space and
further mine feature relationships. We design the TIFG to be an automated
framework that continuously optimizes the feature generation process, adapts to
new data inputs, and improves downstream task performance over iterations. A
broad range of experiments in various downstream tasks showcases that our
approach can generate high-quality and meaningful features, and is
significantly superior to existing methods.",Xinhao Zhang
2024-06-17T07:52:42Z,http://arxiv.org/abs/2406.11290v1,"Iterative Utility Judgment Framework via LLMs Inspired by Relevance in
  Philosophy","Utility and topical relevance are critical measures in information retrieval
(IR), reflecting system and user perspectives, respectively. While topical
relevance has long been emphasized, utility is a higher standard of relevance
and is more useful for facilitating downstream tasks, e.g., in
Retrieval-Augmented Generation (RAG). When we incorporate utility judgments
into RAG, we realize that the topical relevance, utility, and answering in RAG
are closely related to the three types of relevance that Schutz discussed from
a philosophical perspective. They are topical relevance, interpretational
relevance, and motivational relevance, respectively. Inspired by the dynamic
iterations of the three types of relevance, we propose an Iterative utiliTy
judgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We
conducted extensive experiments on multi-grade passage retrieval and factoid
question-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental
results demonstrate significant improvements in utility judgments, ranking of
topical relevance, and answer generation upon representative baselines,
including multiple single-shot utility judging approaches. Our code and
benchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.",Hengran Zhang
2024-06-17T11:22:25Z,http://arxiv.org/abs/2406.11424v1,"Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG
  Systems: A Comparative Study of Performance and Scalability","This paper presents an analysis of open-source large language models (LLMs)
and their application in Retrieval-Augmented Generation (RAG) tasks, specific
for enterprise-specific data sets scraped from their websites. With the
increasing reliance on LLMs in natural language processing, it is crucial to
evaluate their performance, accessibility, and integration within specific
organizational contexts. This study examines various open-source LLMs, explores
their integration into RAG frameworks using enterprise-specific data, and
assesses the performance of different open-source embeddings in enhancing the
retrieval and generation process. Our findings indicate that open-source LLMs,
combined with effective embedding techniques, can significantly improve the
accuracy and efficiency of RAG systems, offering a viable alternative to
proprietary solutions for enterprises.",Gautam B
2024-06-17T13:01:12Z,http://arxiv.org/abs/2406.11497v3,"CrAM: Credibility-Aware Attention Modification in LLMs for Combating
  Misinformation in RAG","Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of ""credibility-aware RAG"", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen1.5-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.",Boyi Deng
2024-06-17T20:14:16Z,http://arxiv.org/abs/2406.12069v2,Satyrn: A Platform for Analytics Augmented Generation,"Large language models (LLMs) are capable of producing documents, and
retrieval augmented generation (RAG) has shown itself to be a powerful method
for improving accuracy without sacrificing fluency. However, not all
information can be retrieved from text. We propose an approach that uses the
analysis of structured data to generate fact sets that are used to guide
generation in much the same way that retrieved documents are used in RAG. This
analytics augmented generation (AAG) approach supports the ability to utilize
standard analytic techniques to generate facts that are then converted to text
and passed to an LLM. We present a neurosymbolic platform, Satyrn, that
leverages AAG to produce accurate, fluent, and coherent reports grounded in
large scale databases. In our experiments, we find that Satyrn generates
reports in which over 86% of claims are accurate while maintaining high levels
of fluency and coherence, even when using smaller language models such as
Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims
are accurate.",Marko Sterbentz
2024-06-18T00:41:41Z,http://arxiv.org/abs/2406.12169v1,"Intermediate Distillation: Data-Efficient Distillation from Black-Box
  LLMs for Information Retrieval","Recent research has explored distilling knowledge from large language models
(LLMs) to optimize retriever models, especially within the retrieval-augmented
generation (RAG) framework. However, most existing training methods rely on
extracting supervision signals from LLMs' weights or their output
probabilities, which is not only resource-intensive but also incompatible with
black-box LLMs. In this paper, we introduce \textit{Intermediate Distillation},
a data-efficient knowledge distillation training scheme that treats LLMs as
black boxes and distills their knowledge via an innovative LLM-ranker-retriever
pipeline, solely using LLMs' ranking generation as the supervision signal.
Extensive experiments demonstrate that our proposed method can significantly
improve the performance of retriever models with only 1,000 training instances.
Moreover, our distilled retriever model significantly boosts performance in
question-answering tasks within the RAG framework, demonstrating the potential
of LLMs to economically and effectively train smaller models.",Zizhong Li
2024-06-18T06:54:28Z,http://arxiv.org/abs/2406.12331v1,"Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text
  Understanding","Current Large Language Models (LLMs) face inherent limitations due to their
pre-defined context lengths, which impede their capacity for multi-hop
reasoning within extensive textual contexts. While existing techniques like
Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by
sourcing external information, they fall short when direct answers are not
readily available. We introduce a novel approach that re-imagines information
retrieval through dynamic in-context editing, inspired by recent breakthroughs
in knowledge editing. By treating lengthy contexts as malleable external
knowledge, our method interactively gathers and integrates relevant
information, thereby enabling LLMs to perform sophisticated reasoning steps.
Experimental results demonstrate that our method effectively empowers
context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with
improved performance, which outperforms state-of-the-art context window
extrapolation methods and even compares favorably to more advanced commercial
long-context models. Our interactive method not only enhances reasoning
capabilities but also mitigates the associated training and computational
costs, making it a pragmatic solution for enhancing LLMs' reasoning within
expansive contexts.",Weizhi Fei
2024-06-18T17:22:48Z,http://arxiv.org/abs/2406.12806v1,"Identifying Performance-Sensitive Configurations in Software Systems
  through Code Analysis with LLM Agents","Configuration settings are essential for tailoring software behavior to meet
specific performance requirements. However, incorrect configurations are
widespread, and identifying those that impact system performance is challenging
due to the vast number and complexity of possible settings. In this work, we
present PerfSense, a lightweight framework that leverages Large Language Models
(LLMs) to efficiently identify performance-sensitive configurations with
minimal overhead. PerfSense employs LLM agents to simulate interactions between
developers and performance engineers using advanced prompting techniques such
as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of
seven open-source Java systems demonstrates that PerfSense achieves an average
accuracy of 64.77% in classifying performance-sensitive configurations,
outperforming both our LLM baseline (50.36%) and the previous state-of-the-art
method (61.75%). Notably, our prompt chaining technique improves recall by 10%
to 30% while maintaining similar precision levels. Additionally, a manual
analysis of 362 misclassifications reveals common issues, including LLMs'
misunderstandings of requirements (26.8%). In summary, PerfSense significantly
reduces manual effort in classifying performance-sensitive configurations and
offers valuable insights for future LLM-based code analysis research.",Zehao Wang
2024-06-18T17:46:08Z,http://arxiv.org/abs/2406.12824v1,"From RAGs to rich parameters: Probing how language models utilize
  external knowledge over parametric information for factual queries","Retrieval Augmented Generation (RAG) enriches the ability of language models
to reason using external context to augment responses for a given user prompt.
This approach has risen in popularity due to practical applications in various
applications of language models in search, question/answering, and chat-bots.
However, the exact nature of how this approach works isn't clearly understood.
In this paper, we mechanistically examine the RAG pipeline to highlight that
language models take shortcut and have a strong bias towards utilizing only the
context information to answer the question, while relying minimally on their
parametric memory. We probe this mechanistic behavior in language models with:
(i) Causal Mediation Analysis to show that the parametric memory is minimally
utilized when answering a question and (ii) Attention Contributions and
Knockouts to show that the last token residual stream do not get enriched from
the subject token in the question, but gets enriched from other informative
tokens in the context. We find this pronounced shortcut behaviour true across
both LLaMa and Phi family of models.",Hitesh Wadhwa
2024-06-19T04:53:48Z,http://arxiv.org/abs/2406.13213v2,"Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database
  Filtering with LLM-Extracted Metadata","The retrieval-augmented generation (RAG) enables retrieval of relevant
information from an external knowledge source and allows large language models
(LLMs) to answer queries over previously unseen document collections. However,
it was demonstrated that traditional RAG applications perform poorly in
answering multi-hop questions, which require retrieving and reasoning over
multiple elements of supporting evidence. We introduce a new method called
Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to
improve the RAG selection of the relevant documents from various sources,
relevant to the question. While database filtering is specific to a set of
questions from a particular domain and format, we found out that Multi-Meta-RAG
greatly improves the results on the MultiHop-RAG benchmark. The code is
available at https://github.com/mxpoliakov/Multi-Meta-RAG.",Mykhailo Poliakov
2024-06-19T08:29:54Z,http://arxiv.org/abs/2406.13331v2,Improving Zero-shot LLM Re-Ranker with Risk Minimization,"In the Retrieval-Augmented Generation (RAG) system, advanced Large Language
Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an
unsupervised way, which re-rank documents based on the probability of
generating the query given the content of a document. However, directly
prompting LLMs to approximate QLMs inherently is biased, where the estimated
distribution might diverge from the actual document-specific distribution. In
this study, we introduce a novel framework, $\mathrm{UR^3}$, which leverages
Bayesian decision theory to both quantify and mitigate this estimation bias.
Specifically, $\mathrm{UR^3}$ reformulates the problem as maximizing the
probability of document generation, thereby harmonizing the optimization of
query and document generation probabilities under a unified risk minimization
objective. Our empirical results indicate that $\mathrm{UR^3}$ significantly
enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits
the QA tasks by achieving higher accuracy with fewer input documents.",Xiaowei Yuan
2024-06-19T21:07:35Z,http://arxiv.org/abs/2406.13840v1,"StackRAG Agent: Improving Developer Answers with Retrieval-Augmented
  Generation","Developers spend much time finding information that is relevant to their
questions. Stack Overflow has been the leading resource, and with the advent of
Large Language Models (LLMs), generative models such as ChatGPT are used
frequently. However, there is a catch in using each one separately. Searching
for answers is time-consuming and tedious, as shown by the many tools developed
by researchers to address this issue. On the other, using LLMs is not reliable,
as they might produce irrelevant or unreliable answers (i.e., hallucination).
In this work, we present StackRAG, a retrieval-augmented Multiagent generation
tool based on LLMs that combines the two worlds: aggregating the knowledge from
SO to enhance the reliability of the generated answers. Initial evaluations
show that the generated answers are correct, accurate, relevant, and useful.",Davit Abrahamyan
2024-06-20T12:59:27Z,http://arxiv.org/abs/2406.14277v2,"QPaug: Question and Passage Augmentation for Open-Domain Question
  Answering of LLMs","Retrieval-augmented generation (RAG) has received much attention for
Open-domain question-answering (ODQA) tasks as a means to compensate for the
parametric knowledge of large language models (LLMs). While previous approaches
focused on processing retrieved passages to remove irrelevant context, they
still rely heavily on the quality of retrieved passages which can degrade if
the question is ambiguous or complex. In this paper, we propose a simple yet
efficient method called question and passage augmentation (QPaug) via LLMs for
open-domain QA. QPaug first decomposes the original questions into
multiple-step sub-questions. By augmenting the original question with detailed
sub-questions and planning, we are able to make the query more specific on what
needs to be retrieved, improving the retrieval performance. In addition, to
compensate for the case where the retrieved passages contain distracting
information or divided opinions, we augment the retrieved passages with
self-generated passages by LLMs to guide the answer extraction. Experimental
results show that QPaug outperforms the previous state-of-the-art and achieves
significant performance gain over existing RAG methods. The source code is
available at \url{https://github.com/kmswin1/QPaug}.",Minsang Kim
2024-06-20T15:12:41Z,http://arxiv.org/abs/2406.14394v1,SEC-QA: A Systematic Evaluation Corpus for Financial QA,"The financial domain frequently deals with large numbers of long documents
that are essential for daily operations. Significant effort is put towards
automating financial data analysis. However, a persistent challenge, not
limited to the finance domain, is the scarcity of datasets that accurately
reflect real-world tasks for model evaluation. Existing datasets are often
constrained by size, context, or relevance to practical applications. Moreover,
LLMs are currently trained on trillions of tokens of text, limiting access to
novel data or documents that models have not encountered during training for
unbiased evaluation. We propose SEC-QA, a continuous dataset generation
framework with two key features: 1) the semi-automatic generation of
Question-Answer (QA) pairs spanning multiple long context financial documents,
which better represent real-world financial scenarios; 2) the ability to
continually refresh the dataset using the most recent public document
collections, not yet ingested by LLMs. Our experiments show that current
retrieval augmented generation methods systematically fail to answer these
challenging multi-document questions. In response, we introduce a QA system
based on program-of-thought that improves the ability to perform complex
information retrieval and quantitative reasoning pipelines, thereby increasing
QA accuracy.",Viet Dac Lai
2024-06-20T20:55:38Z,http://arxiv.org/abs/2406.14732v2,"TTQA-RS- A break-down prompting approach for Multi-hop Table-Text
  Question Answering with Reasoning and Summarization","Question answering (QA) over tables and text has gained much popularity over
the years. Multi-hop table-text QA requires multiple hops between the table and
text, making it a challenging QA task. Although several works have attempted to
solve the table-text QA task, most involve training the models and requiring
labeled data. In this paper, we have proposed a Retrieval Augmented Generation
(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop
Table-Text Question Answering with Reasoning and Summarization. Our model uses
an enhanced retriever for table-text information retrieval and uses augmented
knowledge, including table-text summary with decomposed sub-questions with
answers for a reasoning-based table-text QA. Using open-source language models,
our model outperformed all existing prompting methods for table-text QA tasks
on existing table-text QA datasets, such as HybridQA and OTT-QA's development
set. Our experiments demonstrate the potential of prompt-based approaches using
open-source LLMs. Additionally, by using LLaMA3-70B, our model achieved
state-of-the-art performance for prompting-based methods on multi-hop
table-text QA.",Jayetri Bardhan
2024-06-21T08:31:02Z,http://arxiv.org/abs/2406.14972v1,A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems,"Retrieval Augmented Generation (RAG) represents a significant advancement in
artificial intelligence combining a retrieval phase with a generative phase,
with the latter typically being powered by large language models (LLMs). The
current common practices in RAG involve using ""instructed"" LLMs, which are
fine-tuned with supervised training to enhance their ability to follow
instructions and are aligned with human preferences using state-of-the-art
techniques. Contrary to popular belief, our study demonstrates that base models
outperform their instructed counterparts in RAG tasks by 20% on average under
our experimental settings. This finding challenges the prevailing assumptions
about the superiority of instructed LLMs in RAG applications. Further
investigations reveal a more nuanced situation, questioning fundamental aspects
of RAG and suggesting the need for broader discussions on the topic; or, as
Fromm would have it, ""Seldom is a glance at the statistics enough to understand
the meaning of the figures"".",Florin Cuconasu
2024-06-21T10:48:21Z,http://arxiv.org/abs/2406.15045v2,"Integrating Knowledge Retrieval and Large Language Models for Clinical
  Report Correction","This study proposes an approach for error correction in radiology reports,
leveraging large language models (LLMs) and retrieval-augmented generation
(RAG) techniques. The proposed framework employs a novel internal+external
retrieval mechanism to extract relevant medical entities and relations from the
report of interest and an external knowledge source. A three-stage inference
process is introduced, decomposing the task into error detection, localization,
and correction subtasks, which enhances the explainability and performance of
the system. The effectiveness of the approach is evaluated using a benchmark
dataset created by corrupting real-world radiology reports with realistic
errors, guided by domain experts. Experimental results demonstrate the benefits
of the proposed methods, with the combination of internal and external
retrieval significantly improving the accuracy of error detection,
localization, and correction across various state-of-the-art LLMs. The findings
contribute to the development of more robust and reliable error correction
systems for clinical documentation.",Jinge Wu
2024-06-24T07:17:59Z,http://arxiv.org/abs/2406.16367v1,"On the Role of Long-tail Knowledge in Retrieval Augmented Large Language
  Models","Retrieval augmented generation (RAG) exhibits outstanding performance in
promoting the knowledge capabilities of large language models (LLMs) with
retrieved documents related to user queries. However, RAG only focuses on
improving the response quality of LLMs via enhancing queries indiscriminately
with retrieved information, paying little attention to what type of knowledge
LLMs really need to answer original queries more accurately. In this paper, we
suggest that long-tail knowledge is crucial for RAG as LLMs have already
remembered common world knowledge during large-scale pre-training. Based on our
observation, we propose a simple but effective long-tail knowledge detection
method for LLMs. Specifically, the novel Generative Expected Calibration Error
(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on
both statistics and semantics. Hence, we retrieve relevant documents and infuse
them into the model for patching knowledge loopholes only when the input query
relates to long-tail knowledge. Experiments show that, compared to existing RAG
pipelines, our method achieves over 4x speedup in average inference time and
consistent performance improvement in downstream tasks.",Dongyang Li
2024-06-24T07:52:05Z,http://arxiv.org/abs/2406.16383v2,"Context-augmented Retrieval: A Novel Framework for Fast Information
  Retrieval based Response Generation using Large Language Model","Generating high-quality answers consistently by providing contextual
information embedded in the prompt passed to the Large Language Model (LLM) is
dependent on the quality of information retrieval. As the corpus of contextual
information grows, the answer/inference quality of Retrieval Augmented
Generation (RAG) based Question Answering (QA) systems declines. This work
solves this problem by combining classical text classification with the Large
Language Model (LLM) to enable quick information retrieval from the vector
store and ensure the relevancy of retrieved information. For the same, this
work proposes a new approach Context Augmented retrieval (CAR), where
partitioning of vector database by real-time classification of information
flowing into the corpus is done. CAR demonstrates good quality answer
generation along with significant reduction in information retrieval and answer
generation time.",Sai Ganesh
2024-06-26T07:38:24Z,http://arxiv.org/abs/2406.18134v1,"Assessing ""Implicit"" Retrieval Robustness of Large Language Models","Retrieval-augmented generation has gained popularity as a framework to
enhance large language models with external knowledge. However, its
effectiveness hinges on the retrieval robustness of the model. If the model
lacks retrieval robustness, its performance is constrained by the accuracy of
the retriever, resulting in significant compromises when the retrieved context
is irrelevant. In this paper, we evaluate the ""implicit"" retrieval robustness
of various large language models, instructing them to directly output the final
answer without explicitly judging the relevance of the retrieved context. Our
findings reveal that fine-tuning on a mix of gold and distracting context
significantly enhances the model's robustness to retrieval inaccuracies, while
still maintaining its ability to extract correct answers when retrieval is
accurate. This suggests that large language models can implicitly handle
relevant or irrelevant retrieved context by learning solely from the
supervision of the final answer in an end-to-end manner. Introducing an
additional process for explicit relevance judgment can be unnecessary and
disrupts the end-to-end approach.",Xiaoyu Shen
2024-06-27T05:14:34Z,http://arxiv.org/abs/2406.18894v1,"Assessing the Effectiveness of LLMs in Android Application Vulnerability
  Analysis","The increasing frequency of attacks on Android applications coupled with the
recent popularity of large language models (LLMs) necessitates a comprehensive
understanding of the capabilities of the latter in identifying potential
vulnerabilities, which is key to mitigate the overall risk. To this end, the
work at hand compares the ability of nine state-of-the-art LLMs to detect
Android code vulnerabilities listed in the latest Open Worldwide Application
Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open
dataset of over 100 vulnerable code samples, including obfuscated ones,
assessing each model's ability to identify key vulnerabilities. Our analysis
reveals the strengths and weaknesses of each LLM, identifying important factors
that contribute to their performance. Additionally, we offer insights into
context augmentation with retrieval-augmented generation (RAG) for detecting
Android code vulnerabilities, which in turn may propel secure application
development. Finally, while the reported findings regarding code vulnerability
analysis show promise, they also reveal significant discrepancies among the
different LLMs.",Vasileios Kouliaridis
2024-06-27T13:08:35Z,http://arxiv.org/abs/2406.19150v1,RAVEN: Multitask Retrieval Augmented Vision-Language Learning,"The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.",Varun Nagaraj Rao
2024-06-27T14:38:33Z,http://arxiv.org/abs/2406.19215v1,"SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented
  Generation","This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel
adaptive RAG model that extracts self-aware uncertainty of LLMs from their
internal states. SeaKR activates retrieval when the LLMs present high
self-aware uncertainty for generation. To effectively integrate retrieved
knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty
to preserve the snippet that reduces their uncertainty to the utmost. To
facilitate solving complex tasks that require multiple retrievals, SeaKR
utilizes their self-aware uncertainty to choose among different reasoning
strategies. Our experiments on both complex and simple Question Answering
datasets show that SeaKR outperforms existing adaptive RAG methods. We release
our code at https://github.com/THU-KEG/SeaKR.",Zijun Yao
2024-06-27T16:33:40Z,http://arxiv.org/abs/2406.19309v2,"Which Neurons Matter in IR? Applying Integrated Gradients-based Methods
  to Understand Cross-Encoders","With the recent addition of Retrieval-Augmented Generation (RAG), the scope
and importance of Information Retrieval (IR) has expanded. As a result, the
importance of a deeper understanding of IR models also increases. However,
interpretability in IR remains under-explored, especially when it comes to the
models' inner mechanisms. In this paper, we explore the possibility of adapting
Integrated Gradient-based methods in an IR context to identify the role of
individual neurons within the model. In particular, we provide new insights
into the role of what we call ""relevance"" neurons, as well as how they deal
with unseen data. Finally, we carry out an in-depth pruning study to validate
our findings.",Mathias Vast
2024-06-29T10:46:01Z,http://arxiv.org/abs/2407.00396v1,"A Study on Effect of Reference Knowledge Choice in Generating Technical
  Content Relevant to SAPPhIRE Model Using Large Language Model","Representation of systems using the SAPPhIRE model of causality can be an
inspirational stimulus in design. However, creating a SAPPhIRE model of a
technical or a natural system requires sourcing technical knowledge from
multiple technical documents regarding how the system works. This research
investigates how to generate technical content accurately relevant to the
SAPPhIRE model of causality using a Large Language Model, also called LLM. This
paper, which is the first part of the two-part research, presents a method for
hallucination suppression using Retrieval Augmented Generating with LLM to
generate technical content supported by the scientific information relevant to
a SAPPhIRE con-struct. The result from this research shows that the selection
of reference knowledge used in providing context to the LLM for generating the
technical content is very important. The outcome of this research is used to
build a software support tool to generate the SAPPhIRE model of a given
technical system.",Kausik Bhattacharya
2024-07-01T15:53:29Z,http://arxiv.org/abs/2407.01403v1,"Optimization of Retrieval-Augmented Generation Context with Outlier
  Detection","In this paper, we focus on methods to reduce the size and improve the quality
of the prompt context required for question-answering systems. Attempts to
increase the number of retrieved chunked documents and thereby enlarge the
context related to the query can significantly complicate the processing and
decrease the performance of a Large Language Model (LLM) when generating
responses to queries. It is well known that a large set of documents retrieved
from a database in response to a query may contain irrelevant information,
which often leads to hallucinations in the resulting answers. Our goal is to
select the most semantically relevant documents, treating the discarded ones as
outliers. We propose and evaluate several methods for identifying outliers by
creating features that utilize the distances of embedding vectors, retrieved
from the vector database, to both the centroid and the query vectors. The
methods were evaluated by comparing the similarities of the retrieved LLM
responses to ground-truth answers obtained using the OpenAI GPT-4o model. It
was found that the greatest improvements were achieved with increasing
complexity of the questions and answers.",Vitaly Bulgakov
2024-06-27T15:45:29Z,http://arxiv.org/abs/2407.01449v3,ColPali: Efficient Document Retrieval with Vision Language Models,"Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.",Manuel Faysse
2024-07-01T20:47:47Z,http://arxiv.org/abs/2407.01796v1,"Ground Every Sentence: Improving Retrieval-Augmented LLMs with
  Interleaved Reference-Claim Generation","Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large
Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text
Generation (ATG) has attracted growing attention, which provides citations to
support the model's responses in RAG, so as to enhance the credibility of
LLM-generated content and facilitate verification. Prior methods mainly adopt
coarse-grained attributions, linking to passage-level references or providing
paragraph-level citations. However, these methods still fall short in
verifiability and require certain time costs for fact checking. This paper
proposes a fine-grained ATG method called ReClaim(Refer & Claim), which
alternates the generation of references and answers step by step. Unlike
traditional coarse-grained attribution, ReClaim allows the model to add
sentence-level fine-grained citations to each answer sentence in long-form
question-answering tasks. Our experiments encompass various training and
inference methods and multiple LLMs, verifying the effectiveness of our
approach.",Sirui Xia
2024-07-02T12:57:42Z,http://arxiv.org/abs/2407.02233v2,Synthetic Multimodal Question Generation,"Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to
question-answering over multimodal documents. A key challenge with evaluating
MMRAG is the paucity of high-quality datasets matching the question styles and
modalities of interest. In light of this, we propose SMMQG, a synthetic data
generation framework. SMMQG leverages interplay between a retriever, large
language model (LLM) and large multimodal model (LMM) to generate question and
answer pairs directly from multimodal documents, with the questions conforming
to specified styles and modalities. We use SMMQG to generate an MMRAG dataset
of 1024 questions over Wikipedia documents and evaluate state-of-the-art models
using it, revealing insights into model performance that are attainable only
through style- and modality-specific evaluation data. Next, we measure the
quality of data produced by SMMQG via a human study. We find that the quality
of SMMQG-generated synthetic data is on par with the quality of the
crowdsourced benchmark MMQA and that downstream evaluation results using both
datasets strongly concur.",Ian Wu
2024-07-04T13:52:23Z,http://arxiv.org/abs/2407.03937v2,"TongGu: Mastering Classical Chinese Understanding with
  Knowledge-Grounded Large Language Models","Classical Chinese is a gateway to the rich heritage and wisdom of ancient
China, yet its complexities pose formidable comprehension barriers for most
modern people without specialized knowledge. While Large Language Models (LLMs)
have shown remarkable capabilities in Natural Language Processing (NLP), they
struggle with Classical Chinese Understanding (CCU), especially in
data-demanding and knowledge-intensive tasks. In response to this dilemma, we
propose \textbf{TongGu} (mean understanding ancient and modern), the first
CCU-specific LLM, underpinned by three core contributions. First, we construct
a two-stage instruction-tuning dataset ACCN-INS derived from rich classical
Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we
propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,
enabling TongGu to acquire new capabilities while preserving its foundational
knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)
technique to reduce hallucinations based on knowledge-grounding. Extensive
experiments across 24 diverse CCU tasks validate TongGu's superior ability,
underscoring the effectiveness of RAT and CCU-RAG. The model and dataset are
available at \url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",Jiahuan Cao
2024-07-05T14:16:47Z,http://arxiv.org/abs/2407.04528v4,"GPT vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning","Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.",Aleksander Ficek
2024-07-06T17:25:11Z,http://arxiv.org/abs/2407.05138v1,Vortex under Ripplet: An Empirical Study of RAG-enabled Applications,"Large language models (LLMs) enhanced by retrieval-augmented generation (RAG)
provide effective solutions in various application scenarios. However,
developers face challenges in integrating RAG-enhanced LLMs into software
systems, due to lack of interface specification, requirements from software
context, and complicated system management. In this paper, we manually studied
100 open-source applications that incorporate RAG-enhanced LLMs, and their
issue reports. We have found that more than 98% of applications contain
multiple integration defects that harm software functionality, efficiency, and
security. We have also generalized 19 defect patterns and proposed guidelines
to tackle them. We hope this work could aid LLM-enabled software development
and motivate future research.",Yuchen Shao
2024-07-07T21:26:36Z,http://arxiv.org/abs/2407.05502v2,"Faux Polyglot: A Study on Information Disparity in Multilingual Large
  Language Models","With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are
playing a pivotal role in information search and are being adopted globally.
Although the multilingual capability of LLMs offers new opportunities to bridge
the language barrier, do these capabilities translate into real-life scenarios
where linguistic divide and knowledge conflicts between multilingual sources
are known occurrences? In this paper, we studied LLM's linguistic preference in
a RAG-based information search setting. We found that LLMs displayed systemic
bias towards information in the same language as the query language in both
information retrieval and answer generation. Furthermore, in scenarios where
there is little information in the language of the query, LLMs prefer documents
in high-resource languages, reinforcing the dominant views. Such bias exists
for both factual and opinion-based queries. Our results highlight the
linguistic divide within multilingual LLMs in information search systems. The
seemingly beneficial multilingual capability of LLMs may backfire on
information parity by reinforcing language-specific information cocoons or
filter bubbles further marginalizing low-resource views.",Nikhil Sharma
2024-07-11T13:22:17Z,http://arxiv.org/abs/2407.08488v2,Lynx: An Open Source Hallucination Evaluation Model,"Retrieval Augmented Generation (RAG) techniques aim to mitigate
hallucinations in Large Language Models (LLMs). However, LLMs can still produce
information that is unsupported or contradictory to the retrieved contexts. We
introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced
reasoning on challenging real-world hallucination scenarios. To evaluate LYNX,
we present HaluBench, a comprehensive hallucination evaluation benchmark,
consisting of 15k samples sourced from various real-world domains. Our
experiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and
closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,
HaluBench and our evaluation code for public access.",Selvan Sunitha Ravi
2024-07-11T13:29:28Z,http://arxiv.org/abs/2407.08495v2,"Investigating LLMs as Voting Assistants via Contextual Augmentation: A
  Case Study on the European Parliament Elections 2024","In light of the recent 2024 European Parliament elections, we are
investigating if LLMs can be used as Voting Advice Applications (VAAs). We
audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the
stance of political parties based on the latest ""EU and I"" voting assistance
questionnaire. Furthermore, we explore alternatives to improve models'
performance by augmenting the input context via Retrieval-Augmented Generation
(RAG) relying on web search, and Self-Reflection using staged conversations
that aim to re-collect relevant content from the model's internal memory. We
find that MIXTRAL is highly accurate with an 82% accuracy on average with a
significant performance disparity across different political groups (50-95%).
Augmenting the input context with expert-curated information can lead to a
significant boost of approx. 9%, which remains an open challenge for automated
RAG approaches, even considering curated content.",Ilias Chalkidis
2024-07-12T06:06:54Z,http://arxiv.org/abs/2407.09014v3,CompAct: Compressing Retrieved Documents Actively for Question Answering,"Retrieval-augmented generation supports language models to strengthen their
factual groundings by providing external contexts. However, language models
often face challenges when given extensive information, diminishing their
effectiveness in solving questions. Context compression tackles this issue by
filtering out irrelevant information, but current methods still struggle in
realistic scenarios where crucial information cannot be captured with a
single-step approach. To overcome this limitation, we introduce CompAct, a
novel framework that employs an active strategy to condense extensive documents
without losing key information. Our experiments demonstrate that CompAct brings
significant improvements in both performance and compression rate on multi-hop
question-answering benchmarks. CompAct flexibly operates as a cost-efficient
plug-in module with various off-the-shelf retrievers or readers, achieving
exceptionally high compression rates (47x).",Chanwoong Yoon
2024-07-12T13:30:44Z,http://arxiv.org/abs/2407.09252v3,Context Embeddings for Efficient Answer Generation in RAG,"Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.",David Rau
2024-05-17T12:23:19Z,http://arxiv.org/abs/2407.09977v1,"Mitigating Interpretation Bias in Rock Records with Large Language
  Models: Insights from Paleoenvironmental Analysis","The reconstruction of Earth's history faces significant challenges due to the
nonunique interpretations often derived from rock records. The problem has long
been recognized but there are no systematic solutions in practice. This study
introduces an innovative approach that leverages Large Language Models (LLMs)
along with retrieval augmented generation and real-time search capabilities to
counteract interpretation biases, thereby enhancing the accuracy and
reliability of geological analyses. By applying this framework to sedimentology
and paleogeography, we demonstrate its effectiveness in mitigating
interpretations biases through the generation and evaluation of multiple
hypotheses for the same data, which can effectively reduce human bias. Our
research illuminates the transformative potential of LLMs in refining
paleoenvironmental studies and extends their applicability across various
sub-disciplines of Earth sciences, enabling a deeper and more accurate
depiction of Earth's evolution.",Luoqi Wang
2024-07-14T15:25:08Z,http://arxiv.org/abs/2407.10245v1,"GenSco: Can Question Decomposition based Passage Alignment improve
  Question Answering?","Retrieval augmented generation (RAG) with large language models (LLMs) for
Question Answering (QA) entails furnishing relevant context within the prompt
to facilitate the LLM in answer generation. During the generation, inaccuracies
or hallucinations frequently occur due to two primary factors: inadequate or
distracting context in the prompts, and the inability of LLMs to effectively
reason through the facts. In this paper, we investigate whether providing
aligned context via a carefully selected passage sequence leads to better
answer generation by the LLM for multi-hop QA. We introduce, ""GenSco"", a novel
approach of selecting passages based on the predicted decomposition of the
multi-hop questions}. The framework consists of two distinct LLMs: (i)
Generator LLM, which is used for question decomposition and final answer
generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to
semantically guide the Generator for passage selection. The generator is
invoked only once for the answer generation, resulting in a cost-effective and
efficient approach. We evaluate on three broadly established multi-hop question
answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve
an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect
to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.",Barah Fazili
2024-07-15T17:30:31Z,http://arxiv.org/abs/2407.10930v2,"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together","Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai",Dilara Soylu
2024-06-28T10:57:50Z,http://arxiv.org/abs/2407.12025v1,"LLM4DESIGN: An Automated Multi-Modal System for Architectural and
  Environmental Design","This study introduces LLM4DESIGN, a highly automated system for generating
architectural and environmental design proposals. LLM4DESIGN, relying solely on
site conditions and design requirements, employs Multi-Agent systems to foster
creativity, Retrieval Augmented Generation (RAG) to ground designs in realism,
and Visual Language Models (VLM) to synchronize all information. This system
resulting in coherent, multi-illustrated, and multi-textual design schemes. The
system meets the dual needs of narrative storytelling and objective drawing
presentation in generating architectural and environmental design proposals.
Extensive comparative and ablation experiments confirm the innovativeness of
LLM4DESIGN's narrative and the grounded applicability of its plans,
demonstrating its superior performance in the field of urban renewal design.
Lastly, we have created the first cross-modal design scheme dataset covering
architecture, landscape, interior, and urban design, providing rich resources
for future research.",Ran Chen
2024-07-01T05:37:17Z,http://arxiv.org/abs/2407.12036v2,Exploring Advanced Large Language Models with LLMsuite,"This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.",Giorgio Roffo
2024-07-11T05:04:44Z,http://arxiv.org/abs/2407.12057v1,"NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker
  and AWS Trainium and Inferentia2","Retrieval-augmented generation (RAG) techniques are widely used today to
retrieve and present information in a conversational format. This paper
presents a set of enhancements to traditional RAG techniques, focusing on large
language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI
chips via SageMaker. These chips are characterized by their elasticity,
affordability, and efficient performance for AI compute tasks. Besides enabling
deployment on these chips, this work aims to improve tool usage, add citation
capabilities, and mitigate the risks of hallucinations and unsafe responses due
to context bias. We benchmark our RAG system's performance on the Natural
Questions and HotPotQA datasets, achieving an accuracy of 62% and 59%
respectively, exceeding other models such as DBRX and Mixtral Instruct.",Tengfei Xue
2024-07-17T13:11:28Z,http://arxiv.org/abs/2407.12529v2,Crafting the Path: Robust Query Rewriting for Information Retrieval,"Query rewriting aims to generate a new query that can complement the original
query to improve the information retrieval system. Recent studies on query
rewriting, such as query2doc, query2expand and querey2cot, rely on the internal
knowledge of Large Language Models (LLMs) to generate a relevant passage to add
information to the query. Nevertheless, the efficacy of these methodologies may
markedly decline in instances where the requisite knowledge is not encapsulated
within the model's intrinsic parameters. In this paper, we propose a novel
structured query rewriting method called Crafting the Path tailored for
retrieval systems. Crafting the Path involves a three-step process that crafts
query-related information necessary for finding the passages to be searched in
each step. Specifically, the Crafting the Path begins with Query Concept
Comprehension, proceeds to Query Type Identification, and finally conducts
Expected Answer Extraction. Experimental results show that our method
outperforms previous rewriting methods, especially in less familiar domains for
LLMs. We demonstrate that our method is less dependent on the internal
parameter knowledge of the model and generates queries with fewer factual
inaccuracies. Furthermore, we observe that \name{} demonstrates superior
performance in the retrieval-augmented generation scenarios.",Ingeol Baek
2024-07-17T16:55:42Z,http://arxiv.org/abs/2407.12735v4,EchoSight: Advancing Visual-Language Models with Wiki Knowledge,"Knowledge-based Visual Question Answering (KVQA) tasks require answering
questions about images using extensive background knowledge. Despite
significant advancements, generative models often struggle with these tasks due
to the limited integration of external knowledge. In this paper, we introduce
EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework
that enables large language models (LLMs) to answer visual questions requiring
fine-grained encyclopedic knowledge. To strive for high-performing retrieval,
EchoSight first searches wiki articles by using visual-only information,
subsequently, these candidate articles are further reranked according to their
relevance to the combined text-image query. This approach significantly
improves the integration of multimodal knowledge, leading to enhanced retrieval
outcomes and more accurate VQA responses. Our experimental results on the
Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes
new state-of-the-art results in knowledge-based VQA, achieving an accuracy of
41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",Yibin Yan
2024-07-18T02:19:00Z,http://arxiv.org/abs/2407.13101v1,"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach","Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.",Zhouyu Jiang
2024-07-18T21:49:32Z,http://arxiv.org/abs/2407.13909v1,PRAGyan -- Connecting the Dots in Tweets,"As social media platforms grow, understanding the underlying reasons behind
events and statements becomes crucial for businesses, policymakers, and
researchers. This research explores the integration of Knowledge Graphs (KGs)
with Large Language Models (LLMs) to perform causal analysis of tweets dataset.
The LLM aided analysis techniques often lack depth in uncovering the causes
driving observed effects. By leveraging KGs and LLMs, which encode rich
semantic relationships and temporal information, this study aims to uncover the
complex interplay of factors influencing causal dynamics and compare the
results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented
Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data
format, to retrieve relevant context for causal reasoning. Our approach
demonstrates that the KG-enhanced LLM RAG can provide improved results when
compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus
increases in size. Our qualitative analysis highlights the advantages of
combining KGs with LLMs for improved interpretability and actionable insights,
facilitating informed decision-making across various domains. Whereas,
quantitative analysis using metrics such as BLEU and cosine similarity show
that our approach outperforms the baseline by 10\%.",Rahul Ravi
2024-07-19T03:02:51Z,http://arxiv.org/abs/2407.13998v2,"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval
  Augmented Question Answering","Question answering based on retrieval augmented generation (RAG-QA) is an
important research topic in NLP and has a wide range of real-world
applications. However, most existing datasets for this task are either
constructed using a single source corpus or consist of short extractive
answers, which fall short of evaluating large language model (LLM) based RAG-QA
systems on cross-domain generalization. To address these limitations, we create
Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form
answers that integrate short extractive answers from multiple documents into a
single, coherent narrative, covering 26K queries and large corpora across seven
different domains. We further propose RAG-QA Arena by directly comparing
model-generated answers against LFRQA's answers using LLMs as evaluators. We
show via extensive experiments that RAG-QA Arena and human judgments on answer
quality are highly correlated. Moreover, only 41.3% of the most competitive
LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a
challenging evaluation platform for future research.",Rujun Han
2024-07-19T08:33:07Z,http://arxiv.org/abs/2407.14116v1,AuditNet: A Conversational AI-based Security Assistant [DEMO],"In the age of information overload, professionals across various fields face
the challenge of navigating vast amounts of documentation and ever-evolving
standards. Ensuring compliance with standards, regulations, and contractual
obligations is a critical yet complex task across various professional fields.
We propose a versatile conversational AI assistant framework designed to
facilitate compliance checking on the go, in diverse domains, including but not
limited to network infrastructure, legal contracts, educational standards,
environmental regulations, and government policies. By leveraging
retrieval-augmented generation using large language models, our framework
automates the review, indexing, and retrieval of relevant, context-aware
information, streamlining the process of verifying adherence to established
guidelines and requirements. This AI assistant not only reduces the manual
effort involved in compliance checks but also enhances accuracy and efficiency,
supporting professionals in maintaining high standards of practice and ensuring
regulatory compliance in their respective fields. We propose and demonstrate
AuditNet, the first conversational AI security assistant designed to assist IoT
network security experts by providing instant access to security standards,
policies, and regulations.",Shohreh Deldari
2024-07-20T17:37:51Z,http://arxiv.org/abs/2407.14944v1,"Automatic Generation of Fashion Images using Prompting in Generative
  Machine Learning Models","The advent of artificial intelligence has contributed in a groundbreaking
transformation of the fashion industry, redefining creativity and innovation in
unprecedented ways. This work investigates methodologies for generating
tailored fashion descriptions using two distinct Large Language Models and a
Stable Diffusion model for fashion image creation. Emphasizing adaptability in
AI-driven fashion creativity, we depart from traditional approaches and focus
on prompting techniques, such as zero-shot and few-shot learning, as well as
Chain-of-Thought (CoT), which results in a variety of colors and textures,
enhancing the diversity of the outputs. Central to our methodology is
Retrieval-Augmented Generation (RAG), enriching models with insights from
fashion sources to ensure contemporary representations. Evaluation combines
quantitative metrics such as CLIPscore with qualitative human judgment,
highlighting strengths in creativity, coherence, and aesthetic appeal across
diverse styles. Among the participants, RAG and few-shot learning techniques
are preferred for their ability to produce more relevant and appealing fashion
descriptions. Our code is provided at https://github.com/georgiarg/AutoFashion.",Georgia Argyrou
2024-07-22T07:15:49Z,http://arxiv.org/abs/2407.15428v1,"Decoding BACnet Packets: A Large Language Model Approach for Packet
  Interpretation","The Industrial Control System (ICS) environment encompasses a wide range of
intricate communication protocols, posing substantial challenges for Security
Operations Center (SOC) analysts tasked with monitoring, interpreting, and
addressing network activities and security incidents. Conventional monitoring
tools and techniques often struggle to provide a clear understanding of the
nature and intent of ICS-specific communications. To enhance comprehension, we
propose a software solution powered by a Large Language Model (LLM). This
solution currently focused on BACnet protocol, processes a packet file data and
extracts context by using a mapping database, and contemporary context
retrieval methods for Retrieval Augmented Generation (RAG). The processed
packet information, combined with the extracted context, serves as input to the
LLM, which generates a concise packet file summary for the user. The software
delivers a clear, coherent, and easily understandable summary of network
activities, enabling SOC analysts to better assess the current state of the
control system.",Rashi Sharma
2024-07-22T15:37:41Z,http://arxiv.org/abs/2407.15734v1,"TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON","TaskGen is an open-sourced agentic framework which uses an Agent to solve an
arbitrary task by breaking them down into subtasks. Each subtask is mapped to
an Equipped Function or another Agent to execute. In order to reduce verbosity
(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from
the Large Language Model (LLM), along with additional features such as type
checking and iterative error correction. Key to the philosophy of TaskGen is
the management of information/memory on a need-to-know basis. We empirically
evaluate TaskGen on various environments such as 40x40 dynamic maze navigation
with changing obstacle locations (100% solve rate), TextWorld escape room
solving with dense rewards and detailed goals (96% solve rate), web browsing
(69% of actions successful), solving the MATH dataset (71% solve rate over 100
Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset
(F1 score of 47.03%)",John Chong Min Tan
2024-07-22T17:50:31Z,http://arxiv.org/abs/2407.15831v1,"NV-Retriever: Improving text embedding models with effective
  hard-negative mining","Text embedding models have been popular for information retrieval
applications such as semantic search and Question-Answering systems based on
Retrieval-Augmented Generation (RAG). Those models are typically Transformer
models that are fine-tuned with contrastive learning objectives. Many papers
introduced new embedding model architectures and training approaches, however,
one of the key ingredients, the process of mining negative passages, remains
poorly explored or described. One of the challenging aspects of fine-tuning
embedding models is the selection of high quality hard-negative passages for
contrastive learning. In this paper we propose a family of positive-aware
mining methods that leverage the positive relevance score for more effective
false negatives removal. We also provide a comprehensive ablation study on
hard-negative mining methods over their configurations, exploring different
teacher and base models. We demonstrate the efficacy of our proposed methods by
introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval
(BEIR) benchmark and 0.65 points higher than previous methods. The model placed
1st when it was published to MTEB Retrieval on July 07, 2024.",Gabriel de Souza P. Moreira
2024-07-21T16:42:45Z,http://arxiv.org/abs/2407.18333v1,"AutoVCoder: A Systematic Framework for Automated Verilog Code Generation
  using LLMs","Recently, the use of large language models (LLMs) for software code
generation, e.g., C/C++ and Python, has proven a great success. However, LLMs
still suffer from low syntactic and functional correctness when it comes to the
generation of register-transfer level (RTL) code, such as Verilog. To address
this issue, in this paper, we develop AutoVCoder, a systematic open-source
framework that significantly improves the LLMs' correctness of generating
Verilog code and enhances the quality of its output at the same time. Our
framework integrates three novel techniques, including a high-quality hardware
dataset generation approach, a two-round LLM fine-tuning method and a
domain-specific retrieval-augmented generation (RAG) mechanism. Experimental
results demonstrate that AutoVCoder outperforms both industrial and academic
LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%
improvement in functional correctness on the EvalMachine and EvalHuman
benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax
correctness and a 3.4% increase in functional correctness on the RTLLM
benchmark compared with RTLCoder.",Mingzhe Gao
2024-07-29T08:38:14Z,http://arxiv.org/abs/2407.19794v2,Introducing a new hyper-parameter for RAG: Context Window Utilization,"This paper introduces a new hyper-parameter for Retrieval-Augmented
Generation (RAG) systems called Context Window Utilization. RAG systems enhance
generative models by incorporating relevant information retrieved from external
knowledge bases, improving the factual accuracy and contextual relevance of
generated responses. The size of the text chunks retrieved and processed is a
critical factor influencing RAG performance. This study aims to identify the
optimal chunk size that maximizes answer generation quality. Through systematic
experimentation, we analyze the effects of varying chunk sizes on the
efficiency and effectiveness of RAG frameworks. Our findings reveal that an
optimal chunk size balances the trade-off between providing sufficient context
and minimizing irrelevant information. These insights are crucial for enhancing
the design and implementation of RAG systems, underscoring the importance of
selecting an appropriate chunk size to achieve superior performance.",Kush Juvekar
2024-07-29T13:26:43Z,http://arxiv.org/abs/2407.19994v3,"A Study on the Implementation Method of an Agent-Based Advanced RAG
  System Using Graph","This study aims to improve knowledge-based question-answering (QA) systems by
overcoming the limitations of existing Retrieval-Augmented Generation (RAG)
models and implementing an advanced RAG system based on Graph technology to
develop high-quality generative AI services. While existing RAG models
demonstrate high accuracy and fluency by utilizing retrieved information, they
may suffer from accuracy degradation as they generate responses using
pre-loaded knowledge without reprocessing. Additionally, they cannot
incorporate real-time data after the RAG configuration stage, leading to issues
with contextual understanding and biased information. To address these
limitations, this study implemented an enhanced RAG system utilizing Graph
technology. This system is designed to efficiently search and utilize
information. Specifically, it employs LangGraph to evaluate the reliability of
retrieved information and synthesizes diverse data to generate more accurate
and enhanced responses. Furthermore, the study provides a detailed explanation
of the system's operation, key implementation steps, and examples through
implementation code and validation results, thereby enhancing the understanding
of advanced RAG technology. This approach offers practical guidelines for
implementing advanced RAG systems in corporate services, making it a valuable
resource for practical application.",Cheonsu Jeong
2024-07-30T09:53:55Z,http://arxiv.org/abs/2407.20700v1,"Industrial-Grade Smart Troubleshooting through Causal Technical Language
  Processing: a Proof of Concept","This paper describes the development of a causal diagnosis approach for
troubleshooting an industrial environment on the basis of the technical
language expressed in Return on Experience records. The proposed method
leverages the vectorized linguistic knowledge contained in the distributed
representation of a Large Language Model, and the causal associations entailed
by the embedded failure modes and mechanisms of the industrial assets. The
paper presents the elementary but essential concepts of the solution, which is
conceived as a causality-aware retrieval augmented generation system, and
illustrates them experimentally on a real-world Predictive Maintenance setting.
Finally, it discusses avenues of improvement for the maturity of the utilized
causal technology to meet the robustness challenges of increasingly complex
scenarios in the industry.",Alexandre Trilla
2024-07-26T03:45:30Z,http://arxiv.org/abs/2407.21059v1,"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks","Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.",Yunfan Gao
2024-07-31T01:51:24Z,http://arxiv.org/abs/2407.21276v2,Multi-Level Querying using A Knowledge Pyramid,"This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.",Rubing Chen
2024-07-31T03:00:59Z,http://arxiv.org/abs/2407.21300v3,Implementing Streaming algorithm and k-means clusters to RAG,"Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.",Haoyu Kang
2024-07-31T21:33:56Z,http://arxiv.org/abs/2408.00167v2,Finch: Prompt-guided Key-Value Cache Compression,"Recent large language model applications, such as Retrieval-Augmented
Generation and chatbots, have led to an increased need to process longer input
contexts. However, this requirement is hampered by inherent limitations.
Architecturally, models are constrained by a context window defined during
training. Additionally, processing extensive texts requires substantial GPU
memory. We propose a novel approach, Finch, to compress the input context by
leveraging the pre-trained model weights of the self-attention. Given a prompt
and a long text, Finch iteratively identifies the most relevant Key (K) and
Value (V) pairs over chunks of the text conditioned on the prompt. Only such
pairs are stored in the KV cache, which, within the space constrained by the
context window, ultimately contains a compressed version of the long text. Our
proposal enables models to consume large inputs even with high compression (up
to 93x) while preserving semantic integrity without the need for fine-tuning.",Giulio Corallo
2024-07-20T06:10:46Z,http://arxiv.org/abs/2408.00798v1,"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation
  for Industrial Knowledge Base","This paper introduces Golden-Retriever, designed to efficiently navigate vast
industrial knowledge bases, overcoming challenges in traditional LLM
fine-tuning and RAG frameworks with domain-specific jargon and context
interpretation. Golden-Retriever incorporates a reflection-based question
augmentation step before document retrieval, which involves identifying jargon,
clarifying its meaning based on context, and augmenting the question
accordingly. Specifically, our method extracts and lists all jargon and
abbreviations in the input question, determines the context against a
pre-defined list, and queries a jargon dictionary for extended definitions and
descriptions. This comprehensive augmentation ensures the RAG framework
retrieves the most relevant documents by providing clear context and resolving
ambiguities, significantly improving retrieval accuracy. Evaluations using
three open-source LLMs on a domain-specific question-answer dataset demonstrate
Golden-Retriever's superior performance, providing a robust solution for
efficiently integrating and querying industrial knowledge bases.",Zhiyu An
2024-08-02T08:37:03Z,http://arxiv.org/abs/2408.01107v2,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.",Chengrui Wang
2024-08-02T13:35:11Z,http://arxiv.org/abs/2408.01262v4,RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,"Retrieval-Augmented Generation (RAG) is a powerful approach that enables
large language models (LLMs) to incorporate external knowledge. However,
evaluating the effectiveness of RAG systems in specialized scenarios remains
challenging due to the high costs of data construction and the lack of suitable
evaluation metrics. This paper introduces RAGEval, a framework designed to
assess RAG systems across diverse scenarios by generating high-quality
documents, questions, answers, and references through a schema-based pipeline.
With a focus on factual accuracy, we propose three novel metrics Completeness,
Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses.
Experimental results show that RAGEval outperforms zero-shot and one-shot
methods in terms of clarity, safety, conformity, and richness of generated
samples. Furthermore, the use of LLMs for scoring the proposed metrics
demonstrates a high level of consistency with human evaluations. RAGEval
establishes a new paradigm for evaluating RAG systems in real-world
applications.",Kunlun Zhu
2024-08-02T17:54:34Z,http://arxiv.org/abs/2408.01419v1,DebateQA: Evaluating Question Answering on Debatable Knowledge,"The rise of large language models (LLMs) has enabled us to seek answers to
inherently debatable questions on LLM chatbots, necessitating a reliable way to
evaluate their ability. However, traditional QA benchmarks assume fixed answers
are inadequate for this purpose. To address this, we introduce DebateQA, a
dataset of 2,941 debatable questions, each accompanied by multiple
human-annotated partial answers that capture a variety of perspectives. We
develop two metrics: Perspective Diversity, which evaluates the
comprehensiveness of perspectives, and Dispute Awareness, which assesses if the
LLM acknowledges the question's debatable nature. Experiments demonstrate that
both metrics align with human preferences and are stable across different
underlying models. Using DebateQA with two metrics, we assess 12 popular LLMs
and retrieval-augmented generation methods. Our findings reveal that while LLMs
generally excel at recognizing debatable issues, their ability to provide
comprehensive answers encompassing diverse perspectives varies considerably.",Rongwu Xu
2024-08-07T14:42:13Z,http://arxiv.org/abs/2408.03811v1,"Generative Language Models with Retrieval Augmented Generation for
  Automated Short Answer Scoring","Automated Short Answer Scoring (ASAS) is a critical component in educational
assessment. While traditional ASAS systems relied on rule-based algorithms or
complex deep learning methods, recent advancements in Generative Language
Models (GLMs) offer new opportunities for improvement. This study explores the
application of GLMs to ASAS, leveraging their off-the-shelf capabilities and
performance in various domains. We propose a novel pipeline that combines
vector databases, transformer-based encoders, and GLMs to enhance short answer
scoring accuracy. Our approach stores training responses in a vector database,
retrieves semantically similar responses during inference, and employs a GLM to
analyze these responses and determine appropriate scores. We further optimize
the system through fine-tuned retrieval processes and prompt engineering.
Evaluation on the SemEval 2013 dataset demonstrates a significant improvement
on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,
highlighting the potential of GLMs in advancing ASAS technology.",Zifan Wang
2024-08-08T06:57:49Z,http://arxiv.org/abs/2408.04259v2,EfficientRAG: Efficient Retriever for Multi-Hop Question Answering,"Retrieval-augmented generation (RAG) methods encounter difficulties when
addressing complex questions like multi-hop queries. While iterative retrieval
methods improve performance by gathering additional information, current
approaches often rely on multiple calls of large language models (LLMs). In
this paper, we introduce EfficientRAG, an efficient retriever for multi-hop
question answering. EfficientRAG iteratively generates new queries without the
need for LLM calls at each iteration and filters out irrelevant information.
Experimental results demonstrate that EfficientRAG surpasses existing RAG
methods on three open-domain multi-hop question-answering datasets.",Ziyuan Zhuang
2024-08-08T09:59:30Z,http://arxiv.org/abs/2408.04342v1,"Towards Explainable Network Intrusion Detection using Large Language
  Models","Large Language Models (LLMs) have revolutionised natural language processing
tasks, particularly as chat agents. However, their applicability to threat
detection problems remains unclear. This paper examines the feasibility of
employing LLMs as a Network Intrusion Detection System (NIDS), despite their
high computational requirements, primarily for the sake of explainability.
Furthermore, considerable resources have been invested in developing LLMs, and
they may offer utility for NIDS. Current state-of-the-art NIDS rely on
artificial benchmarking datasets, resulting in skewed performance when applied
to real-world networking environments. Therefore, we compare the GPT-4 and
LLama3 models against traditional architectures and transformer-based models to
assess their ability to detect malicious NetFlows without depending on
artificially skewed datasets, but solely on their vast pre-trained acquired
knowledge. Our results reveal that, although LLMs struggle with precise attack
detection, they hold significant potential for a path towards explainable NIDS.
Our preliminary exploration shows that LLMs are unfit for the detection of
Malicious NetFlows. Most promisingly, however, these exhibit significant
potential as complementary agents in NIDS, particularly in providing
explanations and aiding in threat response when integrated with Retrieval
Augmented Generation (RAG) and function calling capabilities.",Paul R. B. Houssel
2024-08-09T23:17:56Z,http://arxiv.org/abs/2408.05379v1,Temporal Analysis and Repair of Flaky Dockerfiles,"Dockerfile flakiness, characterized by inconsistent build behavior without
Dockerfile or project source code changes, poses significant challenges in
Continuous Integration and Delivery (CI/CD) pipelines. This issue can lead to
unreliable deployments and increased debugging efforts, yet it remains
underexplored in current research. We conduct a systematic analysis of
Dockerfile flakiness, presenting a comprehensive taxonomy of common flakiness
categories, including dependency-related errors and server connectivity issues.
Furthermore, we introduce FlakiDock, a tool leveraging large language models
and retrieval-augmented generation techniques with dynamic analysis and an
iterative feedback loop to automatically repair flaky Dockerfiles. Our
evaluation shows that FlakiDock achieves a 73.55% repair accuracy,
outperforming existing tools such as PARFUM by 12,581% and GPT-4-based
prompting by 94.63%. These results underscore the effectiveness of FlakiDock in
addressing Dockerfile flakiness and improving build reliability.",Taha Shabani
2024-08-13T14:59:44Z,http://arxiv.org/abs/2408.06941v2,OpenResearcher: Unleashing AI for Accelerated Scientific Research,"The rapid growth of scientific literature imposes significant challenges for
researchers endeavoring to stay updated with the latest advancements in their
fields and delve into new areas. We introduce OpenResearcher, an innovative
platform that leverages Artificial Intelligence (AI) techniques to accelerate
the research process by answering diverse questions from researchers.
OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to
integrate Large Language Models (LLMs) with up-to-date, domain-specific
knowledge. Moreover, we develop various tools for OpenResearcher to understand
researchers' queries, search from the scientific literature, filter retrieved
information, provide accurate and comprehensive answers, and self-refine these
answers. OpenResearcher can flexibly use these tools to balance efficiency and
effectiveness. As a result, OpenResearcher enables researchers to save time and
increase their potential to discover new insights and drive scientific
breakthroughs. Demo, video, and code are available at:
https://github.com/GAIR-NLP/OpenResearcher.",Yuxiang Zheng
2024-08-15T10:54:55Z,http://arxiv.org/abs/2408.08073v1,Extracting Sentence Embeddings from Pretrained Transformer Models,"Background/introduction: Pre-trained transformer models shine in many natural
language processing tasks and therefore are expected to bear the representation
of the input sentence or text meaning. These sentence-level embeddings are also
important in retrieval-augmented generation. But do commonly used plain
averaging or prompt templates surface it enough?
  Methods: Given 110M parameters BERT's hidden representations from multiple
layers and multiple tokens we tried various ways to extract optimal sentence
representations. We tested various token aggregation and representation
post-processing techniques. We also tested multiple ways of using a general
Wikitext dataset to complement BERTs sentence representations. All methods were
tested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12
classification tasks. We also evaluated our representation-shaping techniques
on other static models, including random token representations.
  Results: Proposed representation extraction methods improved the performance
on STS and clustering tasks for all models considered. Very high improvements
for static token-based models, especially random embeddings for STS tasks
almost reach the performance of BERT-derived representations.
  Conclusions: Our work shows that for multiple tasks simple baselines with
representation shaping techniques reach or even outperform more complex
BERT-based models or are able to contribute to their performance.",Lukas Stankevičius
2024-08-15T22:34:44Z,http://arxiv.org/abs/2408.08444v1,"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question
  Answering","In knowledge-intensive tasks such as open-domain question answering (OpenQA),
Large Language Models (LLMs) often struggle to generate factual answers relying
solely on their internal (parametric) knowledge. To address this limitation,
Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving
relevant information from external sources, thereby positioning the retriever
as a pivotal component. Although dense retrieval demonstrates state-of-the-art
performance, its training poses challenges due to the scarcity of ground-truth
evidence, largely attributed to the high costs of human annotation. In this
paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create
weakly labeled data for training dense retrievers. Specifically, we rerank the
top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will
generate the correct answer based on the question and each passage. The
highest-ranking passages are then used as positive training examples for dense
retrieval. Our comprehensive experiments across four publicly available OpenQA
datasets demonstrate that our approach enhances both retrieval and OpenQA
performance compared to baseline models.",Jinming Nian
2024-08-16T04:32:10Z,http://arxiv.org/abs/2408.08521v1,"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement
  Framework for Multimodal Question Answering","Recent advancements in retrieval-augmented generation (RAG) have demonstrated
impressive performance in the question-answering (QA) task. However, most
previous works predominantly focus on text-based answers. While some studies
address multimodal data, they still fall short in generating comprehensive
multimodal answers, particularly for explaining concepts or providing
step-by-step tutorials on how to accomplish specific goals. This capability is
especially valuable for applications such as enterprise chatbots and settings
such as customer service and educational systems, where the answers are sourced
from multimodal data. In this paper, we introduce a simple and effective
framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR
enhances text-based answers by retrieving relevant multimodal data and refining
the responses to create coherent multimodal answers. This framework can be
easily extended to support multimodal answers in enterprise chatbots with
minimal modifications. Human evaluation results indicate that multimodal
answers generated by MuRAR are more useful and readable compared to plain text
answers.",Zhengyuan Zhu
2024-08-15T16:53:05Z,http://arxiv.org/abs/2408.08925v1,"Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building
  E-commerce Chat Assistants","This work presents Retail-GPT, an open-source RAG-based chatbot designed to
enhance user engagement in retail e-commerce by guiding users through product
recommendations and assisting with cart operations. The system is
cross-platform and adaptable to various e-commerce domains, avoiding reliance
on specific chat applications or commercial activities. Retail-GPT engages in
human-like conversations, interprets user demands, checks product availability,
and manages cart operations, aiming to serve as a virtual sales agent and test
the viability of such assistants across different retail businesses.",Bruno Amaral Teixeira de Freitas
2024-08-16T22:00:00Z,http://arxiv.org/abs/2408.09031v1,A Primer on Generative AI for Telecom: From Theory to Practice,"The rise of generative artificial intelligence (GenAI) is transforming the
telecom industry. GenAI models, particularly large language models (LLMs), have
emerged as powerful tools capable of driving innovation, improving efficiency,
and delivering superior customer services in telecom. This paper provides an
overview of GenAI for telecom from theory to practice. We review GenAI models
and discuss their practical applications in telecom. Furthermore, we describe
the key technology enablers and best practices for applying GenAI to telecom
effectively. We highlight the importance of retrieval augmented generation
(RAG) in connecting LLMs to telecom domain specific data sources to enhance the
accuracy of the LLMs' responses. We present a real-world use case on RAG-based
chatbot that can answer open radio access network (O-RAN) specific questions.
The demonstration of the chatbot to the O-RAN Alliance has triggered immense
interest in the industry. We have made the O-RAN RAG chatbot publicly
accessible on GitHub.",Xingqin Lin
2024-08-17T13:32:32Z,http://arxiv.org/abs/2408.09199v1,TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems,"In the pursuit of enhancing domain-specific Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) emerges as a promising solution to
mitigate issues such as hallucinations, outdated knowledge, and limited
expertise in highly specialized queries. However, existing approaches to RAG
fall short by neglecting system state variables, which are crucial for ensuring
adaptive control, retrieval halting, and system convergence. In this paper, we
introduce the TC-RAG through rigorous proof, a novel framework that addresses
these challenges by incorporating a Turing Complete System to manage state
variables, thereby enabling more efficient and accurate knowledge retrieval. By
leveraging a memory stack system with adaptive retrieval, reasoning, and
planning capabilities, TC-RAG not only ensures the controlled halting of
retrieval processes but also mitigates the accumulation of erroneous knowledge
via Push and Pop actions. In the case study of the medical domain, our
extensive experiments on real-world healthcare datasets demonstrate the
superiority of TC-RAG over existing methods in accuracy by over 7.20\%. Our
dataset and code have been available at
https://https://github.com/Artessay/SAMA.git.",Xinke Jiang
2024-08-17T19:17:00Z,http://arxiv.org/abs/2408.09277v1,"Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case
  Study at Ericsson","This paper presents our experience developing a Llama-based chatbot for
question answering about continuous integration and continuous delivery (CI/CD)
at Ericsson, a multinational telecommunications company. Our chatbot is
designed to handle the specificities of CI/CD documents at Ericsson, employing
a retrieval-augmented generation (RAG) model to enhance accuracy and relevance.
Our empirical evaluation of the chatbot on industrial CI/CD-related questions
indicates that an ensemble retriever, combining BM25 and embedding retrievers,
yields the best performance. When evaluated against a ground truth of 72 CI/CD
questions and answers at Ericsson, our most accurate chatbot configuration
provides fully correct answers for 61.11% of the questions, partially correct
answers for 26.39%, and incorrect answers for 12.50%. Through an error analysis
of the partially correct and incorrect answers, we discuss the underlying
causes of inaccuracies and provide insights for further refinement. We also
reflect on lessons learned and suggest future directions for further improving
our chatbot's accuracy.",Daksh Chaudhary
2024-08-05T00:43:56Z,http://arxiv.org/abs/2408.11058v1,LLM Agents Improve Semantic Code Search,"Code Search is a key task that many programmers often have to perform while
developing solutions to problems. Current methodologies suffer from an
inability to perform accurately on prompts that contain some ambiguity or ones
that require additional context relative to a code-base. We introduce the
approach of using Retrieval Augmented Generation (RAG) powered agents to inject
information into user prompts allowing for better inputs into embedding models.
By utilizing RAG, agents enhance user queries with relevant details from GitHub
repositories, making them more informative and contextually aligned.
Additionally, we introduce a multi-stream ensemble approach which when paired
with agentic workflow can obtain improved retrieval accuracy, which we deploy
on application called repo-rift.com. Experimental results on the CodeSearchNet
dataset demonstrate that RepoRift significantly outperforms existing methods,
achieving an 78.2% success rate at Success@10 and a 34.6% success rate at
Success@1. This research presents a substantial advancement in semantic code
search, highlighting the potential of agentic LLMs and RAG to enhance code
retrieval systems.",Sarthak Jain
2024-08-21T18:00:21Z,http://arxiv.org/abs/2408.11903v2,"Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for
  Ancient Indian Philosophy","LLMs have revolutionized the landscape of information retrieval and knowledge
dissemination. However, their application in specialized areas is often
hindered by factual inaccuracies and hallucinations, especially in long-tail
knowledge distributions. We explore the potential of retrieval-augmented
generation (RAG) models for long-form question answering (LFQA) in a
specialized knowledge domain. We present VedantaNY-10M, a dataset curated from
extensive public discourses on the ancient Indian philosophy of Advaita
Vedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,
focusing on transcription, retrieval, and generation performance. Human
evaluations by computational linguists and domain experts show that the RAG
model significantly outperforms the standard model in producing factual and
comprehensive responses having fewer hallucinations. In addition, a
keyword-based hybrid retriever that emphasizes unique low-frequency terms
further improves results. Our study provides insights into effectively
integrating modern large language models with ancient knowledge systems.
Project page with dataset and code: https://sites.google.com/view/vedantany-10m",Priyanka Mandikal
2024-08-21T21:34:01Z,http://arxiv.org/abs/2408.12003v1,"RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and
  Personalization","With the development of the modern social economy, tourism has become an
important way to meet people's spiritual needs, bringing development
opportunities to the tourism industry. However, existing large language models
(LLMs) face challenges in personalized recommendation capabilities and the
generation of content that can sometimes produce hallucinations. This study
proposes an optimization scheme for Tibet tourism LLMs based on
retrieval-augmented generation (RAG) technology. By constructing a database of
tourist viewpoints and processing the data using vectorization techniques, we
have significantly improved retrieval accuracy. The application of RAG
technology effectively addresses the hallucination problem in content
generation. The optimized model shows significant improvements in fluency,
accuracy, and relevance of content generation. This research demonstrates the
potential of RAG technology in the standardization of cultural tourism
information and data analysis, providing theoretical and technical support for
the development of intelligent cultural tourism service systems.",Jinhu Qi
2024-08-22T12:21:22Z,http://arxiv.org/abs/2408.12333v2,Graph Retrieval Augmented Trustworthiness Reasoning,"Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game ""Werewolf,"" comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.",Ying Zhu
2024-08-23T20:51:04Z,http://arxiv.org/abs/2408.13366v1,"CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations
  of Research Papers","This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.",Ekaterina Trofimova
2024-08-24T03:18:42Z,http://arxiv.org/abs/2408.13450v1,vitaLITy 2: Reviewing Academic Literature Using Large Language Models,"Academic literature reviews have traditionally relied on techniques such as
keyword searches and accumulation of relevant back-references, using databases
like Google Scholar or IEEEXplore. However, both the precision and accuracy of
these search techniques is limited by the presence or absence of specific
keywords, making literature review akin to searching for needles in a haystack.
We present vitaLITy 2, a solution that uses a Large Language Model or LLM-based
approach to identify semantically relevant literature in a textual embedding
space. We include a corpus of 66,692 papers from 1970-2023 which are searchable
through text embeddings created by three language models. vitaLITy 2
contributes a novel Retrieval Augmented Generation (RAG) architecture and can
be interacted with through an LLM with augmented prompts, including
summarization of a collection of papers. vitaLITy 2 also provides a chat
interface that allow users to perform complex queries without learning any new
programming language. This also enables users to take advantage of the
knowledge captured in the LLM from its enormous training corpus. Finally, we
demonstrate the applicability of vitaLITy 2 through two usage scenarios.
vitaLITy 2 is available as open-source software at
https://vitality-vis.github.io.",Hongye An
2024-08-24T09:23:01Z,http://arxiv.org/abs/2408.13533v1,"Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the
  Role of RAG Noise in Large Language Models","Retrieval-Augmented Generation (RAG) has emerged as a crucial method for
addressing hallucinations in large language models (LLMs). While recent
research has extended RAG models to complex noisy scenarios, these explorations
often confine themselves to limited noise types and presuppose that noise is
inherently detrimental to LLMs, potentially deviating from real-world retrieval
environments and restricting practical applicability. In this paper, we define
seven distinct noise types from a linguistic perspective and establish a Noise
RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing
multiple datasets and reasoning tasks. Through empirical evaluation of eight
representative LLMs with diverse architectures and scales, we reveal that these
noises can be further categorized into two practical groups: noise that is
beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs
(aka harmful noise). While harmful noise generally impairs performance,
beneficial noise may enhance several aspects of model capabilities and overall
performance. Our analysis offers insights for developing more robust, adaptable
RAG solutions and mitigating hallucinations across diverse retrieval scenarios.",Jinyang Wu
2024-08-25T11:09:15Z,http://arxiv.org/abs/2408.13808v1,"Towards Reliable Medical Question Answering: Techniques and Challenges
  in Mitigating Hallucinations in Language Models","The rapid advancement of large language models (LLMs) has significantly
impacted various domains, including healthcare and biomedicine. However, the
phenomenon of hallucination, where LLMs generate outputs that deviate from
factual accuracy or context, poses a critical challenge, especially in
high-stakes domains. This paper conducts a scoping study of existing techniques
for mitigating hallucinations in knowledge-based task in general and especially
for medical domains. Key methods covered in the paper include
Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback
loops, supervised fine-tuning, and prompt engineering. These techniques, while
promising in general contexts, require further adaptation and optimization for
the medical domain due to its unique demands for up-to-date, specialized
knowledge and strict adherence to medical guidelines. Addressing these
challenges is crucial for developing trustworthy AI systems that enhance
clinical decision-making and patient safety as well as accuracy of biomedical
scientific research.",Duy Khoa Pham
2024-08-26T14:45:03Z,http://arxiv.org/abs/2408.14317v1,Claim Verification in the Age of Large Language Models: A Survey,"The large and ever-increasing amount of data available on the Internet
coupled with the laborious task of manual claim and fact verification has
sparked the interest in the development of automated claim verification
systems. Several deep learning and transformer-based models have been proposed
for this task over the years. With the introduction of Large Language Models
(LLMs) and their superior performance in several NLP tasks, we have seen a
surge of LLM-based approaches to claim verification along with the use of novel
methods such as Retrieval Augmented Generation (RAG). In this survey, we
present a comprehensive account of recent claim verification frameworks using
LLMs. We describe the different components of the claim verification pipeline
used in these frameworks in detail including common approaches to retrieval,
prompting, and fine-tuning. Finally, we describe publicly available English
datasets created for this task.",Alphaeus Dmonte
2024-08-26T16:00:41Z,http://arxiv.org/abs/2408.14380v1,Probing Causality Manipulation of Large Language Models,"Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.",Chenyang Zhang
2024-08-11T18:04:56Z,http://arxiv.org/abs/2408.15264v1,"Validation Requirements for AI-based Intervention-Evaluation in Aging
  and Longevity Research and Practice","The field of aging and longevity research is overwhelmed by vast amounts of
data, calling for the use of Artificial Intelligence (AI), including Large
Language Models (LLMs), for the evaluation of geroprotective interventions.
Such evaluations should be correct, useful, comprehensive, explainable, and
they should consider causality, interdisciplinarity, adherence to standards,
longitudinal data and known aging biology. In particular, comprehensive
analyses should go beyond comparing data based on canonical biomedical
databases, suggesting the use of AI to interpret changes in biomarkers and
outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and
dedicated workflows employing, e.g., Retrieval-Augmented Generation. While
naive trust in the responses of AI tools can cause harm, adding our
requirements to LLM queries can improve response quality, calling for
benchmarking efforts and justifying the informed use of LLMs for advice on
longevity interventions.",Georg Fuellen
2024-08-30T08:26:55Z,http://arxiv.org/abs/2408.17095v2,"RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation
  and Retrieval-Guidance","Diffusion-based models demonstrate impressive generation capabilities.
However, they also have a massive number of parameters, resulting in enormous
model sizes, thus making them unsuitable for deployment on resource-constraint
devices. Block-wise generation can be a promising alternative for designing
compact-sized (parameter-efficient) deep generative models since the model can
generate one block at a time instead of generating the whole image at once.
However, block-wise generation is also considerably challenging because
ensuring coherence across generated blocks can be non-trivial. To this end, we
design a retrieval-augmented generation (RAG) approach and leverage the
corresponding blocks of the images retrieved by the RAG module to condition the
training and generation stages of a block-wise denoising diffusion model. Our
conditioning schemes ensure coherence across the different blocks during
training and, consequently, during generation. While we showcase our approach
using the latent diffusion model (LDM) as the base model, it can be used with
other variants of denoising diffusion models. We validate the solution of the
coherence problem through the proposed approach by reporting substantive
experiments to demonstrate our approach's effectiveness in compact model size
and excellent generation quality.",Avideep Mukherjee
2024-09-04T01:14:04Z,http://arxiv.org/abs/2409.02361v1,"Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented
  Ambiguous Question Answering","The retrieval augmented generation (RAG) framework addresses an ambiguity in
user queries in QA systems by retrieving passages that cover all plausible
interpretations and generating comprehensive responses based on the passages.
However, our preliminary studies reveal that a single retrieval process often
suffers from low quality results, as the retrieved passages frequently fail to
capture all plausible interpretations. Although the iterative RAG approach has
been proposed to address this problem, it comes at the cost of significantly
reduced efficiency. To address these issues, we propose the
diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved
passages to encompass diverse interpretations. Subsequently, DIVA verifies the
quality of the passages and adapts the most suitable approach tailored to their
quality. This approach improves the QA systems accuracy and robustness by
handling low quality retrieval issue in ambiguous questions, while enhancing
efficiency.",Yeonjun In
2024-09-05T05:34:16Z,http://arxiv.org/abs/2409.03258v3,"GraphInsight: Unlocking Insights in Large Language Models for Graph
  Structure Understanding","Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.",Yukun Cao
2024-09-10T15:39:32Z,http://arxiv.org/abs/2409.06595v1,"GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question
  Answering","Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.",Sacha Muller
2024-09-11T08:56:27Z,http://arxiv.org/abs/2409.07110v1,"Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and
  Education","This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed
to enhance user interaction for educational and research purposes. Leveraging
cutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as
a sophisticated AI assistant, exploiting the capabilities of traditional models
like ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval
Augmented Generation (RAG) through three primary methods: integration of
preprocessed documents, real-time processing of user-uploaded files, and
information retrieval from any specified website. Additionally, the chatbot
incorporates image generation via a Stable Diffusion Model (SDM), image
understanding and response generation through LLAVA, and search functionality
on the internet powered by secure search engine such as DuckDuckGo. To provide
comprehensive support, Bio-Eng-LMM offers text summarization, website content
summarization, and both text and voice interaction. The chatbot maintains
session memory to ensure contextually relevant and coherent responses. This
integrated platform builds upon the strengths of RAG-GPT and Web-Based RAG
Query (WBRQ) where the system fetches relevant information directly from the
web to enhance the LLMs response generation.",Ali Forootani
2024-09-04T19:00:59Z,http://arxiv.org/abs/2409.07487v2,MoA is All You Need: Building LLM Research Team using Mixture of Agents,"Large Language Models (LLMs) research in the financial domain is particularly
complex due to the sheer number of approaches proposed in literature.
Retrieval-Augmented Generation (RAG) has emerged as one of the leading methods
in the sector due to its inherent groundedness and data source variability. In
this work, we introduce a RAG framework called Mixture of Agents (MoA) and
demonstrate its viability as a practical, customizable, and highly effective
approach for scaling RAG applications. MoA is essentially a layered network of
individually customized small language models (Hoffmann et al., 2022)
collaborating to answer questions and extract information. While there are many
theoretical propositions for such an architecture and even a few libraries for
generally applying the structure in practice, there are limited documented
studies evaluating the potential of this framework considering real business
constraints such as cost and speed. We find that the MoA framework, consisting
of small language models (Hoffmann et al., 2022), produces higher quality and
more grounded responses across various financial domains that are core to
Vanguard's business while simultaneously maintaining low costs.",Sandy Chen
2024-09-12T02:40:28Z,http://arxiv.org/abs/2409.07713v1,"Experimenting with Legal AI Solutions: The Case of Question-Answering
  for Access to Justice","Generative AI models, such as the GPT and Llama series, have significant
potential to assist laypeople in answering legal questions. However, little
prior work focuses on the data sourcing, inference, and evaluation of these
models in the context of laypersons. To this end, we propose a human-centric
legal NLP pipeline, covering data sourcing, inference, and evaluation. We
introduce and release a dataset, LegalQA, with real and specific legal
questions spanning from employment law to criminal law, corresponding answers
written by legal experts, and citations for each answer. We develop an
automatic evaluation protocol for this dataset, then show that
retrieval-augmented generation from only 850 citations in the train set can
match or outperform internet-wide retrieval, despite containing 9 orders of
magnitude less data. Finally, we propose future directions for open-sourced
efforts, which fall behind closed-sourced models.",Jonathan Li
2024-09-13T02:08:47Z,http://arxiv.org/abs/2409.08479v2,"Exploring Information Retrieval Landscapes: An Investigation of a Novel
  Evaluation Techniques and Comparative Document Splitting Methods","The performance of Retrieval-Augmented Generation (RAG) systems in
information retrieval is significantly influenced by the characteristics of the
documents being processed. In this study, the structured nature of textbooks,
the conciseness of articles, and the narrative complexity of novels are shown
to require distinct retrieval strategies. A comparative evaluation of multiple
document-splitting methods reveals that the Recursive Character Splitter
outperforms the Token-based Splitter in preserving contextual integrity. A
novel evaluation technique is introduced, utilizing an open-source model to
generate a comprehensive dataset of question-and-answer pairs, simulating
realistic retrieval scenarios to enhance testing efficiency and metric
reliability. The evaluation employs weighted scoring metrics, including
SequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy
and relevance. This approach establishes a refined standard for evaluating the
precision of RAG systems, with future research focusing on optimizing chunk and
overlap sizes to improve retrieval accuracy and efficiency.",Esmaeil Narimissa
2024-09-13T13:34:32Z,http://arxiv.org/abs/2409.08820v1,"A RAG Approach for Generating Competency Questions in Ontology
  Engineering","Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.",Xueli Pan
2024-09-14T03:11:00Z,http://arxiv.org/abs/2409.09281v1,"Language Models ""Grok"" to Copy","We examine the pre-training dynamics of language models, focusing on their
ability to copy text from preceding context--a fundamental skill for various
LLM applications, including in-context learning (ICL) and retrieval-augmented
generation (RAG). We propose a novel perspective that Transformer-based
language models develop copying abilities similarly to grokking, which refers
to sudden generalization on test set long after the model fit to the training
set. Our experiments yield three arguments: (1) The pre-training loss decreases
rapidly, while the context copying ability of models initially lags and then
abruptly saturates. (2) The speed of developing copying ability is independent
of the number of tokens trained, similarly to how grokking speed is unaffected
by dataset size as long as the data distribution is preserved. (3) Induction
heads, the attention heads responsible for copying, form from shallow to deep
layers during training, mirroring the development of circuits in deeper layers
during grokking. We contend that the connection between grokking and context
copying can provide valuable insights for more effective language model
training, ultimately improving in-context performance. For example, we
demonstrated that techniques that enhance grokking, such as regularization,
either accelerate or enhance the development of context copying.",Ang Lv
2024-09-14T17:40:35Z,http://arxiv.org/abs/2409.09493v1,"Hacking, The Lazy Way: LLM Augmented Pentesting","Security researchers are continually challenged by the need to stay current
with rapidly evolving cybersecurity research, tools, and techniques. This
constant cycle of learning, unlearning, and relearning, combined with the
repetitive tasks of sifting through documentation and analyzing data, often
hinders productivity and innovation. This has led to a disparity where only
organizations with substantial resources can access top-tier security experts,
while others rely on firms with less skilled researchers who focus primarily on
compliance rather than actual security.
  We introduce ""LLM Augmented Pentesting,"" demonstrated through a tool named
""Pentest Copilot,"" to address this gap. This approach integrates Large Language
Models into penetration testing workflows. Our research includes a ""chain of
thought"" mechanism to streamline token usage and boost performance, as well as
unique Retrieval Augmented Generation implementation to minimize hallucinations
and keep models aligned with the latest techniques. Additionally, we propose a
novel file analysis approach, enabling LLMs to understand files. Furthermore,
we highlight a unique infrastructure system that supports if implemented, can
support in-browser assisted penetration testing, offering a robust platform for
cybersecurity professionals, These advancements mark a significant step toward
bridging the gap between automated tools and human expertise, offering a
powerful solution to the challenges faced by modern cybersecurity teams.",Dhruva Goyal
2024-09-17T01:37:57Z,http://arxiv.org/abs/2409.10825v3,"Unveiling and Mitigating Bias in Large Language Model Recommendations: A
  Path to Fairness","excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.",Anindya Bijoy Das
2024-09-17T07:44:06Z,http://arxiv.org/abs/2409.10955v1,"Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style","Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.",Yuepei Li
2024-09-17T14:47:33Z,http://arxiv.org/abs/2409.11242v2,"Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded
  Attributions and Learning to Refuse","LLMs are an integral component of retrieval-augmented generation (RAG)
systems. While many studies focus on evaluating the overall quality of
end-to-end RAG systems, there is a gap in understanding the appropriateness of
LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic
metric that evaluates the trustworthiness of LLMs within the RAG framework. Our
results show that various prompting methods, such as in-context learning, fail
to effectively adapt LLMs to the RAG task as measured by Trust-Score.
Consequently, we propose Trust-Align, a method to align LLMs for improved
Trust-Score performance. The LLaMA-3 family, aligned using our method,
significantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0),
QAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of
Trust-Align across different open-weight models, including the LLaMA series (1b
to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at
\url{https://anonymous.4open.science/r/trust-align}",Maojia Song
2024-09-19T07:39:22Z,http://arxiv.org/abs/2409.12524v1,"Should RAG Chatbots Forget Unimportant Conversations? Exploring
  Importance and Forgetting with Psychological Insights","While Retrieval-Augmented Generation (RAG) has shown promise in enhancing
long-term conversations, the increasing memory load as conversations progress
degrades retrieval accuracy. Drawing on psychological insights, we propose
LUFY, a simple yet effective method that focuses on emotionally arousing
memories and retains less than 10% of the conversation. In the user experiment,
participants interacted with three types of RAG chatbots, each for 2 hours over
4 sessions, marking the most extensive assessment of a chatbot's long-term
capabilities to date -- more than four times longer than any existing
benchmark. The results demonstrate that prioritizing arousing memories while
forgetting the majority of the conversation significantly enhances user
experience. This study pushes the frontier of long-term conversations and
highlights the importance of forgetting unimportant parts of conversations.
Code and Dataset: https://github.com/ryuichi-sumida/LUFY",Ryuichi Sumida
2024-09-20T14:30:45Z,http://arxiv.org/abs/2409.13537v1,"ShizishanGPT: An Agricultural Large Language Model Integrating Tools and
  Resources","Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",Shuting Yang
2024-09-10T17:51:21Z,http://arxiv.org/abs/2409.13741v1,Knowing When to Ask -- Bridging Large Language Models and Data,"Large Language Models (LLMs) are prone to generating factually incorrect
information when responding to queries that involve numerical and statistical
data or other timely facts. In this paper, we present an approach for enhancing
the accuracy of LLMs by integrating them with Data Commons, a vast, open-source
repository of public statistics from trusted organizations like the United
Nations (UN), Center for Disease Control and Prevention (CDC) and global census
bureaus. We explore two primary methods: Retrieval Interleaved Generation
(RIG), where the LLM is trained to produce natural language queries to retrieve
data from Data Commons, and Retrieval Augmented Generation (RAG), where
relevant data tables are fetched from Data Commons and used to augment the
LLM's prompt. We evaluate these methods on a diverse set of queries,
demonstrating their effectiveness in improving the factual accuracy of LLM
outputs. Our work represents an early step towards building more trustworthy
and reliable LLMs that are grounded in verifiable statistical data and capable
of complex factual reasoning.",Prashanth Radhakrishnan
2024-09-21T03:03:09Z,http://arxiv.org/abs/2409.13992v1,"SMART-RAG: Selection using Determinantal Matrices for Augmented
  Retrieval","Retrieval-Augmented Generation (RAG) has greatly improved large language
models (LLMs) by enabling them to generate accurate, contextually grounded
responses through the integration of external information. However,
conventional RAG approaches, which prioritize top-ranked documents based solely
on query-context relevance, often introduce redundancy and conflicting
information. This issue is particularly evident in unsupervised retrieval
settings, where there are no mechanisms to effectively mitigate these problems,
leading to suboptimal context selection. To address this, we propose Selection
using Matrices for Augmented Retrieval (SMART) in question answering tasks, a
fully unsupervised and training-free framework designed to optimize context
selection in RAG. SMART leverages Determinantal Point Processes (DPPs) to
simultaneously model relevance, diversity and conflict, ensuring the selection
of potentially high-quality contexts. Experimental results across multiple
datasets demonstrate that SMART significantly enhances QA performance and
surpasses previous unsupervised context selection methods, showing a promising
strategy for RAG.",Jiatao Li
2024-09-21T09:36:14Z,http://arxiv.org/abs/2409.14083v1,"SURf: Teaching Large Vision-Language Models to Selectively Utilize
  Retrieved Information","Large Vision-Language Models (LVLMs) have become pivotal at the intersection
of computer vision and natural language processing. However, the full potential
of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains
underutilized. Existing works either focus solely on the text modality or are
limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize
retrieved information and are sensitive to irrelevant or misleading references.
To address these challenges, we propose a self-refinement framework designed to
teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically,
when given questions that are incorrectly answered by the LVLM backbone, we
obtain references that help correct the answers (positive references) and those
that do not (negative references). We then fine-tune the LVLM backbone using a
combination of these positive and negative references. Our experiments across
three tasks and seven datasets demonstrate that our framework significantly
enhances LVLMs ability to effectively utilize retrieved multimodal references
and improves their robustness against irrelevant or misleading information. The
source code is available at https://github.com/GasolSun36/SURf.",Jiashuo Sun
2024-09-21T15:32:10Z,http://arxiv.org/abs/2409.14175v1,"QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and
  Option Shuffling","Large Language models (LLMs) have brought about substantial advancements in
the field of Question Answering (QA) systems. These models do remarkably well
in addressing intricate inquiries in a variety of disciplines. However, because
of domain-specific vocabulary, complex technological concepts, and the
requirement for exact responses applying LLMs to specialized sectors like
telecommunications presents additional obstacles. GPT-3.5 has been used in
recent work, to obtain noteworthy accuracy for telecom-related questions in a
Retrieval Augmented Generation (RAG) framework. Notwithstanding these
developments, the practical use of models such as GPT-3.5 is restricted by
their proprietary nature and high computing demands. This paper introduces
QMOS, an innovative approach which uses a Question-Masked loss and Option
Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice
Questions in the telecommunications domain. Our focus was on using opensource,
smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.
Our multi-faceted approach involves several enhancements to the whole LLM-RAG
pipeline of finetuning, retrieval, prompt engineering and inference. Our
approaches significantly outperform existing results, achieving accuracy
improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%
to 84.65% with Phi-2.",Blessed Guda
2024-09-21T16:46:15Z,http://arxiv.org/abs/2409.14192v2,"Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic
  Extraction","Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.",Hossein Sholehrasa
2024-09-23T00:09:34Z,http://arxiv.org/abs/2409.14634v2,"Scideator: Human-LLM Scientific Idea Generation Grounded in
  Research-Paper Facet Recombination","The scientific ideation process often involves blending salient aspects of
existing papers to create new ideas. To see if large language models (LLMs) can
assist this process, we contribute Scideator, a novel mixed-initiative tool for
scientific ideation. Starting from a user-provided set of papers, Scideator
extracts key facets (purposes, mechanisms, and evaluations) from these and
relevant papers, allowing users to explore the idea space by interactively
recombining facets to synthesize inventive ideas. Scideator also helps users to
gauge idea novelty by searching the literature for potential overlaps and
showing automated novelty assessments and explanations. To support these tasks,
Scideator introduces four LLM-powered retrieval-augmented generation (RAG)
modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty
Checker, and Idea Novelty Iterator. In a within-subjects user study, 19
computer-science researchers identified significantly more interesting ideas
using Scideator compared to a strong baseline combining a scientific search
engine with LLM interaction.",Marissa Radensky
2024-09-23T10:23:19Z,http://arxiv.org/abs/2409.14878v1,"InterMind: A Doctor-Patient-Family Interactive Depression Assessment
  System Empowered by Large Language Models","Depression poses significant challenges to patients and healthcare
organizations, necessitating efficient assessment methods. Existing paradigms
typically focus on a patient-doctor way that overlooks multi-role interactions,
such as family involvement in the evaluation and caregiving process. Moreover,
current automatic depression detection (ADD) methods usually model depression
detection as a classification or regression task, lacking interpretability for
the decision-making process. To address these issues, we developed InterMind, a
doctor-patient-family interactive depression assessment system empowered by
large language models (LLMs). Our system enables patients and families to
contribute descriptions, generates assistive diagnostic reports for doctors,
and provides actionable insights, improving diagnostic precision and
efficiency. To enhance LLMs' performance in psychological counseling and
diagnostic interpretability, we integrate retrieval-augmented generation (RAG)
and chain-of-thoughts (CoT) techniques for data augmentation, which mitigates
the hallucination issue of LLMs in specific scenarios after instruction
fine-tuning. Quantitative experiments and professional assessments by
clinicians validate the effectiveness of our system.",Zhiyuan Zhou
2024-09-09T07:28:14Z,http://arxiv.org/abs/2409.15337v1,Revisiting the Solution of Meta KDD Cup 2024: CRAG,"This paper presents the solution of our team APEX in the Meta KDD CUP 2024:
CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the
limitations of existing QA benchmarks in evaluating the diverse and dynamic
challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a
more comprehensive assessment of RAG performance and contributes to advancing
research in this field. We propose a routing-based domain and dynamic adaptive
RAG pipeline, which performs specific processing for the diverse and dynamic
nature of the question in all three stages: retrieval, augmentation, and
generation. Our method achieved superior performance on CRAG and ranked 2nd for
Task 2&3 on the final competition leaderboard. Our implementation is available
at this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.",Jie Ouyang
2024-09-23T20:05:12Z,http://arxiv.org/abs/2409.15515v1,"Learning When to Retrieve, What to Rewrite, and How to Respond in
  Conversational QA","Augmenting Large Language Models (LLMs) with information retrieval
capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial
for knowledge-intensive tasks. However, understanding users' contextual search
intent when generating responses is an understudied topic for conversational
question answering (QA). This conversational extension leads to additional
concerns when compared to single-turn QA as it is more challenging for systems
to comprehend conversational context and manage retrieved passages over
multiple turns. In this work, we propose a method for enabling LLMs to decide
when to retrieve in RAG settings given a conversational context. When retrieval
is deemed necessary, the LLM then rewrites the conversation for passage
retrieval and judges the relevance of returned passages before response
generation. Operationally, we build on the single-turn SELF-RAG framework (Asai
et al., 2023) and propose SELF-multi-RAG for conversational settings.
SELF-multi-RAG demonstrates improved capabilities over single-turn variants
with respect to retrieving relevant passages (by using summarized
conversational context) and assessing the quality of generated responses.
Experiments on three conversational QA datasets validate the enhanced response
generation capabilities of SELF-multi-RAG, with improvements of ~13% measured
by human annotation.",Nirmal Roy
2024-09-24T15:20:39Z,http://arxiv.org/abs/2409.16176v1,Cyber Knowledge Completion Using Large Language Models,"The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.",Braden K Webb
2024-09-24T17:37:54Z,http://arxiv.org/abs/2409.16266v1,"REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial
  Task Allocation in Multi-Human Multi-Robot Teams","Multi-human multi-robot teams combine the complementary strengths of humans
and robots to tackle complex tasks across diverse applications. However, the
inherent heterogeneity of these teams presents significant challenges in
initial task allocation (ITA), which involves assigning the most suitable tasks
to each team member based on their individual capabilities before task
execution. While current learning-based methods have shown promising results,
they are often computationally expensive to train, and lack the flexibility to
incorporate user preferences in multi-objective optimization and adapt to
last-minute changes in real-world dynamic environments. To address these
issues, we propose REBEL, an LLM-based ITA framework that integrates rule-based
and experience-enhanced learning. By leveraging Retrieval-Augmented Generation,
REBEL dynamically retrieves relevant rules and past experiences, enhancing
reasoning efficiency. Additionally, REBEL can complement pre-trained RL-based
ITA policies, improving situational awareness and overall team performance.
Extensive experiments validate the effectiveness of our approach across various
settings. More details are available at https://sites.google.com/view/ita-rebel .",Arjun Gupte
2024-09-25T09:41:46Z,http://arxiv.org/abs/2409.16779v1,LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ,"Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements.",Marc-Antoine Allard
2024-09-26T06:53:29Z,http://arxiv.org/abs/2409.17580v1,"Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case
  Study","Extracting meaningful insights from large and complex datasets poses
significant challenges, particularly in ensuring the accuracy and relevance of
retrieved information. Traditional data retrieval methods such as sequential
search and index-based retrieval often fail when handling intricate and
interconnected data structures, resulting in incomplete or misleading outputs.
To overcome these limitations, we introduce Structured-GraphRAG, a versatile
framework designed to enhance information retrieval across structured datasets
in natural language queries. Structured-GraphRAG utilizes multiple knowledge
graphs, which represent data in a structured format and capture complex
relationships between entities, enabling a more nuanced and comprehensive
retrieval of information. This graph-based approach reduces the risk of errors
in language model outputs by grounding responses in a structured format,
thereby enhancing the reliability of results. We demonstrate the effectiveness
of Structured-GraphRAG by comparing its performance with that of a recently
published method using traditional retrieval-augmented generation. Our findings
show that Structured-GraphRAG significantly improves query processing
efficiency and reduces response times. While our case study focuses on soccer
data, the framework's design is broadly applicable, offering a powerful tool
for data analysis and enhancing language model applications across various
structured domains.",Zahra Sepasdar
2024-09-26T08:55:21Z,http://arxiv.org/abs/2409.17648v3,"Efficient In-Domain Question Answering for Resource-Constrained
  Environments","Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.",Isaac Chung
2024-09-27T09:20:42Z,http://arxiv.org/abs/2409.18575v1,Corpus-informed Retrieval Augmented Generation of Clarifying Questions,"This study aims to develop models that generate corpus informed clarifying
questions for web search, in a way that ensures the questions align with the
available information in the retrieval corpus. We demonstrate the effectiveness
of Retrieval Augmented Language Models (RAG) in this process, emphasising their
ability to (i) jointly model the user query and retrieval corpus to pinpoint
the uncertainty and ask for clarifications end-to-end and (ii) model more
evidence documents, which can be used towards increasing the breadth of the
questions asked. However, we observe that in current datasets search intents
are largely unsupported by the corpus, which is problematic both for training
and evaluation. This causes question generation models to ``hallucinate'', ie.
suggest intents that are not in the corpus, which can have detrimental effects
in performance. To address this, we propose dataset augmentation methods that
align the ground truth clarifications with the retrieval corpus. Additionally,
we explore techniques to enhance the relevance of the evidence pool during
inference, but find that identifying ground truth intents within the corpus
remains challenging. Our analysis suggests that this challenge is partly due to
the bias of current datasets towards clarification taxonomies and calls for
data that can support generating corpus-informed clarifications.",Antonios Minas Krasakis
2024-09-16T20:36:17Z,http://arxiv.org/abs/2409.18986v1,"Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test
  Interpretation in Clinical Medicine","Accurate interpretation of lab results is crucial in clinical medicine, yet
most patient portals use universal normal ranges, ignoring factors like age and
gender. This study introduces Lab-AI, an interactive system that offers
personalized normal ranges using Retrieval-Augmented Generation (RAG) from
credible health sources. Lab-AI has two modules: factor retrieval and normal
range retrieval. We tested these on 68 lab tests-30 with conditional factors
and 38 without. For tests with factors, normal ranges depend on
patient-specific information. Our results show that GPT-4-turbo with RAG
achieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal
range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by
29.1% in factor retrieval and showed 60.9% and 52.9% improvements in
question-level and lab-level performance, respectively, for normal range
retrieval. These findings highlight Lab-AI's potential to enhance patient
understanding of lab results.",Xiaoyu Wang
2024-09-28T23:59:46Z,http://arxiv.org/abs/2409.19487v3,"HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations","In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.",Ziyu Wang
2024-09-29T16:08:45Z,http://arxiv.org/abs/2409.19753v2,"CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex
  Knowledge Graph Question Answering","Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.",Yike Wu
2024-09-30T07:48:55Z,http://arxiv.org/abs/2409.20042v2,"Beyond Scores: A Modular RAG-Based System for Automatic Short Answer
  Scoring with Feedback","Automatic short answer scoring (ASAS) helps reduce the grading burden on
educators but often lacks detailed, explainable feedback. Existing methods in
ASAS with feedback (ASAS-F) rely on fine-tuning language models with limited
datasets, which is resource-intensive and struggles to generalize across
contexts. Recent approaches using large language models (LLMs) have focused on
scoring without extensive fine-tuning. However, they often rely heavily on
prompt engineering and either fail to generate elaborated feedback or do not
adequately evaluate it. In this paper, we propose a modular retrieval augmented
generation based ASAS-F system that scores answers and generates feedback in
strict zero-shot and few-shot learning scenarios. We design our system to be
adaptable to various educational tasks without extensive prompt engineering
using an automatic prompt generation framework. Results show an improvement in
scoring accuracy by 9\% on unseen questions compared to fine-tuning, offering a
scalable and cost-effective solution.",Menna Fateen
2024-09-30T15:53:38Z,http://arxiv.org/abs/2409.20434v1,"QAEncoder: Towards Aligned Representation Learning in Question Answering
  System","Modern QA systems entail retrieval-augmented generation (RAG) for accurate
and trustworthy responses. However, the inherent gap between user queries and
relevant documents hinders precise matching. Motivated by our conical
distribution hypothesis, which posits that potential queries and documents form
a cone-like structure in the embedding space, we introduce QAEncoder, a
training-free approach to bridge this gap. Specifically, QAEncoder estimates
the expectation of potential queries in the embedding space as a robust
surrogate for the document embedding, and attaches document fingerprints to
effectively distinguish these embeddings. Extensive experiments on fourteen
embedding models across six languages and eight datasets validate QAEncoder's
alignment capability, which offers a plug-and-play solution that seamlessly
integrates with existing RAG architectures and training-based methods.",Zhengren Wang
2024-10-01T07:18:34Z,http://arxiv.org/abs/2410.00454v1,UniAdapt: A Universal Adapter for Knowledge Calibration,"Large Language Models (LLMs) require frequent updates to correct errors and
keep pace with continuously evolving knowledge in a timely and effective
manner. Recent research in it model editing has highlighted the challenges in
balancing generalization and locality, especially in the context of lifelong
model editing. We discover that inserting knowledge directly into the model
often causes conflicts and potentially disrupts other unrelated pre-trained
knowledge. To address this problem, we introduce UniAdapt, a universal adapter
for knowledge calibration. Inspired by the Mixture of Experts architecture and
Retrieval-Augmented Generation, UniAdapt is designed with a vector-assisted
router that is responsible for routing inputs to appropriate experts. The
router maintains a vector store, including multiple shards, to construct
routing vectors based on semantic similarity search results. UniAdapt is fully
model-agnostic and designed for seamless plug-and-play integration.
Experimental results show that UniAdapt outperforms existing lifelong model
editors and achieves exceptional results in most metrics.",Tai D. Nguyen
2024-10-03T03:06:42Z,http://arxiv.org/abs/2410.02163v1,"Controlled Generation of Natural Adversarial Documents for Stealthy
  Retrieval Poisoning","Recent work showed that retrieval based on embedding similarity (e.g., for
retrieval-augmented generation) is vulnerable to poisoning: an adversary can
craft malicious documents that are retrieved in response to broad classes of
queries. We demonstrate that previous, HotFlip-based techniques produce
documents that are very easy to detect using perplexity filtering. Even if
generation is constrained to produce low-perplexity text, the resulting
documents are recognized as unnatural by LLMs and can be automatically filtered
from the retrieval corpus.
  We design, implement, and evaluate a new controlled generation technique that
combines an adversarial objective (embedding similarity) with a ""naturalness""
objective based on soft scores computed using an open-source, surrogate LLM.
The resulting adversarial documents (1) cannot be automatically detected using
perplexity filtering and/or other LLMs, except at the cost of significant false
positives in the retrieval corpus, yet (2) achieve similar poisoning efficacy
to easily-detectable documents generated using HotFlip, and (3) are
significantly more effective than prior methods for energy-guided generation,
such as COLD.",Collin Zhang
2024-10-03T09:48:09Z,http://arxiv.org/abs/2410.02338v2,How Much Can RAG Help the Reasoning of LLM?,"Retrieval-Augmented Generation (RAG) has gained significant popularity in
modern Large Language Models (LLMs) due to its effectiveness in introducing new
knowledge and reducing hallucinations. However, the deep understanding of RAG
remains limited, how does RAG help the reasoning process and can RAG help
improve the reasoning capability remains question. While external documents are
typically considered as a method to incorporate domain-specific information,
they also contain intermediate reasoning results related to the query, this
suggests that documents could enhance the reasoning capability of LLMs, which
has not been previously explored. In this paper, we investigate this issue in
depth and find that while RAG can assist with reasoning, the help is limited.
If we conceptualize the reasoning process as a tree with fixed depth, then RAG
struggles to assist LLMs in performing deeper reasoning. Additionally, the
information in the documents requires preprocessing to filter out noise. We
demonstrate that this preprocessing is difficult to achieve simply fine-tuning
of the LLM, it often necessitates numerous additional transformer layers to
solve the problem. To simplify the problem, we propose DPrompt tuning, which
effectively resolves the issue within just limited transformer layers, leading
to improved performance.",Jingyu Liu
2024-10-03T14:55:22Z,http://arxiv.org/abs/2410.02551v1,"ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration","We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.",Zixiang Wang
2024-10-03T16:34:46Z,http://arxiv.org/abs/2410.02650v1,Undesirable Memorization in Large Language Models: A Survey,"While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.",Ali Satvaty
2024-10-03T17:55:09Z,http://arxiv.org/abs/2410.02742v2,"Grounding Large Language Models In Embodied Environment With Imperfect
  World Models","Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.",Haolan Liu
2024-10-03T19:25:05Z,http://arxiv.org/abs/2410.02932v1,Intrinsic Evaluation of RAG Systems for Deep-Logic Questions,"We introduce the Overall Performance Index (OPI), an intrinsic metric to
evaluate retrieval-augmented generation (RAG) mechanisms for applications
involving deep-logic queries. OPI is computed as the harmonic mean of two key
metrics: the Logical-Relation Correctness Ratio and the average of BERT
embedding similarity scores between ground-truth and generated answers. We
apply OPI to assess the performance of LangChain, a popular RAG tool, using a
logical relations classifier fine-tuned from GPT-4o on the RAG-Dataset-12000
from Hugging Face. Our findings show a strong correlation between BERT
embedding similarity scores and extrinsic evaluation scores. Among the commonly
used retrievers, the cosine similarity retriever using BERT-based embeddings
outperforms others, while the Euclidean distance-based retriever exhibits the
weakest performance. Furthermore, we demonstrate that combining multiple
retrievers, either algorithmically or by merging retrieved sentences, yields
superior performance compared to using any single retriever alone.",Junyi Hu
2024-10-04T15:54:49Z,http://arxiv.org/abs/2410.03537v1,Ward: Provable RAG Dataset Inference via LLM Watermarks,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",Nikola Jovanović
2024-09-30T06:27:53Z,http://arxiv.org/abs/2410.03727v2,"FaithEval: Can Your Language Model Stay Faithful to Context, Even If
  ""The Moon is Made of Marshmallows""","Ensuring faithfulness to context in large language models (LLMs) and
retrieval-augmented generation (RAG) systems is crucial for reliable deployment
in real-world applications, as incorrect or unsupported information can erode
user trust. Despite advancements on standard benchmarks, faithfulness
hallucination-where models generate responses misaligned with the provided
context-remains a significant challenge. In this work, we introduce FaithEval,
a novel and comprehensive benchmark tailored to evaluate the faithfulness of
LLMs in contextual scenarios across three diverse tasks: unanswerable,
inconsistent, and counterfactual contexts. These tasks simulate real-world
challenges where retrieval mechanisms may surface incomplete, contradictory, or
fabricated information. FaithEval comprises 4.9K high-quality problems in
total, validated through a rigorous four-stage context construction and
validation framework, employing both LLM-based auto-evaluation and human
validation. Our extensive study across a wide range of open-source and
proprietary models reveals that even state-of-the-art models often struggle to
remain faithful to the given context, and that larger models do not necessarily
exhibit improved faithfulness.Project is available at:
\url{https://github.com/SalesforceAIResearch/FaithEval}.",Yifei Ming
2024-10-02T05:24:49Z,http://arxiv.org/abs/2410.03754v1,Enhancing Retrieval in QA Systems with Derived Feature Association,"Retrieval augmented generation (RAG) has become the standard in long context
question answering (QA) systems. However, typical implementations of RAG rely
on a rather naive retrieval mechanism, in which texts whose embeddings are most
similar to that of the query are deemed most relevant. This has consequences in
subjective QA tasks, where the most relevant text may not directly contain the
answer. In this work, we propose a novel extension to RAG systems, which we
call Retrieval from AI Derived Documents (RAIDD). RAIDD leverages the full
power of the LLM in the retrieval process by deriving inferred features, such
as summaries and example questions, from the documents at ingest. We
demonstrate that this approach significantly improves the performance of RAG
systems on long-context QA tasks.",Keyush Shah
2024-10-03T15:26:50Z,http://arxiv.org/abs/2410.03780v1,Reward-RAG: Enhancing RAG with Reward Driven Supervision,"In this paper, we introduce Reward-RAG, a novel approach designed to enhance
the Retrieval-Augmented Generation (RAG) model through Reward-Driven
Supervision. Unlike previous RAG methodologies, which focus on training
language models (LMs) to utilize external knowledge retrieved from external
sources, our method adapts retrieval information to specific domains by
employing CriticGPT to train a dedicated reward model. This reward model
generates synthesized datasets for fine-tuning the RAG encoder, aligning its
outputs more closely with human preferences. The versatility of our approach
allows it to be effectively applied across various domains through
domain-specific fine-tuning. We evaluate Reward-RAG on publicly available
benchmarks from multiple domains, comparing it to state-of-the-art methods. Our
experimental results demonstrate significant improvements in performance,
highlighting the effectiveness of Reward-RAG in improving the relevance and
quality of generated responses. These findings underscore the potential of
integrating reward models with RAG to achieve superior outcomes in natural
language generation tasks.",Thang Nguyen
2024-10-04T18:22:58Z,http://arxiv.org/abs/2410.03845v2,ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD,"Open-source Electronic Design Automation (EDA) tools are rapidly transforming
chip design by addressing key barriers of commercial EDA tools such as
complexity, costs, and access. Recent advancements in Large Language Models
(LLMs) have further enhanced efficiency in chip design by providing user
assistance across a range of tasks like setup, decision-making, and flow
automation. This paper introduces ORAssistant, a conversational assistant for
OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to
improve the user experience for the OpenROAD flow, from RTL-GDSII by providing
context-specific responses to common user queries, including installation,
command usage, flow setup, and execution, in prose format. Currently,
ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and
KLayout. The data model is built from publicly available documentation and
GitHub resources. The proposed architecture is scalable, supporting extensions
to other open-source tools, operating modes, and LLM models. We use Google
Gemini as the base LLM model to build and test ORAssistant. Early evaluation
results of the RAG-based model show notable improvements in performance and
accuracy compared to non-fine-tuned LLMs.",Aviral Kaintura
2024-10-05T15:13:22Z,http://arxiv.org/abs/2410.04194v1,Consistent Autoformalization for Constructing Mathematical Libraries,"Autoformalization is the task of automatically translating mathematical
content written in natural language to a formal language expression. The
growing language interpretation capabilities of Large Language Models (LLMs),
including in formal languages, are lowering the barriers for autoformalization.
However, LLMs alone are not capable of consistently and reliably delivering
autoformalization, in particular as the complexity and specialization of the
target domain grows. As the field evolves into the direction of systematically
applying autoformalization towards large mathematical libraries, the need to
improve syntactic, terminological and semantic control increases. This paper
proposes the coordinated use of three mechanisms, most-similar retrieval
augmented generation (MS-RAG), denoising steps, and auto-correction with syntax
error feedback (Auto-SEF) to improve autoformalization quality. The empirical
analysis, across different models, demonstrates that these mechanisms can
deliver autoformalizaton results which are syntactically, terminologically and
semantically more consistent. These mechanisms can be applied across different
LLMs and have shown to deliver improve results across different model types.",Lan Zhang
2024-10-06T11:23:56Z,http://arxiv.org/abs/2410.04452v1,"MindScope: Exploring cognitive biases in large language models through
  Multi-Agent Systems","Detecting cognitive biases in large language models (LLMs) is a fascinating
task that aims to probe the existing cognitive biases within these models.
Current methods for detecting cognitive biases in language models generally
suffer from incomplete detection capabilities and a restricted range of
detectable bias types. To address this issue, we introduced the 'MindScope'
dataset, which distinctively integrates static and dynamic elements. The static
component comprises 5,170 open-ended questions spanning 72 cognitive bias
categories. The dynamic component leverages a rule-based, multi-agent
communication framework to facilitate the generation of multi-round dialogues.
This framework is flexible and readily adaptable for various psychological
experiments involving LLMs. In addition, we introduce a multi-agent detection
method applicable to a wide range of detection tasks, which integrates
Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement
learning-based decision module. Demonstrating substantial effectiveness, this
method has shown to improve detection accuracy by as much as 35.10% compared to
GPT-4. Codes and appendix are available at
https://github.com/2279072142/MindScope.",Zhentao Xie
2024-10-07T04:15:02Z,http://arxiv.org/abs/2410.04739v2,TableRAG: Million-Token Table Understanding with Language Models,"Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.",Si-An Chen
2024-10-07T05:27:22Z,http://arxiv.org/abs/2410.04759v1,"Driving with Regulation: Interpretable Decision-Making for Autonomous
  Vehicles with Retrieval-Augmented Reasoning via LLM","This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease.",Tianhui Cai
2024-10-07T13:03:45Z,http://arxiv.org/abs/2410.05004v1,Fast State Restoration in LLM Serving with HCache,"The growing complexity of LLM usage today, e.g., multi-round conversation and
retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)
reusable across user requests. Given the capacity constraints of GPU memory,
only a limited number of contexts can be cached on GPU for reusing. Existing
inference systems typically evict part of the KV cache and restore it by
recomputing it from the original tokens or offloading it to host storage for
later retrieval, both of which introduce substantial computational or I/O
overheads. We propose HCache, a novel LLM state restoration method. Its key
idea is to restore LLM states from intermediate activations and thus utilize
computational and I/O resources with low overhead. We enhance HCache with two
techniques, including i) a bubble-free restoration scheduler that integrates
resource-complementary methods to optimize the balance between computation and
IO tasks; and ii) a chunk-based storage manager to address the layout mismatch
issue (i.e., layer-before-token saving versus token-before-layer restoration).
Our evaluations, conducted using real-world tasks, show that HCache reduces the
TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less
storage space; compared to token recomputation, HCache achieves up to 5.73X
reduction in TTFT.",Shiwei Gao
2024-10-07T16:14:47Z,http://arxiv.org/abs/2410.05162v1,"Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models","Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.",Mehrdad Farahani
2024-10-08T11:33:09Z,http://arxiv.org/abs/2410.05930v1,"Fortify Your Foundations: Practical Privacy and Security for Foundation
  Model Deployments In The Cloud","Foundation Models (FMs) display exceptional performance in tasks such as
natural language processing and are being applied across a growing range of
disciplines. Although typically trained on large public datasets, FMs are often
fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,
which rely on private data. This access, along with their size and costly
training, heightens the risk of intellectual property theft. Moreover,
multimodal FMs may expose sensitive information. In this work, we examine the
FM threat model and discuss the practicality and comprehensiveness of various
approaches for securing against them, such as ML-based methods and trusted
execution environments (TEEs). We demonstrate that TEEs offer an effective
balance between strong security properties, usability, and performance.
Specifically, we present a solution achieving less than 10\% overhead versus
bare metal for the full Llama2 7B and 13B inference pipelines running inside
\intel\ SGX and \intel\ TDX. We also share our configuration files and insights
from our implementation. To our knowledge, our work is the first to show the
practicality of TEEs for securing FMs.",Marcin Chrapek
2024-10-08T15:22:36Z,http://arxiv.org/abs/2410.06121v1,"Less is More: Making Smaller Language Models Competent Subgraph
  Retrievers for Multi-hop KGQA","Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.",Wenyu Huang
2024-10-09T15:10:00Z,http://arxiv.org/abs/2410.06972v1,"Diamond of Thought: A Design Thinking-Based Framework for LLMs in
  Wearable Design","Wearable design is an interdisciplinary field that balances technological
innovation, human factors, and human-computer interactions. Despite
contributions from various disciplines, many projects lack stable
interdisciplinary teams, which often leads to design failures. Large language
models (LLMs) integrate diverse information and generate innovative solutions,
making them a valuable tool for enhancing design processes. Thus, we have
explored the use of LLMs in wearable design by combining design-thinking
principles with LLM capabilities. We have developed the ""Diamond of Thought""
framework and analysed 1,603 prototypes and 1,129 products from a body-centric
perspective to create a comprehensive database. We employed retrieval-augmented
generation to input database details into the LLMs, ensuring applicability to
wearable design challenges and integration of embodied cognition into the
process. Our LLM-based methodology for wearables has been experimentally
validated, demonstrating the potential of LLMs for the advancement of design
practices. This study offers new tools and methods for future wearable designs.",Qiyang Miao
2024-10-10T02:48:06Z,http://arxiv.org/abs/2410.07551v1,KRAG Framework for Enhancing LLMs in the Legal Domain,"This paper introduces Knowledge Representation Augmented Generation (KRAG), a
novel framework designed to enhance the capabilities of Large Language Models
(LLMs) within domain-specific applications. KRAG points to the strategic
inclusion of critical knowledge entities and relationships that are typically
absent in standard data sets and which LLMs do not inherently learn. In the
context of legal applications, we present Soft PROLEG, an implementation model
under KRAG, which uses inference graphs to aid LLMs in delivering structured
legal reasoning, argumentation, and explanations tailored to user inquiries.
The integration of KRAG, either as a standalone framework or in tandem with
retrieval augmented generation (RAG), markedly improves the ability of language
models to navigate and solve the intricate challenges posed by legal texts and
terminologies. This paper details KRAG's methodology, its implementation
through Soft PROLEG, and potential broader applications, underscoring its
significant role in advancing natural language understanding and processing in
specialized knowledge domains.",Nguyen Ha Thanh
2024-10-10T02:58:52Z,http://arxiv.org/abs/2410.07561v2,"AI-Press: A Multi-Agent News Generating and Feedback Simulation System
  Powered by Large Language Models","The rise of various social platforms has transformed journalism. The growing
demand for news content has led to the increased use of large language models
(LLMs) in news production due to their speed and cost-effectiveness. However,
LLMs still encounter limitations in professionalism and ethical judgment in
news generation. Additionally, predicting public feedback is usually difficult
before news is released. To tackle these challenges, we introduce AI-Press, an
automated news drafting and polishing system based on multi-agent collaboration
and Retrieval-Augmented Generation. We develop a feedback simulation system
that generates public feedback considering demographic distributions. Through
extensive quantitative and qualitative evaluations, our system shows
significant improvements in news-generating capabilities and verifies the
effectiveness of public feedback simulation.",Xiawei Liu
2024-10-11T11:41:02Z,http://arxiv.org/abs/2410.08731v1,"Developing a Pragmatic Benchmark for Assessing Korean Legal Language
  Understanding in Large Language Models","Large language models (LLMs) have demonstrated remarkable performance in the
legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However
their efficacy remains limited for non-standardized tasks and tasks in
languages other than English. This underscores the need for careful evaluation
of LLMs within each legal system before application. Here, we introduce KBL, a
benchmark for assessing the Korean legal language understanding of LLMs,
consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning
tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510
examples). First two datasets were developed in close collaboration with
lawyers to evaluate LLMs in practical scenarios in a certified manner.
Furthermore, considering legal practitioners' frequent use of extensive legal
documents for research, we assess LLMs in both a closed book setting, where
they rely solely on internal knowledge, and a retrieval-augmented generation
(RAG) setting, using a corpus of Korean statutes and precedents. The results
indicate substantial room and opportunities for improvement.",Yeeun Kim
2024-10-11T13:52:44Z,http://arxiv.org/abs/2410.08815v2,"StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via
  Inference-time Hybrid Information Structurization","Retrieval-augmented generation (RAG) is a key means to effectively enhance
large language models (LLMs) in many knowledge-based tasks. However, existing
RAG methods struggle with knowledge-intensive reasoning tasks, because useful
information required to these tasks are badly scattered. This characteristic
makes it difficult for existing RAG methods to accurately identify key
information and perform global reasoning with such noisy augmentation. In this
paper, motivated by the cognitive theories that humans convert raw information
into various structured knowledge when tackling knowledge-intensive reasoning,
we proposes a new framework, StructRAG, which can identify the optimal
structure type for the task at hand, reconstruct original documents into this
structured format, and infer answers based on the resulting structure.
Extensive experiments across various knowledge-intensive tasks show that
StructRAG achieves state-of-the-art performance, particularly excelling in
challenging scenarios, demonstrating its potential as an effective solution for
enhancing LLMs in complex real-world applications.",Zhuoqun Li
2024-10-11T17:32:59Z,http://arxiv.org/abs/2410.09019v1,"MedMobile: A mobile-sized language model with expert-level clinical
  capabilities","Language models (LMs) have demonstrated expert-level reasoning and recall
abilities in medicine. However, computational costs and privacy concerns are
mounting barriers to wide-scale implementation. We introduce a parsimonious
adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of
running on a mobile device, for medical applications. We demonstrate that
MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for
physicians (~60%), and approaching the scores of models 100 times its size. We
subsequently perform a careful set of ablations, and demonstrate that chain of
thought, ensembling, and fine-tuning lead to the greatest performance gains,
while unexpectedly retrieval augmented generation fails to demonstrate
significant improvements",Krithik Vishwanath
2024-10-11T17:10:10Z,http://arxiv.org/abs/2410.09136v1,"Underutilized land and sustainable development: effects on employment,
  economic output, and mitigation of CO2 emissions","Climate change, deforestation, and biodiversity loss are calling for
innovative approaches to effective reforestation and afforestation. This paper
explores the integration of artificial intelligence and remote sensing
technologies for optimizing tree planting strategies, estimating labor
requirements, and determining space needs for various tree species in Gabala
District of Azerbaijan. The study employs YOLOv8 for precise identification of
potential planting sites and a Retrieval-Augmented Generation approach,
combined with the Gemini API, to provide tailored species recommendations. The
methodology incorporates time-series modeling to forecast the impact of
reforestation on CO2 emissions reduction, utilizing Holt-Winters for
predictions. Our results indicate that the AI model can effectively identify
suitable locations and species, offering valuable insights into the potential
economic and environmental benefits of large-scale tree planting thus fostering
sustainable economic development and helping to mitigate the adverse effects of
global warming and climate change.",Seymur Garibov
2024-10-12T19:38:09Z,http://arxiv.org/abs/2410.09629v1,"Synthetic Knowledge Ingestion: Towards Knowledge Refinement and
  Injection for Enhancing Large Language Models","Large language models (LLMs) are proficient in capturing factual knowledge
across various domains. However, refining their capabilities on previously seen
knowledge or integrating new knowledge from external sources remains a
significant challenge. In this work, we propose a novel synthetic knowledge
ingestion method called Ski, which leverages fine-grained synthesis,
interleaved generation, and assemble augmentation strategies to construct
high-quality data representations from raw knowledge sources. We then integrate
Ski and its variations with three knowledge injection techniques: Retrieval
Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual
Pre-training (CPT) to inject and refine knowledge in language models. Extensive
empirical experiments are conducted on various question-answering tasks
spanning finance, biomedicine, and open-generation domains to demonstrate that
Ski significantly outperforms baseline methods by facilitating effective
knowledge injection. We believe that our work is an important step towards
enhancing the factual accuracy of LLM outputs by refining knowledge
representation and injection capabilities.",Jiaxin Zhang
2024-10-13T17:53:50Z,http://arxiv.org/abs/2410.09942v1,"Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization","This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and retrieval-augmentation strategy.
We introduce an iterative approach where the search engine generates retrieval
results for these RAG agents and gathers feedback on the quality of the
retrieved documents during an offline phase. This feedback is then used to
iteratively optimize the search engine using a novel expectation-maximization
algorithm, with the goal of maximizing each agent's utility function.
Additionally, we adapt this approach to an online setting, allowing the search
engine to refine its behavior based on real-time individual agents feedback to
better serve the results for each of them. Experiments on diverse datasets from
the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our
approach significantly on average outperforms competitive baselines across 18
RAG models. We also demonstrate that our method effectively ``personalizes''
the retrieval process for each RAG agent based on the collected feedback.
Finally, we provide a comprehensive ablation study to explore various aspects
of our method.",Alireza Salemi
2024-10-14T04:06:22Z,http://arxiv.org/abs/2410.10136v1,"Beyond-RAG: Question Identification and Answer Generation in Real-Time
  Conversations","In customer contact centers, human agents often struggle with long average
handling times (AHT) due to the need to manually interpret queries and retrieve
relevant knowledge base (KB) articles. While retrieval augmented generation
(RAG) systems using large language models (LLMs) have been widely adopted in
industry to assist with such tasks, RAG faces challenges in real-time
conversations, such as inaccurate query formulation and redundant retrieval of
frequently asked questions (FAQs). To address these limitations, we propose a
decision support system that can look beyond RAG by first identifying customer
questions in real time. If the query matches an FAQ, the system retrieves the
answer directly from the FAQ database; otherwise, it generates answers via RAG.
Our approach reduces reliance on manual queries, providing responses to agents
within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva
CQ, this system improves efficiency, reduces AHT, and lowers operational costs.
We also introduce an automated LLM-agentic workflow to identify FAQs from
historical transcripts when no predefined FAQs exist.",Garima Agrawal
2024-10-14T08:47:21Z,http://arxiv.org/abs/2410.10293v1,FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG,"Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the retrieval
paradigm that we design and use remains flat, which treats the retrieval
procedures as a one-off deal with constant granularity. Despite effectiveness,
we argue that they suffer from two limitations: (1) flat retrieval exerts a
significant burden on one retriever; (2) constant granularity limits the
ceiling of retrieval performance. In this work, we propose a progressive
retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,
so as to balance effectiveness and efficiency. Specifically, FunnelRAG
establishes a progressive retrieval pipeline by collaborating coarse-to-fine
granularity, large-to-small quantity, and low-to-high capacity, which can
relieve the burden on one retriever and also promote the ceiling of retrieval
performance. Extensive experiments manifest that FunnelRAG achieves comparable
retrieval performance while the time overhead is reduced by nearly 40 percent.",Xinping Zhao
2024-10-14T10:26:57Z,http://arxiv.org/abs/2410.10360v2,"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented
  Language Models with Parameter Decoupling and Tailored Tuning","Retrieval-Augmented Generation (RAG) offers an effective solution to the
issues faced by Large Language Models (LLMs) in hallucination generation and
knowledge obsolescence by incorporating externally retrieved knowledge.
However, existing methods lack effective control mechanisms for integrating
internal and external knowledge. Inspired by human cognitive processes, we
propose Parenting, a novel framework that decouples, identifies, and
purposefully optimizes parameter subspaces related to adherence and robustness.
Specifically, Parenting utilizes a key parameter mining method that combines
forward and backward propagation signals to localize subspaces representing
different capabilities. Then, Parenting employs a type-tailored tuning
strategy, applying specific and appropriate optimizations to different
subspaces, aiming to achieve a balanced enhancement of both adherence and
robustness. Extensive experiments on various datasets and models validate the
effectiveness and generalizability of our method.",Yongxin Xu
2024-10-14T12:45:10Z,http://arxiv.org/abs/2410.10450v1,KBLaM: Knowledge Base augmented Language Model,"In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a
new method for augmenting Large Language Models (LLMs) with external knowledge.
KBLaM works with a knowledge base (KB) constructed from a corpus of documents,
transforming each piece of knowledge in the KB into continuous key-value vector
pairs via pre-trained sentence encoders with linear adapters and integrating
them into pre-trained LLMs via a specialized rectangular attention mechanism.
Unlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval
modules, and unlike in-context learning, its computational overhead scales
linearly with KB size rather than quadratically. Our approach enables
integrating a large KB of more than 10K triples into an 8B pre-trained LLM of
only 8K context window on one single A100 80GB GPU and allows for dynamic
updates without model fine-tuning or retraining. Experiments demonstrate
KBLaM's effectiveness in various tasks, including question-answering and
open-ended reasoning, while providing interpretable insights into its use of
the augmented knowledge.",Xi Wang
2024-10-14T13:18:20Z,http://arxiv.org/abs/2410.10481v1,"Model-Based Differentially Private Knowledge Transfer for Large Language
  Models","As large language models (LLMs) become increasingly prevalent in web
services, effectively leveraging domain-specific knowledge while ensuring
privacy has become critical. Existing methods, such as retrieval-augmented
generation (RAG) and differentially private data synthesis, often compromise
either the utility of domain knowledge or the privacy of sensitive data,
limiting their applicability in specialized domains. To address these
challenges, we propose \textit{Llamdex}, a novel framework that integrates
privacy-preserving, domain-specific models into LLMs. Our approach
significantly enhances the accuracy of domain-specific tasks, achieving up to a
26\% improvement compared to existing methods under the same differential
privacy constraints. Experimental results show that Llamdex not only improves
the accuracy of LLM responses but also maintains comparable inference
efficiency to the original LLM, highlighting its potential for real-world
applications.",Zhaomin Wu
2024-10-14T14:56:01Z,http://arxiv.org/abs/2410.10584v1,"STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with
  FeedBack","Large Language Models (LLMs) often generate incorrect or outdated
information, especially in low-resource settings or when dealing with private
data. To address this, Retrieval-Augmented Generation (RAG) uses external
knowledge bases (KBs), but these can also suffer from inaccuracies. We
introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base
editing with FEEDback approach that iteratively refines the KB based on expert
feedback using a multi-actor, centralized critic reinforcement learning
framework. Each document is assigned to an actor, modeled as a ReACT agent,
which performs structured edits based on document-specific targeted
instructions from a centralized critic. Experimental results show that
STACKFEED significantly improves KB quality and RAG system performance,
enhancing accuracy by up to 8% over baselines.",Naman Gupta
2024-10-14T04:57:32Z,http://arxiv.org/abs/2410.10913v2,"Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined
  Knowledge Base","Recent advances in audio understanding tasks leverage the reasoning
capabilities of LLMs. However, adapting LLMs to learn audio concepts requires
massive training data and substantial computational resources. To address these
challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs
from a knowledge base (KB) and augments them with query audio to generate
accurate textual responses. In RAG, the relevance of the retrieved information
plays a crucial role in effectively processing the input. In this paper, we
analyze how different retrieval methods and knowledge bases impact the
relevance of audio-text pairs and the performance of audio captioning with RAG.
We propose generative pair-to-pair retrieval, which uses the generated caption
as a text query to accurately find relevant audio-text pairs to the query
audio, thereby improving the relevance and accuracy of retrieved information.
Additionally, we refine the large-scale knowledge base to retain only
audio-text pairs that align with the contextualized intents. Our approach
achieves state-of-the-art results on benchmarks including AudioCaps, Clotho,
and Auto-ACD, with detailed ablation studies validating the effectiveness of
our retrieval and KB construction methods.",Choi Changin
2024-10-14T23:38:51Z,http://arxiv.org/abs/2410.11141v1,Can Structured Data Reduce Epistemic Uncertainty?,"In this work, we present a framework that utilizes ontology alignment to
improve the learning process of deep learning models. With this approach we
show that models fine-tuned using ontologies learn a downstream task at a
higher rate with better performance on a sequential classification task
compared to the native version of the model. Additionally, we extend our work
to showcase how subsumption mappings retrieved during the process of ontology
alignment can help enhance Retrieval-Augmented Generation in Large Language
Models. The results show that the responses obtained by using subsumption
mappings show an increase of 8.97% in contextual similarity and a 1% increase
in factual accuracy. We also use these scores to define our Hallucination Index
and show that this approach reduces hallucination in LLMs by 4.847%.",Shriram M S
2024-10-15T02:18:01Z,http://arxiv.org/abs/2410.11195v1,"Athena: Retrieval-augmented Legal Judgment Prediction with Large
  Language Models","Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have
prevailed in countless domains, including legal scenarios. With LLMs' rapid
technological progress, the development of prompt engineering (PE) as an
interface between the LLMs and real-world applications has drawn the attention
of all developers. Various PE methods have been proposed to overcome real-world
challenges, such as few-shot prompting, chain-of-thought, and
retrieval-augmented generation (RAG). However, RAG for legal judgment
prediction (LJP) is still underexplored. To address this, we propose ""Athena"",
a novel framework cultivating RAG as a core preprocess component to enhance
LLMs' performance on specialized tasks. Athena constructs a knowledge base for
accusations, attached with a semantic retrieval mechanism through
vectorization. Our experiments show that Athena's overall performance has
improved significantly, achieving state-of-the-art results on the CAIL2018
dataset. Our ablation study on the in-context window size parameter further
reproduces LLMs' ""lost-in-the-middle"" phenomenon with a relative positional
variation. And with moderate hyper-parameter-tuning, we can achieve at most 95%
of accuracy accordingly. We also study the impact of query rewriting and data
distribution, providing possible directions for future research based on former
analyses.",Xiao Peng
2024-10-15T03:04:26Z,http://arxiv.org/abs/2410.11217v1,On the Capacity of Citation Generation by Large Language Models,"Retrieval-augmented generation (RAG) appears as a promising method to
alleviate the ""hallucination"" problem in large language models (LLMs), since it
can incorporate external traceable resources for response generation. The
essence of RAG in combating the hallucination issue lies in accurately
attributing claims in responses to the corresponding retrieved documents.
However, most of existing works focus on improving the quality of generated
responses from the LLM, while largely overlooked its ability to attribute
sources accurately. In this study, we conduct a systematic analysis about the
capabilities of LLMs in generating citations within response generation, and
further introduce a novel method to enhance their citation generation
abilities. Specifically, we evaluate both the correctness and citation quality
for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce
new citation evaluation metrics to eliminate the over-penalization of
unnecessary and excessive citations in existing metrics. Furthermore, we
propose a Generate-then-Refine method that completes relevant citations and
removes irrelevant ones without altering the response text. The results on
WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves
the quality of citations in responses generated by LLMs.",Haosheng Qian
2024-10-15T08:39:12Z,http://arxiv.org/abs/2410.11395v1,"Synthetic Interlocutors. Experiments with Generative AI to Prolong
  Ethnographic Encounters","This paper introduces ""Synthetic Interlocutors"" for ethnographic research.
Synthetic Interlocutors are chatbots ingested with ethnographic textual
material (interviews and observations) by using Retrieval Augmented Generation
(RAG). We integrated an open-source large language model with ethnographic data
from three projects to explore two questions: Can RAG digest ethnographic
material and act as ethnographic interlocutor? And, if so, can Synthetic
Interlocutors prolong encounters with the field and extend our analysis?
Through reflections on the process of building our Synthetic Interlocutors and
an experimental collaborative workshop, we suggest that RAG can digest
ethnographic materials, and it might lead to prolonged, yet uneasy ethnographic
encounters that allowed us to partially recreate and re-visit fieldwork
interactions while facilitating opportunities for novel analytic insights.
Synthetic Interlocutors can produce collaborative, ambiguous and serendipitous
moments.",Johan Irving Søltoft
2024-10-15T09:50:19Z,http://arxiv.org/abs/2410.11446v1,"AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a
  simple RAG task","This paper describes our $3^{rd}$ place submission in the AVeriTeC shared
task in which we attempted to address the challenge of fact-checking with
evidence retrieved in the wild using a simple scheme of Retrieval-Augmented
Generation (RAG) designed for the task, leveraging the predictive power of
Large Language Models. We release our codebase and explain its two modules -
the Retriever and the Evidence & Label generator - in detail, justifying their
features such as MMR-reranking and Likert-scale confidence estimation. We
evaluate our solution on AVeriTeC dev and test set and interpret the results,
picking the GPT-4o as the most appropriate model for our pipeline at the time
of our publication, with Llama 3.1 70B being a promising open-source
alternative. We perform an empirical error analysis to see that faults in our
predictions often coincide with noise in the data or ambiguous fact-checks,
provoking further research and data augmentation.",Herbert Ullrich
2024-10-15T10:57:12Z,http://arxiv.org/abs/2410.11494v1,DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG,"In the rapidly evolving landscape of language, resolving new linguistic
expressions in continuously updating knowledge bases remains a formidable
challenge. This challenge becomes critical in retrieval-augmented generation
(RAG) with knowledge bases, as emerging expressions hinder the retrieval of
relevant documents, leading to generator hallucinations. To address this issue,
we introduce a novel task aimed at resolving emerging mentions to dynamic
entities and present DynamicER benchmark. Our benchmark includes dynamic entity
mention resolution and entity-centric knowledge-intensive QA task, evaluating
entity linking and RAG model's adaptability to new expressions, respectively.
We discovered that current entity linking models struggle to link these new
expressions to entities. Therefore, we propose a temporal segmented clustering
method with continual adaptation, effectively managing the temporal dynamics of
evolving entities and emerging mentions. Extensive experiments demonstrate that
our method outperforms existing baselines, enhancing RAG model performance on
QA task with resolved mentions.",Jinyoung Kim
2024-10-15T11:20:42Z,http://arxiv.org/abs/2410.11507v2,"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic
  Evaluation Framework for LLMs","While various vertical domain large language models (LLMs) have been
developed, the challenge of automatically evaluating their performance across
different domains remains significant. Current benchmark-based evaluation
methods exhibit rigid, aimless interactions and rely on pre-collected static
datasets that are costly to build, inflexible across domains, and misaligned
with practical user needs. To address this issue, we revisit the evaluation
components and introduce two concepts: Benchmark+, which extends traditional
question-answer benchmark into a more flexible ""strategy-criterion"" format; and
Assessment+, which enhances the interaction process, enabling deeper
exploration and supporting both quantitative metrics and qualitative insights.
These concepts capture the nuanced behaviors of LLMs through richer, multi-turn
interactions. We propose an agent-based evaluation framework called TestAgent,
which implements these concepts through retrieval augmented generation and
reinforcement learning. Experiments on tasks ranging from constructing vertical
domain evaluation to activating existing benchmarks demonstrate the
effectiveness of TestAgent across various scenarios. We believe this work
offers an interesting perspective on automatic evaluation for LLMs.",Wanying Wang
2024-10-06T17:11:29Z,http://arxiv.org/abs/2410.11859v1,"SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health
  Support and Assessment, Based on a Systematic Application Survey","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide accessible, stigma-free, personalized, and real-time
mental health support through cutting-edge AI technologies. It makes the
following contributions: (1) Conducting an extensive survey of recent mental
health support methods to identify prevalent functionalities and unmet needs.
(2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM
technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering,
and domain knowledge. This system offers advanced features such as Suicide Risk
Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized
profile uploads and Conversational Information Extraction. (3) Developing novel
evaluation approaches to assess preliminary assessments and suicide risk
detection, utilizing annotated real-life interview data and professionally
labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator
Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance
model performance and usability through context-sensitive response adjustments
and semantic coherence evaluations. This study contributes to advancing mental
health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-15T21:10:01Z,http://arxiv.org/abs/2410.12069v1,De-jargonizing Science for Journalists with GPT-4: A Pilot Study,"This study offers an initial evaluation of a human-in-the-loop system
leveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented
Generation (RAG) to identify and define jargon terms in scientific abstracts,
based on readers' self-reported knowledge. The system achieves fairly high
recall in identifying jargon and preserves relative differences in readers'
jargon identification, suggesting personalization as a feasible use-case for
LLMs to support sense-making of complex information. Surprisingly, using only
abstracts for context to generate definitions yields slightly more accurate and
higher quality definitions than using RAG-based context from the fulltext of an
article. The findings highlight the potential of generative AI for assisting
science reporters, and can inform future work on developing tools to simplify
dense documents.",Sachita Nishal
2024-10-16T11:43:17Z,http://arxiv.org/abs/2410.12475v2,"Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional
  Safety Engineering","Functional safety is a critical aspect of automotive engineering,
encompassing all phases of a vehicle's lifecycle, including design,
development, production, operation, and decommissioning. This domain involves
highly knowledge-intensive tasks. This paper introduces Aegis: An Advanced
LLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is
specifically designed to support complex functional safety tasks within the
automotive sector. It is tailored to perform Hazard Analysis and Risk
Assessment(HARA), document Functional Safety Requirements(FSR), and plan test
cases for Automatic Emergency Braking(AEB) systems. The most advanced version,
Aegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective
mechanisms to enhance its capability in managing complex, knowledge-intensive
tasks. Additionally, targeted prompt refinement by professional functional
safety practitioners can significantly optimize Aegis's performance in the
functional safety domain. This paper demonstrates the potential of Aegis to
improve the efficiency and effectiveness of functional safety processes in
automotive engineering.",Lu Shi
2024-10-16T13:10:27Z,http://arxiv.org/abs/2410.12532v2,"MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration","Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.",Jinjie Wei
2024-10-11T19:49:05Z,http://arxiv.org/abs/2410.12859v1,"Enhancing Long Context Performance in LLMs Through Inner Loop Query
  Mechanism","Transformers have a quadratic scaling of computational complexity with input
size, which limits the input context window size of large language models
(LLMs) in both training and inference. Meanwhile, retrieval-augmented
generation (RAG) besed models can better handle longer contexts by using a
retrieval system to filter out unnecessary information. However, most RAG
methods only perform retrieval based on the initial query, which may not work
well with complex questions that require deeper reasoning. We introduce a novel
approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving
inner-loop queries, based not only on the query question itself but also on
intermediate findings. At inference time, our model retrieves information from
the RAG system, integrating data from lengthy documents at various levels of
abstraction. Based on the information retrieved, the LLM generates texts stored
in an area named Short-Term Memory (STM) which is then used to formulate the
next query. This retrieval process is repeated until the text in STM converged.
Our experiments demonstrate that retrieval with STM offers improvements over
traditional retrieval-augmented LLMs, particularly in long context tests such
as Multi-Needle In A Haystack (M-NIAH) and BABILong.",Yimin Tang
2024-10-16T08:43:39Z,http://arxiv.org/abs/2410.12890v1,"REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via
  Model Fusion of Embedding Models","Retrieval augmented generation (RAG) pipelines are commonly used in tasks
such as question-answering (QA), relying on retrieving relevant documents from
a vector store computed using a pretrained embedding model. However, if the
retrieved context is inaccurate, the answers generated using the large language
model (LLM) may contain errors or hallucinations. Although pretrained embedding
models have advanced, adapting them to new domains remains challenging.
Fine-tuning is a potential solution, but industry settings often lack the
necessary fine-tuning data. To address these challenges, we propose REFINE, a
novel technique that generates synthetic data from available documents and then
uses a model fusion approach to fine-tune embeddings for improved retrieval
performance in new domains, while preserving out-of-domain capability. We
conducted experiments on the two public datasets: SQUAD and RAG-12000 and a
proprietary TOURISM dataset. Results demonstrate that even the standard
fine-tuning with the proposed data augmentation technique outperforms the
vanilla pretrained model. Furthermore, when combined with model fusion, the
proposed approach achieves superior performance, with a 5.76% improvement in
recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and
RAG-12000 respectively.",Ambuje Gupta
2024-10-17T04:30:46Z,http://arxiv.org/abs/2410.13210v1,"FaithBench: A Diverse Hallucination Benchmark for Summarization by
  Modern LLMs","Summarization is one of the most common tasks performed by large language
models (LLMs), especially in applications like Retrieval-Augmented Generation
(RAG). However, existing evaluations of hallucinations in LLM-generated
summaries, and evaluations of hallucination detection models both suffer from a
lack of diversity and recency in the LLM and LLM families considered. This
paper introduces FaithBench, a summarization hallucination benchmark comprising
challenging hallucinations made by 10 modern LLMs from 8 different families,
with ground truth annotations by human experts. ``Challenging'' here means
summaries on which popular, state-of-the-art hallucination detection models,
including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and
GPT-3.5-Turbo produce the least hallucinations. However, even the best
hallucination detection models have near 50\% accuracies on FaithBench,
indicating lots of room for future improvement. The repo is
https://github.com/vectara/FaithBench",Forrest Sheng Bao
2024-10-17T08:48:54Z,http://arxiv.org/abs/2410.13339v1,"Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval","Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.",Ingeol Baek
2024-10-17T13:51:03Z,http://arxiv.org/abs/2410.13553v1,"Integrating Temporal Representations for Dynamic Memory Retrieval and
  Management in Large Language Models","Conventional dialogue agents often struggle with effective memory recall,
leading to redundant retrieval and inadequate management of unique user
associations. To address this, we propose SynapticRAG, a novel approach
integrating synaptic dynamics into Retrieval-Augmented Generation (RAG).
SynapticRAG integrates temporal representations into memory vectors, mimicking
biological synapses by differentiating events based on occurrence times and
dynamically updating memory significance. This model employs temporal scoring
for memory connections and a synaptic-inspired propagation control mechanism.
Experiments across English, Japanese, and Chinese datasets demonstrate
SynapticRAG's superiority over existing methods, including traditional RAG,
with up to 14.66\% improvement in memory retrieval accuracy. Our approach
advances context-aware dialogue AI systems by enhancing long-term context
maintenance and specific information extraction from conversations.",Yuki Hou
2024-10-17T15:29:57Z,http://arxiv.org/abs/2410.13671v1,"HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World
  Multilingual Settings","Assessing the capabilities and limitations of large language models (LLMs)
has garnered significant interest, yet the evaluation of multiple models in
real-world scenarios remains rare. Multilingual evaluation often relies on
translated benchmarks, which typically do not capture linguistic and cultural
nuances present in the source language. This study provides an extensive
assessment of 24 LLMs on real world data collected from Indian patients
interacting with a medical chatbot in Indian English and 4 other Indic
languages. We employ a uniform Retrieval Augmented Generation framework to
generate responses, which are evaluated using both automated techniques and
human evaluators on four specific metrics relevant to our application. We find
that models vary significantly in their performance and that instruction tuned
Indic models do not always perform well on Indic language queries. Further, we
empirically show that factual correctness is generally lower for responses to
Indic queries compared to English queries. Finally, our qualitative work shows
that code-mixed and culturally relevant queries in our dataset pose challenges
to evaluated models.",Varun Gumma
2024-10-17T21:56:22Z,http://arxiv.org/abs/2410.14057v1,"Towards Cross-Cultural Machine Translation with Retrieval-Augmented
  Generation from Multilingual Knowledge Graphs","Translating text that contains entity names is a challenging task, as
cultural-related references can vary significantly across languages. These
variations may also be caused by transcreation, an adaptation process that
entails more than transliteration and word-for-word translation. In this paper,
we address the problem of cross-cultural translation on two fronts: (i) we
introduce XC-Translate, the first large-scale, manually-created benchmark for
machine translation that focuses on text that contains potentially
culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end
method to integrate information from a multilingual knowledge graph into a
neural machine translation model by leveraging a dense retrieval mechanism. Our
experiments and analyses show that current machine translation systems and
large language models still struggle to translate texts containing entity
names, whereas KG-MT outperforms state-of-the-art approaches by a large margin,
obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4,
respectively.",Simone Conia
2024-10-18T16:11:29Z,http://arxiv.org/abs/2410.14567v2,ScopeQA: A Framework for Generating Out-of-Scope Questions for RAG,"Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{DBLP:conf/emnlp/MinMHZ20}. RAG agents need high-quality data to
improve their responses to confusing questions. This paper presents a novel
guided hallucination-based method to efficiently generate a diverse set of
borderline out-of-scope confusing questions for a given document corpus. We
conduct an empirical comparative evaluation of several large language models as
RAG agents to measure the accuracy of confusion detection and appropriate
response generation. We contribute a benchmark dataset to the public domain.",Zhiyuan Peng
2024-10-19T01:29:12Z,http://arxiv.org/abs/2410.14926v1,"Aligning LLMs with Human Instructions and Stock Market Feedback in
  Financial Sentiment Analysis","Financial sentiment analysis is crucial for trading and investment
decision-making. This study introduces an adaptive retrieval augmented
framework for Large Language Models (LLMs) that aligns with human instructions
through Instruction Tuning and incorporates market feedback to dynamically
adjust weights across various knowledge sources within the Retrieval-Augmented
Generation (RAG) module. Building upon foundational models like LLaMA 2, we
fine-tune a series of LLMs ranging from 7B to 70B in size, enriched with
Instruction Tuning and RAG, and further optimized through direct feedback and
Reinforcement Learning (RL)-based refinement methods applied to the source
weights of RAG.Through extensive evaluation, we demonstrate that the sentiment
outputs from our LLMs more accurately mirror the intrinsic sentiment of textual
data, showcasing a 1% to 6% boost in accuracy and F1 score over existing
state-of-the-art models and leading conversational AI systems. Moreover, the
sentiments extracted are more indicative of the directions in stock price
movements. On top of that, we successfully construct portfolios that yield a
3.61% higher Sharpe ratio compared to the S&P 500 baseline in bullish markets.
These portfolios also demonstrate resilience in bearish markets, with a 5x
reduction in return losses compared to those typically experienced by the S&P
500.",Zijie Zhao
2024-10-19T01:35:26Z,http://arxiv.org/abs/2410.14931v1,"""Ghost of the past"": identifying and resolving privacy leakage from
  LLM's memory through proactive user interaction","Memories, encompassing past inputs in context window and retrieval-augmented
generation (RAG), frequently surface during human-LLM interactions, yet users
are often unaware of their presence and the associated privacy risks. To
address this, we propose MemoAnalyzer, a system for identifying, visualizing,
and managing private information within memories. A semi-structured interview
(N=40) revealed that low privacy awareness was the primary challenge, while
proactive privacy control emerged as the most common user need. MemoAnalyzer
uses a prompt-based method to infer and identify sensitive information from
aggregated past inputs, allowing users to easily modify sensitive content.
Background color temperature and transparency are mapped to inference
confidence and sensitivity, streamlining privacy adjustments. A 5-day
evaluation (N=36) comparing MemoAnalyzer with the default GPT setting and a
manual modification baseline showed MemoAnalyzer significantly improved privacy
awareness and protection without compromising interaction speed. Our study
contributes to privacy-conscious LLM design, offering insights into privacy
protection for Human-AI interactions.",Shuning Zhang
2024-10-20T04:48:12Z,http://arxiv.org/abs/2410.15284v1,Customized FinGPT Search Agents Using Foundation Models,"Current large language models (LLMs) have proven useful for analyzing
financial data, but most existing models, such as BloombergGPT and FinGPT, lack
customization for specific user needs. In this paper, we address this gap by
developing FinGPT Search Agents tailored for two types of users: individuals
and institutions. For individuals, we leverage Retrieval-Augmented Generation
(RAG) to integrate local documents and user-specified data sources. For
institutions, we employ dynamic vector databases and fine-tune models on
proprietary data. There are several key issues to address, including data
privacy, the time-sensitive nature of financial information, and the need for
fast responses. Experiments show that FinGPT agents outperform existing models
in accuracy, relevance, and response time, making them practical for real-world
applications.",Felix Tian
2024-10-20T04:51:24Z,http://arxiv.org/abs/2410.15285v1,"Contextual Augmented Multi-Model Programming (CAMP): A Hybrid
  Local-Cloud Copilot Framework","The advancements in cloud-based Large Languages Models (LLMs) have
revolutionized AI-assisted programming. However, their integration into certain
local development environments like ones within the Apple software ecosystem
(e.g., iOS apps, macOS) remains challenging due to computational demands and
sandboxed constraints. This paper presents CAMP, a multi-model AI-assisted
programming framework that consists of a local model that employs
Retrieval-Augmented Generation (RAG) to retrieve contextual information from
the codebase to facilitate context-aware prompt construction thus optimizing
the performance of the cloud model, empowering LLMs' capabilities in local
Integrated Development Environments (IDEs). The methodology is actualized in
Copilot for Xcode, an AI-assisted programming tool crafted for Xcode that
employs the RAG module to address software constraints and enables diverse
generative programming tasks, including automatic code completion,
documentation, error detection, and intelligent user-agent interaction. The
results from objective experiments on generated code quality and subjective
experiments on user adoption collectively demonstrate the pilot success of the
proposed system and mark its significant contributions to the realm of
AI-assisted programming.",Yuchen Wang
2024-10-20T08:42:29Z,http://arxiv.org/abs/2410.15332v1,"EPIC: Efficient Position-Independent Context Caching for Serving Large
  Language Models","Large Language Models (LLMs) are critical for a wide range of applications,
but serving them efficiently becomes increasingly challenging as inputs become
more complex. Context caching improves serving performance by exploiting
inter-request dependency and reusing key-value (KV) cache across requests, thus
improving time-to-first-token (TTFT). However, existing prefix-based context
caching requires exact token prefix matches, limiting cache reuse in few-shot
learning, multi-document QA, or retrieval-augmented generation, where prefixes
may vary. In this paper, we present EPIC, an LLM serving system that introduces
position-independent context caching (PIC), enabling modular KV cache reuse
regardless of token chunk position (or prefix). EPIC features two key designs:
AttnLink, which leverages static attention sparsity to minimize recomputation
for accuracy recovery, and KVSplit, a customizable chunking method that
preserves semantic coherence. Our experiments demonstrate that Epic delivers up
to 8x improvements in TTFT and 7x throughput over existing systems, with
negligible or no accuracy loss. By addressing the limitations of traditional
caching approaches, Epic enables more scalable and efficient LLM inference.",Junhao Hu
2024-10-20T16:08:54Z,http://arxiv.org/abs/2410.15438v1,"Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based
  LLMs","Retrieval-Augmented Generation (RAG) significantly improved the ability of
Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing
research seeks to enhance RAG performance by retrieving higher-quality
documents or designing RAG-specific LLMs, the internal mechanisms within LLMs
that contribute to the effectiveness of RAG systems remain underexplored. In
this paper, we aim to investigate these internal mechanisms within the popular
Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by
examining expert activations in these LLMs. Our controlled experiments reveal
that several core groups of experts are primarily responsible for RAG-related
behaviors. The activation of these core experts can signify the model's
inclination towards external/internal knowledge and adjust its behavior. For
instance, we identify core experts that can (1) indicate the sufficiency of the
model's internal knowledge, (2) assess the quality of retrieved documents, and
(3) enhance the model's ability to utilize context. Based on these findings, we
propose several strategies to enhance RAG's efficiency and effectiveness
through expert activation. Experimental results across various datasets and
MoE-based LLMs show the effectiveness of our method.",Xin Zhou
2024-10-20T21:17:05Z,http://arxiv.org/abs/2410.15511v1,"ConTReGen: Context-driven Tree-structured Retrieval for Open-domain
  Long-form Text Generation","Open-domain long-form text generation requires generating coherent,
comprehensive responses that address complex queries with both breadth and
depth. This task is challenging due to the need to accurately capture diverse
facets of input queries. Existing iterative retrieval-augmented generation
(RAG) approaches often struggle to delve deeply into each facet of complex
queries and integrate knowledge from various sources effectively. This paper
introduces ConTReGen, a novel framework that employs a context-driven,
tree-structured retrieval approach to enhance the depth and relevance of
retrieved content. ConTReGen integrates a hierarchical, top-down in-depth
exploration of query facets with a systematic bottom-up synthesis, ensuring
comprehensive coverage and coherent integration of multifaceted information.
Extensive experiments on multiple datasets, including LFQA and ODSUM, alongside
a newly introduced dataset, ODSUM-WikiHow, demonstrate that ConTReGen
outperforms existing state-of-the-art RAG models.",Kashob Kumar Roy
2024-10-20T22:59:34Z,http://arxiv.org/abs/2410.15531v1,"Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses
  with Sub-Question Coverage","Evaluating retrieval-augmented generation (RAG) systems remains challenging,
particularly for open-ended questions that lack definitive answers and require
coverage of multiple sub-topics. In this paper, we introduce a novel evaluation
framework based on sub-question coverage, which measures how well a RAG system
addresses different facets of a question. We propose decomposing questions into
sub-questions and classifying them into three types -- core, background, and
follow-up -- to reflect their roles and importance. Using this categorization,
we introduce a fine-grained evaluation protocol that provides insights into the
retrieval and generation characteristics of RAG systems, including three
commercial generative answer engines: You.com, Perplexity AI, and Bing Chat.
Interestingly, we find that while all answer engines cover core sub-questions
more often than background or follow-up ones, they still miss around 50% of
core sub-questions, revealing clear opportunities for improvement. Further,
sub-question coverage metrics prove effective for ranking responses, achieving
82% accuracy compared to human preference annotations. Lastly, we also
demonstrate that leveraging core sub-questions enhances both retrieval and
answer generation in a RAG system, resulting in a 74% win rate over the
baseline that lacks sub-questions.",Kaige Xie
2024-10-21T06:11:38Z,http://arxiv.org/abs/2410.15667v1,RAC: Efficient LLM Factuality Correction with Retrieval Augmentation,"Large Language Models (LLMs) exhibit impressive results across a wide range
of natural language processing (NLP) tasks, yet they can often produce
factually incorrect outputs. This paper introduces a simple but effective
low-latency post-correction method, \textbf{Retrieval Augmented Correction
(RAC)}, aimed at enhancing the factual performance of LLMs without requiring
additional fine-tuning. Our method is general and can be used with any
instruction-tuned LLM, and has greatly reduced latency compared to prior
approaches. RAC decomposes the LLM's output into atomic facts and applies a
fine-grained verification and correction process with retrieved content to
verify and correct the LLM-generated output. Our extensive experiments show
that RAC yields up to 30\% improvements over state-of-the-art baselines across
two popular factuality evaluation datasets, validating its efficacy and
robustness in both with and without the integration of Retrieval-Augmented
Generation (RAG) across different LLMs.\footnote{Our code is at
\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}",Changmao Li
2024-10-21T07:56:45Z,http://arxiv.org/abs/2410.15737v1,Who's Who: Large Language Models Meet Knowledge Conflicts in Practice,"Retrieval-augmented generation (RAG) methods are viable solutions for
addressing the static memory limits of pre-trained language models.
Nevertheless, encountering conflicting sources of information within the
retrieval context is an inevitable practical challenge. In such situations, the
language models are recommended to transparently inform users about the
conflicts rather than autonomously deciding what to present based on their
inherent biases. To analyze how current large language models (LLMs) align with
our recommendation, we introduce WhoQA, a public benchmark dataset to examine
model's behavior in knowledge conflict situations. We induce conflicts by
asking about a common property among entities having the same name, resulting
in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K
questions across 13 Wikidata property types and 150K Wikipedia entities. Our
experiments show that despite the simplicity of WhoQA questions, knowledge
conflicts significantly degrades LLMs' performance in RAG settings.",Quang Hieu Pham
2024-10-21T09:18:30Z,http://arxiv.org/abs/2410.15801v1,Improve Dense Passage Retrieval with Entailment Tuning,"Retrieval module can be plugged into many downstream NLP tasks to improve
their performance, such as open-domain question answering and
retrieval-augmented generation. The key to a retrieval system is to calculate
relevance scores to query and passage pairs. However, the definition of
relevance is often ambiguous. We observed that a major class of relevance
aligns with the concept of entailment in NLI tasks. Based on this observation,
we designed a method called entailment tuning to improve the embedding of dense
retrievers. Specifically, we unify the form of retrieval data and NLI data
using existence claim as a bridge. Then, we train retrievers to predict the
claims entailed in a passage with a variant task of masked prediction. Our
method can be efficiently plugged into current dense retrieval methods, and
experiments show the effectiveness of our method.",Lu Dai
2024-10-21T11:02:18Z,http://arxiv.org/abs/2410.15884v1,"Using GPT Models for Qualitative and Quantitative News Analytics in the
  2024 US Presidental Election Process","The paper considers an approach of using Google Search API and GPT-4o model
for qualitative and quantitative analyses of news through retrieval-augmented
generation (RAG). This approach was applied to analyze news about the 2024 US
presidential election process. Different news sources for different time
periods have been analyzed. Quantitative scores generated by GPT model have
been analyzed using Bayesian regression to derive trend lines. The
distributions found for the regression parameters allow for the analysis of
uncertainty in the election process. The obtained results demonstrate that
using the GPT models for news analysis, one can get informative analytics and
provide key insights that can be applied in further analyses of election
processes.",Bohdan M. Pavlyshenko
2024-10-05T14:37:35Z,http://arxiv.org/abs/2410.16285v1,"Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for
  Your Job?","The current paper presents the development and validation of SelfScore, a
novel benchmark designed to assess the performance of automated Large Language
Model (LLM) agents on help desk and professional consultation tasks. Given the
increasing integration of AI in industries, particularly within customer
service, SelfScore fills a crucial gap by enabling the comparison of automated
agents and human workers. The benchmark evaluates agents on problem complexity
and response helpfulness, ensuring transparency and simplicity in its scoring
system. The study also develops automated LLM agents to assess SelfScore and
explores the benefits of Retrieval-Augmented Generation (RAG) for
domain-specific tasks, demonstrating that automated LLM agents incorporating
RAG outperform those without. All automated LLM agents were observed to perform
better than the human control group. Given these results, the study raises
concerns about the potential displacement of human workers, especially in areas
where AI technologies excel. Ultimately, SelfScore provides a foundational tool
for understanding the impact of AI in help desk environments while advocating
for ethical considerations in the ongoing transition towards automation.",John Mavi
2024-10-22T09:25:21Z,http://arxiv.org/abs/2410.16843v1,"Trustworthy Alignment of Retrieval-Augmented Large Language Models via
  Reinforcement Learning","Trustworthiness is an essential prerequisite for the real-world application
of large language models. In this paper, we focus on the trustworthiness of
language models with respect to retrieval augmentation. Despite being supported
with external evidence, retrieval-augmented generation still suffers from
hallucinations, one primary cause of which is the conflict between contextual
and parametric knowledge. We deem that retrieval-augmented language models have
the inherent capabilities of supplying response according to both contextual
and parametric knowledge. Inspired by aligning language models with human
preference, we take the first step towards aligning retrieval-augmented
language models to a status where it responds relying merely on the external
evidence and disregards the interference of parametric knowledge. Specifically,
we propose a reinforcement learning based algorithm Trustworthy-Alignment,
theoretically and experimentally demonstrating large language models'
capability of reaching a trustworthy status without explicit supervision on how
to respond. Our work highlights the potential of large language models on
exploring its intrinsic abilities by its own and expands the application
scenarios of alignment from fulfilling human preference to creating trustworthy
agents.",Zongmeng Zhang
2024-10-08T16:26:18Z,http://arxiv.org/abs/2410.18104v1,"ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless
  Environment Perception","Large language models (LLMs) hold significant promise in advancing network
management and orchestration in 6G and beyond networks. However, existing LLMs
are limited in domain-specific knowledge and their ability to handle
multi-modal sensory data, which is critical for real-time situational awareness
in dynamic wireless environments. This paper addresses this gap by introducing
ENWAR, an ENvironment-aWARe retrieval augmented generation-empowered
multi-modal LLM framework. ENWAR seamlessly integrates multi-modal sensory
inputs to perceive, interpret, and cognitively process complex wireless
environments to provide human-interpretable situational awareness. ENWAR is
evaluated on the GPS, LiDAR, and camera modality combinations of DeepSense6G
dataset with state-of-the-art LLMs such as Mistral-7b/8x7b and
LLaMa3.1-8/70/405b. Compared to general and often superficial environmental
descriptions of these vanilla LLMs, ENWAR delivers richer spatial analysis,
accurately identifies positions, analyzes obstacles, and assesses line-of-sight
between vehicles. Results show that ENWAR achieves key performance indicators
of up to 70% relevancy, 55% context recall, 80% correctness, and 86%
faithfulness, demonstrating its efficacy in multi-modal perception and
interpretation.",Ahmad M. Nazar
2024-10-08T17:36:48Z,http://arxiv.org/abs/2410.18105v1,"Improving Embedding Accuracy for Document Retrieval Using Entity
  Relationship Maps and Model-Aware Contrastive Sampling","In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic
eXtraction), a 7-billion parameter decoder-only text Feature Extraction Model,
specifically designed for Document Retrieval-Augmented Generation (RAG) tasks.
Our approach employs two training techniques that yield an emergent improvement
in factual focus: (1) Pre-convergence interrupted fine-tuning using Structured
Entity Relationship Maps as training data input: designed to shift the model's
attention and create a bias towards factual content rather than semantic style
- this enhances plain text performance despite not being directly trained for
it; and (2) Model-Aware Contrastive Sampling, creating a balanced and evenly
distributed collation map of hard and soft negatives directly informed by the
base model's competency. This combined methodology yields significant
improvements, enhancing plain text query/document pair retrieval to achieve an
absolute rank@1 accuracy of 90.86% (an increase of 6.26% compared to the next
leading model) in our evaluation, and reducing training data input context size
by an average of 37.71% compared to plain text for both queries and document
texts. Based on our evaluations, our model establishes a new state-of-the-art
standard in text feature extraction for longer context document retrieval
tasks.",Thea Aviss
2024-10-09T16:35:41Z,http://arxiv.org/abs/2410.18251v1,Context-Augmented Code Generation Using Programming Knowledge Graphs,"Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.",Iman Saberi
2024-10-24T17:13:39Z,http://arxiv.org/abs/2410.18926v1,"LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor
  Search","Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets.",Elias Jääsaari
2024-10-25T14:07:53Z,http://arxiv.org/abs/2410.19572v4,ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems,"Retrieval-Augmented Generation (RAG) systems using large language models
(LLMs) often generate inaccurate responses due to the retrieval of irrelevant
or loosely related information. Existing methods, which operate at the document
level, fail to effectively filter out such content. We propose LLM-driven chunk
filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and
filtering retrieved information at the chunk level. Our approach employs
semantic chunking to divide documents into coherent sections and utilizes
LLM-based relevance scoring to assess each chunk's alignment with the user's
query. By filtering out less pertinent chunks before the generation phase, we
significantly reduce hallucinations and improve factual accuracy. Experiments
show that our method outperforms existing RAG models, achieving higher accuracy
on tasks requiring precise information retrieval. This advancement enhances the
reliability of RAG systems, making them particularly beneficial for
applications like fact-checking and multi-hop reasoning.",Ishneet Sukhvinder Singh
2024-10-26T03:07:22Z,http://arxiv.org/abs/2410.20056v1,Multi-Field Adaptive Retrieval,"Document retrieval for tasks such as search and retrieval-augmented
generation typically involves datasets that are unstructured: free-form text
without explicit internal structure in each document. However, documents can
have a structured form, consisting of fields such as an article title, message
body, or HTML header. To address this gap, we introduce Multi-Field Adaptive
Retrieval (MFAR), a flexible framework that accommodates any number of and any
type of document indices on structured data. Our framework consists of two main
steps: (1) the decomposition of an existing document into fields, each indexed
independently through dense and lexical methods, and (2) learning a model which
adaptively predicts the importance of a field by conditioning on the document
query, allowing on-the-fly weighting of the most likely field(s). We find that
our approach allows for the optimized use of dense versus lexical
representations across field types, significantly improves in document ranking
over a number of existing retrievers, and achieves state-of-the-art performance
for multi-field structured data.",Millicent Li
2024-10-27T00:42:21Z,http://arxiv.org/abs/2410.20299v1,"EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge
  Update","Large Language Models are revolutionizing Web, mobile, and Web of Things
systems, driving intelligent and scalable solutions. However, as
Retrieval-Augmented Generation (RAG) systems expand, they encounter significant
challenges related to scalability, including increased delay and communication
overhead. To address these issues, we propose EACO-RAG, an edge-assisted
distributed RAG system that leverages adaptive knowledge updates and inter-node
collaboration. By distributing vector datasets across edge nodes and optimizing
retrieval processes, EACO-RAG significantly reduces delay and resource
consumption while enhancing response accuracy. The system employs a multi-armed
bandit framework with safe online Bayesian methods to balance performance and
cost. Extensive experimental evaluation demonstrates that EACO-RAG outperforms
traditional centralized RAG systems in both response time and resource
efficiency. EACO-RAG effectively reduces delay and resource expenditure to
levels comparable to, or even lower than, those of local RAG systems, while
significantly improving accuracy. This study presents the first systematic
exploration of edge-assisted distributed RAG architectures, providing a
scalable and cost-effective solution for large-scale distributed environments.",Jiaxing Li
2024-10-28T02:55:03Z,http://arxiv.org/abs/2410.20695v2,"Combining Domain-Specific Models and LLMs for Automated Disease
  Phenotyping from Survey Data","This exploratory pilot study investigated the potential of combining a
domain-specific model, BERN2, with large language models (LLMs) to enhance
automated disease phenotyping from research survey data. Motivated by the need
for efficient and accurate methods to harmonize the growing volume of survey
data with standardized disease ontologies, we employed BERN2, a biomedical
named entity recognition and normalization model, to extract disease
information from the ORIGINS birth cohort survey data. After rigorously
evaluating BERN2's performance against a manually curated ground truth dataset,
we integrated various LLMs using prompt engineering, Retrieval-Augmented
Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's
outputs. BERN2 demonstrated high performance in extracting and normalizing
disease mentions, and the integration of LLMs, particularly with Few Shot
Inference and RAG orchestration, further improved accuracy. This approach,
especially when incorporating structured examples, logical reasoning prompts,
and detailed context, offers a promising avenue for developing tools to enable
efficient cohort profiling and data harmonization across large, heterogeneous
research datasets.",Gal Beeri
2024-10-28T12:50:27Z,http://arxiv.org/abs/2410.20975v1,"Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base
  for Geospatial Code Generation Tasks Using Large Language Models","The rise of spatiotemporal data and the need for efficient geospatial
modeling have spurred interest in automating these tasks with large language
models (LLMs). However, general LLMs often generate errors in geospatial code
due to a lack of domain-specific knowledge on functions and operators. To
address this, a retrieval-augmented generation (RAG) approach, utilizing an
external knowledge base of geospatial functions and operators, is proposed.
This study introduces a framework to construct such a knowledge base,
leveraging geospatial script semantics. The framework includes: Function
Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination
Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like
Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and
align geospatial functions. An example knowledge base, Geo-FuB, built from
154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics
show a high accuracy, reaching 88.89% overall, with structural and semantic
accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize
geospatial code generation through the RAG and fine-tuning paradigms is
highlighted.",Shuyang Hou
2024-10-28T14:29:11Z,http://arxiv.org/abs/2410.21067v1,"CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and
  Retrieval-Augmented Translation with Large Language Models","Large language models (LLMs) have shown great promise in machine translation,
but they still struggle with contextually dependent terms, such as new or
domain-specific words. This leads to inconsistencies and errors that are
difficult to address. Existing solutions often depend on manual identification
of such terms, which is impractical given the complexity and evolving nature of
language. While Retrieval-Augmented Generation (RAG) could provide some
assistance, its application to translation is limited by issues such as
hallucinations from information overload. In this paper, we propose CRAT, a
novel multi-agent translation framework that leverages RAG and
causality-enhanced self-reflection to address these challenges. This framework
consists of several specialized agents: the Unknown Terms Identification agent
detects unknown terms within the context, the Knowledge Graph (KG) Constructor
agent extracts relevant internal knowledge about these terms and retrieves
bilingual information from external sources, the Causality-enhanced Judge agent
validates the accuracy of the information, and the Translator agent
incorporates the refined information into the final output. This automated
process allows for more precise and consistent handling of key terms during
translation. Our results show that CRAT significantly improves translation
accuracy, particularly in handling context-sensitive terms and emerging
vocabulary.",Meiqi Chen
2024-10-28T17:04:18Z,http://arxiv.org/abs/2410.21220v1,"Vision Search Assistant: Empower Vision-Language Models as Multimodal
  Search Engines","Search engines enable the retrieval of unknown information with texts.
However, traditional methods fall short when it comes to understanding
unfamiliar visual content, such as identifying an object that the model has
never seen before. This challenge is particularly pronounced for large
vision-language models (VLMs): if the model has not been exposed to the object
depicted in an image, it struggles to generate reliable answers to the user's
question regarding that image. Moreover, as new objects and events continuously
emerge, frequently updating VLMs is impractical due to heavy computational
burdens. To address this limitation, we propose Vision Search Assistant, a
novel framework that facilitates collaboration between VLMs and web agents.
This approach leverages VLMs' visual understanding capabilities and web agents'
real-time information access to perform open-world Retrieval-Augmented
Generation via the web. By integrating visual and textual representations
through this collaboration, the model can provide informed responses even when
the image is novel to the system. Extensive experiments conducted on both
open-set and closed-set QA benchmarks demonstrate that the Vision Search
Assistant significantly outperforms the other models and can be widely applied
to existing VLMs.",Zhixin Zhang
2024-10-29T09:02:37Z,http://arxiv.org/abs/2410.21868v2,Improving In-Context Learning with Small Language Model Ensembles,"Large language models (LLMs) have shown impressive capabilities across
various tasks, but their performance on domain-specific tasks remains limited.
While methods like retrieval augmented generation and fine-tuning can help to
address this, they require significant resources. In-context learning (ICL) is
a cheap and efficient alternative but cannot match the accuracies of advanced
methods. We present Ensemble SuperICL, a novel approach that enhances ICL by
leveraging the expertise of multiple fine-tuned small language models (SLMs).
Ensemble SuperICL achieves state of the art (SoTA) results on several natural
language understanding benchmarks. Additionally, we test it on a medical-domain
labelling task and showcase its practicality by using off-the-shelf SLMs
fine-tuned on a general language task, achieving superior accuracy in
large-scale data labelling compared to all baselines. Finally, we conduct an
ablation study and sensitivity analyses to elucidate the underlying mechanism
of Ensemble SuperICL. Our research contributes to the growing demand for
efficient domain specialisation methods in LLMs, offering a cheap and effective
method for practitioners.",M. Mehdi Mojarradi
2024-10-29T11:03:31Z,http://arxiv.org/abs/2410.21943v1,"Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial
  Applications","Large Language Models (LLMs) have demonstrated impressive capabilities in
answering questions, but they lack domain-specific knowledge and are prone to
hallucinations. Retrieval Augmented Generation (RAG) is one approach to address
these challenges, while multimodal models are emerging as promising AI
assistants for processing both text and images. In this paper we describe a
series of experiments aimed at determining how to best integrate multimodal
models into RAG systems for the industrial domain. The purpose of the
experiments is to determine whether including images alongside text from
documents within the industrial domain increases RAG performance and to find
the optimal configuration for such a multimodal RAG system. Our experiments
include two approaches for image processing and retrieval, as well as two LLMs
(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies
involve the use of multimodal embeddings and the generation of textual
summaries from images. We evaluate our experiments with an LLM-as-a-Judge
approach. Our results reveal that multimodal RAG can outperform single-modality
RAG settings, although image retrieval poses a greater challenge than text
retrieval. Additionally, leveraging textual summaries from images presents a
more promising approach compared to the use of multimodal embeddings, providing
more opportunities for future advancements.",Monica Riedler
2024-10-30T09:15:51Z,http://arxiv.org/abs/2410.22832v1,"HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language
  Models","Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge, making them adaptable and
cost-effective for various applications. However, the growing reliance on these
systems also introduces potential security risks. In this work, we reveal a
novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which
enables attackers to manipulate the retrieval mechanisms of RAG systems by
injecting malicious texts into the knowledge database. When the RAG system
encounters target questions, it generates the attacker's pre-determined answers
instead of the correct ones, undermining the integrity and trustworthiness of
the system. We formalize HijackRAG as an optimization problem and propose both
black-box and white-box attack strategies tailored to different levels of the
attacker's knowledge. Extensive experiments on multiple benchmark datasets show
that HijackRAG consistently achieves high attack success rates, outperforming
existing baseline attacks. Furthermore, we demonstrate that the attack is
transferable across different retriever models, underscoring the widespread
risk it poses to RAG systems. Lastly, our exploration of various defense
mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing
the urgent need for more robust security measures to protect RAG systems in
real-world deployments.",Yucheng Zhang
2024-10-30T10:11:53Z,http://arxiv.org/abs/2410.22874v1,"Eliciting Critical Reasoning in Retrieval-Augmented Language Models via
  Contrastive Explanations","Retrieval-augmented generation (RAG) has emerged as a critical mechanism in
contemporary NLP to support Large Language Models(LLMs) in systematically
accessing richer factual context. However, the integration of RAG mechanisms
brings its inherent challenges, as LLMs need to deal with potentially noisy
contexts. Recent studies have shown that LLMs still struggle to critically
analyse RAG-based in-context information, a limitation that may lead to
incorrect inferences and hallucinations. In this paper, we investigate how to
elicit critical reasoning in RAG via contrastive explanations. In particular,
we propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant
documents given a query, (ii) selects and exemplifies relevant passages, and
(iii) generates explanations that explicitly contrast the relevance of the
passages to (iv) support the final answer. We show the impact of C-RAG building
contrastive reasoning demonstrations from LLMs to instruct smaller models for
retrieval-augmented tasks. Extensive experiments demonstrate that C-RAG
improves state-of-the-art RAG models while (a) requiring significantly fewer
prompts and demonstrations and (b) being robust to perturbations in the
retrieved documents.",Leonardo Ranaldi
2024-10-30T20:28:10Z,http://arxiv.org/abs/2410.23437v1,Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment,"Retrieval-Augmented Generation (RAG) systems enhance text generation by
incorporating external knowledge but often struggle when retrieving context
across different text modalities due to semantic gaps. We introduce a
generalized projection-based method, inspired by adapter modules in transfer
learning, that efficiently bridges these gaps between various text types, such
as programming code and pseudocode, or English and French sentences. Our
approach emphasizes speed, accuracy, and data efficiency, requiring minimal
resources for training and inference. By aligning embeddings from heterogeneous
text modalities into a unified space through a lightweight projection network,
our model significantly outperforms traditional retrieval methods like the
Okapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while
approaching the accuracy of Sentence Transformers. Extensive evaluations
demonstrate the effectiveness and generalizability of our method across
different tasks, highlighting its potential for real-time, resource-constrained
applications.",Arihan Yadav
2024-10-30T23:35:21Z,http://arxiv.org/abs/2410.23511v1,"Dynamic Strategy Planning for Efficient Question Answering with Large
  Language Models","Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),
planning (e.g., SelfAsk), and retrieval augmented generation strategies to
improve the performance of Large Language Models (LLMs) on various tasks, such
as question answering. However, using a single fixed strategy to answer
different kinds of questions is suboptimal in performance and inefficient in
terms of generated output tokens and performed retrievals. In our work, we
propose a novel technique DyPlan, to induce a dynamic strategy selection
process in LLMs, to improve performance and reduce costs in question-answering.
DyPlan incorporates an initial decision step to select the most suitable
strategy conditioned on the input question and guides the LLM's response
generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal
verification and correction process to further enrich the generated answer.
Experiments on three prominent multi-hop question answering (MHQA) datasets
reveal how DyPlan can improve model performance by 7-13% while reducing the
cost by 11-32% relative to the best baseline model.",Tanmay Parekh
2024-10-31T00:18:05Z,http://arxiv.org/abs/2410.23526v1,"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve
  Factualness in Large Language Models","Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.",Hieu Tran
2024-10-31T18:43:12Z,http://arxiv.org/abs/2411.00142v1,"JudgeRank: Leveraging Large Language Models for Reasoning-Intensive
  Reranking","Accurate document retrieval is crucial for the success of retrieval-augmented
generation (RAG) applications, including open-domain question answering and
code completion. While large language models (LLMs) have been employed as dense
encoders or listwise rerankers in RAG systems, they often struggle with
reasoning-intensive tasks because they lack nuanced analysis when judging
document relevance. To address this limitation, we introduce JudgeRank, a novel
agentic reranker that emulates human cognitive processes when assessing
document relevance. Our approach consists of three key steps: (1) query
analysis to identify the core problem, (2) document analysis to extract a
query-aware summary, and (3) relevance judgment to provide a concise assessment
of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT
benchmark, demonstrating substantial performance improvements over first-stage
retrieval methods and outperforming other popular reranking approaches. In
addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers
on the popular BEIR benchmark, validating its zero-shot generalization
capability. Through comprehensive ablation studies, we demonstrate that
JudgeRank's performance generalizes well across LLMs of various sizes while
ensembling them yields even more accurate reranking than individual models.",Tong Niu
2024-11-01T01:11:58Z,http://arxiv.org/abs/2411.00294v2,"LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models","Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.",Kazi Ahmed Asif Fuad
2024-11-01T20:44:59Z,http://arxiv.org/abs/2411.01022v1,"Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM
  Generation Output","We present a light-weight approach for detecting nonfactual outputs from
retrieval-augmented generation (RAG). Given a context and putative output, we
compute a factuality score that can be thresholded to yield a binary decision
to check the results of LLM-based question-answering, summarization, or other
systems. Unlike factuality checkers that themselves rely on LLMs, we use
compact, open-source natural language inference (NLI) models that yield a
freely accessible solution with low latency and low cost at run-time, and no
need for LLM fine-tuning. The approach also enables downstream mitigation and
correction of hallucinations, by tracing them back to specific context chunks.
Our experiments show high area under the ROC curve (AUC) across a wide range of
relevant open source datasets, indicating the effectiveness of our method for
fact-checking RAG output.",Hithesh Sankararaman
2024-11-01T23:03:40Z,http://arxiv.org/abs/2411.01073v1,"AttackQA: Development and Adoption of a Dataset for Assisting
  Cybersecurity Operations using Fine-tuned and Open-Source LLMs","Retrieval-augmented generation (RAG) on specialized domain datasets has shown
improved performance when large language models (LLMs) are fine-tuned for
generating responses to user queries. In this study, we develop a cybersecurity
question-answering (Q\&A) dataset, called AttackQA, and employ it to build a
RAG-based Q\&A system designed for analysts in security operations centers. The
dataset comprises 25,335 Q\&A pairs, accompanied by rationales to facilitate
fine-tuning and evaluation. 80\% of the dataset was generated with help of a
lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per
second with full 16-bit precision on SambaNova System's SN40L specialized
hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and
reject low-quality Q\&A pairs. In using the dataset for RAG, we demonstrate
that fine-tuning open-source embeddings and LLMs can yield superior accuracy
compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).
Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,
enabling the creation of a fully open-source, high-speed RAG and evaluation
pipeline with a benchmark for model accuracy.",Varun Badrinath Krishna
2024-11-02T02:09:01Z,http://arxiv.org/abs/2411.01106v1,"LoRA-Contextualizing Adaptation of Large Multimodal Models for Long
  Document Understanding","Large multimodal models (LMMs) have recently shown great progress in
text-rich image understanding, yet they still struggle with complex,
multi-page, visually-rich documents. Traditional methods using document parsers
for retrieval-augmented generation suffer from performance and efficiency
limitations, while directly presenting all pages to LMMs leads to
inefficiencies, especially with lengthy documents. In this work, we present a
novel framework named LoRA-Contextualizing Adaptation of Large multimodal
models (LoCAL), which broadens the capabilities of any LMM to support
long-document understanding. We demonstrate that LMMs can effectively serve as
multimodal retrievers, fetching relevant pages to answer user questions based
on these pages. LoCAL is implemented with two specific LMM adapters: one for
evidence page retrieval and another for question answering. Empirical results
show state-of-the-art performance on public benchmarks, demonstrating the
effectiveness of LoCAL.",Jian Chen
2024-11-05T06:44:15Z,http://arxiv.org/abs/2411.02850v1,"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene","This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.",Simon Kloker
2024-11-05T22:37:43Z,http://arxiv.org/abs/2411.03538v1,Long Context RAG Performance of Large Language Models,"Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.",Quinn Leng
2024-11-06T00:23:55Z,http://arxiv.org/abs/2411.03572v1,"Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge
  Reasoning and Text Generation","This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.",Yuxin Dong
2024-11-06T22:10:18Z,http://arxiv.org/abs/2411.04284v1,Enhancing Security Control Production With Generative AI,"Security controls are mechanisms or policies designed for cloud based
services to reduce risk, protect information, and ensure compliance with
security regulations. The development of security controls is traditionally a
labor-intensive and time-consuming process. This paper explores the use of
Generative AI to accelerate the generation of security controls. We
specifically focus on generating Gherkin codes which are the domain-specific
language used to define the behavior of security controls in a structured and
understandable format. By leveraging large language models and in-context
learning, we propose a structured framework that reduces the time required for
developing security controls from 2-3 days to less than one minute. Our
approach integrates detailed task descriptions, step-by-step instructions, and
retrieval-augmented generation to enhance the accuracy and efficiency of the
generated Gherkin code. Initial evaluations on AWS cloud services demonstrate
promising results, indicating that GenAI can effectively streamline the
security control development process, thus providing a robust and dynamic
safeguard for cloud-based infrastructures.",Chen Ling
2024-11-07T06:51:24Z,http://arxiv.org/abs/2411.04473v1,ML-Promise: A Multilingual Dataset for Corporate Promise Verification,"Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.",Yohei Seki
2024-11-08T06:12:56Z,http://arxiv.org/abs/2411.05349v1,"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent
  Cluster Diagnosis System and Evaluation Framework","Recent advancements in Large Language Models (LLMs) and related technologies
such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
enabled the creation of autonomous intelligent systems capable of performing
cluster diagnostics and troubleshooting. By integrating these technologies with
self-play methodologies, we have developed an LLM-agent system designed to
autonomously diagnose and resolve issues within AI clusters. Our innovations
include a knowledge base tailored for cluster diagnostics, enhanced LLM
algorithms, practical deployment strategies for agents, and a benchmark
specifically designed for evaluating LLM capabilities in this domain. Through
extensive experimentation across multiple dimensions, we have demonstrated the
superiority of our system in addressing the challenges faced in cluster
diagnostics, particularly in detecting and rectifying performance issues more
efficiently and accurately than traditional methods.",Honghao Shi
2024-11-09T13:17:39Z,http://arxiv.org/abs/2411.06175v2,"Clustering Algorithms and RAG Enhancing Semi-Supervised Text
  Classification with Large LLMs","This paper introduces a novel semi-supervised learning framework specifically
designed for text classification tasks, effectively addressing the challenge of
vast datasets with limited labeled examples. By integrating multi-level
similarity based data augmentation techniques from Retrieval-Augmented
Generation (RAG) to Large Language Model (LLM) rewriting and traditional word
substitution-we constructed an intelligent augmentation pipeline. This
framework innovatively employs the selection of representative landmarks
through clustering, which serve as intermediaries in the retrieval and
rewriting processes, ensuring that the augmented data maintains a distribution
similar to the original dataset. Empirical results show that even in complex
text document classification scenarios with over 100 categories, our method
achieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and
Web of Science datasets, respectively. These findings highlight the
effectiveness and broad applicability of our semi-supervised learning approach
for text classification tasks.",Shan Zhong
2024-11-10T15:21:30Z,http://arxiv.org/abs/2411.06493v2,LProtector: An LLM-driven Vulnerability Detection System,"This paper presents LProtector, an automated vulnerability detection system
for C/C++ codebases driven by the large language model (LLM) GPT-4o and
Retrieval-Augmented Generation (RAG). As software complexity grows, traditional
methods face challenges in detecting vulnerabilities effectively. LProtector
leverages GPT-4o's powerful code comprehension and generation capabilities to
perform binary classification and identify vulnerabilities within target
codebases. We conducted experiments on the Big-Vul dataset, showing that
LProtector outperforms two state-of-the-art baselines in terms of F1 score,
demonstrating the potential of integrating LLMs with vulnerability detection.",Ze Sheng
2024-11-11T17:33:51Z,http://arxiv.org/abs/2411.07156v1,"A Primer on Word Embeddings: AI Techniques for Text Analysis in Social
  Work","Word embeddings represent a transformative technology for analyzing text data
in social work research, offering sophisticated tools for understanding case
notes, policy documents, research literature, and other text-based materials.
This methodological paper introduces word embeddings to social work
researchers, explaining how these mathematical representations capture meaning
and relationships in text data more effectively than traditional keyword-based
approaches. We discuss fundamental concepts, technical foundations, and
practical applications, including semantic search, clustering, and retrieval
augmented generation. The paper demonstrates how embeddings can enhance
research workflows through concrete examples from social work practice, such as
analyzing case notes for housing instability patterns and comparing social work
licensing examinations across languages. While highlighting the potential of
embeddings for advancing social work research, we acknowledge limitations
including information loss, training data constraints, and potential biases. We
conclude that successfully implementing embedding technologies in social work
requires developing domain-specific models, creating accessible tools, and
establishing best practices aligned with social work's ethical principles. This
integration can enhance our ability to analyze complex patterns in text data
while supporting more effective services and interventions.",Brian E. Perron
2024-11-11T22:06:51Z,http://arxiv.org/abs/2411.07396v1,Toward Optimal Search and Retrieval for RAG,"Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.",Alexandria Leto
2024-11-12T12:03:57Z,http://arxiv.org/abs/2411.07739v1,Unlocking Legal Knowledge with Multi-Layered Embedding-Based Retrieval,"This work addresses the challenge of capturing the complexities of legal
knowledge by proposing a multi-layered embedding-based retrieval method for
legal and legislative texts. Creating embeddings not only for individual
articles but also for their components (paragraphs, clauses) and structural
groupings (books, titles, chapters, etc), we seek to capture the subtleties of
legal information through the use of dense vectors of embeddings, representing
it at varying levels of granularity. Our method meets various information needs
by allowing the Retrieval Augmented Generation system to provide accurate
responses, whether for specific segments or entire sections, tailored to the
user's query. We explore the concepts of aboutness, semantic chunking, and
inherent hierarchy within legal texts, arguing that this method enhances the
legal information retrieval. Despite the focus being on Brazil's legislative
methods and the Brazilian Constitution, which follow a civil law tradition, our
findings should in principle be applicable across different legal systems,
including those adhering to common law traditions. Furthermore, the principles
of the proposed method extend beyond the legal domain, offering valuable
insights for organizing and retrieving information in any field characterized
by information encoded in hierarchical text.",João Alberto de Oliveira Lima
2024-11-12T14:12:45Z,http://arxiv.org/abs/2411.07820v2,"Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models","We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.",Youan Cong
2024-11-12T15:26:17Z,http://arxiv.org/abs/2411.07870v6,"Trustful LLMs: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders","Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.",Xiaofeng Zhu
2024-11-12T23:55:11Z,http://arxiv.org/abs/2411.08249v1,Retrieval Augmented Time Series Forecasting,"Retrieval-augmented generation (RAG) is a central component of modern LLM
systems, particularly in scenarios where up-to-date information is crucial for
accurately responding to user queries or when queries exceed the scope of the
training data. The advent of time-series foundation models (TSFM), such as
Chronos, and the need for effective zero-shot forecasting performance across
various time-series domains motivates the question: Do benefits of RAG
similarly carry over to time series forecasting? In this paper, we advocate
that the dynamic and event-driven nature of time-series data makes RAG a
crucial component of TSFMs and introduce a principled RAG framework for
time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within
RAF, we develop efficient strategies for retrieving related time-series
examples and incorporating them into forecast. Through experiments and
mechanistic studies, we demonstrate that RAF indeed improves the forecasting
accuracy across diverse time series domains and the improvement is more
significant for larger TSFM sizes.",Kutay Tire
2024-11-13T04:20:20Z,http://arxiv.org/abs/2411.08324v1,"Are LLMs Prescient? A Continuous Evaluation using Daily News as the
  Oracle","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict ""future"" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.",Hui Dai
2024-11-13T05:40:24Z,http://arxiv.org/abs/2411.08348v1,"Refining Translations with LLMs: A Constraint-Aware Iterative Prompting
  Approach","Large language models (LLMs) have demonstrated remarkable proficiency in
machine translation (MT), even without specific training on the languages in
question. However, translating rare words in low-resource or domain-specific
contexts remains challenging for LLMs. To address this issue, we propose a
multi-step prompt chain that enhances translation faithfulness by prioritizing
key terms crucial for semantic accuracy. Our method first identifies these
keywords and retrieves their translations from a bilingual dictionary,
integrating them into the LLM's context using Retrieval-Augmented Generation
(RAG). We further mitigate potential output hallucinations caused by long
prompts through an iterative self-checking mechanism, where the LLM refines its
translations based on lexical and semantic constraints. Experiments using Llama
and Qwen as base models on the FLORES-200 and WMT datasets demonstrate
significant improvements over baselines, highlighting the effectiveness of our
approach in enhancing translation faithfulness and robustness, particularly in
low-resource scenarios.",Shangfeng Chen
2024-11-13T09:11:56Z,http://arxiv.org/abs/2411.08449v2,Towards Evaluating Large Language Models for Graph Query Generation,"Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.",Siraj Munir
2024-11-13T09:40:37Z,http://arxiv.org/abs/2411.08469v2,"Building Trustworthy AI: Transparent AI Systems via Large Language
  Models, Ontologies, and Logical Reasoning (TranspNet)","Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their ""black box"" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.",Fadi Al Machot
2024-11-13T12:44:41Z,http://arxiv.org/abs/2411.08574v1,"Practitioners' Discussions on Building LLM-based Applications for
  Production","\textit{Background}: Large language models (LLMs) have become a paramount
interest of researchers and practitioners alike, yet a comprehensive overview
of key considerations for those developing LLM-based systems is lacking. This
study addresses this gap by collecting and mapping the topics practitioners
discuss online, offering practical insights into where priorities lie in
developing LLM-based applications. \textit{Method}: We collected 189 videos
from 2022 to 2024 from practitioners actively developing such systems and
discussing various aspects they encounter during development and deployment of
LLMs in production. We analyzed the transcripts using BERTopic, then manually
sorted and merged the generated topics into themes, leading to a total of 20
topics in 8 themes. \textit{Results}: The most prevalent topics fall within the
theme Design \& Architecture, with a strong focus on retrieval-augmented
generation (RAG) systems. Other frequently discussed topics include model
capabilities and enhancement techniques (e.g., fine-tuning, prompt
engineering), infrastructure and tooling, and risks and ethical challenges.
\textit{Implications}: Our results highlight current discussions and challenges
in deploying LLMs in production. This way, we provide a systematic overview of
key aspects practitioners should be aware of when developing LLM-based
applications. We further pale off topics of interest for academics where
further research is needed.",Alina Mailach
2024-11-04T08:15:22Z,http://arxiv.org/abs/2411.08724v1,"QCG-Rerank: Chunks Graph Rerank with Query Expansion in
  Retrieval-Augmented LLMs for Tourism Domain","Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in
Large Language Models (LLMs) by integrating information retrieval techniques.
However, in the tourism domain, since the query is usually brief and the
content in the database is diverse, existing RAG may contain a significant
amount of irrelevant or contradictory information contents after retrieval. To
address this challenge, we propose the QCG-Rerank model. This model first
performs an initial retrieval to obtain candidate chunks and then enhances
semantics by extracting critical information to expand the original query.
Next, we utilize the expanded query and candidate chunks to calculate
similarity scores as the initial transition probability and construct the
chunks graph. Subsequently, We iteratively compute the transition probabilities
based on an initial estimate until convergence. The chunks with the highest
score are selected and input into the LLMs to generate responses. We evaluate
the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.
The experimental results demonstrate the effectiveness and superiority of the
QCG-Rerank method.",Qikai Wei
2024-11-14T17:01:24Z,http://arxiv.org/abs/2411.09590v1,Adopting RAG for LLM-Aided Future Vehicle Design,"In this paper, we explore the integration of Large Language Models (LLMs)
with Retrieval-Augmented Generation (RAG) to enhance automated design and
software development in the automotive industry. We present two case studies: a
standardization compliance chatbot and a design copilot, both utilizing RAG to
provide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,
LLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and
execution time. Our results demonstrate that while GPT-4 offers superior
performance, LLAMA3 and Mistral also show promising capabilities for local
deployment, addressing data privacy concerns in automotive applications. This
study highlights the potential of RAG-augmented LLMs in improving design
workflows and compliance in automotive engineering.",Vahid Zolfaghari
2024-11-19T07:03:19Z,http://arxiv.org/abs/2411.12280v1,"Large Language Models for Material Property Predictions: elastic
  constant tensor prediction and materials design","Efficient and accurate prediction of material properties is critical for
advancing materials design and applications. The rapid-evolution of large
language models (LLMs) presents a new opportunity for material property
predictions, complementing experimental measurements and multi-scale
computational methods. We focus on predicting the elastic constant tensor, as a
case study, and develop domain-specific LLMs for predicting elastic constants
and for materials discovery. The proposed ElaTBot LLM enables simultaneous
prediction of elastic constant tensors, bulk modulus at finite temperatures,
and the generation of new materials with targeted properties. Moreover, the
capabilities of ElaTBot are further enhanced by integrating with general LLMs
(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized
variant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,
reduces the prediction errors by 33.1% compared with domain-specific, material
science LLMs (Darwin) trained on the same dataset. This natural language-based
approach lowers the barriers to computational materials science and highlights
the broader potential of LLMs for material property predictions and inverse
design.",Siyu Liu
2024-11-19T07:16:48Z,http://arxiv.org/abs/2411.12287v2,"CUE-M: Contextual Understanding and Enhanced Search with Multimodal
  Large Language Model","The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large
Language Models (MLLMs) has revolutionized information retrieval and expanded
the practical applications of AI. However, current systems struggle in
accurately interpreting user intent, employing diverse retrieval strategies,
and effectively filtering unintended or inappropriate responses, limiting their
effectiveness. This paper introduces Contextual Understanding and Enhanced
Search with MLLM (CUE-M), a novel multimodal search framework that addresses
these challenges through a multi-stage pipeline comprising image context
enrichment, intent refinement, contextual query generation, external API
integration, and relevance-based filtering. CUE-M incorporates a robust
filtering pipeline combining image-based, text-based, and multimodal
classifiers, dynamically adapting to instance- and category-specific concern
defined by organizational policies. Evaluations on a multimodal Q&A dataset and
a public safety benchmark demonstrate that CUE-M outperforms baselines in
accuracy, knowledge integration, and safety, advancing the capabilities of
multimodal retrieval systems.",Dongyoung Go
2024-11-19T16:54:45Z,http://arxiv.org/abs/2411.12644v2,"CodeXEmbed: A Generalist Embedding Model Family for Multiligual and
  Multi-task Code Retrieval","Despite the success of text retrieval in many NLP tasks, code retrieval
remains a largely underexplored area. Most text retrieval systems are tailored
for natural language queries, often neglecting the specific challenges of
retrieving code. This gap leaves existing models unable to effectively capture
the diversity of programming languages and tasks across different domains,
highlighting the need for more focused research in code retrieval. To address
this, we introduce CodeXEmbed, a family of large-scale code embedding models
ranging from 400M to 7B parameters. Our novel training pipeline unifies
multiple programming languages and transforms various code-related tasks into a
common retrieval framework, enhancing model generalizability and retrieval
performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,
outperforming the previous leading model, Voyage-Code, by over 20% on CoIR
benchmark. In addition to excelling in code retrieval, our models demonstrate
competitive performance on the widely adopted BeIR text retrieval benchmark,
offering versatility across domains. Experimental results demonstrate that
improving retrieval performance significantly enhances end-to-end
Retrieval-Augmented Generation (RAG) performance for code-related tasks.",Ye Liu
2024-11-20T09:43:30Z,http://arxiv.org/abs/2411.13154v1,DMQR-RAG: Diverse Multi-Query Rewriting for RAG,"Large language models often encounter challenges with static knowledge and
hallucinations, which undermine their reliability. Retrieval-augmented
generation (RAG) mitigates these issues by incorporating external information.
However, user queries frequently contain noise and intent deviations,
necessitating query rewriting to improve the relevance of retrieved documents.
In this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework
designed to improve the performance of both document retrieval and final
responses in RAG. Specifically, we investigate how queries with varying
information quantities can retrieve a diverse array of documents, presenting
four rewriting strategies that operate at different levels of information to
enhance the performance of baseline approaches. Additionally, we propose an
adaptive strategy selection method that minimizes the number of rewrites while
optimizing overall performance. Our methods have been rigorously validated
through extensive experiments conducted in both academic and industry settings.",Zhicong Li
2024-11-20T11:41:08Z,http://arxiv.org/abs/2411.13226v1,"AIDBench: A benchmark for evaluating the authorship identification
  capability of large language models","As large language models (LLMs) rapidly advance and integrate into daily
life, the privacy risks they pose are attracting increasing attention. We focus
on a specific privacy risk where LLMs may help identify the authorship of
anonymous texts, which challenges the effectiveness of anonymity in real-world
systems such as anonymous peer review systems. To investigate these risks, we
present AIDBench, a new benchmark that incorporates several author
identification datasets, including emails, blogs, reviews, articles, and
research papers. AIDBench utilizes two evaluation methods: one-to-one
authorship identification, which determines whether two texts are from the same
author; and one-to-many authorship identification, which, given a query text
and a list of candidate texts, identifies the candidate most likely written by
the same author as the query text. We also introduce a Retrieval-Augmented
Generation (RAG)-based method to enhance the large-scale authorship
identification capabilities of LLMs, particularly when input lengths exceed the
models' context windows, thereby establishing a new baseline for authorship
identification using LLMs. Our experiments with AIDBench demonstrate that LLMs
can correctly guess authorship at rates well above random chance, revealing new
privacy risks posed by these powerful models. The source code and data will be
made publicly available after acceptance.",Zichen Wen
2024-11-19T05:15:19Z,http://arxiv.org/abs/2411.14476v1,"StreetviewLLM: Extracting Geographic Information Using a
  Chain-of-Thought Multimodal Large Language Model","Geospatial predictions are crucial for diverse fields such as disaster
management, urban planning, and public health. Traditional machine learning
methods often face limitations when handling unstructured or multi-modal data
like street view imagery. To address these challenges, we propose
StreetViewLLM, a novel framework that integrates a large language model with
the chain-of-thought reasoning and multimodal data sources. By combining street
view imagery with geographic coordinates and textual data, StreetViewLLM
improves the precision and granularity of geospatial predictions. Using
retrieval-augmented generation techniques, our approach enhances geographic
information extraction, enabling a detailed analysis of urban environments. The
model has been applied to seven global cities, including Hong Kong, Tokyo,
Singapore, Los Angeles, New York, London, and Paris, demonstrating superior
performance in predicting urban indicators, including population density,
accessibility to healthcare, normalized difference vegetation index, building
height, and impervious surface. The results show that StreetViewLLM
consistently outperforms baseline models, offering improved predictive accuracy
and deeper insights into the built environment. This research opens new
opportunities for integrating the large language model into urban analytics,
decision-making in urban planning, infrastructure management, and environmental
monitoring.",Zongrong Li
2024-11-21T21:22:58Z,http://arxiv.org/abs/2411.14592v2,G-RAG: Knowledge Expansion in Material Science,"In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.",Radeen Mostafa
2024-11-23T14:47:10Z,http://arxiv.org/abs/2411.15577v1,"From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive
  Grammars","Recent advances in language modeling have demonstrated significant
improvements in zero-shot capabilities, including in-context learning,
instruction following, and machine translation for extremely under-resourced
languages (Tanzer et al., 2024). However, many languages with limited written
resources rely primarily on formal descriptions of grammar and vocabulary.
  In this paper, we introduce a set of benchmarks to evaluate how well models
can extract and classify information from the complex descriptions found in
linguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based
approach that leverages these descriptions for downstream tasks such as machine
translation. Our benchmarks encompass linguistic descriptions for 248 languages
across 142 language families, focusing on typological features from WALS and
Grambank.
  This set of benchmarks offers the first comprehensive evaluation of language
models' in-context ability to accurately interpret and extract linguistic
features, providing a critical resource for scaling NLP to low-resource
languages. The code and data are publicly available at
\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.",Albert Kornilov
2024-11-25T13:53:36Z,http://arxiv.org/abs/2411.16391v2,"Human-Calibrated Automated Testing and Validation of Generative Language
  Models","This paper introduces a comprehensive framework for the evaluation and
validation of generative language models (GLMs), with a focus on
Retrieval-Augmented Generation (RAG) systems deployed in high-stakes domains
such as banking. GLM evaluation is challenging due to open-ended outputs and
subjective quality assessments. Leveraging the structured nature of RAG
systems, where generated responses are grounded in a predefined document
collection, we propose the Human-Calibrated Automated Testing (HCAT) framework.
HCAT integrates a) automated test generation using stratified sampling, b)
embedding-based metrics for explainable assessment of functionality, risk and
safety attributes, and c) a two-stage calibration approach that aligns
machine-generated evaluations with human judgments through probability
calibration and conformal prediction.
  In addition, the framework includes robustness testing to evaluate model
performance against adversarial, out-of-distribution, and varied input
conditions, as well as targeted weakness identification using marginal and
bivariate analysis to pinpoint specific areas for improvement. This
human-calibrated, multi-layered evaluation framework offers a scalable,
transparent, and interpretable approach to GLM assessment, providing a
practical and reliable solution for deploying GLMs in applications where
accuracy, transparency, and regulatory compliance are paramount.",Agus Sudjianto
2024-11-21T19:01:07Z,http://arxiv.org/abs/2411.16707v1,"Enhancing LLMs for Power System Simulations: A Feedback-driven
  Multi-agent Framework","The integration of experimental technologies with large language models
(LLMs) is transforming scientific research, positioning AI as a versatile
research assistant rather than a mere problem-solving tool. In the field of
power systems, however, managing simulations -- one of the essential
experimental technologies -- remains a challenge for LLMs due to their limited
domain-specific knowledge, restricted reasoning capabilities, and imprecise
handling of simulation parameters. To address these limitations, we propose a
feedback-driven, multi-agent framework that incorporates three proposed
modules: an enhanced retrieval-augmented generation (RAG) module, an improved
reasoning module, and a dynamic environmental acting module with an
error-feedback mechanism. Validated on 69 diverse tasks from Daline and
MATPOWER, this framework achieves success rates of 93.13% and 96.85%,
respectively, significantly outperforming the latest LLMs (ChatGPT 4o and
o1-preview), which achieved a 27.77% success rate on standard simulation tasks
and 0% on complex tasks. Additionally, our framework also supports rapid,
cost-effective task execution, completing each simulation in approximately 30
seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable
framework lays a foundation for developing intelligent LLM-based assistants for
human researchers, facilitating power system research and beyond.",Mengshuo Jia
2024-11-27T10:48:37Z,http://arxiv.org/abs/2411.18216v1,"Evaluating and Improving the Robustness of Security Attack Detectors
  Generated by LLMs","Large Language Models (LLMs) are increasingly used in software development to
generate functions, such as attack detectors, that implement security
requirements. However, LLMs struggle to generate accurate code, resulting,
e.g., in attack detectors that miss well-known attacks when used in practice.
This is most likely due to the LLM lacking knowledge about some existing
attacks and to the generated code being not evaluated in real usage scenarios.
We propose a novel approach integrating Retrieval Augmented Generation (RAG)
and Self-Ranking into the LLM pipeline. RAG enhances the robustness of the
output by incorporating external knowledge sources, while the Self-Ranking
technique, inspired to the concept of Self-Consistency, generates multiple
reasoning paths and creates ranks to select the most robust detector. Our
extensive empirical study targets code generated by LLMs to detect two
prevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL
injection (SQLi). Results show a significant improvement in detection
performance compared to baselines, with an increase of up to 71%pt and 37%pt in
the F2-Score for XSS and SQLi detection, respectively.",Samuele Pasini
2024-11-28T06:28:45Z,http://arxiv.org/abs/2411.18947v1,ICLERB: In-Context Learning Embedding and Reranker Benchmark,"In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new
tasks by conditioning on prompts with relevant information. Retrieval-Augmented
Generation (RAG) enhances ICL by incorporating retrieved documents into the
LLM's context at query time. However, traditional retrieval methods focus on
semantic relevance, treating retrieval as a search problem. In this paper, we
propose reframing retrieval for ICL as a recommendation problem, aiming to
select documents that maximize utility in ICL tasks. We introduce the
In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel
evaluation framework that compares retrievers based on their ability to enhance
LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement
Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune
retrieval models using minimal feedback from the LLM. Our experimental results
reveal notable differences between ICLERB and existing benchmarks, and
demonstrate that small models fine-tuned with our RLRAIF algorithm outperform
large state-of-the-art retrieval models. These findings highlight the
limitations of existing evaluation methods and the need for specialized
benchmarks and training strategies adapted to ICL.",Marie Al Ghossein
2024-11-29T04:25:31Z,http://arxiv.org/abs/2411.19463v1,"Towards Understanding Retrieval Accuracy and Prompt Quality in RAG
  Systems","Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the
capability of large language models (LLMs) and has demonstrated promising
efficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show
superior performance, they face unique challenges in stability and reliability.
Their complexity hinders developers' efforts to design, maintain, and optimize
effective RAG systems. Therefore, it is crucial to understand how RAG's
performance is impacted by its design. In this work, we conduct an early
exploratory study toward a better understanding of the mechanism of RAG
systems, covering three code datasets, three QA datasets, and two LLMs. We
focus on four design factors: retrieval document type, retrieval recall,
document selection, and prompt techniques. Our study uncovers how each factor
impacts system correctness and confidence, providing valuable insights for
developing an accurate and reliable RAG system. Based on these findings, we
present nine actionable guidelines for detecting defects and optimizing the
performance of RAG systems. We hope our early exploration can inspire further
advancements in engineering, improving and maintaining LLM-driven intelligent
software systems for greater efficiency and reliability.",Shengming Zhao
2024-11-30T10:58:56Z,http://arxiv.org/abs/2412.00431v2,Multi-Agent System for Cosmological Parameter Analysis,"Multi-agent systems (MAS) utilizing multiple Large Language Model agents with
Retrieval Augmented Generation and that can execute code locally may become
beneficial in cosmological data analysis. Here, we illustrate a first small
step towards AI-assisted analyses and a glimpse of the potential of MAS to
automate and optimize scientific workflows in Cosmology. The system
architecture of our example package, that builds upon the autogen/ag2
framework, can be applied to MAS in any area of quantitative scientific
research. The particular task we apply our methods to is the cosmological
parameter analysis of the Atacama Cosmology Telescope lensing power spectrum
likelihood using Monte Carlo Markov Chains. Our work-in-progress code is open
source and available at https://github.com/CMBAgents/cmbagent.",Andrew Laverick
2024-12-02T16:55:07Z,http://arxiv.org/abs/2412.01709v1,"Query Performance Explanation through Large Language Model for HTAP
  Systems","In hybrid transactional and analytical processing (HTAP) systems, users often
struggle to understand why query plans from one engine (OLAP or OLTP) perform
significantly slower than those from another. Although optimizers provide plan
details via the EXPLAIN function, these explanations are frequently too
technical for non-experts and offer limited insights into performance
differences across engines. To address this, we propose a novel framework that
leverages large language models (LLMs) to explain query performance in HTAP
systems. Built on Retrieval-Augmented Generation (RAG), our framework
constructs a knowledge base that stores historical query executions and
expert-curated explanations. To enable efficient retrieval of relevant
knowledge, query plans are embedded using a lightweight tree-CNN classifier.
This augmentation allows the LLM to generate clear, context-aware explanations
of performance differences between engines. Our approach demonstrates the
potential of LLMs in hybrid engine systems, paving the way for further
advancements in database optimization and user support.",Haibo Xiu
2024-12-03T00:59:56Z,http://arxiv.org/abs/2412.02065v1,"Leveraging Large Language Models to Democratize Access to Costly
  Financial Datasets for Academic Research","Unequal access to costly datasets essential for empirical research has long
hindered researchers from disadvantaged institutions, limiting their ability to
contribute to their fields and advance their careers. Recent breakthroughs in
Large Language Models (LLMs) have the potential to democratize data access by
automating data collection from unstructured sources. We develop and evaluate a
novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation
(RAG) framework to collect data from corporate disclosures. Our approach
achieves human-level accuracy in collecting CEO pay ratios from approximately
10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000
10-K filings, with LLM processing times of 9 and 40 minutes respectively, each
at a cost under $10. This stands in stark contrast to the hundreds of hours
needed for manual collection or the thousands of dollars required for
commercial database subscriptions. To foster a more inclusive research
community by empowering researchers with limited resources to explore new
avenues of inquiry, we share our methodology and the resulting datasets.",Julian Junyan Wang
2024-12-03T08:34:42Z,http://arxiv.org/abs/2412.02262v1,"Composing Open-domain Vision with RAG for Ocean Monitoring and
  Conservation","Climate change's destruction of marine biodiversity is threatening
communities and economies around the world which rely on healthy oceans for
their livelihoods. The challenge of applying computer vision to niche,
real-world domains such as ocean conservation lies in the dynamic and diverse
environments where traditional top-down learning struggle with long-tailed
distributions, generalization, and domain transfer. Scalable species
identification for ocean monitoring is particularly difficult due to the need
to adapt models to new environments and identify rare or unseen species. To
overcome these limitations, we propose leveraging bottom-up, open-domain
learning frameworks as a resilient, scalable solution for image and video
analysis in marine applications. Our preliminary demonstration uses pretrained
vision-language models (VLMs) combined with retrieval-augmented generation
(RAG) as grounding, leaving the door open for numerous architectural, training
and engineering optimizations. We validate this approach through a preliminary
application in classifying fish from video onboard fishing vessels,
demonstrating impressive emergent retrieval and prediction capabilities without
domain-specific training or knowledge of the task itself.",Sepand Dyanatkar
2024-12-05T14:24:07Z,http://arxiv.org/abs/2412.04185v1,"Leveraging Large Language Models to Generate Course-specific
  Semantically Annotated Learning Objects","Background: Over the past few decades, the process and methodology of
automated question generation (AQG) have undergone significant transformations.
Recent progress in generative natural language models has opened up new
potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs)
for generating computer science questions that are sufficiently annotated for
automatic learner model updates, are fully situated in the context of a
particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT,
our approach involves more targeted strategies such as retrieval-augmented
generation (RAG) to produce contextually relevant and pedagogically meaningful
learning objects.
  Results and Conclusions: Our results show that generating structural,
semantic annotations works well. However, this success was not reflected in the
case of relational annotations. The quality of the generated questions often
did not meet educational standards, highlighting that although LLMs can
contribute to the pool of learning materials, their current level of
performance requires significant human intervention to refine and validate the
generated content.",Dominic Lohr
2024-12-05T15:11:12Z,http://arxiv.org/abs/2412.04235v1,"Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM
  Chatbots","I combine detection and mitigation techniques to addresses hallucinations in
Large Language Models (LLMs). Mitigation is achieved in a question-answering
Retrieval-Augmented Generation (RAG) framework while detection is obtained by
introducing the Negative Missing Information Scoring System (NMISS), which
accounts for contextual relevance in responses. While RAG mitigates
hallucinations by grounding answers in external data, NMISS refines the
evaluation by identifying cases where traditional metrics incorrectly flag
contextually accurate responses as hallucinations. I use Italian health news
articles as context to evaluate LLM performance. Results show that Gemma2 and
GPT-4 outperform the other models, with GPT-4 producing answers closely aligned
with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral
benefit significantly from NMISS, highlighting their ability to provide richer
contextual information. This combined approach offers new insights into the
reduction and more accurate assessment of hallucinations in LLMs, with
applications in real-world healthcare tasks and other domains.",Maria Paola Priola
2024-12-05T17:00:32Z,http://arxiv.org/abs/2412.04342v1,Retrieval-Augmented Machine Translation with Unstructured Knowledge,"Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",Jiaan Wang
2024-12-05T23:10:56Z,http://arxiv.org/abs/2412.04661v1,"HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and
  Representation Learning","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external document retrieval to provide domain-specific or
up-to-date knowledge. The effectiveness of RAG depends on the relevance of
retrieved documents, which is influenced by the semantic alignment of
embeddings with the domain's specialized content. Although full fine-tuning can
align language models to specific domains, it is computationally intensive and
demands substantial data. This paper introduces Hierarchical Embedding
Alignment Loss (HEAL), a novel method that leverages hierarchical fuzzy
clustering with matrix factorization within contrastive learning to efficiently
align LLM embeddings with domain-specific content. HEAL computes
level/depth-wise contrastive losses and incorporates hierarchical penalties to
align embeddings with the underlying relationships in label hierarchies. This
approach enhances retrieval relevance and document classification, effectively
reducing hallucinations in LLM outputs. In our experiments, we benchmark and
evaluate HEAL across diverse domains, including Healthcare, Material Science,
Cyber-security, and Applied Maths.",Manish Bhattarai
2024-12-06T17:04:21Z,http://arxiv.org/abs/2412.05184v1,QueEn: A Large Language Model for Quechua-English Translation,"Recent studies show that large language models (LLMs) are powerful tools for
working with natural language, bringing advances in many areas of computational
linguistics. However, these models face challenges when applied to low-resource
languages due to limited training data and difficulty in understanding cultural
nuances. In this paper, we propose QueEn, a novel approach for Quechua-English
translation that combines Retrieval-Augmented Generation (RAG) with
parameter-efficient fine-tuning techniques. Our method leverages external
linguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for
efficient model adaptation. Experimental results show that our approach
substantially exceeds baseline models, with a BLEU score of 17.6 compared to
1.5 for standard GPT models. The integration of RAG with fine-tuning allows our
system to address the challenges of low-resource language translation while
maintaining computational efficiency. This work contributes to the broader goal
of preserving endangered languages through advanced language technologies.",Junhao Chen
2024-12-06T17:54:54Z,http://arxiv.org/abs/2412.05223v1,100% Hallucination Elimination Using Acurai,"The issue of hallucinations in large language models (LLMs) remains a
critical barrier to the adoption of AI in enterprise and other high-stakes
applications. Despite advancements in retrieval-augmented generation (RAG)
systems, current state-of-the-art methods fail to achieve more than 80%
accuracy in generating faithful and factually correct outputs, even when
provided with relevant and accurate context. In this work, we introduce Acurai,
a novel systematic approach that achieves 100% hallucination-free responses in
LLMs by reformatting queries and context data prior to input. Leveraging a deep
understanding of LLM internal representations, the importance of noun-phrase
dominance, and the role of discrete functional units (DFUs), Acurai ensures
alignment between input context and generated output. We validate this method
using the RAGTruth corpus, demonstrating its ability to eliminate 100%
hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for
achieving consistent, accurate, and faithful AI responses, marking a
significant step forward in the development of trustworthy AI systems.",Michael C. Wood
2024-12-06T22:05:39Z,http://arxiv.org/abs/2412.05447v1,"A Graph-Based Approach for Conversational AI-Driven Personal Memory
  Capture and Retrieval in a Real-world Application","TOBU is a novel mobile application that captures and retrieves `personal
memories' (pictures/videos together with stories and context around those
moments) in a user-engaging AI-guided conversational approach. Our initial
prototype showed that existing retrieval techniques such as retrieval-augmented
generation (RAG) systems fall short due to their limitations in understanding
memory relationships, causing low recall, hallucination, and unsatisfactory
user experience. We design TOBUGraph, a novel graph-based retrieval approach.
During capturing, TOBUGraph leverages large language models (LLMs) to
automatically create a dynamic knowledge graph of memories, establishing
context and relationships of those memories. During retrieval, TOBUGraph
combines LLMs with the memory graph to achieve comprehensive recall through
graph traversal. Our evaluation using real user data demonstrates that
TOBUGraph outperforms multiple RAG implementations in both precision and
recall, significantly improving user experience through improved retrieval
accuracy and reduced hallucination.",Savini Kashmira
2024-12-07T05:49:14Z,http://arxiv.org/abs/2412.05547v1,"KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large
  Language Models","Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.",Weijie Chen
2024-12-08T13:36:42Z,http://arxiv.org/abs/2412.05937v1,"Accelerating Manufacturing Scale-Up from Material Discovery Using
  Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering
  Schematics Design","Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (PIDs)
are critical tools for industrial process design, control, and safety. However,
the generation of precise and regulation-compliant diagrams remains a
significant challenge, particularly in scaling breakthroughs from material
discovery to industrial production in an era of automation and digitalization.
This paper introduces an autonomous agentic framework to address these
challenges through a twostage approach involving knowledge acquisition and
generation. The framework integrates specialized sub-agents for retrieving and
synthesizing multimodal data from publicly available online sources and
constructs ontological knowledge graphs using a Graph Retrieval-Augmented
Generation (Graph RAG) paradigm. These capabilities enable the automation of
diagram generation and open-domain question answering (ODQA) tasks with high
contextual accuracy. Extensive empirical experiments demonstrate the frameworks
ability to deliver regulation-compliant diagrams with minimal expert
intervention, highlighting its practical utility for industrial applications.",Sakhinana Sagar Srinivas
2024-12-08T17:53:43Z,http://arxiv.org/abs/2412.06009v1,"1-800-SHARED-TASKS at RegNLP: Lexical Reranking of Semantic Retrieval
  (LeSeR) for Regulatory Question Answering","This paper presents the system description of our entry for the COLING 2025
RegNLP RIRAG (Regulatory Information Retrieval and Answer Generation)
challenge, focusing on leveraging advanced information retrieval and answer
generation techniques in regulatory domains. We experimented with a combination
of embedding models, including Stella, BGE, CDE, and Mpnet, and leveraged
fine-tuning and reranking for retrieving relevant documents in top ranks. We
utilized a novel approach, LeSeR, which achieved competitive results with a
recall@10 of 0.8201 and map@10 of 0.6655 for retrievals. This work highlights
the transformative potential of natural language processing techniques in
regulatory applications, offering insights into their capabilities for
implementing a retrieval augmented generation system while identifying areas
for future improvement in robustness and domain adaptation.",Jebish Purbey
2024-12-10T11:05:26Z,http://arxiv.org/abs/2412.07412v1,"Generating Knowledge Graphs from Large Language Models: A Comparative
  Study of GPT-4, LLaMA 2, and BERT","Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a
form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks
requiring structured reasoning and semantic understanding. However, creating
KGs for GraphRAGs remains a significant challenge due to accuracy and
scalability limitations of traditional methods. This paper introduces a novel
approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and
BERT to generate KGs directly from unstructured data, bypassing traditional
pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit
Distance, and Semantic Similarity, we evaluate the models' ability to generate
high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic
fidelity and structural accuracy, LLaMA 2 excels in lightweight,
domain-specific graphs, and BERT provides insights into challenges in
entity-relationship modeling. This study underscores the potential of LLMs to
streamline KG creation and enhance GraphRAG accessibility for real-world
applications, while setting a foundation for future advancements.",Ahan Bhatt
2024-12-13T17:53:29Z,http://arxiv.org/abs/2412.10313v1,MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation,"Regulatory documents are rich in nuanced terminology and specialized
semantics. FRAG systems: Frozen retrieval-augmented generators utilizing
pre-trained (or, frozen) components face consequent challenges with both
retriever and answering performance. We present a system that adapts the
retriever performance to the target domain using a multi-stage tuning (MST)
strategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders
used in vector stores using hard negative mining, (b) then uses a hybrid
retriever, combining sparse and dense retrievers using reciprocal rank fusion,
and then (c) adapts the cross-attention encoder by fine-tuning only the top-k
retrieved results. We benchmark the system performance on the dataset released
for the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We
achieve significant performance gains obtaining a top rank on the RegNLP
challenge leaderboard. We also show that a trivial answering approach games the
RePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing
this anomaly, we present important takeaways for future research.",Yash Malviya
2024-12-13T20:39:30Z,http://arxiv.org/abs/2412.10543v1,RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation,"RAG (Retrieval Augmented Generation) allows LLMs (large language models) to
generate better responses with external knowledge, but using more external
knowledge often improves generation quality at the expense of response delay.
Prior work either reduces the response delay (through better scheduling of RAG
queries) or strives to maximize quality (which involves tuning the RAG
workflow), but they fall short in optimizing the tradeoff between the delay and
quality of RAG responses. This paper presents RAGServe, the first RAG system
that jointly schedules queries and adapts the key RAG configurations of each
query, such as the number of retrieved text chunks and synthesis methods, in
order to balance quality optimization and response delay reduction. Using 4
popular RAG-QA datasets, we show that compared with the state-of-the-art RAG
optimization schemes, RAGServe reduces the generation latency by
$1.64-2.54\times$ without sacrificing generation quality.",Siddhant Ray
2024-12-14T05:06:43Z,http://arxiv.org/abs/2412.10684v1,Inference Scaling for Bridging Retrieval and Augmented Generation,"Retrieval-augmented generation (RAG) has emerged as a popular approach to
steering the output of a large language model (LLM) by incorporating retrieved
contexts as inputs. However, existing work observed the generator bias, such
that improving the retrieval results may negatively affect the outcome. In this
work, we show such bias can be mitigated, from inference scaling, aggregating
inference calls from the permuted order of retrieved contexts. The proposed
Mixture-of-Intervention (MOI) explicitly models the debiased utility of each
passage with multiple forward passes to construct a new ranking. We also show
that MOI can leverage the retriever's prior knowledge to reduce the
computational cost by minimizing the number of permutations considered and
lowering the cost per LLM call. We showcase the effectiveness of MOI on diverse
RAG tasks, improving ROUGE-L on MS MARCO and EM on HotpotQA benchmarks by ~7
points.",Youngwon Lee
2024-12-14T17:30:33Z,http://arxiv.org/abs/2412.10906v1,"SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability
  Report Generation","The rapid growth of the financial sector and the rising focus on
Environmental, Social, and Governance (ESG) considerations highlight the need
for advanced NLP tools. However, open-source LLMs proficient in both finance
and ESG domains remain scarce. To address this gap, we introduce SusGen-30K, a
category-balanced dataset comprising seven financial NLP tasks and ESG report
generation, and propose TCFD-Bench, a benchmark for evaluating sustainability
report generation. Leveraging this dataset, we developed SusGen-GPT, a suite of
models achieving state-of-the-art performance across six adapted and two
off-the-shelf tasks, trailing GPT-4 by only 2% despite using 7-8B parameters
compared to GPT-4's 1,700B. Based on this, we propose the SusGen system,
integrated with Retrieval-Augmented Generation (RAG), to assist in
sustainability report generation. This work demonstrates the efficiency of our
approach, advancing research in finance and ESG.",Qilong Wu
2024-12-16T12:44:42Z,http://arxiv.org/abs/2412.11722v2,"GHIssuemarket: A Sandbox Environment for SWE-Agents Economic
  Experimentation","Software engineering agents (swe-agents), as key innovations in intelligent
software engineering, are poised in the industry's end-of-programming debate to
transcend from assistance to primary roles. we argue the importance of
swe-agents' economic viability to their transcendence -- defined as their
capacity to maintain efficient operations in constrained environments -- and
propose its exploration via software engineering economics experimentation.we
introduce ghissuemarket sandbox, a controlled virtual environment for
swe-agents' economic experimentation, simulating the environment of an
envisioned peer-to-peer multiagent system for github issues outsourcing
auctions. in this controlled setting, autonomous swe-agents auction and bid on
github issues, leveraging real-time communication, a built-in
retrieval-augmented generation (rag) interface for effective decision-making,
and instant cryptocurrency micropayments. we open-source our software
artifacts, discuss our sandbox engineering decisions, and advocate towards
swe-agents' economic exploration -- an emerging field we intend to pursue under
the term intelligent software engineering economics (isee).",Mohamed A. Fouad
2024-12-16T16:03:25Z,http://arxiv.org/abs/2412.11919v1,"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained
  Evidence within Generation","Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.",Xiaoxi Li
2024-12-16T19:11:55Z,http://arxiv.org/abs/2412.12300v1,Unanswerability Evaluation for Retreival Augmented Generation,"Existing evaluation frameworks for retrieval-augmented generation (RAG)
systems focus on answerable queries, but they overlook the importance of
appropriately rejecting unanswerable requests. In this paper, we introduce
UAEval4RAG, a framework designed to evaluate whether RAG systems can handle
unanswerable queries effectively. We define a taxonomy with six unanswerable
categories, and UAEval4RAG automatically synthesizes diverse and challenging
queries for any given knowledge base with unanswered ratio and acceptable ratio
metrics. We conduct experiments with various RAG components, including
retrieval models, rewriting methods, rerankers, language models, and prompting
strategies, and reveal hidden trade-offs in performance of RAG systems. Our
findings highlight the critical role of component selection and prompt design
in optimizing RAG systems to balance the accuracy of answerable queries with
high rejection rates of unanswerable ones. UAEval4RAG provides valuable
insights and tools for developing more robust and reliable RAG systems.",Xiangyu Peng
2024-12-16T19:40:26Z,http://arxiv.org/abs/2412.12322v1,"RAG Playground: A Framework for Systematic Evaluation of Retrieval
  Strategies and Prompt Engineering in RAG Systems","We present RAG Playground, an open-source framework for systematic evaluation
of Retrieval-Augmented Generation (RAG) systems. The framework implements and
compares three retrieval approaches: naive vector search, reranking, and hybrid
vector-keyword search, combined with ReAct agents using different prompting
strategies. We introduce a comprehensive evaluation framework with novel
metrics and provide empirical results comparing different language models
(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our
experiments demonstrate significant performance improvements through hybrid
search methods and structured self-evaluation prompting, achieving up to 72.7%
pass rate on our multi-metric evaluation framework. The results also highlight
the importance of prompt engineering in RAG systems, with our custom-prompted
agents showing consistent improvements in retrieval accuracy and response
quality.",Ioannis Papadimitriou
2024-12-16T21:36:03Z,http://arxiv.org/abs/2412.12364v1,"LogBabylon: A Unified Framework for Cross-Log File Integration and
  Analysis","Logs are critical resources that record events, activities, or messages
produced by software applications, operating systems, servers, and network
devices. However, consolidating the heterogeneous logs and cross-referencing
them is challenging and complicated. Manually analyzing the log data is
time-consuming and prone to errors. LogBabylon is a centralized log data
consolidating solution that leverages Large Language Models (LLMs) integrated
with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the
log data in a human-readable way and adds insight analysis of the system
performance and anomaly alerts. It provides a paramount view of the system
landscape, enabling proactive management and rapid incident response.
LogBabylon consolidates diverse log sources and enhances the extracted
information's accuracy and relevancy. This facilitates a deeper understanding
of log data, supporting more effective decision-making and operational
efficiency. Furthermore, LogBabylon streamlines the log analysis process,
significantly reducing the time and effort required to interpret complex
datasets. Its capabilities extend to generating context-aware insights,
offering an invaluable tool for continuous monitoring, performance
optimization, and security assurance in dynamic computing environments.",Rabimba Karanjai
2024-12-17T01:23:45Z,http://arxiv.org/abs/2412.12447v2,"PERC: Plan-As-Query Example Retrieval for Underrepresented Code
  Generation","Code generation with large language models has shown significant promise,
especially when employing retrieval-augmented generation (RAG) with few-shot
examples. However, selecting effective examples that enhance generation quality
remains a challenging task, particularly when the target programming language
(PL) is underrepresented. In this study, we present two key findings: (1)
retrieving examples whose presented algorithmic plans can be referenced for
generating the desired behavior significantly improves generation accuracy, and
(2) converting code into pseudocode effectively captures such algorithmic
plans, enhancing retrieval quality even when the source and the target PLs are
different. Based on these findings, we propose Plan-as-query Example Retrieval
for few-shot prompting in Code generation (PERC), a novel framework that
utilizes algorithmic plans to identify and retrieve effective examples. We
validate the effectiveness of PERC through extensive experiments on the
CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms
the state-of-the-art RAG methods in code generation, both when the source and
target programming languages match or differ, highlighting its adaptability and
robustness in diverse coding environments.",Jaeseok Yoo
2024-12-17T07:49:49Z,http://arxiv.org/abs/2412.12632v1,"What External Knowledge is Preferred by LLMs? Characterizing and
  Exploring Chain of Evidence in Imperfect Context","Incorporating external knowledge into large language models (LLMs) has
emerged as a promising approach to mitigate outdated knowledge and
hallucination in LLMs. However, external knowledge is often imperfect. In
addition to useful knowledge, external knowledge is rich in irrelevant or
misinformation in the context that can impair the reliability of LLM responses.
This paper focuses on LLMs' preferred external knowledge in imperfect contexts
when handling multi-hop QA. Inspired by criminal procedural law's Chain of
Evidence (CoE), we characterize that knowledge preferred by LLMs should
maintain both relevance to the question and mutual support among knowledge
pieces. Accordingly, we propose an automated CoE discrimination approach and
explore LLMs' preferences from their effectiveness, faithfulness and
robustness, as well as CoE's usability in a naive Retrieval-Augmented
Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances
LLMs through more accurate generation, stronger answer faithfulness, better
robustness against knowledge conflict, and improved performance in a popular
RAG case.",Zhiyuan Chang
2024-12-18T08:04:57Z,http://arxiv.org/abs/2412.13582v1,EvoWiki: Evaluating LLMs on Evolving Knowledge,"Knowledge utilization is a critical aspect of LLMs, and understanding how
they adapt to evolving knowledge is essential for their effective deployment.
However, existing benchmarks are predominantly static, failing to capture the
evolving nature of LLMs and knowledge, leading to inaccuracies and
vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an
evolving dataset designed to reflect knowledge evolution by categorizing
information into stable, evolved, and uncharted states. EvoWiki is fully
auto-updatable, enabling precise evaluation of continuously changing knowledge
and newly released LLMs. Through experiments with Retrieval-Augmented
Generation (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs
adapt to evolving knowledge. Our results indicate that current models often
struggle with evolved knowledge, frequently providing outdated or incorrect
responses. Moreover, the dataset highlights a synergistic effect between RAG
and CL, demonstrating their potential to better adapt to evolving knowledge.
EvoWiki provides a robust benchmark for advancing future research on the
knowledge evolution capabilities of large language models.",Wei Tang
2024-12-18T11:00:58Z,http://arxiv.org/abs/2412.13720v1,"Federated Learning and RAG Integration: A Scalable Approach for Medical
  Large Language Models","This study analyzes the performance of domain-specific Large Language Models
(LLMs) for the medical field by integrating Retrieval-Augmented Generation
(RAG) systems within a federated learning framework. Leveraging the inherent
advantages of federated learning, such as preserving data privacy and enabling
distributed computation, this research explores the integration of RAG systems
with models trained under varying client configurations to optimize
performance. Experimental results demonstrate that the federated learning-based
models integrated with RAG systems consistently outperform their non-integrated
counterparts across all evaluation metrics. This study highlights the potential
of combining federated learning and RAG systems for developing domain-specific
LLMs in the medical field, providing a scalable and privacy-preserving solution
for enhancing text generation capabilities.",Jincheol Jung
2024-12-18T15:07:23Z,http://arxiv.org/abs/2412.13924v1,Language verY Rare for All,"In the quest to overcome language barriers, encoder-decoder models like NLLB
have expanded machine translation to rare languages, with some models (e.g.,
NLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform
well in translation, open LLMs prove highly competitive when fine-tuned for
specific tasks involving unknown corpora. We introduce LYRA (Language verY Rare
for All), a novel approach that combines open LLM fine-tuning,
retrieval-augmented generation (RAG), and transfer learning from related
high-resource languages. This study is exclusively focused on single-GPU
training to facilitate ease of adoption. Our study focuses on two-way
translation between French and Mon\'egasque, a rare language unsupported by
existing translation tools due to limited corpus availability. Our results
demonstrate LYRA's effectiveness, frequently surpassing and consistently
matching state-of-the-art encoder-decoder models in rare language translation.",Ibrahim Merad
2024-12-18T16:07:32Z,http://arxiv.org/abs/2412.13988v1,RAG for Effective Supply Chain Security Questionnaire Automation,"In an era where digital security is crucial, efficient processing of
security-related inquiries through supply chain security questionnaires is
imperative. This paper introduces a novel approach using Natural Language
Processing (NLP) and Retrieval-Augmented Generation (RAG) to automate these
responses. We developed QuestSecure, a system that interprets diverse document
formats and generates precise responses by integrating large language models
(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure
significantly improves response accuracy and operational efficiency. By
employing advanced NLP techniques and tailored retrieval mechanisms, the system
consistently produces contextually relevant and semantically rich responses,
reducing cognitive load on security teams and minimizing potential errors. This
research offers promising avenues for automating complex security management
tasks, enhancing organizational security processes.",Zaynab Batool Reza
2024-12-10T21:52:35Z,http://arxiv.org/abs/2412.14191v1,"Ontology-Aware RAG for Improved Question-Answering in Cybersecurity
  Education","Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.",Chengshuai Zhao
2024-12-19T11:30:07Z,http://arxiv.org/abs/2412.14751v1,"Query pipeline optimization for cancer patient question answering
  systems","Retrieval-augmented generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by using query pipelines to retrieve relevant external
information and grounding responses in retrieved knowledge. However, query
pipeline optimization for cancer patient question-answering (CPQA) systems
requires separately optimizing multiple components with domain-specific
considerations. We propose a novel three-aspect optimization approach for the
RAG query pipeline in CPQA systems, utilizing public biomedical databases like
PubMed and PubMed Central. Our optimization includes: (1) document retrieval,
utilizing a comparative analysis of NCBI resources and introducing Hybrid
Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,
identifying optimal pairings of dense retrievers and rerankers; and (3)
semantic representation, introducing Semantic Enhanced Overlap Segmentation
(SEOS) for improved contextual understanding. On a custom-developed dataset
tailored for cancer-related inquiries, our optimized RAG approach improved the
answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and
about 3% over a naive RAG setup. This study highlights the importance of
domain-specific query optimization in realizing the full potential of RAG and
provides a robust framework for building more accurate and reliable CPQA
systems, advancing the development of RAG-based biomedical systems.",Maolin He
2024-12-19T15:44:01Z,http://arxiv.org/abs/2412.14964v1,Knowledge Injection via Prompt Distillation,"In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.",Kalle Kujanpää
2024-12-19T17:48:23Z,http://arxiv.org/abs/2412.15101v1,"Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering
  with Temporal Adaptability","Retrieve-augmented generation (RAG) frameworks have emerged as a promising
solution to multi-hop question answering(QA) tasks since it enables large
language models (LLMs) to incorporate external knowledge and mitigate their
inherent knowledge deficiencies. Despite this progress, existing RAG
frameworks, which usually follows the retrieve-then-read paradigm, often
struggle with multi-hop QA with temporal information since it has difficulty
retrieving and synthesizing accurate time-related information. To address the
challenge, this paper proposes a novel framework called review-then-refine,
which aims to enhance LLM performance in multi-hop QA scenarios with temporal
information. Our approach begins with a review phase, where decomposed
sub-queries are dynamically rewritten with temporal information, allowing for
subsequent adaptive retrieval and reasoning process. In addition, we implement
adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing
the potential for hallucinations. In the subsequent refine phase, the LLM
synthesizes the retrieved information from each sub-query along with its
internal knowledge to formulate a coherent answer. Extensive experimental
results across multiple datasets demonstrate the effectiveness of our proposed
framework, highlighting its potential to significantly improve multi-hop QA
capabilities in LLMs.",Xiangsen Chen
2024-12-19T18:57:11Z,http://arxiv.org/abs/2412.15189v1,"Face the Facts! Evaluating RAG-based Fact-checking Pipelines in
  Realistic Settings","Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under
more realistic scenarios, RAG-based methods for the generation of verdicts -
i.e., short texts discussing the veracity of a claim - evaluating them on
stylistically complex claims and heterogeneous, yet reliable, knowledge bases.
Our findings show a complex landscape, where, for example, LLM-based retrievers
outperform other retrieval techniques, though they still struggle with
heterogeneous knowledge bases; larger models excel in verdict faithfulness,
while smaller models provide better context adherence, with human evaluations
favouring zero-shot and one-shot approaches for informativeness, and fine-tuned
models for emotional alignment.",Daniel Russo
2024-12-16T12:04:22Z,http://arxiv.org/abs/2412.15258v1,DisEmbed: Transforming Disease Understanding through Embeddings,"The medical domain is vast and diverse, with many existing embedding models
focused on general healthcare applications. However, these models often
struggle to capture a deep understanding of diseases due to their broad
generalization across the entire medical field. To address this gap, I present
DisEmbed, a disease-focused embedding model. DisEmbed is trained on a synthetic
dataset specifically curated to include disease descriptions, symptoms, and
disease-related Q\&A pairs, making it uniquely suited for disease-related
tasks. For evaluation, I benchmarked DisEmbed against existing medical models
using disease-specific datasets and the triplet evaluation method. My results
demonstrate that DisEmbed outperforms other models, particularly in identifying
disease-related contexts and distinguishing between similar diseases. This
makes DisEmbed highly valuable for disease-specific use cases, including
retrieval-augmented generation (RAG) tasks, where its performance is
particularly robust.",Salman Faroz
2024-12-17T11:18:14Z,http://arxiv.org/abs/2412.15271v1,"A MapReduce Approach to Effectively Utilize Long Context Information in
  Retrieval Augmented Language Models","While holding great promise for improving and facilitating healthcare, large
language models (LLMs) struggle to produce up-to-date responses on evolving
topics due to outdated knowledge or hallucination. Retrieval-augmented
generation (RAG) is a pivotal innovation that improves the accuracy and
relevance of LLM responses by integrating LLMs with a search engine and
external sources of knowledge. However, the quality of RAG responses can be
largely impacted by the rank and density of key information in the retrieval
results, such as the ""lost-in-the-middle"" problem. In this work, we aim to
improve the robustness and reliability of the RAG workflow in the medical
domain. Specifically, we propose a map-reduce strategy, BriefContext, to combat
the ""lost-in-the-middle"" issue without modifying the model weights. We
demonstrated the advantage of the workflow with various LLM backbones and on
multiple QA datasets. This method promises to improve the safety and
reliability of LLMs deployed in healthcare domains.",Gongbo Zhang
2024-12-18T04:08:18Z,http://arxiv.org/abs/2412.15280v1,Context-DPO: Aligning Language Models for Context-Faithfulness,"Reliable responses from large language models (LLMs) require adherence to
user instructions and retrieved information. While alignment techniques help
LLMs align with human intentions and values, improving context-faithfulness
through alignment remains underexplored. To address this, we propose
$\textbf{Context-DPO}$, the first alignment method specifically designed to
enhance LLMs' context-faithfulness. We introduce $\textbf{ConFiQA}$, a
benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with
knowledge conflicts to evaluate context-faithfulness. By leveraging faithful
and stubborn responses to questions with provided context from ConFiQA, our
Context-DPO aligns LLMs through direct preference optimization. Extensive
experiments demonstrate that our Context-DPO significantly improves
context-faithfulness, achieving 35% to 280% improvements on popular open-source
models. Further analysis demonstrates that Context-DPO preserves LLMs'
generative capabilities while providing interpretable insights into context
utilization. Our code and data are released at
https://github.com/byronBBL/Context-DPO",Baolong Bi
2024-12-20T03:58:27Z,http://arxiv.org/abs/2412.15540v1,"MRAG: A Modular Retrieval Framework for Time-Sensitive Question
  Answering","Understanding temporal relations and answering time-sensitive questions is
crucial yet a challenging task for question-answering systems powered by large
language models (LLMs). Existing approaches either update the parametric
knowledge of LLMs with new facts, which is resource-intensive and often
impractical, or integrate LLMs with external knowledge retrieval (i.e.,
retrieval-augmented generation). However, off-the-shelf retrievers often
struggle to identify relevant documents that require intensive temporal
reasoning. To systematically study time-sensitive question answering, we
introduce the TempRAGEval benchmark, which repurposes existing datasets by
incorporating temporal perturbations and gold evidence labels. As anticipated,
all existing retrieval methods struggle with these temporal reasoning-intensive
questions. We further propose Modular Retrieval (MRAG), a trainless framework
that includes three modules: (1) Question Processing that decomposes question
into a main content and a temporal constraint; (2) Retrieval and Summarization
that retrieves evidence and uses LLMs to summarize according to the main
content; (3) Semantic-Temporal Hybrid Ranking that scores each evidence
summarization based on both semantic and temporal relevance. On TempRAGEval,
MRAG significantly outperforms baseline retrievers in retrieval performance,
leading to further improvements in final answer accuracy.",Zhang Siyue
2024-12-20T06:58:32Z,http://arxiv.org/abs/2412.15605v1,"Don't Do RAG: When Cache-Augmented Generation is All You Need for
  Knowledge Tasks","Retrieval-augmented generation (RAG) has gained traction as a powerful
approach for enhancing language models by integrating external knowledge
sources. However, RAG introduces challenges such as retrieval latency,
potential errors in document selection, and increased system complexity. With
the advent of large language models (LLMs) featuring significantly extended
context windows, this paper proposes an alternative paradigm, cache-augmented
generation (CAG) that bypasses real-time retrieval. Our method involves
preloading all relevant resources, especially when the documents or knowledge
for retrieval are of a limited and manageable size, into the LLM's extended
context and caching its runtime parameters. During inference, the model
utilizes these preloaded parameters to answer queries without additional
retrieval steps. Comparative analyses reveal that CAG eliminates retrieval
latency and minimizes retrieval errors while maintaining context relevance.
Performance evaluations across multiple benchmarks highlight scenarios where
long-context LLMs either outperform or complement traditional RAG pipelines.
These findings suggest that, for certain applications, particularly those with
a constrained knowledge base, CAG provide a streamlined and efficient
alternative to RAG, achieving comparable or superior results with reduced
complexity.",Brian J Chan
2024-12-20T13:54:57Z,http://arxiv.org/abs/2412.15902v1,"On the Suitability of pre-trained foundational LLMs for Analysis in
  German Legal Education","We show that current open-source foundational LLMs possess instruction
capability and German legal background knowledge that is sufficient for some
legal analysis in an educational context. However, model capability breaks down
in very specific tasks, such as the classification of ""Gutachtenstil"" appraisal
style components, or with complex contexts, such as complete legal opinions.
Even with extended context and effective prompting strategies, they cannot
match the Bag-of-Words baseline. To combat this, we introduce a Retrieval
Augmented Generation based prompt example selection method that substantially
improves predictions in high data availability scenarios. We further evaluate
the performance of pre-trained LLMs on two standard tasks for argument mining
and automated essay scoring and find it to be more adequate. Throughout,
pre-trained LLMs improve upon the baseline in scenarios with little or no
labeled data with Chain-of-Thought prompting further helping in the zero-shot
case.",Lorenz Wendlinger
2024-12-20T17:33:50Z,http://arxiv.org/abs/2412.16086v1,"Towards Interpretable Radiology Report Generation via Concept
  Bottlenecks using a Multi-Agentic RAG","Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",Hasan Md Tusfiqur Alam
2024-12-09T17:22:40Z,http://arxiv.org/abs/2412.16176v1,"Efficient VoIP Communications through LLM-based Real-Time Speech
  Reconstruction and Call Prioritization for Emergency Services","Emergency communication systems face disruptions due to packet loss,
bandwidth constraints, poor signal quality, delays, and jitter in VoIP systems,
leading to degraded real-time service quality. Victims in distress often
struggle to convey critical information due to panic, speech disorders, and
background noise, further complicating dispatchers' ability to assess
situations accurately. Staffing shortages in emergency centers exacerbate
delays in coordination and assistance. This paper proposes leveraging Large
Language Models (LLMs) to address these challenges by reconstructing incomplete
speech, filling contextual gaps, and prioritizing calls based on severity. The
system integrates real-time transcription with Retrieval-Augmented Generation
(RAG) to generate contextual responses, using Twilio and AssemblyAI APIs for
seamless implementation. Evaluation shows high precision, favorable BLEU and
ROUGE scores, and alignment with real-world needs, demonstrating the model's
potential to optimize emergency response workflows and prioritize critical
cases effectively.",Danush Venkateshperumal
2024-12-21T00:34:52Z,http://arxiv.org/abs/2412.16412v1,"InfoTech Assistant : A Multimodal Conversational Agent for
  InfoTechnology Web Portal Queries","This pilot study presents the development of the InfoTech Assistant, a
domain-specific, multimodal chatbot engineered to address queries in bridge
evaluation and infrastructure technology. By integrating web data scraping,
large language models (LLMs), and Retrieval-Augmented Generation (RAG), the
InfoTech Assistant provides accurate and contextually relevant responses. Data,
including textual descriptions and images, are sourced from publicly available
documents on the InfoTechnology website and organized in JSON format to
facilitate efficient querying. The architecture of the system includes an
HTML-based interface and a Flask back end connected to the Llama 3.1 model via
LLM Studio. Evaluation results show approximately 95 percent accuracy on
domain-specific tasks, with high similarity scores confirming the quality of
response matching. This RAG-enhanced setup enables the InfoTech Assistant to
handle complex, multimodal queries, offering both textual and visual
information in its responses. The InfoTech Assistant demonstrates strong
potential as a dependable tool for infrastructure professionals, delivering
high accuracy and relevance in its domain-specific outputs.",Sai Surya Gadiraju
2024-12-22T14:17:12Z,http://arxiv.org/abs/2412.17032v1,"MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on
  New and Tail Knowledge","Large language models (LLMs) have demonstrated impressive capabilities in
various reasoning tasks but face significant challenges with complex,
knowledge-intensive multi-hop queries, particularly those involving new or
long-tail knowledge. Existing benchmarks often fail to fully address these
challenges. To bridge this gap, we introduce MINTQA (Multi-hop Question
Answering on New and Tail Knowledge), a comprehensive benchmark to evaluate
LLMs' capabilities in multi-hop reasoning across four critical dimensions:
question handling strategy, sub-question generation, retrieval-augmented
generation, and iterative or dynamic decomposition and retrieval. MINTQA
comprises 10,479 question-answer pairs for evaluating new knowledge and 17,887
pairs for assessing long-tail knowledge, with each question equipped with
corresponding sub-questions and answers. Our systematic evaluation of 22
state-of-the-art LLMs on MINTQA reveals significant limitations in their
ability to handle complex knowledge base queries, particularly in handling new
or unpopular knowledge. Our findings highlight critical challenges and offer
insights for advancing multi-hop reasoning capabilities. The MINTQA benchmark
is available at https://github.com/probe2/multi-hop/.",Jie He
2024-12-23T11:24:04Z,http://arxiv.org/abs/2412.17483v1,"A Silver Bullet or a Compromise for Full Attention? A Comprehensive
  Study of Gist Token-based Context Compression","In this work, we provide a thorough investigation of gist-based context
compression methods to improve long-context processing in large language
models. We focus on two key questions: (1) How well can these methods replace
full attention models? and (2) What potential failure patterns arise due to
compression? Through extensive experiments, we show that while gist-based
compression can achieve near-lossless performance on tasks like
retrieval-augmented generation and long-document QA, it faces challenges in
tasks like synthetic recall. Furthermore, we identify three key failure
patterns: lost by the boundary, lost if surprise, and lost along the way. To
mitigate these issues, we propose two effective strategies: fine-grained
autoencoding, which enhances the reconstruction of original token information,
and segment-wise token importance estimation, which adjusts optimization based
on token dependencies. Our work provides valuable insights into the
understanding of gist token-based context compression and offers practical
strategies for improving compression capabilities.",Chenlong Deng
2024-12-23T13:26:04Z,http://arxiv.org/abs/2412.17558v1,A Survey of Query Optimization in Large Language Models,"\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the
efficiency and quality of Large Language Models (LLMs) in understanding and
answering queries, especially complex ones in scenarios like
Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the
limitations of LLMs by dynamically retrieving and leveraging up-to-date
relevant information, which provides a cost-effective solution to the challenge
of LLMs producing plausible but potentially inaccurate responses. Recently, as
RAG evolves and incorporates multiple components that influence its
performance, QO has emerged as a critical element, playing a pivotal role in
determining the effectiveness of RAG's retrieval stage in accurately sourcing
the necessary multiple pieces of evidence to answer queries correctly. In this
paper, we trace the evolution of QO techniques by summarizing and analyzing
significant studies. Through an organized framework and categorization, we aim
to consolidate existing QO techniques in RAG, elucidate their technological
foundations, and highlight their potential to enhance the versatility and
applications of LLMs.",Mingyang Song
2024-12-23T16:16:30Z,http://arxiv.org/abs/2412.17690v2,"RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF
  for Conversational QA over KGs with RAG","Conversational question answering (ConvQA) is a convenient means of searching
over RDF knowledge graphs (KGs), where a prevalent approach is to translate
natural language questions to SPARQL queries. However, SPARQL has certain
shortcomings: (i) it is brittle for complex intents and conversational
questions, and (ii) it is not suitable for more abstract needs. Instead, we
propose a novel two-pronged system where we fuse: (i) SQL-query results over a
database automatically derived from the KG, and (ii) text-search results over
verbalizations of KG facts. Our pipeline supports iterative retrieval: when the
results of any branch are found to be unsatisfactory, the system can
automatically opt for further rounds. We put everything together in a retrieval
augmented generation (RAG) setup, where an LLM generates a coherent response
from accumulated search results. We demonstrate the superiority of our proposed
system over several baselines on a knowledge graph of BMW automobiles.",Rishiraj Saha Roy
2024-12-23T19:54:28Z,http://arxiv.org/abs/2412.17942v1,"Contrato360 2.0: A Document and Database-Driven Question-Answer System
  using Large Language Models and Agents","We present a question-and-answer (Q\&A) application designed to support the
contract management process by leveraging combined information from contract
documents (PDFs) and data retrieved from contract management systems
(database). This data is processed by a large language model (LLM) to provide
precise and relevant answers. The accuracy of these responses is further
enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL
techniques, and agents that dynamically orchestrate the workflow. These
techniques eliminate the need to retrain the language model. Additionally, we
employed Prompt Engineering to fine-tune the focus of responses. Our findings
demonstrate that this multi-agent orchestration and combination of techniques
significantly improve the relevance and accuracy of the answers, offering a
promising direction for future information systems.",Antony Seabra
2024-12-23T20:28:20Z,http://arxiv.org/abs/2412.17964v1,"Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source
  Question-Answer Systems using Large Language Models","We propose a methodology that combines several advanced techniques in Large
Language Model (LLM) retrieval to support the development of robust,
multi-source question-answer systems. This methodology is designed to integrate
information from diverse data sources, including unstructured documents (PDFs)
and structured databases, through a coordinated multi-agent orchestration and
dynamic retrieval approach. Our methodology leverages specialized agents-such
as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents -
that dynamically select the most appropriate retrieval strategy based on the
nature of each query. To further improve accuracy and contextual relevance, we
employ dynamic prompt engineering, which adapts in real time to query-specific
contexts. The methodology's effectiveness is demonstrated within the domain of
Contract Management, where complex queries often require seamless interaction
between unstructured and structured data. Our results indicate that this
approach enhances response accuracy and relevance, offering a versatile and
scalable framework for developing question-answer systems that can operate
across various domains and data sources.",Antony Seabra
2024-12-24T00:55:59Z,http://arxiv.org/abs/2412.18069v1,Improving Factuality with Explicit Working Memory,"Large language models can generate factually inaccurate content, a problem
known as hallucination. Recent works have built upon retrieved-augmented
generation to improve factuality through iterative prompting but these methods
are limited by the traditional RAG design. To address these challenges, we
introduce EWE (Explicit Working Memory), a novel approach that enhances
factuality in long-form text generation by integrating a working memory that
receives real-time feedback from external resources. The memory is refreshed
based on online fact-checking and retrieval feedback, allowing EWE to rectify
false claims during the generation process and ensure more accurate and
reliable outputs. Our experiments demonstrate that Ewe outperforms strong
baselines on four fact-seeking long-form generation datasets, increasing the
factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing
the helpfulness of the responses. Further analysis reveals that the design of
rules for memory updates, configurations of memory units, and the quality of
the retrieval datastore are crucial factors for influencing model performance.",Mingda Chen
2024-12-24T02:08:38Z,http://arxiv.org/abs/2412.18093v1,"Molly: Making Large Language Model Agents Solve Python Problem More
  Logically","Applying large language models (LLMs) as teaching assists has attracted much
attention as an integral part of intelligent education, particularly in
computing courses. To reduce the gap between the LLMs and the computer
programming education expert, fine-tuning and retrieval augmented generation
(RAG) are the two mainstream methods in existing researches. However,
fine-tuning for specific tasks is resource-intensive and may diminish the
model`s generalization capabilities. RAG can perform well on reducing the
illusion of LLMs, but the generation of irrelevant factual content during
reasoning can cause significant confusion for learners. To address these
problems, we introduce the Molly agent, focusing on solving the proposed
problem encountered by learners when learning Python programming language. Our
agent automatically parse the learners' questioning intent through a
scenario-based interaction, enabling precise retrieval of relevant documents
from the constructed knowledge base. At generation stage, the agent reflect on
the generated responses to ensure that they not only align with factual content
but also effectively answer the user's queries. Extensive experimentation on a
constructed Chinese Python QA dataset shows the effectiveness of the Molly
agent, indicating an enhancement in its performance for providing useful
responses to Python questions.",Rui Xiao
2024-12-24T09:03:57Z,http://arxiv.org/abs/2412.18295v1,Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases,"The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in
several real-world services triggers severe concerns about their security. A
RAG system improves the generative capabilities of a Large Language Models
(LLM) by a retrieval mechanism which operates on a private knowledge base,
whose unintended exposure could lead to severe consequences, including breaches
of private and sensitive information. This paper presents a black-box attack to
force a RAG system to leak its private knowledge base which, differently from
existing approaches, is adaptive and automatic. A relevance-based mechanism and
an attacker-side open-source LLM favor the generation of effective queries to
leak most of the (hidden) knowledge base. Extensive experimentation proves the
quality of the proposed algorithm in different RAG pipelines and domains,
comparing to very recent related approaches, which turn out to be either not
fully black-box, not adaptive, or not based on open-source models. The findings
from our study remark the urgent need for more robust privacy safeguards in the
design and deployment of RAG systems.",Christian Di Maio
2020-06-09T17:09:29Z,http://arxiv.org/abs/2006.05405v5,Retrieval-Augmented Generation for Code Summarization via Hybrid GNN,"Source code summarization aims to generate natural language summaries from
structured code snippets for better understanding code functionalities.
However, automatic code summarization is challenging due to the complexity of
the source code and the language gap between the source code and natural
language summaries. Most previous approaches either rely on retrieval-based
(which can take advantage of similar examples seen from the retrieval database,
but have low generalization performance) or generation-based methods (which
have better generalization performance, but cannot take advantage of similar
examples). This paper proposes a novel retrieval-augmented mechanism to combine
the benefits of both worlds. Furthermore, to mitigate the limitation of Graph
Neural Networks (GNNs) on capturing global graph structure information of
source code, we propose a novel attention-based dynamic graph to complement the
static graph representation of the source code, and design a hybrid message
passing GNN for capturing both the local and global structural information. To
evaluate the proposed approach, we release a new challenging benchmark, crawled
from diversified large-scale open-source C projects (total 95k+ unique
functions in the dataset). Our method achieves the state-of-the-art
performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of
BLEU-4, ROUGE-L and METEOR.",Shangqing Liu
2021-04-17T18:24:51Z,http://arxiv.org/abs/2104.08610v1,Zero-shot Slot Filling with DPR and RAG,"The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.",Michael Glass
2022-10-06T13:58:03Z,http://arxiv.org/abs/2210.02928v2,"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question
  Answering over Images and Text","While language Models store a massive amount of world knowledge implicitly in
their parameters, even very large models often fail to encode information about
rare entities and events, while incurring huge computational costs. Recently,
retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated
world knowledge into language generation by leveraging an external
non-parametric index and have demonstrated impressive performance with
constrained model sizes. However, these methods are restricted to retrieving
only textual knowledge, neglecting the ubiquitous amount of knowledge in other
modalities like images -- much of which contains information not covered by any
text. To address this limitation, we propose the first Multimodal
Retrieval-Augmented Transformer (MuRAG), which accesses an external
non-parametric multimodal memory to augment language generation. MuRAG is
pre-trained with a mixture of large-scale image-text and text-only corpora
using a joint contrastive and generative loss. We perform experiments on two
different datasets that require retrieving and reasoning over both images and
text to answer a given query: WebQA, and MultimodalQA. Our results show that
MuRAG achieves state-of-the-art accuracy, outperforming existing models by
10-20\% absolute on both datasets and under both distractor and full-wiki
settings.",Wenhu Chen
2022-10-23T16:34:39Z,http://arxiv.org/abs/2210.12777v4,"Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful
  Clinical Medicine","Language models (LMs), including large language models (such as ChatGPT),
have the potential to assist clinicians in generating various clinical notes.
However, LMs are prone to produce ``hallucinations'', i.e., generated content
that is not aligned with facts and knowledge. In this paper, we propose the
Re$^3$Writer method with retrieval-augmented generation and knowledge-grounded
reasoning to enable LMs to generate faithful clinical texts. We demonstrate the
effectiveness of our method in generating patient discharge instructions. It
requires the LMs not to only understand the patients' long clinical documents,
i.e., the health records during hospitalization, but also to generate critical
instructional information provided both to carers and to the patient at the
time of discharge. The proposed Re$^3$Writer imitates the working patterns of
physicians to first \textbf{re}trieve related working experience from
historical instructions written by physicians, then \textbf{re}ason related
medical knowledge. Finally, it \textbf{re}fines the retrieved working
experience and reasoned medical knowledge to extract useful information, which
is used to generate the discharge instructions for previously-unseen patients.
Our experiments show that, using our method, the performance of five
representative LMs can be substantially boosted across all metrics. Meanwhile,
we show results from human evaluations to measure the effectiveness in terms of
fluency, faithfulness, and comprehensiveness.",Fenglin Liu
2023-01-06T06:47:21Z,http://arxiv.org/abs/2301.02401v1,"You Truly Understand What I Need: Intellectual and Friendly Dialogue
  Agents grounding Knowledge and Persona","To build a conversational agent that interacts fluently with humans, previous
studies blend knowledge or personal profile into the pre-trained language
model. However, the model that considers knowledge and persona at the same time
is still limited, leading to hallucination and a passive way of using personas.
We propose an effective dialogue agent that grounds external knowledge and
persona simultaneously. The agent selects the proper knowledge and persona to
use for generating the answers with our candidate scoring implemented with a
poly-encoder. Then, our model generates the utterance with lesser hallucination
and more engagingness utilizing retrieval augmented generation with
knowledge-persona enhanced query. We conduct experiments on the
persona-knowledge chat and achieve state-of-the-art performance in grounding
and generation tasks on the automatic metrics. Moreover, we validate the
answers from the models regarding hallucination and engagingness through human
evaluation and qualitative results. We show our retriever's effectiveness in
extracting relevant documents compared to the other previous retrievers, along
with the comparison of multiple candidate scoring methods. Code is available at
https://github.com/dlawjddn803/INFO",Jungwoo Lim
2023-08-09T02:02:46Z,http://arxiv.org/abs/2308.04662v3,"VulLibGen: Generating Names of Vulnerability-Affected Packages via a
  Large Language Model","Security practitioners maintain vulnerability reports (e.g., GitHub Advisory)
to help developers mitigate security risks. An important task for these
databases is automatically extracting structured information mentioned in the
report, e.g., the affected software packages, to accelerate the defense of the
vulnerability ecosystem.
  However, it is challenging for existing work on affected package
identification to achieve a high accuracy. One reason is that all existing work
focuses on relatively smaller models, thus they cannot harness the knowledge
and semantic capabilities of large language models.
  To address this limitation, we propose VulLibGen, the first method to use LLM
for affected package identification. In contrast to existing work, VulLibGen
proposes the novel idea to directly generate the affected package. To improve
the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval
augmented generation (RAG) and a local search algorithm. The local search
algorithm is a novel postprocessing algorithm we introduce for reducing the
hallucination of the generated packages. Our evaluation results show that
VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages
in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go)
while the best average accuracy in previous work is 0.721. Additionally,
VulLibGen has high value to security practice: we submitted 60 <vulnerability,
affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them
have been accepted and merged and 20 are pending approval. Our code and dataset
can be found in the attachments.",Tianyu Chen
2023-09-03T07:03:17Z,http://arxiv.org/abs/2309.01105v2,"A Study on the Implementation of Generative AI Services Using an
  Enterprise Data-Based LLM Application Architecture","This study presents a method for implementing generative AI services by
utilizing the Large Language Models (LLM) application architecture. With recent
advancements in generative AI technology, LLMs have gained prominence across
various domains. In this context, the research addresses the challenge of
information scarcity and proposes specific remedies by harnessing LLM
capabilities. The investigation delves into strategies for mitigating the issue
of inadequate data, offering tailored solutions. The study delves into the
efficacy of employing fine-tuning techniques and direct document integration to
alleviate data insufficiency. A significant contribution of this work is the
development of a Retrieval-Augmented Generation (RAG) model, which tackles the
aforementioned challenges. The RAG model is carefully designed to enhance
information storage and retrieval processes, ensuring improved content
generation. The research elucidates the key phases of the information storage
and retrieval methodology underpinned by the RAG model. A comprehensive
analysis of these steps is undertaken, emphasizing their significance in
addressing the scarcity of data. The study highlights the efficacy of the
proposed method, showcasing its applicability through illustrative instances.
By implementing the RAG model for information storage and retrieval, the
research not only contributes to a deeper comprehension of generative AI
technology but also facilitates its practical usability within enterprises
utilizing LLMs. This work holds substantial value in advancing the field of
generative AI, offering insights into enhancing data-driven content generation
and fostering active utilization of LLM-based services within corporate
settings.",Cheonsu Jeong
2023-09-15T22:12:44Z,http://arxiv.org/abs/2309.08788v2,"BioinspiredLLM: Conversational Large Language Model for the Mechanics of
  Biological and Bio-inspired Materials","The study of biological materials and bio-inspired materials science is well
established; however, surprisingly little knowledge has been systematically
translated to engineering solutions. To accelerate discovery and guide
insights, an open-source autoregressive transformer large language model (LLM),
BioinspiredLLM, is reported. The model was finetuned with a corpus of over a
thousand peer-reviewed articles in the field of structural biological and
bio-inspired materials and can be prompted to recall information, assist with
research tasks, and function as an engine for creativity. The model has proven
that it is able to accurately recall information about biological materials and
is further enhanced with enhanced reasoning ability, as well as with
retrieval-augmented generation to incorporate new data during generation that
can also help to traceback sources, update the knowledge base, and connect
knowledge domains. BioinspiredLLM also has been shown to develop sound
hypotheses regarding biological materials design and remarkably so for
materials that have never been explicitly studied before. Lastly, the model
showed impressive promise in collaborating with other generative artificial
intelligence models in a workflow that can reshape the traditional materials
design process. This collaborative generative artificial intelligence method
can stimulate and enhance bio-inspired materials design workflows. Biological
materials are at a critical intersection of multiple scientific fields and
models like BioinspiredLLM help to connect knowledge domains.",Rachel K. Luu
2023-09-29T17:26:03Z,http://arxiv.org/abs/2309.17415v3,"Intuitive or Dependent? Investigating LLMs' Behavior Style to
  Conflicting Prompts","This study investigates the behaviors of Large Language Models (LLMs) when
faced with conflicting prompts versus their internal memory. This will not only
help to understand LLMs' decision mechanism but also benefit real-world
applications, such as retrieval-augmented generation (RAG). Drawing on
cognitive theory, we target the first scenario of decision-making styles where
there is no superiority in the conflict and categorize LLMs' preference into
dependent, intuitive, and rational/irrational styles. Another scenario of
factual robustness considers the correctness of prompt and memory in
knowledge-intensive tasks, which can also distinguish if LLMs behave rationally
or irrationally in the first scenario. To quantify them, we establish a
complete benchmarking framework including a dataset, a robustness evaluation
pipeline, and corresponding metrics. Extensive experiments with seven LLMs
reveal their varying behaviors. And, with role play intervention, we can change
the styles, but different models present distinct adaptivity and upper-bound.
One of our key takeaways is to optimize models or the prompts according to the
identified style. For instance, RAG models with high role play adaptability may
dynamically adjust the interventions according to the quality of retrieval
results -- being dependent to better leverage informative context; and, being
intuitive when external prompt is noisy.",Jiahao Ying
2023-09-28T15:32:36Z,http://arxiv.org/abs/2310.01429v1,Chatmap : Large Language Model Interaction with Cartographic Data,"The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.",Eren Unlu
2023-10-08T01:43:39Z,http://arxiv.org/abs/2310.04963v3,LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,"Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. The goal of this work is to automatically generate tests
and use these tests to validate and verify compiler implementations of a
directive-based parallel programming paradigm, OpenACC. To do so, in this
paper, we explore the capabilities of state-of-the-art LLMs, including
open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama,
Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and
GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using
our own testsuite dataset along with using the OpenACC specification. We also
explored these LLMs using various prompt engineering techniques that include
code template, template with retrieval-augmented generation (RAG), one-shot
example, one-shot with RAG, expressive prompt with code template and RAG. This
paper highlights our findings from over 5000 tests generated via all the above
mentioned methods. Our contributions include: (a) exploring the capabilities of
the latest and relevant LLMs for code generation, (b) investigating fine-tuning
and prompt methods, and (c) analyzing the outcome of LLMs generated tests
including manually analysis of representative set of tests. We found the LLM
Deepseek-Coder-33b-Instruct produced the most passing tests followed by
GPT-4-Turbo.",Christian Munley
2023-10-09T11:34:41Z,http://arxiv.org/abs/2310.05628v3,"Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models","Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Publicly released information on sustainability
practices is often disclosed in diverse, unstructured, and multi-modal
documentation. This poses a challenge in efficiently gathering and aligning the
data into a unified framework to derive insights related to Corporate Social
Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes
an intuitive choice for delivering insightful and actionable data to
stakeholders. In this study, we employ Large Language Models (LLMs), In-Context
Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract
structured insights related to ESG aspects from companies' sustainability
reports. We then leverage graph-based representations to conduct statistical
analyses concerning the extracted insights. These analyses revealed that ESG
criteria cover a wide range of topics, exceeding 500, often beyond those
considered in existing categorizations, and are addressed by companies through
a variety of initiatives. Moreover, disclosure similarities emerged among
companies from the same region or sector, validating ongoing hypotheses in the
ESG literature. Lastly, by incorporating additional company attributes into our
analyses, we investigated which factors impact the most on companies' ESG
ratings, showing that ESG disclosure affects the obtained ratings more than
other financial or company data.",Marco Bronzini
2023-10-10T00:39:04Z,http://arxiv.org/abs/2310.06225v2,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",Bruno Silva
2023-10-11T18:27:12Z,http://arxiv.org/abs/2310.07793v5,"GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large
  Language Models","The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.",Ruotong Liao
2023-10-13T13:17:03Z,http://arxiv.org/abs/2310.09089v2,"Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large
  Language Model","Integrating large language models (LLMs) into healthcare holds great
potential but faces challenges. Pre-training LLMs from scratch for domains like
medicine is resource-heavy and often unfeasible. On the other hand, sole
reliance on Supervised Fine-tuning (SFT) can result in overconfident
predictions and may not tap into domain-specific insights. In response, we
present a multi-stage training method combining Domain-specific Continued
Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In
addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing
medical question answering, plain texts, knowledge graphs, and dialogues,
segmented into three training stages. The medical LLM trained with our
pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and
SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,
respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by
7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the
Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in
BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the
model's performance through the Retrieval Augmented Generation (RAG) approach.
Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%
on CMExam. These results highlight the contribution of our novel training
approach in building LLMs for medical applications.",Qichen Ye
2023-10-14T08:46:24Z,http://arxiv.org/abs/2310.09536v1,"CarExpert: Leveraging Large Language Models for In-Car Conversational
  Question Answering","Large language models (LLMs) have demonstrated remarkable performance by
following natural language instructions without fine-tuning them on
domain-specific tasks and data. However, leveraging LLMs for domain-specific
question answering suffers from severe limitations. The generated answer tends
to hallucinate due to the training data collection time (when using
off-the-shelf), complex user utterance and wrong retrieval (in
retrieval-augmented generation). Furthermore, due to the lack of awareness
about the domain and expected output, such LLMs may generate unexpected and
unsafe answers that are not tailored to the target domain. In this paper, we
propose CarExpert, an in-car retrieval-augmented conversational
question-answering system leveraging LLMs for different tasks. Specifically,
CarExpert employs LLMs to control the input, provide domain-specific documents
to the extractive and generative answering components, and controls the output
to ensure safe and domain-specific answers. A comprehensive empirical
evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in
generating natural, safe and car-specific answers.",Md Rashad Al Hasan Rony
2023-10-16T14:29:35Z,http://arxiv.org/abs/2310.10445v1,"MechGPT, a language-based strategy for mechanics and materials modeling
  that connects knowledge across scales, disciplines and modalities","For centuries, researchers have sought out ways to connect disparate areas of
knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across
fields, specialization has taken hold later. With the advent of Artificial
Intelligence, we can now explore relationships across areas (e.g.,
mechanics-biology) or disparate domains (e.g., failure mechanics-art). To
achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset
of knowledge in multiscale materials failure. The approach includes the use of
a general-purpose LLM to distill question-answer pairs from raw sources
followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used
in a series of computational experiments to explore its capacity for knowledge
retrieval, various language tasks, hypothesis generation, and connecting
knowledge across disparate areas. While the model has some ability to recall
knowledge from training, we find that LLMs are particularly useful to extract
structural insights through Ontological Knowledge Graphs. These interpretable
graph structures provide explanatory insights, frameworks for new research
questions, and visual representations of knowledge that also can be used in
retrieval-augmented generation. Three versions of MechGPT are discussed,
featuring different sizes from 13 billion to 70 billion parameters, and
reaching context lengths of more than 10,000 tokens. This provides ample
capacity for sophisticated retrieval augmented strategies, as well as
agent-based modeling where multiple LLMs interact collaboratively and/or
adversarially, the incorporation of new data from the literature or web
searches, as well as multimodality.",Markus J. Buehler
2023-10-20T22:47:18Z,http://arxiv.org/abs/2310.13848v2,"FABULA: Intelligence Report Generation Using Retrieval-Augmented
  Narrative Construction","Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.",Priyanka Ranade
2023-10-31T03:52:08Z,http://arxiv.org/abs/2310.20158v1,GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval,"Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.",Daman Arora
2023-11-05T08:34:26Z,http://arxiv.org/abs/2311.02597v1,"FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented
  Generation with an LLM","Fast disaster impact reporting is crucial in planning humanitarian
assistance. Large Language Models (LLMs) are well known for their ability to
write coherent text and fulfill a variety of tasks relevant to impact
reporting, such as question answering or text summarization. However, LLMs are
constrained by the knowledge within their training data and are prone to
generating inaccurate, or ""hallucinated"", information. To address this, we
introduce a sophisticated pipeline embodied in our tool FloodBrain
(floodbrain.com), specialized in generating flood disaster impact reports by
extracting and curating information from the web. Our pipeline assimilates
information from web search results to produce detailed and accurate reports on
flood events. We test different LLMs as backbones in our tool and compare their
generated reports to human-written reports on different metrics. Similar to
other studies, we find a notable correlation between the scores assigned by
GPT-4 and the scores given by human evaluators when comparing our generated
reports to human-authored ones. Additionally, we conduct an ablation study to
test our single pipeline components and their relevancy for the final reports.
With our tool, we aim to advance the use of LLMs for disaster impact reporting
and reduce the time for coordination of humanitarian efforts in the wake of
flood disasters.",Grace Colverd
2023-11-05T21:43:02Z,http://arxiv.org/abs/2311.02775v3,"AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using
  Open-Source LLMs","Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of AI-TA, an
intelligent QA assistant customizable for courses with an online QA platform",Yann Hicke
2023-11-13T18:22:32Z,http://arxiv.org/abs/2311.07536v3,"A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual
  Question Answering","The emergence of multimodal large models (MLMs) has significantly advanced
the field of visual understanding, offering remarkable capabilities in the
realm of visual question answering (VQA). Yet, the true challenge lies in the
domain of knowledge-intensive VQA tasks, which necessitate not just recognition
of visual elements, but also a deep comprehension of the visual information in
conjunction with a vast repository of learned knowledge. To uncover such
capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we
provide an in-depth evaluation from three perspectives: 1) Commonsense
Knowledge, which assesses how well models can understand visual cues and
connect to general knowledge; 2) Fine-grained World Knowledge, which tests the
model's skill in reasoning out specific knowledge from images, showcasing their
proficiency across various specialized fields; 3) Comprehensive Knowledge with
Decision-making Rationales, which examines model's capability to provide
logical explanations for its inference, facilitating a deeper analysis from the
interpretability perspective. Additionally, we utilize a visual
knowledge-enhanced training strategy and multimodal retrieval-augmented
generation approach to enhance MLMs, highlighting the future need for
advancements in this research direction. Extensive experiments indicate that:
a) GPT-4V demonstrates enhanced explanation generation when using composite
images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations
when dealing with world knowledge; c) Visual knowledge enhanced training and
prompting technicals present potential to improve performance. Codes:
https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper",Yunxin Li
2023-11-16T01:21:33Z,http://arxiv.org/abs/2311.10776v5,"Chemist-X: Large Language Model-empowered Agent for Reaction Condition
  Recommendation in Chemical Synthesis","Recent AI research plots a promising future of automatic chemical reactions
within the chemistry society. This study proposes Chemist-X, a transformative
AI agent that automates the reaction condition recommendation (RCR) task in
chemical synthesis with retrieval-augmented generation (RAG) technology. To
emulate expert chemists' strategies when solving RCR tasks, Chemist-X utilizes
advanced RAG schemes to interrogate online molecular databases and distill
critical data from the latest literature database. Further, the agent leverages
state-of-the-art computer-aided design (CAD) tools with a large language model
(LLM) supervised programming interface. With the ability to utilize updated
chemical knowledge and CAD tools, our agent significantly outperforms
conventional synthesis AIs confined to the fixed knowledge within its training
data. Chemist-X considerably reduces chemists' workload and allows them to
focus on more fundamental and creative problems, thereby bringing closer
computational techniques and chemical research and making a remarkable leap
toward harnessing AI's full capabilities in scientific discovery.",Kexin Chen
2023-11-22T17:24:21Z,http://arxiv.org/abs/2311.13538v5,"AlignedCoT: Prompting Large Language Models via Native-Speaking
  Demonstrations","Large Language Models prompting, such as using in-context demonstrations, is
a mainstream technique for invoking LLMs to perform high-performance and solid
complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and
has the potential for further human-machine collaborative scientific findings.
However, current LLMs are delicate and elusive in prompt words and styles. And
there is an unseen gap between LLM understanding and human-written prompts.
This paper introduces Alignedcot, an LLM-acquainted prompting technique that
includes proficient ``native-speaking'' in in-context learning for the LLMs.
Specifically, it achieves consistent and correct step-wise prompts in zero-shot
scenarios by progressively probing, refining, and formatting the LLM chain of
thoughts so that free from handcrafted few-shot demonstrations while
maintaining the prompt quality. We conduct experiments on mathematical
reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform
significantly superior to them with human-crafted demonstrations. We further
apply Alignedcot for rewriting the GSM8K training set, resulting in a
GSM8K-Align dataset. We observe its benefits for retrieval augmented
generation. The code and data can be found at
https://github.com/yangzhch6/AlignedCoT.",Zhicheng Yang
2023-11-29T15:21:35Z,http://arxiv.org/abs/2311.17722v1,SenTest: Evaluating Robustness of Sentence Encoders,"Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.",Tanmay Chavan
2023-11-30T09:48:51Z,http://arxiv.org/abs/2311.18397v1,"IAG: Induction-Augmented Generation Framework for Answering Reasoning
  Questions","Retrieval-Augmented Generation (RAG), by incorporating external knowledge
with parametric memory of language models, has become the state-of-the-art
architecture for open-domain QA tasks. However, common knowledge bases are
inherently constrained by limited coverage and noisy information, making
retrieval-based approaches inadequate to answer implicit reasoning questions.
In this paper, we propose an Induction-Augmented Generation (IAG) framework
that utilizes inductive knowledge along with the retrieved documents for
implicit reasoning. We leverage large language models (LLMs) for deriving such
knowledge via a novel prompting method based on inductive reasoning patterns.
On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,
respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for
answer prediction, while IAG-Student gets rid of dependencies on GPT service at
inference time by incorporating a student inductor model. The inductor is
firstly trained via knowledge distillation and further optimized by
back-propagating the generator feedback via differentiable beam scores.
Experimental results show that IAG outperforms RAG baselines as well as ChatGPT
on two Open-Domain QA tasks. Notably, our best models have won the first place
in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA
(since Jan 8, 2023).",Zhebin Zhang
2023-12-04T17:35:42Z,http://arxiv.org/abs/2312.02073v3,"A Glitch in the Matrix? Locating and Detecting Language Model Grounding
  with Fakepedia","Large language models (LLMs) have an impressive ability to draw on novel
information supplied in their context. Yet the mechanisms underlying this
contextual grounding remain unknown, especially in situations where contextual
information contradicts factual knowledge stored in the parameters, which LLMs
also excel at recalling. Favoring the contextual information is critical for
retrieval-augmented generation methods, which enrich the context with
up-to-date information, hoping that grounding can rectify outdated or noisy
stored knowledge. We present a novel method to study grounding abilities using
Fakepedia, a novel dataset of counterfactual texts constructed to clash with a
model's internal parametric knowledge. In this study, we introduce Fakepedia, a
counterfactual dataset designed to evaluate grounding abilities when the
internal parametric knowledge clashes with the contextual information. We
benchmark various LLMs with Fakepedia and conduct a causal mediation analysis
of LLM components when answering Fakepedia queries, based on our Masked Grouped
Causal Tracing (MGCT) method. Through this analysis, we identify distinct
computational patterns between grounded and ungrounded responses. We finally
demonstrate that distinguishing grounded from ungrounded responses is
achievable through computational analysis alone. Our results, together with
existing findings about factual recall mechanisms, provide a coherent narrative
of how grounding and factual recall mechanisms interact within LLMs.",Giovanni Monea
2023-12-05T21:21:01Z,http://arxiv.org/abs/2312.03141v2,"NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest
  Neighbor Search through Near Data Processing","Approximate nearest neighbor search (ANNS) is a key retrieval technique for
vector database and many data center applications, such as person
re-identification and recommendation systems. It is also fundamental to
retrieval augmented generation (RAG) for large language models (LLM) now. Among
all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall
rate. However, as the size of dataset increases, the graph may require hundreds
of gigabytes of memory, exceeding the main memory capacity of a single
workstation node. Although we can do partitioning and use solid-state drive
(SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades
the performance of the system. To address this challenge, we present NDSEARCH,
a hardware-software co-designed near-data processing (NDP) solution for ANNS
processing. NDSEARCH consists of a novel in-storage computing architecture,
namely, SEARSSD, that supports the ANNS kernels and leverages logic unit
(LUN)-level parallelism inside the NAND flash chips. NDSEARCH also includes a
processing model that is customized for NDP and cooperates with SEARSSD. The
processing model enables us to apply a two-level scheduling to improve the data
locality and exploit the internal bandwidth in NDSEARCH, and a speculative
searching mechanism to further accelerate the ANNS workload. Our results show
that NDSEARCH improves the throughput by up to 31.7x, 14.6x, 7.4x 2.9x over
CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively.
NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than
CPU and GPU.",Yitu Wang
2023-12-21T10:19:58Z,http://arxiv.org/abs/2312.14211v1,"Experimenting with Large Language Models and vector embeddings in NASA
  SciX","Open-source Large Language Models enable projects such as NASA SciX (i.e.,
NASA ADS) to think out of the box and try alternative approaches for
information retrieval and data augmentation, while respecting data copyright
and users' privacy. However, when large language models are directly prompted
with questions without any context, they are prone to hallucination. At NASA
SciX we have developed an experiment where we created semantic vectors for our
large collection of abstracts and full-text content, and we designed a prompt
system to ask questions using contextual chunks from our system. Based on a
non-systematic human evaluation, the experiment shows a lower degree of
hallucination and better responses when using Retrieval Augmented Generation.
Further exploration is required to design new features and data augmentation
processes at NASA SciX that leverages this technology while respecting the high
level of trust and quality that the project holds.",Sergi Blanco-Cuaresma
2023-12-21T23:42:13Z,http://arxiv.org/abs/2312.14335v2,"Context-aware Decoding Reduces Hallucination in Query-focused
  Summarization","Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",Zhichao Xu
2023-12-26T04:49:56Z,http://arxiv.org/abs/2312.15883v2,"HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and
  Reliable Medical LLMs Responses","In this paper, we investigate the retrieval-augmented generation (RAG) based
on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large
Language Models (LLMs). Recent approaches suffer from insufficient and
repetitive knowledge retrieval, tedious and time-consuming query parsing, and
monotonous knowledge utilization. To this end, we develop a Hypothesis
Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful
reasoning capacity to compensate for the incompleteness of user queries,
optimizes the interaction process with LLMs, and provides diverse retrieved
knowledge. Specifically, HyKGE explores the zero-shot capability and the rich
knowledge of LLMs with Hypothesis Outputs to extend feasible exploration
directions in the KGs, as well as the carefully curated prompt to enhance the
density and efficiency of LLMs' responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise while ensuring the
balance between diversity and relevance in retrieved knowledge. Experiments on
two Chinese medical multiple-choice question datasets and one Chinese
open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority
of HyKGE in terms of accuracy and explainability.",Xinke Jiang
2023-12-26T08:24:24Z,http://arxiv.org/abs/2312.16262v1,Dynamic In-Context Learning from Nearest Neighbors for Bundle Generation,"Product bundling has evolved into a crucial marketing strategy in e-commerce.
However, current studies are limited to generating (1) fixed-size or single
bundles, and most importantly, (2) bundles that do not reflect consistent user
intents, thus being less intelligible or useful to users. This paper explores
two interrelated tasks, i.e., personalized bundle generation and the underlying
intent inference based on users' interactions in a session, leveraging the
logical reasoning capability of large language models. We introduce a dynamic
in-context learning paradigm, which enables ChatGPT to seek tailored and
dynamic lessons from closely related sessions as demonstrations while
performing tasks in the target session. Specifically, it first harnesses
retrieval augmented generation to identify nearest neighbor sessions for each
target session. Then, proper prompts are designed to guide ChatGPT to perform
the two tasks on neighbor sessions. To enhance reliability and mitigate the
hallucination issue, we develop (1) a self-correction strategy to foster mutual
improvement in both tasks without supervision signals; and (2) an auto-feedback
mechanism to recurrently offer dynamic supervision based on the distinct
mistakes made by ChatGPT on various neighbor sessions. Thus, the target session
can receive customized and dynamic lessons for improved performance by
observing the demonstrations of its neighbor sessions. Finally, experimental
results on three real-world datasets verify the effectiveness of our methods on
both tasks. Additionally, the inferred intents can prove beneficial for other
intriguing downstream tasks, such as crafting appealing bundle names.",Zhu Sun
2023-12-25T06:44:32Z,http://arxiv.org/abs/2312.17264v1,"ESGReveal: An LLM-based approach for extracting structured data from ESG
  reports","ESGReveal is an innovative method proposed for efficiently extracting and
analyzing Environmental, Social, and Governance (ESG) data from corporate
reports, catering to the critical need for reliable ESG information retrieval.
This approach utilizes Large Language Models (LLM) enhanced with Retrieval
Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG
metadata module for targeted queries, a preprocessing module for assembling
databases, and an LLM agent for data extraction. Its efficacy was appraised
using ESG reports from 166 companies across various sectors listed on the Hong
Kong Stock Exchange in 2022, ensuring comprehensive industry and market
capitalization representation. Utilizing ESGReveal unearthed significant
insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in
data extraction and 83.7% in disclosure analysis, which is an improvement over
baseline models. This highlights the framework's capacity to refine ESG data
analysis precision. Moreover, it revealed a demand for reinforced ESG
disclosures, with environmental and social data disclosures standing at 69.5%
and 57.2%, respectively, suggesting a pursuit for more corporate transparency.
While current iterations of ESGReveal do not process pictorial information, a
functionality intended for future enhancement, the study calls for continued
research to further develop and compare the analytical capabilities of various
LLMs. In summary, ESGReveal is a stride forward in ESG data processing,
offering stakeholders a sophisticated tool to better evaluate and advance
corporate sustainability efforts. Its evolution is promising in promoting
transparency in corporate reporting and aligning with broader sustainable
development aims.",Yi Zou
2023-12-31T04:43:45Z,http://arxiv.org/abs/2401.00396v2,"RAGTruth: A Hallucination Corpus for Developing Trustworthy
  Retrieval-Augmented Language Models","Retrieval-augmented generation (RAG) has become a main technique for
alleviating hallucinations in large language models (LLMs). Despite the
integration of RAG, LLMs may still present unsupported or contradictory claims
to the retrieved contents. In order to develop effective hallucination
prevention strategies under RAG, it is important to create benchmark datasets
that can measure the extent of hallucination. This paper presents RAGTruth, a
corpus tailored for analyzing word-level hallucinations in various domains and
tasks within the standard RAG frameworks for LLM applications. RAGTruth
comprises nearly 18,000 naturally generated responses from diverse LLMs using
RAG. These responses have undergone meticulous manual annotations at both the
individual cases and word levels, incorporating evaluations of hallucination
intensity. We not only benchmark hallucination frequencies across different
LLMs, but also critically assess the effectiveness of several existing
hallucination detection methodologies. Furthermore, we show that using a
high-quality dataset such as RAGTruth, it is possible to finetune a relatively
small LLM and achieve a competitive level of performance in hallucination
detection when compared to the existing prompt-based approaches using
state-of-the-art large language models such as GPT-4.",Cheng Niu
2023-12-31T17:15:25Z,http://arxiv.org/abs/2401.00544v2,"A Reliable Knowledge Processing Framework for Combustion Science using
  Foundation Models","This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.",Vansh Sharma
2024-01-03T12:09:43Z,http://arxiv.org/abs/2401.01701v3,"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks
  via Iterative Grounding","Large language models (LLMs) trained on datasets of publicly available source
code have established a new state of the art in code generation tasks. However,
these models are mostly unaware of the code that exists within a specific
project, preventing the models from making good use of existing APIs. Instead,
LLMs often invent, or ""hallucinate"", non-existent APIs or produce variants of
already existing code. This paper presents De-Hallucinator, a technique that
grounds the predictions of an LLM through a novel combination of retrieving
suitable API references and iteratively querying the model with increasingly
suitable context information in the prompt. The approach exploits the
observation that predictions by LLMs often resemble the desired code, but they
fail to correctly refer to already existing APIs. De-Hallucinator automatically
identifies project-specific API references related to the model's initial
predictions and adds these references into the prompt. Unlike
retrieval-augmented generation (RAG), our approach uses the initial
prediction(s) by the model to iteratively retrieve increasingly suitable API
references. Our evaluation applies the approach to two tasks: predicting API
usages in Python and generating tests in JavaScript. We show that
De-Hallucinator consistently improves the generated code across five LLMs. In
particular, the approach improves the edit distance by 23.3-50.6% and the
recall of correctly predicted API usages by 23.9-61.0% for code completion, and
improves the number of fixed tests that initially failed because of
hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage
for test generation.",Aryaz Eghbali
2024-01-15T05:38:37Z,http://arxiv.org/abs/2401.07483v1,"Graph database while computationally efficient filters out quickly the
  ESG integrated equities in investment management","Design/methodology/approach This research evaluated the databases of SQL,
No-SQL and graph databases to compare and contrast efficiency and performance.
To perform this experiment the data were collected from multiple sources
including stock price and financial news. Python is used as an interface to
connect and query databases (to create database structures according to the
feed file structure, to load data into tables, objects, to read data , to
connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM
(Large language model) including RAG (Retrieval Augmented Generation) with
Machine Learning, deep learning, NLP (natural language processing) or Decision
Analytics are computationally expensive. Finding a better option to consume
less resources and time to get the result. Findings The Graph database of ESG
(Environmental, Social and Governance) is comparatively better and can be
considered for extended analytics to integrate ESG in business and investment.
Practical implications A graph ML with a RAG architecture model can be
introduced as a new framework with less computationally expensive LLM
application in the equity filtering process for portfolio management.
Originality/value Filtering out selective stocks out of two thousand or more
listed companies in any stock exchange for active investment, consuming less
resource consumption especially memory and energy to integrate artificial
intelligence and ESG in business and investment.",Partha Sen
2024-01-15T16:00:50Z,http://arxiv.org/abs/2401.07793v1,"Flexibly Scaling Large Language Models Contexts Through Extensible
  Tokenization","Large language models (LLMs) are in need of sufficient contexts to handle
many critical applications, such as retrieval augmented generation and few-shot
learning. However, due to the constrained window size, the LLMs can only access
to the information within a limited context. Although the size of context
window can be extended by fine-tuning, it will result in a substantial cost in
both training and inference stage. In this paper, we present Extensible
Tokenization as an alternative method which realizes the flexible scaling of
LLMs' context. Extensible Tokenization stands as a midware in between of the
tokenized context and the LLM, which transforms the raw token embeddings into
the extensible embeddings. Such embeddings provide a more compact
representation for the long context, on top of which the LLM is able to
perceive more information with the same context window. Extensible Tokenization
is also featured by its flexibility: the scaling factor can be flexibly
determined within a feasible scope, leading to the extension of an arbitrary
context length at the inference time. Besides, Extensible Tokenization is
introduced as a drop-in component, which can be seamlessly plugged into not
only the LLM itself and but also its fine-tuned derivatives, bringing in the
extended contextual information while fully preserving the LLM's existing
capabilities. We perform comprehensive experiments on long-context language
modeling and understanding tasks, which verify Extensible Tokenization as an
effective, efficient, flexible, and compatible method to extend LLM's context.
Our model and source code will be made publicly available.",Ninglu Shao
2024-01-15T18:25:18Z,http://arxiv.org/abs/2401.07883v1,"The Chronicles of RAG: The Retriever, the Chunk and the Generator","Retrieval Augmented Generation (RAG) has become one of the most popular
paradigms for enabling LLMs to access external data, and also as a mechanism
for grounding to mitigate against hallucinations. When implementing RAG you can
face several challenges like effective integration of retrieval models,
efficient representation learning, data diversity, computational efficiency
optimization, evaluation, and quality of text generation. Given all these
challenges, every day a new technique to improve RAG appears, making it
unfeasible to experiment with all combinations for your problem. In this
context, this paper presents good practices to implement, optimize, and
evaluate RAG for the Brazilian Portuguese language, focusing on the
establishment of a simple pipeline for inference and experiments. We explored a
diverse set of methods to answer questions about the first Harry Potter book.
To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,
gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the
retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to
the baseline. When optimizing the input size in the application, we observed
that it is possible to further enhance it by 2.4%. Finally, we present the
complete architecture of the RAG with our recommendations. As result, we moved
from a baseline of 57.88% to a maximum relative score of 98.61%.",Paulo Finardi
2024-01-17T09:53:50Z,http://arxiv.org/abs/2401.09092v1,"BibSonomy Meets ChatLLMs for Publication Management: From Chat to
  Publication Management: Organizing your related work using BibSonomy & LLMs","The ever-growing corpus of scientific literature presents significant
challenges for researchers with respect to discovery, management, and
annotation of relevant publications. Traditional platforms like Semantic
Scholar, BibSonomy, and Zotero offer tools for literature management, but
largely require manual laborious and error-prone input of tags and metadata.
Here, we introduce a novel retrieval augmented generation system that leverages
chat-based large language models (LLMs) to streamline and enhance the process
of publication management. It provides a unified chat-based interface, enabling
intuitive interactions with various backends, including Semantic Scholar,
BibSonomy, and the Zotero Webscraper. It supports two main use-cases: (1)
Explorative Search & Retrieval - leveraging LLMs to search for and retrieve
both specific and general scientific publications, while addressing the
challenges of content hallucination and data obsolescence; and (2) Cataloguing
& Management - aiding in the organization of personal publication libraries, in
this case BibSonomy, by automating the addition of metadata and tags, while
facilitating manual edits and updates. We compare our system to different LLM
models in three different settings, including a user study, and we can show its
advantages in different metrics.",Tom Völker
2024-01-18T18:59:11Z,http://arxiv.org/abs/2401.10225v5,ChatQA: Surpassing GPT-4 on Conversational QA and RAG,"In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on
retrieval-augmented generation (RAG) and conversational question answering
(QA). To enhance generation, we propose a two-stage instruction tuning method
that significantly boosts the performance of RAG. For effective retrieval, we
introduce a dense retriever optimized for conversational QA, which yields
results comparable to the alternative state-of-the-art query rewriting models,
while substantially reducing deployment costs. We also present the ChatRAG
Bench, which encompasses ten datasets covering comprehensive evaluations on
RAG, table-related QA, arithmetic calculations, and scenarios involving
unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a
weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:
53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without
relying on any synthetic data from OpenAI GPT models. Notably, the
Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,
achieving a 4.4% improvement. To advance research in this field, we
open-sourced the model weights, instruction tuning data, ChatRAG Bench, and
retriever for the community: https://chatqa-project.github.io/.",Zihan Liu
2024-01-20T03:41:23Z,http://arxiv.org/abs/2401.12998v1,"Evaluating and Enhancing Large Language Models Performance in
  Domain-specific Medicine: Osteoarthritis Management with DocOA","The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.",Xi Chen
2024-01-26T14:14:59Z,http://arxiv.org/abs/2401.14887v4,The Power of Noise: Redefining Retrieval for RAG Systems,"Retrieval-Augmented Generation (RAG) has recently emerged as a method to
extend beyond the pre-trained knowledge of Large Language Models by augmenting
the original prompt with relevant passages or documents retrieved by an
Information Retrieval (IR) system. RAG has become increasingly important for
Generative AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized in the LLM. We
argue here that the retrieval component of RAG systems, be it dense or sparse,
deserves increased attention from the research community, and accordingly, we
conduct the first comprehensive and systematic examination of the retrieval
strategy of RAG systems. We focus, in particular, on the type of passages IR
systems within a RAG solution should retrieve. Our analysis considers multiple
factors, such as the relevance of the passages included in the prompt context,
their position, and their number. One counter-intuitive finding of this work is
that the retriever's highest-scoring documents that are not directly relevant
to the query (e.g., do not contain the answer) negatively impact the
effectiveness of the LLM. Even more surprising, we discovered that adding
random documents in the prompt improves the LLM accuracy by up to 35%. These
results highlight the need to investigate the appropriate strategies when
integrating retrieval with LLMs, thereby laying the groundwork for future
research in this area.",Florin Cuconasu
2024-01-30T18:37:45Z,http://arxiv.org/abs/2401.17244v3,"LLaMP: Large Language Model Made Powerful for High-fidelity Materials
  Knowledge Retrieval and Distillation","Reducing hallucination of Large Language Models (LLMs) is imperative for use
in the sciences, where reliability and reproducibility are crucial. However,
LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and
inevitably biased task to fine-tune them on domain-specific literature and
data. Here we introduce LLaMP, a multimodal retrieval-augmented generation
(RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can
dynamically and recursively interact with computational and experimental data
on Materials Project (MP) and run atomistic simulations via high-throughput
workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage
ability to comprehend and integrate various modalities of materials science
concepts, fetch relevant data stores on the fly, process higher-order data
(such as crystal structure and elastic tensor), and streamline complex tasks in
computational materials and chemistry. We propose a simple metric combining
uncertainty and confidence estimates to evaluate the self-consistency of
responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively
mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli,
electronic bandgaps, and formation energies that seem to derive from mixed data
sources. We also demonstrate LLaMP's capability to edit crystal structures and
run annealing molecular dynamics simulations using pre-trained machine-learning
force fields. The framework offers an intuitive and nearly hallucination-free
approach to exploring and scaling materials informatics, and establishes a
pathway for knowledge distillation and fine-tuning other language models. Code
and live demo are available at https://github.com/chiang-yuan/llamp",Yuan Chiang
2024-01-31T07:58:54Z,http://arxiv.org/abs/2401.17645v1,"ReSLLM: Large Language Models are Strong Resource Selectors for
  Federated Search","Federated search, which involves integrating results from multiple
independent search engines, will become increasingly pivotal in the context of
Retrieval-Augmented Generation pipelines empowering LLM-based applications such
as chatbots. These systems often distribute queries among various search
engines, ranging from specialized (e.g., PubMed) to general (e.g., Google),
based on the nature of user utterances. A critical aspect of federated search
is resource selection - the selection of appropriate resources prior to issuing
the query to ensure high-quality and rapid responses, and contain costs
associated with calling the external search engines. However, current SOTA
resource selection methodologies primarily rely on feature-based learning
approaches. These methods often involve the labour intensive and expensive
creation of training labels for each resource. In contrast, LLMs have exhibited
strong effectiveness as zero-shot methods across NLP and IR tasks. We
hypothesise that in the context of federated search LLMs can assess the
relevance of resources without the need for extensive predefined labels or
features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to
drive the selection of resources in federated search in a zero-shot setting. In
addition, we devise an unsupervised fine tuning protocol, the Synthetic Label
Augmentation Tuning (SLAT), where the relevance of previously logged queries
and snippets from resources is predicted using an off-the-shelf LLM and then in
turn used to fine-tune ReSLLM with respect to resource selection. Our empirical
evaluation and analysis details the factors influencing the effectiveness of
LLMs in this context. The results showcase the merits of ReSLLM for resource
selection: not only competitive effectiveness in the zero-shot setting, but
also obtaining large when fine-tuned using SLAT-protocol.",Shuai Wang
2024-02-01T16:40:32Z,http://arxiv.org/abs/2402.00746v7,Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,"Recent advancements in artificial intelligence (AI), especially large
language models (LLMs), have significantly advanced healthcare applications and
demonstrated potentials in intelligent medical treatment. However, there are
conspicuous challenges such as vast data volumes and inconsistent symptom
characterization standards, preventing full integration of healthcare AI
systems with individual patients' needs. To promote professional and
personalized healthcare, we propose an innovative framework, Heath-LLM, which
combines large-scale feature extraction and medical knowledge trade-off
scoring. Compared to traditional health management applications, our system has
three main advantages: (1) It integrates health reports and medical knowledge
into a large model to ask relevant questions to large language model for
disease prediction; (2) It leverages a retrieval augmented generation (RAG)
mechanism to enhance feature extraction; (3) It incorporates a semi-automated
feature updating framework that can merge and delete features to improve
accuracy of disease prediction. We experiment on a large number of health
reports to assess the effectiveness of Health-LLM system. The results indicate
that the proposed system surpasses the existing ones and has the potential to
significantly advance disease prediction and personalized health management.",Mingyu Jin
2024-02-02T06:44:22Z,http://arxiv.org/abs/2402.01176v2,"CorpusLM: Towards a Unified Language Model on Corpus for
  Knowledge-Intensive Tasks","Large language models (LLMs) have gained significant attention in various
fields but prone to hallucination, especially in knowledge-intensive (KI)
tasks. To address this, retrieval-augmented generation (RAG) has emerged as a
popular solution to enhance factual accuracy. However, traditional retrieval
modules often rely on large document index and disconnect with generative
tasks. With the advent of generative retrieval (GR), language models can
retrieve by directly generating document identifiers (DocIDs), offering
superior performance in retrieval tasks. However, the potential relationship
between GR and downstream tasks remains unexplored. In this paper, we propose
\textbf{CorpusLM}, a unified language model that leverages external corpus to
tackle various knowledge-intensive tasks by integrating generative retrieval,
closed-book generation, and RAG through a unified greedy decoding process. We
design the following mechanisms to facilitate effective retrieval and
generation, and improve the end-to-end effectiveness of KI tasks: (1) We
develop a ranking-oriented DocID list generation strategy, which refines GR by
directly learning from a DocID ranking list, to improve retrieval quality. (2)
We design a continuous DocIDs-References-Answer generation strategy, which
facilitates effective and efficient RAG. (3) We employ well-designed
unsupervised DocID understanding tasks, to comprehend DocID semantics and their
relevance to downstream tasks. We evaluate our approach on the widely used KILT
benchmark with two variants of backbone models, i.e., T5 and Llama2.
Experimental results demonstrate the superior performance of our models in both
retrieval and downstream tasks.",Xiaoxi Li
2024-02-02T02:41:28Z,http://arxiv.org/abs/2402.01788v1,LitLLM: A Toolkit for Scientific Literature Review,"Conducting literature reviews for scientific papers is essential for
understanding research, its limitations, and building on existing work. It is a
tedious task which makes an automatic literature review generator appealing.
Unfortunately, many existing works that generate such reviews using Large
Language Models (LLMs) have significant limitations. They tend to
hallucinate-generate non-actual information-and ignore the latest research they
have not been trained on. To address these limitations, we propose a toolkit
that operates on Retrieval Augmented Generation (RAG) principles, specialized
prompting and instructing techniques with the help of LLMs. Our system first
initiates a web search to retrieve relevant papers by summarizing user-provided
abstracts into keywords using an off-the-shelf LLM. Authors can enhance the
search by supplementing it with relevant papers or keywords, contributing to a
tailored retrieval process. Second, the system re-ranks the retrieved papers
based on the user-provided abstract. Finally, the related work section is
generated based on the re-ranked results and the abstract. There is a
substantial reduction in time and effort for literature review compared to
traditional methods, establishing our toolkit as an efficient alternative. Our
open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM
and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)
with the video demo at https://youtu.be/E2ggOZBAFw0.",Shubham Agarwal
2024-02-02T18:23:09Z,http://arxiv.org/abs/2402.01828v1,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed SLM, a joint speech and language model, which fuses a
pretrained foundational speech model and a large language model (LLM), while
preserving the in-context learning capability intrinsic to the pretrained LLM.
In this paper, we apply SLM to speech dialog applications where the dialog
states are inferred directly from the audio signal.
  Task-oriented dialogs often contain domain-specific entities, i.e.,
restaurants, hotels, train stations, and city names, which are difficult to
recognize, however, critical for the downstream applications. Inspired by the
RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented
SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to
retrieve text entities mentioned in the audio. The retrieved entities are then
added as text inputs to the underlying SLM to bias model predictions. We
evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that
this retrieval augmentation boosts model performance, achieving joint goal
accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error
rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach
is broadly applicable to other speech tasks requiring contextual information or
domain-specific entities, such as contextual ASR with biasing capability.",Mingqiu Wang
2024-02-03T03:44:57Z,http://arxiv.org/abs/2402.02008v1,"How well do LLMs cite relevant medical references? An evaluation
  framework and analyses","Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.",Kevin Wu
2024-02-03T05:43:39Z,http://arxiv.org/abs/2402.02044v1,Locally-Adaptive Quantization for Streaming Vector Search,"Retrieving the most similar vector embeddings to a given query among a
massive collection of vectors has long been a key component of countless
real-world applications. The recently introduced Retrieval-Augmented Generation
is one of the most prominent examples. For many of these applications, the
database evolves over time by inserting new data and removing outdated data. In
these cases, the retrieval problem is known as streaming similarity search.
While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector
compression method, yields state-of-the-art search performance for non-evolving
databases, its usefulness in the streaming setting has not been yet
established. In this work, we study LVQ in streaming similarity search. In
support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and
multi-means LVQ that boost its search performance by up to 28% and 27%,
respectively. Our studies show that LVQ and its new variants enable blazing
fast vector search, outperforming its closest competitor by up to 9.4x for
identically distributed data and by up to 8.8x under the challenging scenario
of data distribution shifts (i.e., where the statistical distribution of the
data changes over time). We release our contributions as part of Scalable
Vector Search, an open-source library for high-performance similarity search.",Cecilia Aguerrebere
2024-02-09T19:53:29Z,http://arxiv.org/abs/2402.06764v3,"GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph
  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding","Integrating large language models (LLMs) with knowledge graphs derived from
domain-specific data represents an important advancement towards more powerful
and factual reasoning. As these models grow more capable, it is crucial to
enable them to perform multi-step inferences over real-world knowledge graphs
while minimizing hallucination. While large language models excel at
conversation and text generation, their ability to reason over
domain-specialized graphs of interconnected entities remains limited. For
example, can we query a LLM to identify the optimal contact in a professional
network for a specific goal, based on relationships and attributes in a private
database? The answer is no--such capabilities lie beyond current methods.
However, this question underscores a critical technical gap that must be
addressed. Many high-value applications in areas such as science, security, and
e-commerce rely on proprietary knowledge graphs encoding unique structures,
relationships, and logical constraints. We introduce a fine-tuning framework
for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge
graph into an alternate text representation with labeled question-answer pairs.
We demonstrate that grounding the models in specific graph-based knowledge
expands the models' capacity for structure-based reasoning. Our methodology
leverages the large-language model's generative capabilities to create the
dataset and proposes an efficient alternate to retrieval-augmented generation
styled methods.",Stefan Dernbach
2024-02-10T18:27:28Z,http://arxiv.org/abs/2402.07016v1,"REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models","The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.",Yinghao Zhu
2024-02-12T08:45:08Z,http://arxiv.org/abs/2402.07483v2,T-RAG: Lessons from the LLM Trenches,"Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations, including a Needle in a Haystack test, show that this combination
performs better than a simple RAG or finetuning implementation. Finally, we
share some lessons learned based on our experiences building an LLM application
for real-world use.",Masoomali Fatehkia
2024-02-12T17:17:50Z,http://arxiv.org/abs/2402.07812v2,"Retrieval Augmented Thought Process for Private Data Handling in
  Healthcare","Large Language Models (LLMs) have demonstrated the strong potential to assist
both clinicians and the general public with their extensive medical knowledge.
However, their application in healthcare is constrained due to concerns about
the privacy of data used in training, which prevents the integration of private
and personal information because of security and ethical issues. Moreover, if
their capabilities can be enhanced with information retrieval to access
up-to-date knowledge, the current integration of LLMs with Information
retrieval lacks robustness to imperfect retrieval, which can hinder their
effectiveness and even reduce overall performance. In this work, we address
this challenge by introducing the Retrieval-Augmented Thought Process (RATP).
Given access to external knowledge, RATP formulates the thought generation of
LLMs as a multiple-step decision process. To optimise such a thought process,
RATP leverages Monte-Carlo Tree Search and learns a proxy reward function that
permits cost-efficient inference. On a private dataset of electronic medical
records, deliberately excluded from any LLM training set, RATP achieves 35%
additional accuracy compared to in-context retrieval-augmented generation for
the question-answering task.",Thomas Pouplin
2024-02-15T07:22:04Z,http://arxiv.org/abs/2402.09760v1,Grounding Language Model with Chunking-Free In-Context Retrieval,"This paper presents a novel Chunking-Free In-Context (CFIC) retrieval
approach, specifically tailored for Retrieval-Augmented Generation (RAG)
systems. Traditional RAG systems often struggle with grounding responses using
precise evidence text due to the challenges of processing lengthy documents and
filtering out irrelevant content. Commonly employed solutions, such as document
chunking and adapting language models to handle longer contexts, have their
limitations. These methods either disrupt the semantic coherence of the text or
fail to effectively address the issues of noise and inaccuracy in evidence
retrieval.
  CFIC addresses these challenges by circumventing the conventional chunking
process. It utilizes the encoded hidden states of documents for in-context
retrieval, employing auto-aggressive decoding to accurately identify the
specific evidence text required for user queries, eliminating the need for
chunking. CFIC is further enhanced by incorporating two decoding strategies,
namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies
not only improve the efficiency of the retrieval process but also ensure that
the fidelity of the generated grounding text evidence is maintained. Our
evaluations of CFIC on a range of open QA datasets demonstrate its superiority
in retrieving relevant and accurate evidence, offering a significant
improvement over traditional methods. By doing away with the need for document
chunking, CFIC presents a more streamlined, effective, and efficient retrieval
solution, making it a valuable advancement in the field of RAG systems.",Hongjin Qian
2024-02-15T13:39:55Z,http://arxiv.org/abs/2402.09939v1,Generative AI in the Construction Industry: A State-of-the-art Analysis,"The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.",Ridwan Taiwo
2024-02-16T19:26:09Z,http://arxiv.org/abs/2402.11034v2,"PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal
  Question-Answering","Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. ""Who was the US
president in 1970?""). Little work has studied questions whose temporal context
is relative to the present time (e.g. ""Who was the previous US president?""). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.",Jannat Ara Meem
2024-02-19T17:37:28Z,http://arxiv.org/abs/2402.12317v2,EVOR: Evolving Retrieval for Code Generation,"Recently the retrieval-augmented generation (RAG) has been successfully
applied in code generation. However, existing pipelines for retrieval-augmented
code generation (RACG) employ static knowledge bases with a single source,
limiting the adaptation capabilities of Large Language Models (LLMs) to domains
they have insufficient knowledge of. In this work, we develop a novel pipeline,
EVOR, that employs the synchronous evolution of both queries and diverse
knowledge bases. On two realistic settings where the external knowledge is
required to solve code generation tasks, we compile four new datasets
associated with frequently updated libraries and long-tail programming
languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR
achieves two to four times of execution accuracy compared to other methods such
as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We
demonstrate that EVOR is flexible and can be easily combined with them to
achieve further improvement. Further analysis reveals that EVOR benefits from
the synchronous evolution of queries and documents and the diverse information
sources in the knowledge base. We hope that our studies will inspire more
insights into the design of advanced RACG pipelines in future research. Our
model, code, and data are available at https://arks-codegen.github.io.",Hongjin Su
2024-02-19T18:31:11Z,http://arxiv.org/abs/2402.12352v1,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,"Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.",Julien Delile
2024-02-22T12:13:35Z,http://arxiv.org/abs/2402.14480v1,"MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems
  in LLM Augmented Generation","Augmented generation techniques such as Retrieval-Augmented Generation (RAG)
and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing
large language model (LLM) outputs with external knowledge and cached
information. However, the integration of vector databases, which serve as a
backbone for these augmentations, introduces critical challenges, particularly
in ensuring accurate vector matching. False vector matching in these databases
can significantly compromise the integrity and reliability of LLM outputs,
leading to misinformation or erroneous responses. Despite the crucial impact of
these issues, there is a notable research gap in methods to effectively detect
and address false vector matches in LLM-augmented generation. This paper
presents MeTMaP, a metamorphic testing framework developed to identify false
vector matching in LLM-augmented generation systems. We derive eight
metamorphic relations (MRs) from six NLP datasets, which form our method's
core, based on the idea that semantically similar texts should match and
dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets
for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over
203 vector matching configurations, involving 29 embedding models and 7
distance metrics, uncovers significant inaccuracies. The results, showing a
maximum accuracy of only 41.51\% on our tests compared to the original
datasets, emphasize the widespread issue of false matches in vector matching
methods and the critical need for effective detection and mitigation in
LLM-augmented applications.",Guanyu Wang
2024-02-25T11:24:41Z,http://arxiv.org/abs/2402.16063v3,Citation-Enhanced Generation for LLM-based Chatbots,"Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc Citation-Enhanced Generation (CEG) approach combined with
retrieval argumentation. Unlike previous studies that focus on preventing
hallucinations during generation, our method addresses this issue in a post-hoc
way. It incorporates a retrieval module to search for supporting documents
relevant to the generated content, and employs a natural language
inference-based citation generation module. Once the statements in the
generated content lack of reference, our model can regenerate responses until
all statements are supported by citations. Note that our method is a
training-free plug-and-play plugin that is capable of various LLMs. Experiments
on various hallucination-related datasets show our framework outperforms
state-of-the-art methods in both hallucination detection and response
regeneration on three benchmarks. Our codes and dataset will be publicly
available.",Weitao Li
2024-02-26T23:37:59Z,http://arxiv.org/abs/2402.17081v1,"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI
  Judge","This study presents an innovative enhancement to retrieval-augmented
generation (RAG) systems by seamlessly integrating fine-tuned large language
models (LLMs) with vector databases. This integration capitalizes on the
combined strengths of structured data retrieval and the nuanced comprehension
provided by advanced LLMs. Central to our approach are the LoRA and QLoRA
methodologies, which stand at the forefront of model refinement through
parameter-efficient fine-tuning and memory optimization. A novel feature of our
research is the incorporation of user feedback directly into the training
process, ensuring the model's continuous adaptation to user expectations and
thus, improving its performance and applicability. Additionally, we introduce a
Quantized Influence Measure (QIM) as an innovative ""AI Judge"" mechanism to
enhance the precision of result selection, further refining the system's
accuracy. Accompanied by an executive diagram and a detailed algorithm for
fine-tuning QLoRA, our work provides a comprehensive framework for implementing
these advancements within chatbot technologies. This research contributes
significant insights into LLM optimization for specific uses and heralds new
directions for further development in retrieval-augmented models. Through
extensive experimentation and analysis, our findings lay a robust foundation
for future advancements in chatbot technology and retrieval systems, marking a
significant step forward in the creation of more sophisticated, precise, and
user-centric conversational AI systems.",Keshav Rangan
2024-02-27T13:22:51Z,http://arxiv.org/abs/2402.17497v2,"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering","Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (eg., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness regarding the reliability of external knowledge for LLMs, so
as to adaptively utilize external knowledge in RAG systems. Specially, we
develop a novel architecture for LLM-based RAG systems, by incorporating a
specially designed assessment module that precisely assesses the relevance of
retrieved documents. Furthermore, we propose an improved training method based
on bi-granularity relevance fusion and noise-resistant training. By combining
the improvements in both architecture and training, our proposed REAR can
better utilize external knowledge by effectively perceiving the relevance of
retrieved documents. Experiments on four open-domain QA tasks show that REAR
significantly outperforms previous a number of competitive RAG approaches. Our
codes can be accessed at https://github.com/RUCAIBox/REAR.",Yuhao Wang
2024-02-27T18:42:31Z,http://arxiv.org/abs/2402.17753v1,Evaluating Very Long-Term Conversational Memory of LLM Agents,"Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.",Adyasha Maharana
2024-02-27T21:01:41Z,http://arxiv.org/abs/2402.17887v4,"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability","Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.",Junda Wang
2024-02-29T18:20:37Z,http://arxiv.org/abs/2402.19421v1,"Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based
  Search Engines","In the domain of digital information dissemination, search engines act as
pivotal conduits linking information seekers with providers. The advent of
chat-based search engines utilizing Large Language Models (LLMs) and Retrieval
Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary
leap in the search ecosystem. They demonstrate metacognitive abilities in
interpreting web information and crafting responses with human-like
understanding and creativity. Nonetheless, the intricate nature of LLMs renders
their ""cognitive"" processes opaque, challenging even their designers'
understanding. This research aims to dissect the mechanisms through which an
LLM-powered chat-based search engine, specifically Bing Chat, selects
information sources for its responses. To this end, an extensive dataset has
been compiled through engagements with New Bing, documenting the websites it
cites alongside those listed by the conventional search engine. Employing
natural language processing (NLP) techniques, the research reveals that Bing
Chat exhibits a preference for content that is not only readable and formally
structured, but also demonstrates lower perplexity levels, indicating a unique
inclination towards text that is predictable by the underlying LLM. Further
enriching our analysis, we procure an additional dataset through interactions
with the GPT-4 based knowledge retrieval API, unveiling a congruent text
preference between the RAG API and Bing Chat. This consensus suggests that
these text preferences intrinsically emerge from the underlying language
models, rather than being explicitly crafted by Bing Chat's developers.
Moreover, our investigation documents a greater similarity among websites cited
by RAG technologies compared to those ranked highest by conventional search
engines.",Lijia Ma
2024-02-23T18:45:35Z,http://arxiv.org/abs/2403.00801v2,"Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model","The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.",Qiaoyu Tang
2024-02-29T09:35:41Z,http://arxiv.org/abs/2403.00840v1,EyeGPT: Ophthalmic Assistant with Large Language Models,"Artificial intelligence (AI) has gained significant attention in healthcare
consultation due to its potential to improve clinical workflow and enhance
medical communication. However, owing to the complex nature of medical
information, large language models (LLM) trained with general world knowledge
might not possess the capability to tackle medical-related tasks at an expert
level. Here, we introduce EyeGPT, a specialized LLM designed specifically for
ophthalmology, using three optimization strategies including role-playing,
finetuning, and retrieval-augmented generation. In particular, we proposed a
comprehensive evaluation framework that encompasses a diverse dataset, covering
various subspecialties of ophthalmology, different users, and diverse inquiry
intents. Moreover, we considered multiple evaluation metrics, including
accuracy, understandability, trustworthiness, empathy, and the proportion of
hallucinations. By assessing the performance of different EyeGPT variants, we
identify the most effective one, which exhibits comparable levels of
understandability, trustworthiness, and empathy to human ophthalmologists (all
Ps>0.05). Overall, ur study provides valuable insights for future research,
facilitating comprehensive comparisons and evaluations of different strategies
for developing specialized LLMs in ophthalmology. The potential benefits
include enhancing the patient experience in eye care and optimizing
ophthalmologists' services.",Xiaolan Chen
2024-03-01T07:14:45Z,http://arxiv.org/abs/2403.00872v1,"DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy
  in Large-Scale Databases","The task of converting natural language queries into SQL queries is
intricate, necessitating a blend of precise techniques for an accurate
translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a
significant development in this domain. This paper introduces DFIN (Decomposed
Focused-In-Context), an innovative extension of DIN-SQL that enhances
Text-to-SQL conversion by addressing schema linking errors, which are a major
source of inaccuracies. DFIN uniquely alternates between prompting techniques
and Retrieval-Augmented Generation (RAG), adapting to the size and complexity
of the database schema. A preprocessing phase embeds database definitions and
leverages annotated files, akin to those in the BIRD dataset, facilitating the
runtime retrieval of pertinent schema information. This strategy significantly
reduces the token count for schema linking prompts, enabling the use of a
standard GPT-4 model over its larger context variant, thus handling large-scale
databases more effectively and economically. Our evaluation on the BIRD
dataset, a challenging real-world benchmark, demonstrates that DFIN not only
scales efficiently but also improves accuracy, achieving a score of 51.69. This
improvement surpasses DIN-SQL method (the current third-place), which is the
highest-ranked model employing in-context learning rather than fine-tuning,
previously scoring 50.72. The advancement of DFIN underscores the evolving
capabilities of in-context learning methodologies combined with advanced
language models, offering a promising avenue for future research in complex
Text-to-SQL conversion tasks.",Shai Volvovsky
2024-03-07T08:34:57Z,http://arxiv.org/abs/2403.04317v2,Online Adaptation of Language Models with a Memory of Amortized Contexts,"Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.",Jihoon Tack
2024-03-13T08:50:15Z,http://arxiv.org/abs/2403.08345v1,"From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction","The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.",Vamsi Krishna Kommineni
2024-03-15T06:59:43Z,http://arxiv.org/abs/2403.10059v2,Repoformer: Selective Retrieval for Repository-Level Code Completion,"Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). In this paper, we propose a selective RAG
framework to avoid retrieval when unnecessary. To power this framework, we
design a self-supervised learning approach to enable a code LM to accurately
self-evaluate whether retrieval can improve its output quality and robustly
leverage the potentially noisy retrieved contexts. Using this LM as both the
selective RAG policy and the generation model, our framework achieves
state-of-the-art repository-level code completion performance on diverse
benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new
long-form code completion benchmark. Meanwhile, our analyses show that
selectively retrieving brings as much as 70% inference speedup in the online
serving setting without harming the performance. We further demonstrate that
our framework is able to accommodate different generation models, retrievers,
and programming languages. These advancements position our framework as an
important step towards more accurate and efficient repository-level code
completion.",Di Wu
2024-03-15T15:43:02Z,http://arxiv.org/abs/2403.10408v1,"SocialGenPod: Privacy-Friendly Generative AI Social Web Applications
  with Decentralised Personal Data Stores","We present SocialGenPod, a decentralised and privacy-friendly way of
deploying generative AI Web applications. Unlike centralised Web and data
architectures that keep user data tied to application and service providers, we
show how one can use Solid -- a decentralised Web specification -- to decouple
user data from generative AI applications. We demonstrate SocialGenPod using a
prototype that allows users to converse with different Large Language Models,
optionally leveraging Retrieval Augmented Generation to generate answers
grounded in private documents stored in any Solid Pod that the user is allowed
to access, directly or indirectly. SocialGenPod makes use of Solid access
control mechanisms to give users full control of determining who has access to
data stored in their Pods. SocialGenPod keeps all user data (chat history, app
configuration, personal documents, etc) securely in the user's personal Pod;
separate from specific model or application providers. Besides better privacy
controls, this approach also enables portability across different services and
applications. Finally, we discuss challenges, posed by the large compute
requirements of state-of-the-art models, that future research in this area
should address. Our prototype is open-source and available at:
https://github.com/Vidminas/socialgenpod/.",Vidminas Vizgirda
2024-03-15T17:04:27Z,http://arxiv.org/abs/2403.10588v1,"S3LLM: Large-Scale Scientific Software Understanding with LLMs using
  Source, Metadata, and Document","The understanding of large-scale scientific software poses significant
challenges due to its diverse codebase, extensive code length, and target
computing architectures. The emergence of generative AI, specifically large
language models (LLMs), provides novel pathways for understanding such complex
scientific codes. This paper presents S3LLM, an LLM-based framework designed to
enable the examination of source code, code metadata, and summarized
information in conjunction with textual technical reports in an interactive,
conversational manner through a user-friendly interface. S3LLM leverages
open-source LLaMA-2 models to enhance code analysis through the automatic
transformation of natural language queries into domain-specific language (DSL)
queries. Specifically, it translates these queries into Feature Query Language
(FQL), enabling efficient scanning and parsing of entire code repositories. In
addition, S3LLM is equipped to handle diverse metadata types, including DOT,
SQL, and customized formats. Furthermore, S3LLM incorporates retrieval
augmented generation (RAG) and LangChain technologies to directly query
extensive documents. S3LLM demonstrates the potential of using locally deployed
open-source LLMs for the rapid understanding of large-scale scientific
computing software, eliminating the need for extensive coding expertise, and
thereby making the process more efficient and effective. S3LLM is available at
https://github.com/ResponsibleAILab/s3llm.",Kareem Shaik
2024-03-18T11:19:37Z,http://arxiv.org/abs/2403.11671v1,HDLdebugger: Streamlining HDL debugging with Large Language Models,"In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.",Xufeng Yao
2024-03-19T09:45:33Z,http://arxiv.org/abs/2403.12582v1,"AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented
  Stock-Chain Framework","The task of financial analysis primarily encompasses two key areas: stock
trend prediction and the corresponding financial question answering. Currently,
machine learning and deep learning algorithms (ML&DL) have been widely applied
for stock trend predictions, leading to significant progress. However, these
methods fail to provide reasons for predictions, lacking interpretability and
reasoning processes. Also, they can not integrate textual information such as
financial news or reports. Meanwhile, large language models (LLMs) have
remarkable textual understanding and generation ability. But due to the
scarcity of financial training datasets and limited integration with real-time
knowledge, LLMs still suffer from hallucinations and are unable to keep up with
the latest information. To tackle these challenges, we first release AlphaFin
datasets, combining traditional research datasets, real-time financial data,
and handwritten chain-of-thought (CoT) data. It has a positive impact on
training LLMs for completing financial analysis. We then use AlphaFin datasets
to benchmark a state-of-the-art method, called Stock-Chain, for effectively
tackling the financial analysis task, which integrates retrieval-augmented
generation (RAG) techniques. Extensive experiments are conducted to demonstrate
the effectiveness of our framework on financial analysis.",Xiang Li
2024-03-21T14:17:28Z,http://arxiv.org/abs/2403.14421v3,DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning,"Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.",Jonathan Lebensold
2024-03-22T15:06:45Z,http://arxiv.org/abs/2403.15268v5,"Awakening Augmented Generation: Learning to Awaken Internal Knowledge of
  Large Language Models for Question Answering","Retrieval-Augmented-Generation and Generation-Augmented-Generation have been
proposed to enhance the knowledge required for question answering with Large
Language Models (LLMs) by leveraging richer context. However, the former relies
on external resources, and both require incorporating explicit documents into
the context, which increases execution costs and susceptibility to noise data
during inference. Recent works indicate that LLMs model rich knowledge, but it
is often not effectively activated and awakened. Inspired by this, we propose a
novel knowledge-augmented framework, $\textbf{Awakening-Augmented-Generation}$
(AAG), which mimics the human ability to answer questions using only thinking
and recalling to compensate for knowledge gaps, thereby awaking relevant
knowledge in LLMs without relying on external resources. AAG consists of two
key components for awakening richer context. Explicit awakening fine-tunes a
context generator to create a synthetic, compressed document that functions as
symbolic context. Implicit awakening utilizes a hypernetwork to generate
adapters based on the question and synthetic document, which are inserted into
LLMs to serve as parameter context. Experimental results on three datasets
demonstrate that AAG exhibits significant advantages in both open-domain and
closed-book settings, as well as in out-of-distribution generalization. Our
code will be available at \url{https://github.com/Xnhyacinth/IAG}.",Huanxuan Liao
2024-03-23T05:32:46Z,http://arxiv.org/abs/2403.15729v3,Towards a RAG-based Summarization Agent for the Electron-Ion Collider,"The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.",Karthik Suresh
2024-03-23T06:03:36Z,http://arxiv.org/abs/2403.15736v2,"General LLMs as Instructors for Domain-Specific LLMs: A Sequential
  Fusion Method to Integrate Extraction and Editing","The substantial interest in updating Large Language Models (LLMs) without
retraining from scratch is accompanied by several challenges. This is
particularly true when updating LLMs with datasets that necessitate
domain-expert reasoning across extensive texts, despite limited samples. We
termed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs
(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval
Augmented Generation (RAG) are inadequate for addressing this critical issue,
particularly evident in our exploration of a specific medical dataset that
epitomizes the distinct needs of FDoR-UL. To tackle this challenge, we
introduce a Sequential Fusion method to integrate knowledge from complex
contexts into LLMs. This method employs a two-stage framework: initially
leveraging general LLMs to perform relation extraction for knowledge
acquisition from complex texts, followed by updating domain-specific LLMs
through Knowledge Editing (KE). Employing our method, domain-specific LLMs
achieved a 71.7% accuracy (an average gain of 39.1%) in question-answering
tasks. Furthermore, we expanded our evaluation to a novel economics-management
dataset we developed, where our method achieved a 75.0% accuracy (an average
gain of 45.0%). These findings underscore the effectiveness and flexibility of
our approach in FDoR-UL across various domains.",Xin Zhang
2024-03-28T03:14:18Z,http://arxiv.org/abs/2403.19116v1,MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,"In today's fast-paced industry, professionals face the challenge of
summarizing a large number of documents and extracting vital information from
them on a daily basis. These metrics are frequently hidden away in tables
and/or their nested hyperlinks. To address this challenge, the approach of
Table Question Answering (QA) has been developed to extract the relevant
information. However, traditional Table QA training tasks that provide a table
and an answer(s) from a gold cell coordinate(s) for a question may not always
ensure extracting the accurate answer(s). Recent advancements in Large Language
Models (LLMs) have opened up new possibilities for extracting information from
tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot
Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The
first step involves Few-Shot Learning (FSL), where relevant tables and
associated contexts of hyperlinks are retrieved based on a given question. The
retrieved content is then used to construct few-shot prompts as inputs to an
LLM, such as ChatGPT. To tackle the challenge of answering complex questions,
the second step leverages Chain-of-thought (CoT) prompting to decompose the
complex question into a sequential chain of questions and reasoning thoughts in
a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process
by retrieving relevant tables and contexts of hyperlinks that are relevant to
the resulting reasoning thoughts and questions. These additional contexts are
then used to supplement the prompt used in the first step, resulting in more
accurate answers from an LLM. Empirical results from OTT-QA demonstrate that
our abstractive QA approach significantly improves the accuracy of extractive
Table QA methods.",Che Guan
2024-03-28T08:27:44Z,http://arxiv.org/abs/2403.19216v2,Are Large Language Models Good at Utility Judgments?,"Retrieval-augmented generation (RAG) is considered to be a promising approach
to alleviate the hallucination issue of large language models (LLMs), and it
has received widespread attention from researchers recently. Due to the
limitation in the semantic understanding of retrieval models, the success of
RAG heavily lies on the ability of LLMs to identify passages with utility.
Recent efforts have explored the ability of LLMs to assess the relevance of
passages in retrieval, but there has been limited work on evaluating the
utility of passages in supporting question answering. In this work, we conduct
a comprehensive study about the capabilities of LLMs in utility evaluation for
open-domain QA. Specifically, we introduce a benchmarking procedure and
collection of candidate passages with different characteristics, facilitating a
series of experiments with five representative LLMs. Our experiments reveal
that: (i) well-instructed LLMs can distinguish between relevance and utility,
and that LLMs are highly receptive to newly generated counterfactual passages.
Moreover, (ii) we scrutinize key factors that affect utility judgments in the
instruction design. And finally, (iii) to verify the efficacy of utility
judgments in practical retrieval augmentation applications, we delve into LLMs'
QA capabilities using the evidence judged with utility and direct dense
retrieval results. (iv) We propose a k-sampling, listwise approach to reduce
the dependency of LLMs on the sequence of input passages, thereby facilitating
subsequent answer generation. We believe that the way we formalize and study
the problem along with our findings contributes to a critical assessment of
retrieval-augmented LLMs. Our code and benchmark can be found at
\url{https://github.com/ict-bigdatalab/utility_judgments}.",Hengran Zhang
2024-03-30T22:41:05Z,http://arxiv.org/abs/2404.00486v1,"Dialectical Alignment: Resolving the Tension of 3H and Security Threats
  of LLMs","With the rise of large language models (LLMs), ensuring they embody the
principles of being helpful, honest, and harmless (3H), known as Human
Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,
etc., effectively fine-tune LLMs to match preferences in the preference
dataset, they often lead LLMs to highly receptive human input and external
evidence, even when this information is poisoned. This leads to a tendency for
LLMs to be Adaptive Chameleons when external evidence conflicts with their
parametric memory. This exacerbates the risk of LLM being attacked by external
poisoned data, which poses a significant security risk to LLM system
applications such as Retrieval-augmented generation (RAG). To address the
challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)
utilizes AI feedback to identify optimal strategies for LLMs to navigate
inter-context conflicts and context-memory conflicts with different external
evidence in context window (i.e., different ratios of poisoned factual
contexts); (2) constructs the SFT dataset as well as the preference dataset
based on the AI feedback and strategies above; (3) uses the above datasets for
LLM alignment to defense poisoned context attack while preserving the
effectiveness of in-context knowledge editing. Our experiments show that the
dialectical alignment model improves poisoned data attack defense by 20 and
does not require any additional prompt engineering or prior declaration of
``you may be attacked`` to the LLMs' context window.",Shu Yang
2024-04-05T02:53:51Z,http://arxiv.org/abs/2404.03868v2,"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge
  Graph Construction","In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that, in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schemas
easily exceed the LLMs' context window length. Furthermore, there are scenarios
where a fixed pre-defined schema is not available and we would like the method
to construct a high-quality KG with a succinct self-generated schema. To
address these problems, we propose a three-phase framework named
Extract-Define-Canonicalize (EDC): open information extraction followed by
schema definition and post-hoc canonicalization. EDC is flexible in that it can
be applied to settings where a pre-defined target schema is available and when
it is not; in the latter case, it constructs a schema automatically and applies
self-canonicalization. To further improve performance, we introduce a trained
component that retrieves schema elements relevant to the input text; this
improves the LLMs' extraction performance in a retrieval-augmented
generation-like manner. We demonstrate on three KGC benchmarks that EDC is able
to extract high-quality triplets without any parameter tuning and with
significantly larger schemas compared to prior works. Code for EDC is available
at https://github.com/clear-nus/edc.",Bowen Zhang
2024-04-05T11:55:52Z,http://arxiv.org/abs/2404.04044v2,A Comparison of Methods for Evaluating Generative IR,"Information retrieval systems increasingly incorporate generative components.
For example, in a retrieval augmented generation (RAG) system, a retrieval
component might provide a source of ground truth, while a generative component
summarizes and augments its responses. In other systems, a large language model
(LLM) might directly generate responses without consulting a retrieval
component. While there are multiple definitions of generative information
retrieval (Gen-IR) systems, in this paper we focus on those systems where the
system's response is not drawn from a fixed collection of documents or
passages. The response to a query may be entirely new text. Since traditional
IR evaluation methods break down under this model, we explore various methods
that extend traditional offline evaluation approaches to the Gen-IR context.
Offline IR evaluation traditionally employs paid human assessors, but
increasingly LLMs are replacing human assessment, demonstrating capabilities
similar or superior to crowdsourced labels. Given that Gen-IR systems do not
generate responses from a fixed set, we assume that methods for Gen-IR
evaluation must largely depend on LLM-generated labels. Along with methods
based on binary and graded relevance, we explore methods based on explicit
subtopics, pairwise preferences, and embeddings. We first validate these
methods against human assessments on several TREC Deep Learning Track tasks; we
then apply these methods to evaluate the output of several purely generative
systems. For each method we consider both its ability to act autonomously,
without the need for human labels or other input, and its ability to support
human auditing. To trust these methods, we must be assured that their results
align with human assessments. In order to do so, evaluation criteria must be
transparent, so that outcomes can be audited by human assessors.",Negar Arabzadeh
2024-04-04T02:58:21Z,http://arxiv.org/abs/2404.04287v1,CONFLARE: CONFormal LArge language model REtrieval,"Retrieval-augmented generation (RAG) frameworks enable large language models
(LLMs) to retrieve relevant information from a knowledge base and incorporate
it into the context for generating responses. This mitigates hallucinations and
allows for the updating of knowledge without retraining the LLM. However, RAG
does not guarantee valid responses if retrieval fails to identify the necessary
information as the context for response generation. Also, if there is
contradictory content, the RAG response will likely reflect only one of the two
possible responses. Therefore, quantifying uncertainty in the retrieval process
is crucial for ensuring RAG trustworthiness. In this report, we introduce a
four-step framework for applying conformal prediction to quantify retrieval
uncertainty in RAG frameworks. First, a calibration set of questions answerable
from the knowledge base is constructed. Each question's embedding is compared
against document embeddings to identify the most relevant document chunks
containing the answer and record their similarity scores. Given a
user-specified error rate ({\alpha}), these similarity scores are then analyzed
to determine a similarity score cutoff threshold. During inference, all chunks
with similarity exceeding this threshold are retrieved to provide context to
the LLM, ensuring the true answer is captured in the context with a
(1-{\alpha}) confidence level. We provide a Python package that enables users
to implement the entire workflow proposed in our work, only using LLMs and
without human intervention.",Pouria Rouzrokh
2024-04-08T15:03:57Z,http://arxiv.org/abs/2404.05590v2,"MedExpQA: Multilingual Benchmarking of Large Language Models for Medical
  Question Answering","Large Language Models (LLMs) have the potential of facilitating the
development of Artificial Intelligence technology to assist medical experts for
interactive decision support, which has been demonstrated by their competitive
performances in Medical QA. However, while impressive, the required quality bar
for medical applications remains far from being achieved. Currently, LLMs
remain challenged by outdated knowledge and by their tendency to generate
hallucinated content. Furthermore, most benchmarks to assess medical knowledge
lack reference gold explanations which means that it is not possible to
evaluate the reasoning of LLMs predictions. Finally, the situation is
particularly grim if we consider benchmarking LLMs for languages other than
English which remains, as far as we know, a totally neglected topic. In order
to address these shortcomings, in this paper we present MedExpQA, the first
multilingual benchmark based on medical exams to evaluate LLMs in Medical
Question Answering. To the best of our knowledge, MedExpQA includes for the
first time reference gold explanations written by medical doctors which can be
leveraged to establish various gold-based upper-bounds for comparison with LLMs
performance. Comprehensive multilingual experimentation using both the gold
reference explanations and Retrieval Augmented Generation (RAG) approaches show
that performance of LLMs still has large room for improvement, especially for
languages other than English. Furthermore, and despite using state-of-the-art
RAG methods, our results also demonstrate the difficulty of obtaining and
integrating readily available medical knowledge that may positively impact
results on downstream evaluations for Medical Question Answering. So far the
benchmark is available in four languages, but we hope that this work may
encourage further development to other languages.",Iñigo Alonso
2024-04-09T14:34:48Z,http://arxiv.org/abs/2404.06347v2,RAR-b: Reasoning as Retrieval Benchmark,"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks
have been the two major avenues to record the progress of embedding models in
the past few years. Under the emerging Retrieval-augmented Generation (RAG)
paradigm, we envision the need to evaluate next-level language understanding
abilities of embedding models, and take a conscious look at the reasoning
abilities stored in them. Addressing this, we pose the question: Can retrievers
solve reasoning problems? By transforming reasoning tasks into retrieval tasks,
we find that without specifically trained for reasoning-level language
understanding, current state-of-the-art retriever models may still be far from
being competent for playing the role of assisting LLMs, especially in
reasoning-intensive tasks. Moreover, albeit trained to be aware of
instructions, instruction-aware IR models are often better off without
instructions in inference time for reasoning tasks, posing an overlooked
retriever-LLM behavioral gap for the research community to align. However,
recent decoder-based embedding models show great promise in narrowing the gap,
highlighting the pathway for embedding models to achieve reasoning-level
language understanding. We also show that, although current off-the-shelf
re-ranker models fail on these tasks, injecting reasoning abilities into them
through fine-tuning still appears easier than doing so to bi-encoders, and we
are able to achieve state-of-the-art performance across all tasks by
fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark
(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning
abilities stored in retriever models. RAR-b is available at
https://github.com/gowitheflow-1998/RAR-b.",Chenghao Xiao
2024-04-10T07:56:26Z,http://arxiv.org/abs/2404.06809v3,Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation,"The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.",Ruotong Pan
2024-04-10T16:12:50Z,http://arxiv.org/abs/2404.07135v2,"Towards Robustness of Text-to-Visualization Translation against Lexical
  and Phrasal Variability","Text-to-Vis is an emerging task in the natural language processing (NLP) area
that aims to automatically generate data visualizations from natural language
questions (NLQs). Despite their progress, existing text-to-vis models often
heavily rely on lexical matching between words in the questions and tokens in
data schemas. This overreliance on lexical matching may lead to a diminished
level of model robustness against input variations. In this study, we
thoroughly examine the robustness of current text-to-vis models, an area that
has not previously been explored. In particular, we construct the first
robustness dataset nvBench-Rob, which contains diverse lexical and phrasal
variations based on the original text-to-vis benchmark nvBench. Then, we found
that the performance of existing text-to-vis models on this new dataset
dramatically drops, implying that these methods exhibit inadequate robustness
overall. Finally, we propose a novel framework based on Retrieval-Augmented
Generation (RAG) technique, named GRED, specifically designed to address input
perturbations in these two variants. The framework consists of three parts:
NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and
Annotation-based Debugger, which are used to tackle the challenges posed by
natural language variants, programming style differences and data schema
variants, respectively. Extensive experimental evaluations show that, compared
to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs
better in terms of model robustness, with a 32% increase in accuracy on the
proposed nvBench-Rob dataset.",Jinwei Lu
2024-04-11T21:48:54Z,http://arxiv.org/abs/2404.08137v2,Generative Information Retrieval Evaluation,"This paper is a draft of a chapter intended to appear in a forthcoming book
on generative information retrieval, co-edited by Chirag Shah and Ryen White.
In this chapter, we consider generative information retrieval evaluation from
two distinct but interrelated perspectives. First, large language models (LLMs)
themselves are rapidly becoming tools for evaluation, with current research
indicating that LLMs may be superior to crowdsource workers and other paid
assessors on basic relevance judgement tasks. We review past and ongoing
related research, including speculation on the future of shared task
initiatives, such as TREC, and a discussion on the continuing need for human
assessments. Second, we consider the evaluation of emerging LLM-based
generative information retrieval (GenIR) systems, including retrieval augmented
generation (RAG) systems. We consider approaches that focus both on the
end-to-end evaluation of GenIR systems and on the evaluation of a retrieval
component as an element in a RAG system. Going forward, we expect the
evaluation of GenIR systems to be at least partially based on LLM-based
assessment, creating an apparent circularity, with a system seemingly
evaluating its own output. We resolve this apparent circularity in two ways: 1)
by viewing LLM-based assessment as a form of ""slow search"", where a slower IR
system is used for evaluation and training of a faster production IR system;
and 2) by recognizing a continuing need to ground evaluation in human
assessment, even if the characteristics of that human assessment must change.",Marwah Alaofi
2024-04-13T02:39:36Z,http://arxiv.org/abs/2404.08878v1,"Generative AI Agent for Next-Generation MIMO Design: Fundamentals,
  Challenges, and Vision","Next-generation multiple input multiple output (MIMO) is expected to be
intelligent and scalable. In this paper, we study generative artificial
intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we
provide an overview of the development, fundamentals, and challenges of the
next-generation MIMO. Then, we propose the concept of the generative AI agent,
which is capable of generating tailored and specialized contents with the aid
of large language model (LLM) and retrieval augmented generation (RAG). Next,
we comprehensively discuss the features and advantages of the generative AI
agent framework. More importantly, to tackle existing challenges of
next-generation MIMO, we discuss generative AI agent-enabled next-generation
MIMO design, from the perspective of performance analysis, signal processing,
and resource allocation. Furthermore, we present two compelling case studies
that demonstrate the effectiveness of leveraging the generative AI agent for
performance analysis in complex configuration scenarios. These examples
highlight how the integration of generative AI agents can significantly enhance
the analysis and design of next-generation MIMO systems. Finally, we discuss
important potential research future directions.",Zhe Wang
2024-04-14T03:44:54Z,http://arxiv.org/abs/2404.09134v2,"Generative AI Agents with Large Language Model for Satellite Networks
  via a Mixture of Experts Transmission","In response to the needs of 6G global communications, satellite communication
networks have emerged as a key solution. However, the large-scale development
of satellite communication networks is constrained by the complex system
models, whose modeling is challenging for massive users. Moreover, transmission
interference between satellites and users seriously affects communication
performance. To solve these problems, this paper develops generative artificial
intelligence (AI) agents for model formulation and then applies a mixture of
experts (MoE) approach to design transmission strategies. Specifically, we
leverage large language models (LLMs) to build an interactive modeling paradigm
and utilize retrieval-augmented generation (RAG) to extract satellite expert
knowledge that supports mathematical modeling. Afterward, by integrating the
expertise of multiple specialized components, we propose an MoE-proximal policy
optimization (PPO) approach to solve the formulated problem. Each expert can
optimize the optimization variables at which it excels through specialized
training through its own network and then aggregates them through the gating
network to perform joint optimization. The simulation results validate the
accuracy and effectiveness of employing a generative agent for problem
formulation. Furthermore, the superiority of the proposed MoE-ppo approach over
other benchmarks is confirmed in solving the formulated problem. The
adaptability of MoE-PPO to various customized modeling problems has also been
demonstrated.",Ruichen Zhang
2024-04-14T16:34:31Z,http://arxiv.org/abs/2404.09296v2,"Cross-Data Knowledge Graph Construction for LLM-enabled Educational
  Question-Answering System: A Case Study at HCMUT","In today's rapidly evolving landscape of Artificial Intelligence, large
language models (LLMs) have emerged as a vibrant research topic. LLMs find
applications in various fields and contribute significantly. Despite their
powerful language capabilities, similar to pre-trained language models (PLMs),
LLMs still face challenges in remembering events, incorporating new
information, and addressing domain-specific issues or hallucinations. To
overcome these limitations, researchers have proposed Retrieval-Augmented
Generation (RAG) techniques, some others have proposed the integration of LLMs
with Knowledge Graphs (KGs) to provide factual context, thereby improving
performance and delivering more accurate feedback to user queries.
  Education plays a crucial role in human development and progress. With the
technology transformation, traditional education is being replaced by digital
or blended education. Therefore, educational data in the digital environment is
increasing day by day. Data in higher education institutions are diverse,
comprising various sources such as unstructured/structured text, relational
databases, web/app-based API access, etc. Constructing a Knowledge Graph from
these cross-data sources is not a simple task. This article proposes a method
for automatically constructing a Knowledge Graph from multiple data sources and
discusses some initial applications (experimental trials) of KG in conjunction
with LLMs for question-answering tasks.",Tuan Bui
2024-04-16T12:10:01Z,http://arxiv.org/abs/2404.10496v4,"Spiral of Silence: How is Large Language Model Killing Information
  Retrieval? -- A Case Study on Open Domain Question Answering","The practice of Retrieval-Augmented Generation (RAG), which integrates Large
Language Models (LLMs) with retrieval systems, has become increasingly
prevalent. However, the repercussions of LLM-derived content infiltrating the
web and influencing the retrieval-generation feedback loop are largely
uncharted territories. In this study, we construct and iteratively run a
simulation pipeline to deeply investigate the short-term and long-term effects
of LLM text on RAG systems. Taking the trending Open Domain Question Answering
(ODQA) task as a point of entry, our findings reveal a potential digital
""Spiral of Silence"" effect, with LLM-generated text consistently outperforming
human-authored content in search rankings, thereby diminishing the presence and
impact of human contributions online. This trend risks creating an imbalanced
information ecosystem, where the unchecked proliferation of erroneous
LLM-generated content may result in the marginalization of accurate
information. We urge the academic community to take heed of this potential
issue, ensuring a diverse and authentic digital information landscape.",Xiaoyang Chen
2024-03-23T13:25:01Z,http://arxiv.org/abs/2404.10779v1,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",Mathav Raj J
2024-04-17T23:00:03Z,http://arxiv.org/abs/2404.11792v2,"Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning:
  A Comparative Study","This paper investigates the impact of domain-specific model fine-tuning and
of reasoning mechanisms on the performance of question-answering (Q&A) systems
powered by large language models (LLMs) and Retrieval-Augmented Generation
(RAG). Using the FinanceBench SEC financial filings dataset, we observe that,
for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves
better accuracy than generic models, with relatively greater gains attributable
to fine-tuned embedding models. Additionally, employing reasoning iterations on
top of RAG delivers an even bigger jump in performance, enabling the Q&A
systems to get closer to human-expert quality. We discuss the implications of
such findings, propose a structured technical design space capturing major
technical components of Q&A AI, and provide recommendations for making
high-impact technical choices for such components. We plan to follow up on this
work with actionable guides for AI teams and further investigations into the
impact of domain-specific augmentation in RAG and into agentic AI capabilities
such as advanced planning and reasoning.",Zooey Nguyen
2024-04-19T10:27:40Z,http://arxiv.org/abs/2404.12772v1,"Generating Test Scenarios from NL Requirements using Retrieval-Augmented
  LLMs: An Industrial Study","Test scenarios are specific instances of test cases that describe actions to
validate a particular software functionality. By outlining the conditions under
which the software operates and the expected outcomes, test scenarios ensure
that the software functionality is tested in an integrated manner. Test
scenarios are crucial for systematically testing an application under various
conditions, including edge cases, to identify potential issues and guarantee
overall performance and reliability. Specifying test scenarios is tedious and
requires a deep understanding of software functionality and the underlying
domain. It further demands substantial effort and investment from already time-
and budget-constrained requirements engineers and testing teams. This paper
presents an automated approach (RAGTAG) for test scenario generation using
Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG
allows the integration of specific domain knowledge with LLMs' generation
capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post
with bilingual requirements in German and English. Our results from an
interview survey conducted with four experts on five dimensions -- relevance,
coverage, correctness, coherence and feasibility, affirm the potential of
RAGTAG in automating test scenario generation. Specifically, our results
indicate that, despite the difficult task of analyzing bilingual requirements,
RAGTAG is able to produce scenarios that are well-aligned with the underlying
requirements and provide coverage of different aspects of the intended
functionality. The generated scenarios are easily understandable to experts and
feasible for testing in the project environment. The overall correctness is
deemed satisfactory; however, gaps in capturing exact action sequences and
domain nuances remain, underscoring the need for domain expertise when applying
LLMs.",Chetan Arora
2024-04-22T07:44:20Z,http://arxiv.org/abs/2404.13947v3,"Self-Bootstrapped Visual-Language Model for Knowledge Selection and
  Question Answering","While large visual-language models (LVLM) have shown promising results on
traditional visual question answering benchmarks, it is still challenging for
them to answer complex VQA problems which requires diverse world knowledge.
Motivated by the research of retrieval-augmented generation in the field of
natural language processing, we use Dense Passage Retrieval (DPR) to retrieve
related knowledge to help the model answer questions. However, DPR conduct
retrieving in natural language space, which may not ensure comprehensive
acquisition of image information. Thus, the retrieved knowledge is not truly
conducive to helping answer the question, affecting the performance of the
overall system. To address this issue, we propose a novel framework that
leverages the visual-language model to select the key knowledge retrieved by
DPR and answer questions. The framework consists of two modules: Selector and
Answerer, where both are initialized by the LVLM and parameter-efficiently
finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge
documents using the Selector, and then use them to finetune the Answerer to
predict answers; obtain the pseudo-labels of key knowledge documents based on
the predictions of the Answerer and weak supervision labels, and then finetune
the Selector to select key knowledge; repeat. Our framework significantly
enhances the performance of the baseline on the challenging open-domain
Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of
62.83%. Our code is publicly available at
https://github.com/haodongze/Self-KSel-QAns.",Dongze Hao
2024-04-22T07:49:36Z,http://arxiv.org/abs/2404.13948v2,"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by
  Simulating Documents in the Wild via Low-level Perturbations","The robustness of recent Large Language Models (LLMs) has become increasingly
crucial as their applicability expands across various domains and real-world
applications. Retrieval-Augmented Generation (RAG) is a promising solution for
addressing the limitations of LLMs, yet existing studies on the robustness of
RAG often overlook the interconnected relationships between RAG components or
the potential threats prevalent in real-world databases, such as minor textual
errors. In this work, we investigate two underexplored aspects when assessing
the robustness of RAG: 1) vulnerability to noisy documents through low-level
perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we
introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}),
which targets these aspects. Specifically, GARAG is designed to reveal
vulnerabilities within each component and test the overall system functionality
against noisy documents. We validate RAG robustness by applying our
\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and
LLMs. The experimental results show that GARAG consistently achieves high
attack success rates. Also, it significantly devastates the performance of each
component and their synergy, highlighting the substantial risk that minor
textual inaccuracies pose in disrupting RAG systems in the real world.",Sukmin Cho
2024-04-03T11:37:01Z,http://arxiv.org/abs/2404.15317v1,Concept-Guided LLM Agents for Human-AI Safety Codesign,"Generative AI is increasingly important in software engineering, including
safety engineering, where its use ensures that software does not cause harm to
people. This also leads to high quality requirements for generative AI.
Therefore, the simplistic use of Large Language Models (LLMs) alone will not
meet these quality demands. It is crucial to develop more advanced and
sophisticated approaches that can effectively address the complexities and
safety concerns of software systems. Ultimately, humans must understand and
take responsibility for the suggestions provided by generative AI to ensure
system safety. To this end, we present an efficient, hybrid strategy to
leverage LLMs for safety analysis and Human-AI codesign. In particular, we
develop a customized LLM agent that uses elements of prompt engineering,
heuristic reasoning, and retrieval-augmented generation to solve tasks
associated with predefined safety concepts, in interaction with a system model
graph. The reasoning is guided by a cascade of micro-decisions that help
preserve structured information. We further suggest a graph verbalization which
acts as an intermediate representation of the system model to facilitate
LLM-graph interactions. Selected pairs of prompts and responses relevant for
safety analytics illustrate our method for the use case of a simplified
automated driving system.",Florian Geissler
2024-04-24T18:38:11Z,http://arxiv.org/abs/2404.16130v1,"From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization","The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as ""What are the main themes in the dataset?"", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities
of text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose a Graph RAG approach to question answering over
private text corpora that scales with both the generality of user questions and
the quantity of source text to be indexed. Our approach uses an LLM to build a
graph-based text index in two stages: first to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely-related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that Graph RAG
leads to substantial improvements over a na\""ive RAG baseline for both the
comprehensiveness and diversity of generated answers. An open-source,
Python-based implementation of both global and local Graph RAG approaches is
forthcoming at https://aka.ms/graphrag.",Darren Edge
2024-04-27T13:11:42Z,http://arxiv.org/abs/2404.17897v1,"Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented
  Large Language Models","Large-scale language models (LLMs) have achieved remarkable success across
various language tasks but suffer from hallucinations and temporal
misalignment. To mitigate these shortcomings, Retrieval-augmented generation
(RAG) has been utilized to provide external knowledge to facilitate the answer
generation. However, applying such models to the medical domain faces several
challenges due to the lack of domain-specific knowledge and the intricacy of
real-world scenarios. In this study, we explore LLMs with RAG framework for
knowledge-intensive tasks in the medical field. To evaluate the capabilities of
LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates
the real-world medication consultation scenario and requires LLMs to answer
with retrieved evidence from the medicine database. MedicineQA contains 300
multi-round question-answering pairs, each embedded within a detailed dialogue
history, highlighting the challenge posed by this knowledge-intensive task to
current LLMs. We further propose a new \textit{Distill-Retrieve-Read} framework
instead of the previous \textit{Retrieve-then-Read}. Specifically, the
distillation and retrieval process utilizes a tool calling mechanism to
formulate search queries that emulate the keyword-based inquiries used by
search engines. With experimental results, we show that our framework brings
notable performance improvements and surpasses the previous counterparts in the
evidence retrieval process in terms of evidence retrieval accuracy. This
advancement sheds light on applying RAG to the medical domain.",Zhongzhen Huang
2024-04-28T05:46:28Z,http://arxiv.org/abs/2404.18077v2,"Generative AI for Low-Carbon Artificial Intelligence of Things with
  Large Language Models","By integrating Artificial Intelligence (AI) with the Internet of Things
(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.
However, AIoT is facing the challenges of energy consumption and carbon
emissions due to the continuous advancement of mobile technology. Fortunately,
Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT
due to its excellent reasoning and generation capabilities. In this article, we
explore the potential of GAI for carbon emissions reduction and propose a novel
GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main
impacts that cause carbon emissions in AIoT, and then introduce GAI techniques
and their relations to carbon emissions. We then explore the application
prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon
emissions of network components. Subsequently, we propose a Large Language
Model (LLM)-enabled carbon emission optimization framework, in which we design
pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more
accurate and reliable optimization problems. Furthermore, we utilize Generative
Diffusion Models (GDMs) to identify optimal strategies for carbon emission
reduction. Numerical results demonstrate the effectiveness of the proposed
framework. Finally, we insightfully provide open research directions for
low-carbon AIoT.",Jinbo Wen
2024-04-29T07:11:39Z,http://arxiv.org/abs/2404.18470v2,"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls
  using Large Language Model for Stock Performance Prediction","In the realm of financial analytics, leveraging unstructured data, such as
earnings conference calls (ECCs), to forecast stock volatility is a critical
challenge that has attracted both academics and investors. While previous
studies have used multimodal deep learning-based models to obtain a general
view of ECCs for volatility predicting, they often fail to capture detailed,
complex information. Our research introduces a novel framework: \textbf{ECC
Analyzer}, which utilizes large language models (LLMs) to extract richer, more
predictive content from ECCs to aid the model's prediction performance. We use
the pre-trained large models to extract textual and audio features from ECCs
and implement a hierarchical information extraction strategy to extract more
fine-grained information. This strategy first extracts paragraph-level general
information by summarizing the text and then extracts fine-grained focus
sentences using Retrieval-Augmented Generation (RAG). These features are then
fused through multimodal feature fusion to perform volatility prediction.
Experimental results demonstrate that our model outperforms traditional
analytical benchmarks, confirming the effectiveness of advanced LLM techniques
in financial analysis.",Yupeng Cao
2024-04-30T03:29:30Z,http://arxiv.org/abs/2404.19232v7,"GRAMMAR: Grounded and Modular Methodology for Assessment of
  Closed-Domain Retrieval-Augmented Language Model","Retrieval-Augmented Generation (RAG) systems are widely used across various
industries for querying closed-domain and in-house knowledge bases. However,
evaluating these systems presents significant challenges due to the private
nature of closed-domain data and a scarcity of queries with verifiable ground
truths. Moreover, there is a lack of analytical methods to diagnose problematic
modules and identify types of failure, such as those caused by knowledge
deficits or issues with robustness. To address these challenges, we introduce
GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation
framework comprising a grounded data generation process and an evaluation
protocol that effectively pinpoints defective modules. Our validation
experiments reveal that GRAMMAR provides a reliable approach for identifying
vulnerable modules and supports hypothesis testing for textual form
vulnerabilities. An open-source tool accompanying this framework is available
in our GitHub repository (see https://github.com/xinzhel/grammar), allowing for
easy reproduction of our results and enabling reliable and modular evaluation
in closed-domain settings.",Xinzhe Li
2024-04-30T13:14:51Z,http://arxiv.org/abs/2404.19543v1,"RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural
  Language Processing","Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.",Yucheng Hu
2024-04-30T17:44:44Z,http://arxiv.org/abs/2404.19744v1,"PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for
  Privacy Policy Compliance Verification","Data protection and privacy is becoming increasingly crucial in the digital
era. Numerous companies depend on third-party vendors and service providers to
carry out critical functions within their operations, encompassing tasks such
as data handling and storage. However, this reliance introduces potential
vulnerabilities, as these vendors' security measures and practices may not
always align with the standards expected by regulatory bodies. Businesses are
required, often under the penalty of law, to ensure compliance with the
evolving regulatory rules. Interpreting and implementing these regulations pose
challenges due to their complexity. Regulatory documents are extensive,
demanding significant effort for interpretation, while vendor-drafted privacy
policies often lack the detail required for full legal compliance, leading to
ambiguity. To ensure a concise interpretation of the regulatory requirements
and compliance of organizational privacy policy with said regulations, we
propose a Large Language Model (LLM) and Semantic Web based approach for
privacy compliance. In this paper, we develop the novel Privacy Policy
Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to
efficiently store and retrieve comprehensive information concerning privacy
policies, regulatory frameworks, and domain-specific knowledge pertaining to
the legal landscape of privacy. Using Retrieval Augmented Generation, we
identify the relevant sections in a privacy policy with corresponding
regulatory rules. This information about individual privacy policies is
populated into the PrivComp-KG. Combining this with the domain context and
rules, the PrivComp-KG can be queried to check for compliance with privacy
policies by each vendor against relevant policy regulations. We demonstrate the
relevance of the PrivComp-KG, by verifying compliance of privacy policy
documents for various organizations.",Leon Garza
2024-05-01T12:01:39Z,http://arxiv.org/abs/2405.00465v3,BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine,"Large Language Models (LLMs) have swiftly emerged as vital resources for
different applications in the biomedical and healthcare domains; however, these
models encounter issues such as generating inaccurate information or
hallucinations. Retrieval-augmented generation provided a solution for these
models to update knowledge and enhance their performance. In contrast to
previous retrieval-augmented LMs, which utilize specialized cross-attention
mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler
approach by directly inputting the retrieved chunk-based documents into the
LLM. This straightforward design is easily applicable to existing retrieval and
language models, effectively bypassing noise information in retrieved
documents, particularly in noise-intensive tasks. Moreover, we demonstrate the
potential for utilizing the LLM to supervise the retrieval model in the
biomedical domain, enabling it to retrieve the document that assists the LM in
improving its predictions. Our experiments reveal that with the tuned
scorer,\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP
tasks, encompassing information extraction (triple extraction, relation
extraction), text classification, link prediction, and question-answering,
leveraging over 9 datasets. For instance, in the triple extraction task,
\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1
scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.",Mingchen Li
2024-05-02T14:19:25Z,http://arxiv.org/abs/2405.01310v1,"Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf
  Disease Remediation","This research introduces an innovative AI-driven precision agriculture
system, leveraging YOLOv8 for disease identification and Retrieval Augmented
Generation (RAG) for context-aware diagnosis. Focused on addressing the
challenges of diseases affecting the coffee production sector in Karnataka, The
system integrates sophisticated object detection techniques with language
models to address the inherent constraints associated with Large Language
Models (LLMs). Our methodology not only tackles the issue of hallucinations in
LLMs, but also introduces dynamic disease identification and remediation
strategies. Real-time monitoring, collaborative dataset expansion, and
organizational involvement ensure the system's adaptability in diverse
agricultural settings. The effect of the suggested system extends beyond
automation, aiming to secure food supplies, protect livelihoods, and promote
eco-friendly farming practices. By facilitating precise disease identification,
the system contributes to sustainable and environmentally conscious
agriculture, reducing reliance on pesticides. Looking to the future, the
project envisions continuous development in RAG-integrated object detection
systems, emphasizing scalability, reliability, and usability. This research
strives to be a beacon for positive change in agriculture, aligning with global
efforts toward sustainable and technologically enhanced food production.",Selva Kumar S
2024-05-06T00:18:43Z,http://arxiv.org/abs/2405.03085v1,"Compressing Long Context for Enhancing RAG with AMR-based Concept
  Distillation","Large Language Models (LLMs) have made significant strides in information
acquisition. However, their overreliance on potentially flawed parametric
knowledge leads to hallucinations and inaccuracies, particularly when handling
long-tail, domain-specific queries. Retrieval Augmented Generation (RAG)
addresses this limitation by incorporating external, non-parametric knowledge.
Nevertheless, the retrieved long-context documents often contain noisy,
irrelevant information alongside vital knowledge, negatively diluting LLMs'
attention. Inspired by the supportive role of essential concepts in
individuals' reading comprehension, we propose a novel concept-based RAG
framework with the Abstract Meaning Representation (AMR)-based concept
distillation algorithm. The proposed algorithm compresses the cluttered raw
retrieved documents into a compact set of crucial concepts distilled from the
informative nodes of AMR by referring to reliable linguistic features. The
concepts explicitly constrain LLMs to focus solely on vital information in the
inference process. We conduct extensive experiments on open-domain
question-answering datasets to empirically evaluate the proposed method's
effectiveness. The results indicate that the concept-based RAG framework
outperforms other baseline methods, particularly as the number of supporting
documents increases, while also exhibiting robustness across various backbone
LLMs. This emphasizes the distilled concepts are informative for augmenting the
RAG process by filtering out interference information. To the best of our
knowledge, this is the first work introducing AMR to enhance the RAG,
presenting a potential solution to augment inference performance with
semantic-based context compression.",Kaize Shi
2024-05-06T02:35:10Z,http://arxiv.org/abs/2405.03122v2,"Automatic Retrieval-augmented Generation of 6G Network Specifications
  for Use Cases","6G Open Radio Access Networks (O-RAN) promises to open data interfaces to
enable plug-and-play service Apps, many of which are consumer and
business-facing. Opening up 6G access lowers the barrier to innovation but
raises the challenge that the required communication specifications are not
fully known to all service designers. As such, business innovators must either
be familiar with 6G standards or consult with experts. Enabling consistent,
unbiased, rapid, and low-cost requirement assessment and specification
generation is crucial to the O-RAN innovation ecosystem.
  Here, we discuss our initiative to bridge service specification gaps between
network service providers and business innovators leveraging Large Language
Models (LLMs). We first review the state-of-the-art and motivation in 6G
plug-and-play services, capabilities, potential use cases and LLMs. We identify
an ample innovation space for hybrid use cases that may require diverse and
variational wireless functionalities across its operating time. We show that
the network specification can be automated and present the first automatic
retrieval-augmented network service specification framework for 6G use cases.
To enable public acceptance and feedback, a website interface is published for
the research and industrial community to experiment with the framework. We hope
this review highlights the need for emerging foundation models for this area
and motivates researcher engagement and contribution to the community through
our framework.",Yun Tang
2024-05-06T20:50:17Z,http://arxiv.org/abs/2405.03845v1,Self-Improving Customer Review Response Generation Based on LLMs,"Previous studies have demonstrated that proactive interaction with user
reviews has a positive impact on the perception of app users and encourages
them to submit revised ratings. Nevertheless, developers encounter challenges
in managing a high volume of reviews, particularly in the case of popular apps
with a substantial influx of daily reviews. Consequently, there is a demand for
automated solutions aimed at streamlining the process of responding to user
reviews. To address this, we have developed a new system for generating
automatic responses by leveraging user-contributed documents with the help of
retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
Our solution, named SCRABLE, represents an adaptive customer review response
automation that enhances itself with self-optimizing prompts and a judging
mechanism based on LLMs. Additionally, we introduce an automatic scoring
mechanism that mimics the role of a human evaluator to assess the quality of
responses generated in customer review domains. Extensive experiments and
analyses conducted on real-world datasets reveal that our method is effective
in producing high-quality responses, yielding improvement of more than 8.5%
compared to the baseline. Further validation through manual examination of the
generated responses underscores the efficacy our proposed system.",Guy Azov
2024-05-07T02:49:59Z,http://arxiv.org/abs/2405.03963v4,ERATTA: Extreme RAG for Table To Answers with Large Language Models,"Large language models (LLMs) with retrieval augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. Although RAG implemented with AI agents (agentic-RAG) has been recently
popularized, its suffers from unstable cost and unreliable performances for
Enterprise-level data-practices. Most existing use-cases that incorporate RAG
with LLMs have been either generic or extremely domain specific, thereby
questioning the scalability and generalizability of RAG-LLM approaches. In this
work, we propose a unique LLM-based system where multiple LLMs can be invoked
to enable data authentication, user-query routing, data-retrieval and custom
prompting for question-answering capabilities from Enterprise-data tables. The
source tables here are highly fluctuating and large in size and the proposed
framework enables structured responses in under 10 seconds per query.
Additionally, we propose a five metric scoring module that detects and reports
hallucinations in the LLM responses. Our proposed system and scoring metrics
achieve >90% confidence scores across hundreds of user queries in the
sustainability, financial health and social media domains. Extensions to the
proposed extreme RAG architectures can enable heterogeneous source querying
using LLMs.",Sohini Roychowdhury
2024-05-07T17:59:31Z,http://arxiv.org/abs/2405.04533v1,"ChatHuman: Language-driven 3D Human Understanding with
  Retrieval-Augmented Tool Reasoning","Numerous methods have been proposed to detect, estimate, and analyze
properties of people in images, including the estimation of 3D pose, shape,
contact, human-object interaction, emotion, and more. Each of these methods
works in isolation instead of synergistically. Here we address this problem and
build a language-driven human understanding system -- ChatHuman, which combines
and integrates the skills of many different methods. To do so, we finetune a
Large Language Model (LLM) to select and use a wide variety of existing tools
in response to user inputs. In doing so, ChatHuman is able to combine
information from multiple tools to solve problems more accurately than the
individual tools themselves and to leverage tool output to improve its ability
to reason about humans. The novel features of ChatHuman include leveraging
academic publications to guide the application of 3D human-related tools,
employing a retrieval-augmented generation model to generate
in-context-learning examples for handling new tools, and discriminating and
integrating tool results to enhance 3D human understanding. Our experiments
show that ChatHuman outperforms existing models in both tool selection accuracy
and performance across multiple 3D human-related tasks. ChatHuman is a step
towards consolidating diverse methods for human analysis into a single,
powerful, system for 3D human reasoning.",Jing Lin
2024-05-07T21:14:38Z,http://arxiv.org/abs/2405.04674v1,"Towards Accurate and Efficient Document Analytics with Large Language
  Models","Unstructured data formats account for over 80% of the data currently stored,
and extracting value from such formats remains a considerable challenge. In
particular, current approaches for managing unstructured documents do not
support ad-hoc analytical queries on document collections. Moreover, Large
Language Models (LLMs) directly applied to the documents themselves, or on
portions of documents through a process of Retrieval-Augmented Generation
(RAG), fail to provide high accuracy query results, and in the LLM-only case,
additionally incur high costs. Since many unstructured documents in a
collection often follow similar templates that impart a common semantic
structure, we introduce ZenDB, a document analytics system that leverages this
semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document
collections. ZenDB efficiently extracts semantic hierarchical structures from
such templatized documents, and introduces a novel query engine that leverages
these structures for accurate and cost-effective query execution. Users can
impose a schema on their documents, and query it, all via SQL. Extensive
experiments on three real-world document collections demonstrate ZenDB's
benefits, achieving up to 30% cost savings compared to LLM-based baselines,
while maintaining or improving accuracy, and surpassing RAG-based baselines by
up to 61% in precision and 80% in recall, at a marginally higher cost.",Yiming Lin
2024-05-08T22:23:58Z,http://arxiv.org/abs/2405.05444v1,"Evaluating Students' Open-ended Written Responses with LLMs: Using the
  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large","Evaluating open-ended written examination responses from students is an
essential yet time-intensive task for educators, requiring a high degree of
effort, consistency, and precision. Recent developments in Large Language
Models (LLMs) present a promising opportunity to balance the need for thorough
evaluation with efficient use of educators' time. In our study, we explore the
effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in
assessing university students' open-ended answers to questions made about
reference material they have studied. Each model was instructed to evaluate 54
answers repeatedly under two conditions: 10 times (10-shot) with a temperature
setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of
1,080 evaluations per model and 4,320 evaluations across all models. The RAG
(Retrieval Augmented Generation) framework was used as the framework to make
the LLMs to process the evaluation of the answers. As of spring 2024, our
analysis revealed notable variations in consistency and the grading outcomes
provided by studied LLMs. There is a need to comprehend strengths and
weaknesses of LLMs in educational settings for evaluating open-ended written
responses. Further comparative research is essential to determine the accuracy
and cost-effectiveness of using LLMs for educational assessments.",Jussi S. Jauhiainen
2024-05-09T12:58:22Z,http://arxiv.org/abs/2405.05741v1,Can large language models understand uncommon meanings of common words?,"Large language models (LLMs) like ChatGPT have shown significant advancements
across diverse natural language understanding (NLU) tasks, including
intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged
testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely
comprehend the world' remains unclear, fostering numerous studies and sparking
heated debates. Prevailing research mainly focuses on surface-level NLU,
neglecting fine-grained explorations. However, such explorations are crucial
for understanding their unique comprehension mechanisms, aligning with human
cognition, and finally enhancing LLMs' general NLU capacities. To address this
gap, our study delves into LLMs' nuanced semantic comprehension capabilities,
particularly regarding common words with uncommon meanings. The idea stems from
foundational principles of human communication within psychology, which
underscore accurate shared understandings of word semantics. Specifically, this
paper presents the innovative construction of a Lexical Semantic Comprehension
(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing
both fine-grained and cross-lingual dimensions. Introducing models of both
open-source and closed-source, varied scales and architectures, our extensive
empirical experiments demonstrate the inferior performance of existing models
in this basic lexical-meaning understanding task. Notably, even the
state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%
and 22.3%, respectively. Additionally, multiple advanced prompting techniques
and retrieval-augmented generation are also introduced to help alleviate this
trouble, yet limitations persist. By highlighting the above critical
shortcomings, this research motivates further investigation and offers novel
insights for developing more intelligent LLMs.",Jinyang Wu
2024-05-06T04:42:18Z,http://arxiv.org/abs/2405.06683v1,"ERAGent: Enhancing Retrieval-Augmented Language Models with Improved
  Accuracy, Efficiency, and Personalization","Retrieval-augmented generation (RAG) for language models significantly
improves language understanding systems. The basic retrieval-then-read pipeline
of response generation has evolved into a more extended process due to the
integration of various components, sometimes even forming loop structures.
Despite its advancements in improving response accuracy, challenges like poor
retrieval quality for complex questions that require the search of multifaceted
semantic information, inefficiencies in knowledge re-retrieval during long-term
serving, and lack of personalized responses persist. Motivated by transcending
these limitations, we introduce ERAGent, a cutting-edge framework that embodies
an advancement in the RAG area. Our contribution is the introduction of the
synergistically operated module: Enhanced Question Rewriter and Knowledge
Filter, for better retrieval quality. Retrieval Trigger is incorporated to
curtail extraneous external knowledge retrieval without sacrificing response
quality. ERAGent also personalizes responses by incorporating a learned user
profile. The efficiency and personalization characteristics of ERAGent are
supported by the Experiential Learner module which makes the AI assistant being
capable of expanding its knowledge and modeling user profile incrementally.
Rigorous evaluations across six datasets and three question-answering tasks
prove ERAGent's superior accuracy, efficiency, and personalization, emphasizing
its potential to advance the RAG field and its applicability in practical
systems.",Yunxiao Shi
2024-05-21T13:02:27Z,http://arxiv.org/abs/2405.12750v1,"Generative AI and Large Language Models for Cyber Security: All Insights
  You Need","This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.",Mohamed Amine Ferrag
2024-05-20T11:05:56Z,http://arxiv.org/abs/2405.13057v1,Can Github issues be solved with Tree Of Thoughts?,"While there have been extensive studies in code generation by large language
models (LLM), where benchmarks like HumanEval have been surpassed with an
impressive 96.3% success rate, these benchmarks predominantly judge a model's
performance on basic function-level code generation and lack the critical
thinking and concept of scope required of real-world scenarios such as solving
GitHub issues. This research introduces the application of the Tree of Thoughts
(ToT) language model reasoning framework for enhancing the decision-making and
problem-solving abilities of LLMs for this complex task. Compared to
traditional input-output (IO) prompting and Retrieval Augmented Generation
(RAG) techniques, ToT is designed to improve performance by facilitating a
structured exploration of multiple reasoning trajectories and enabling
self-assessment of potential solutions. We experimentally deploy ToT in
tackling a Github issue contained within an instance of the SWE-bench. However,
our results reveal that the ToT framework alone is not enough to give LLMs the
critical reasoning capabilities to outperform existing methods. In this paper
we analyze the potential causes of these shortcomings and identify key areas
for improvement such as deepening the thought process and introducing agentic
capabilities. The insights of this research are aimed at informing future
directions for refining the application of ToT and better harnessing the
potential of LLMs in real-world problem-solving scenarios.",Ricardo La Rosa
2024-05-23T15:37:06Z,http://arxiv.org/abs/2405.14702v2,"G3: An Effective and Adaptive Framework for Worldwide Geolocalization
  Using Large Multi-Modality Models","Worldwide geolocalization aims to locate the precise location at the
coordinate level of photos taken anywhere on the Earth. It is very challenging
due to 1) the difficulty of capturing subtle location-aware visual semantics,
and 2) the heterogeneous geographical distribution of image data. As a result,
existing studies have clear limitations when scaled to a worldwide context.
They may easily confuse distant images with similar visual contents, or cannot
adapt to various locations worldwide with different amounts of relevant data.
To resolve these limitations, we propose G3, a novel framework based on
Retrieval-Augmented Generation (RAG). In particular, G3 consists of three
steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to
optimize both retrieval and generation phases of worldwide geolocalization.
During Geo-alignment, our solution jointly learns expressive multi-modal
representations for images, GPS and textual descriptions, which allows us to
capture location-aware semantics for retrieving nearby images for a given
query. During Geo-diversification, we leverage a prompt ensembling method that
is robust to inconsistent retrieval performance for different image queries.
Finally, we combine both retrieved and generated GPS candidates in
Geo-verification for location prediction. Experiments on two well-established
datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other
state-of-the-art methods. Our code and data are available online for
reproduction.",Pengyue Jia
2024-05-23T17:47:55Z,http://arxiv.org/abs/2405.14831v2,"HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language
  Models","In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",Bernal Jiménez Gutiérrez
2024-05-25T11:10:04Z,http://arxiv.org/abs/2405.16178v1,"Accelerating Inference of Retrieval-Augmented Generation via Sparse
  Context Selection","Large language models (LLMs) augmented with retrieval exhibit robust
performance and extensive versatility by incorporating external contexts.
However, the input length grows linearly in the number of retrieved documents,
causing a dramatic increase in latency. In this paper, we propose a novel
paradigm named Sparse RAG, which seeks to cut computation costs through
sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel,
which eliminates latency introduced by long-range attention of retrieved
documents. Then, LLMs selectively decode the output by only attending to highly
relevant caches auto-regressively, which are chosen via prompting LLMs with
special control tokens. It is notable that Sparse RAG combines the assessment
of each individual document and the generation of the response into a single
process. The designed sparse mechanism in a RAG system can facilitate the
reduction of the number of documents loaded during decoding for accelerating
the inference of the RAG system. Additionally, filtering out undesirable
contexts enhances the model's focus on relevant context, inherently improving
its generation quality. Evaluation results of two datasets show that Sparse RAG
can strike an optimal balance between generation quality and computational
efficiency, demonstrating its generalizability across both short- and long-form
generation tasks.",Yun Zhu
2024-05-28T16:56:42Z,http://arxiv.org/abs/2405.18359v1,"Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual
  Performance in LLMs","Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs without
extensive training or fine-tuning. Through systematic investigation and
evaluation of diverse languages using popular question-answering (QA) datasets,
we present novel techniques that unlock the true potential of LLMs in a
polyglot landscape. Our approach encompasses three key strategies that yield
significant improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval
Augmented Generation (RAG) with multilingual embeddings and achieves improved
multilingual task performance. Finally, we introduce a novel learning approach
that dynamically selects the optimal prompt strategy, LLM model, and embedding
model per query at run-time. This dynamic adaptation maximizes the efficacy of
LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online
settings, and can seamlessly adapt to new languages and datasets, leading to
substantial advancements in multilingual understanding and generation across
diverse languages.",Somnath Kumar
2024-05-30T15:14:24Z,http://arxiv.org/abs/2405.20139v1,GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning,"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form
of triplets (head, relation, tail), which collectively form a graph. Question
Answering over KGs (KGQA) is the task of answering natural questions grounding
the reasoning to the information provided by the KG. Large Language Models
(LLMs) are the state-of-the-art models for QA tasks due to their remarkable
ability to understand natural language. On the other hand, Graph Neural
Networks (GNNs) have been widely used for KGQA as they can handle the complex
graph information stored in the KG. In this work, we introduce GNN-RAG, a novel
method for combining language understanding abilities of LLMs with the
reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for
a given question. Second, the shortest paths in the KG that connect question
entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with
RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to
extract useful graph information, while the LLM leverages its natural language
processing ability for ultimate KGQA. Furthermore, we develop a retrieval
augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in
two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching
GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop
and multi-entity questions outperforming competing approaches by 8.9--15.5%
points at answer F1.",Costas Mavromatis
2024-05-30T18:00:21Z,http://arxiv.org/abs/2405.20389v1,"Designing an Evaluation Framework for Large Language Models in Astronomy
  Research","Large Language Models (LLMs) are shifting how scientific research is done. It
is imperative to understand how researchers interact with these models and how
scientific sub-communities like astronomy might benefit from them. However,
there is currently no standard for evaluating the use of LLMs in astronomy.
Therefore, we present the experimental design for an evaluation study on how
astronomy researchers interact with LLMs. We deploy a Slack chatbot that can
answer queries from users via Retrieval-Augmented Generation (RAG); these
responses are grounded in astronomy papers from arXiv. We record and anonymize
user questions and chatbot answers, user upvotes and downvotes to LLM
responses, user feedback to the LLM, and retrieved documents and similarity
scores with the query. Our data collection method will enable future dynamic
evaluations of LLM tools for astronomy.",John F. Wu
2024-05-30T20:05:44Z,http://arxiv.org/abs/2405.20455v5,"DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency
  Management","In the era of Large Language Models (LLMs) with their advanced capabilities,
a unique opportunity arises to develop LLM-based digital assistant tools that
can support software developers by facilitating comprehensive reasoning about
software dependencies and open-source libraries before importing them. This
reasoning process is daunting, mandating multiple specialized tools and
dedicated expertise, each focusing on distinct aspects (e.g., security analysis
tools may overlook design flaws such as circular dependencies, which hinder
software maintainability). Creating a significant bottleneck in the software
development lifecycle. In this paper, we introduce DepsRAG, a multi-agent
framework designed to assist developers in reasoning about software
dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG)
that includes both direct and transitive dependencies. Developers can interact
with DepsRAG through a conversational interface, posing queries about the
dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance
these queries by retrieving relevant information from the KG as well as
external sources, such as the Web and vulnerability databases, thus
demonstrating its adaptability to novel scenarios. DepsRAG incorporates a
Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated
responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three
multi-step reasoning tasks, observing a threefold increase in accuracy with the
integration of the Critic-Agent mechanism. DepsRAG demo and implementation are
available: https://github.com/Mohannadcse/DepsRAG.",Mohannad Alhanahnah
2024-05-30T21:19:24Z,http://arxiv.org/abs/2405.20485v2,"Phantom: General Trigger Attacks on Retrieval Augmented Language
  Generation","Retrieval Augmented Generation (RAG) expands the capabilities of modern large
language models (LLMs), by anchoring, adapting, and personalizing their
responses to the most relevant knowledge sources. It is particularly useful in
chatbot applications, allowing developers to customize LLM output without
expensive retraining. Despite their significant utility in various
applications, RAG systems present new security risks. In this work, we propose
new attack vectors that allow an adversary to inject a single malicious
document into a RAG system's knowledge base, and mount a backdoor poisoning
attack. We design Phantom, a general two-stage optimization framework against
RAG systems, that crafts a malicious poisoned document leading to an integrity
violation in the model's output. First, the document is constructed to be
retrieved only when a specific trigger sequence of tokens appears in the
victim's queries. Second, the document is further optimized with crafted
adversarial text that induces various adversarial objectives on the LLM output,
including refusal to answer, reputation damage, privacy violations, and harmful
behaviors. We demonstrate our attacks on multiple LLM architectures, including
Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and
GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's
black-box production RAG system, ""Chat with RTX"".",Harsh Chaudhari
2024-05-31T14:23:49Z,http://arxiv.org/abs/2405.20834v1,"Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits
  Multimodal Reasoning","Large language models equipped with retrieval-augmented generation (RAG)
represent a burgeoning field aimed at enhancing answering capabilities by
leveraging external knowledge bases. Although the application of RAG with
language-only models has been extensively explored, its adaptation into
multimodal vision-language models remains nascent. Going beyond mere answer
generation, the primary goal of multimodal RAG is to cultivate the models'
ability to reason in response to relevant queries. To this end, we introduce a
novel multimodal RAG framework named RMR (Retrieval Meets Reasoning). The RMR
framework employs a bi-modal retrieval module to identify the most relevant
question-answer pairs, which then serve as scaffolds for the multimodal
reasoning process. This training-free approach not only encourages the model to
engage deeply with the reasoning processes inherent in the retrieved content
but also facilitates the generation of answers that are precise and richly
interpretable. Surprisingly, utilizing solely the ScienceQA dataset, collected
from elementary and high school science curricula, RMR significantly boosts the
performance of various vision-language models across a spectrum of benchmark
datasets, including A-OKVQA, MMBench, and SEED. These outcomes highlight the
substantial potential of our multimodal retrieval and reasoning mechanism to
improve the reasoning capabilities of vision-language models.",Cheng Tan
2024-05-31T16:24:53Z,http://arxiv.org/abs/2405.20978v1,"Enhancing Noise Robustness of Retrieval-Augmented Language Models with
  Adaptive Adversarial Training","Large Language Models (LLMs) exhibit substantial capabilities yet encounter
challenges, including hallucination, outdated knowledge, and untraceable
reasoning processes. Retrieval-augmented generation (RAG) has emerged as a
promising solution, integrating knowledge from external databases to mitigate
these challenges. However, inappropriate retrieved passages can potentially
hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine
themselves to a limited set of noise types, deviating from real-world retrieval
environments and limiting practical applicability. In this study, we initially
investigate retrieval noises and categorize them into three distinct types,
reflecting real-world environments. We analyze the impact of these various
retrieval noises on the robustness of LLMs. Subsequently, we propose a novel
RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's
training process in response to retrieval noises. Concurrently, it employs
multi-task learning to ensure the model's capacity to internally recognize
noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model
trained using RAAT exhibits significant improvements in F1 and EM scores under
diverse noise conditions. For reproducibility, we release our code and data at:
https://github.com/calubkk/RAAT.",Feiteng Fang
2024-05-24T20:03:32Z,http://arxiv.org/abs/2406.00031v1,"AMGPT: a Large Language Model for Contextual Querying in Additive
  Manufacturing","Generalized large language models (LLMs) such as GPT-4 may not provide
specific answers to queries formulated by materials science researchers. These
models may produce a high-level outline but lack the capacity to return
detailed instructions on manufacturing and material properties of novel alloys.
Enhancing a smaller model with specialized domain knowledge may provide an
advantage over large language models which cannot be retrained quickly enough
to keep up with the rapid pace of research in metal additive manufacturing
(AM). We introduce ""AMGPT,"" a specialized LLM text generator designed for metal
AM queries. The goal of AMGPT is to assist researchers and users in navigating
the extensive corpus of literature in AM. Instead of training from scratch, we
employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented
Generation (RAG) setup, utilizing it to dynamically incorporate information
from $\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert
these PDF documents into TeX format, facilitating their integration into the
RAG pipeline managed by LlamaIndex. Expert evaluations of this project
highlight that specific embeddings from the RAG setup accelerate response times
and maintain coherence in the generated text.",Achuth Chandrasekhar
2024-05-27T10:53:15Z,http://arxiv.org/abs/2406.00036v1,EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling,"The integration of multimodal Electronic Health Records (EHR) data has
notably advanced clinical predictive capabilities. However, current models that
utilize clinical notes and multivariate time-series EHR data often lack the
necessary medical context for precise clinical tasks. Previous methods using
knowledge graphs (KGs) primarily focus on structured knowledge extraction. To
address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven
framework aimed at enhancing multimodal EHR predictive modeling. Our approach
extracts entities from both time-series data and clinical notes by prompting
Large Language Models (LLMs) and aligns them with professional PrimeKG to
ensure consistency. Beyond triplet relationships, we include entities'
definitions and descriptions to provide richer semantics. The extracted
knowledge is then used to generate task-relevant summaries of patients' health
statuses. These summaries are fused with other modalities utilizing an adaptive
multimodal fusion network with cross-attention. Extensive experiments on the
MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day
readmission tasks demonstrate the superior performance of the EMERGE framework
compared to baseline models. Comprehensive ablation studies and analyses
underscore the efficacy of each designed module and the framework's robustness
to data sparsity. EMERGE significantly enhances the use of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts crucial for
informed clinical predictions.",Yinghao Zhu
2024-05-29T18:19:46Z,http://arxiv.org/abs/2406.00057v2,"Toward Conversational Agents with Context and Time Sensitive Long-term
  Memory","There has recently been growing interest in conversational agents with
long-term memory which has led to the rapid development of language models that
use retrieval-augmented generation (RAG). Until recently, most work on RAG has
focused on information retrieval from large databases of texts, like Wikipedia,
rather than information from long-form conversations. In this paper, we argue
that effective retrieval from long-form conversational data faces two unique
problems compared to static database retrieval: 1) time/event-based queries,
which requires the model to retrieve information about previous conversations
based on time or the order of a conversational event (e.g., the third
conversation on Tuesday), and 2) ambiguous queries that require surrounding
conversational context to understand. To better develop RAG-based agents that
can deal with these challenges, we generate a new dataset of ambiguous and
time-based questions that build upon a recent dataset of long-form, simulated
conversations, and demonstrate that standard RAG based approaches handle such
questions poorly. We then develop a novel retrieval model which combines
chained-of-table search methods, standard vector-database retrieval, and a
prompting method to disambiguate queries, and demonstrate that this approach
substantially improves over current methods at solving these tasks. We believe
that this new dataset and more advanced RAG agent can act as a key benchmark
and stepping stone towards effective memory augmented conversational agents
that can be used in a wide variety of AI applications.",Nick Alonso
2024-06-03T15:26:06Z,http://arxiv.org/abs/2406.01428v2,"Superhuman performance in urology board questions by an explainable
  large language model enabled for context integration of the European
  Association of Urology guidelines: the UroBot study","Large Language Models (LLMs) are revolutionizing medical Question-Answering
(medQA) through extensive use of medical literature. However, their performance
is often hampered by outdated training data and a lack of explainability, which
limits clinical applicability. This study aimed to create and assess UroBot, a
urology-specialized chatbot, by comparing it with state-of-the-art models and
the performance of urologists on urological board questions, ensuring full
clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,
and GPT-4o models, employing retrieval-augmented generation (RAG) and the
latest 2023 guidelines from the European Association of Urology (EAU). The
evaluation included ten runs of 200 European Board of Urology (EBU) In-Service
Assessment (ISA) questions, with performance assessed by the mean Rate of
Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing
GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and
exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).
By comparison, the average performance of urologists on board questions, as
reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and
superior accuracy compared to both existing models and urologists on board
questions highlight its potential for clinical integration. The study also
provides the necessary code and instructions for further development of UroBot.",Martin J. Hetz
2024-06-04T08:36:39Z,http://arxiv.org/abs/2406.02110v1,"UniOQA: A Unified Framework for Knowledge Graph Question Answering with
  Large Language Models","OwnThink stands as the most extensive Chinese open-domain knowledge graph
introduced in recent times. Despite prior attempts in question answering over
OwnThink (OQA), existing studies have faced limitations in model representation
capabilities, posing challenges in further enhancing overall accuracy in
question answering. In this paper, we introduce UniOQA, a unified framework
that integrates two complementary parallel workflows. Unlike conventional
approaches, UniOQA harnesses large language models (LLMs) for precise question
answering and incorporates a direct-answer-prediction process as a
cost-effective complement. Initially, to bolster representation capacity, we
fine-tune an LLM to translate questions into the Cypher query language (CQL),
tackling issues associated with restricted semantic understanding and
hallucinations. Subsequently, we introduce the Entity and Relation Replacement
algorithm to ensure the executability of the generated CQL. Concurrently, to
augment overall accuracy in question answering, we further adapt the
Retrieval-Augmented Generation (RAG) process to the knowledge graph.
Ultimately, we optimize answer accuracy through a dynamic decision algorithm.
Experimental findings illustrate that UniOQA notably advances SpCQL Logical
Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new
state-of-the-art results on this benchmark. Through ablation experiments, we
delve into the superior representation capacity of UniOQA and quantify its
performance breakthrough.",Zhuoyang Li
2024-06-04T20:02:52Z,http://arxiv.org/abs/2406.02746v5,RATT: A Thought Structure for Coherent and Correct LLM Reasoning,"Large Language Models (LLMs) gain substantial reasoning and decision-making
capabilities from thought structures. However, existing methods such as Tree of
Thought and Retrieval Augmented Thoughts often fall short in complex tasks due
to the limitations of insufficient local retrieval of factual knowledge and
inadequate global selection of strategies. These limitations make it
challenging for these methods to balance factual accuracy and comprehensive
logical optimization effectively. To address these limitations, we introduce
the Retrieval Augmented Thought Tree (RATT), a novel thought structure that
considers both overall logical soundness and factual correctness at each step
of the thinking process. Specifically, at every point of a thought branch, RATT
performs planning and lookahead to explore and evaluate multiple potential
reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented
Generation (RAG) with LLM's ability to assess overall strategy. Through this
combination of factual knowledge and strategic feasibility, the RATT adjusts
and integrates the thought tree structure to search for the most promising
branches within the search space. This thought structure significantly enhances
the model's coherence in logical inference and efficiency in decision-making,
and thus increases the limit of the capacity of LLM to generate reliable
inferences and decisions based on thought structures. A broad range of
experiments on different types of tasks showcases that the RATT structure
significantly outperforms existing methods in factual correctness and logical
coherence.",Jinghan Zhang
2024-06-04T23:36:08Z,http://arxiv.org/abs/2406.02818v1,"Chain of Agents: Large Language Models Collaborating on Long-Context
  Tasks","Addressing the challenge of effectively processing long contexts has become a
critical issue for Large Language Models (LLMs). Two common strategies have
emerged: 1) reducing the input length, such as retrieving relevant chunks by
Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit
of LLMs. However, both strategies have drawbacks: input reduction has no
guarantee of covering the part with needed information, while window extension
struggles with focusing on the pertinent information for solving the task. To
mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework
that harnesses multi-agent collaboration through natural language to enable
information aggregation and context reasoning across various LLMs over
long-context tasks. CoA consists of multiple worker agents who sequentially
communicate to handle different segmented portions of the text, followed by a
manager agent who synthesizes these contributions into a coherent final output.
CoA processes the entire input by interleaving reading and reasoning, and it
mitigates long context focus issues by assigning each agent a short context. We
perform comprehensive evaluation of CoA on a wide range of long-context tasks
in question answering, summarization, and code completion, demonstrating
significant improvements by up to 10% over strong baselines of RAG,
Full-Context, and multi-agent LLMs.",Yusen Zhang
2024-06-06T11:14:27Z,http://arxiv.org/abs/2406.03963v1,"A + B: A General Generator-Reader Framework for Optimizing LLMs to
  Unleash Synergy Potential","Retrieval-Augmented Generation (RAG) is an effective solution to supplement
necessary knowledge to large language models (LLMs). Targeting its bottleneck
of retriever performance, ""generate-then-read"" pipeline is proposed to replace
the retrieval stage with generation from the LLM itself. Although promising,
this research direction is underexplored and still cannot work in the scenario
when source knowledge is given. In this paper, we formalize a general ""A + B""
framework with varying combinations of foundation models and types for
systematic investigation. We explore the efficacy of the base and chat versions
of LLMs and found their different functionalities suitable for generator A and
reader B, respectively. Their combinations consistently outperform single
models, especially in complex scenarios. Furthermore, we extend the application
of the ""A + B"" framework to scenarios involving source documents through
continuous learning, enabling the direct integration of external knowledge into
LLMs. This approach not only facilitates effective acquisition of new knowledge
but also addresses the challenges of safety and helpfulness post-adaptation.
The paper underscores the versatility of the ""A + B"" framework, demonstrating
its potential to enhance the practical application of LLMs across various
domains.",Wei Tang
2024-06-07T16:59:38Z,http://arxiv.org/abs/2406.05085v2,Multi-Head RAG: Solving Multi-Aspect Problems with LLMs,"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving motivation is that different attention
heads can learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets that we release
online, and real-world use cases to demonstrate MRAG's effectiveness, showing
improvements of up to 20% in relevance over standard RAG baselines. MRAG can be
seamlessly integrated with existing RAG frameworks and benchmarking tools like
RAGAS as well as different classes of data stores.",Maciej Besta
2024-06-07T17:02:35Z,http://arxiv.org/abs/2406.05087v2,Corpus Poisoning via Approximate Greedy Gradient Descent,"Dense retrievers are widely used in information retrieval and have also been
successfully extended to other knowledge intensive areas such as language
models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they
have recently been shown to be vulnerable to corpus poisoning attacks in which
a malicious user injects a small fraction of adversarial passages into the
retrieval corpus to trick the system into returning these passages among the
top-ranked results for a broad set of user queries. Further study is needed to
understand the extent to which these attacks could limit the deployment of
dense retrievers in real-world applications. In this work, we propose
Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval
systems based on the widely used HotFlip method for efficiently generating
adversarial passages. We demonstrate that AGGD can select a higher quality set
of token-level perturbations than HotFlip by replacing its random token
sampling with a more structured search. Experimentally, we show that our method
achieves a high attack success rate on several datasets and using several
retrievers, and can generalize to unseen queries and new domains. Notably, our
method is extremely effective in attacking the ANCE retrieval model, achieving
attack success rates that are 15.24\% and 17.44\% higher on the NQ and MS MARCO
datasets, respectively, compared to HotFlip. Additionally, we demonstrate
AGGD's potential to replace HotFlip in other adversarial attacks, such as
knowledge poisoning of RAG systems.",Jinyan Su
2024-06-10T15:52:49Z,http://arxiv.org/abs/2406.06399v3,"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt
  LLMs for Dialogue","We study the limitations of Large Language Models (LLMs) for the task of
response generation in human-machine dialogue. Several techniques have been
proposed in the literature for different dialogue types (e.g., Open-Domain).
However, the evaluations of these techniques have been limited in terms of base
LLMs, dialogue types and evaluation metrics. In this work, we extensively
analyze different LLM adaptation techniques when applied to different dialogue
types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue
types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.
We evaluate the performance of in-context learning and fine-tuning techniques
across datasets selected for each dialogue type. We assess the impact of
incorporating external knowledge to ground the generation in both scenarios of
Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent
evaluation and explainability criteria for automatic metrics and human
evaluation protocols. Our analysis shows that there is no universal
best-technique for adapting large language models as the efficacy of each
technique depends on both the base LLM and the specific type of dialogue. Last
but not least, the assessment of the best adaptation technique should include
human evaluation to avoid false expectations and outcomes derived from
automatic metrics.",Simone Alghisi
2024-06-03T07:44:32Z,http://arxiv.org/abs/2406.06566v4,"Natural Language Interaction with a Household Electricity
  Knowledge-based Digital Twin","Domain specific digital twins, representing a digital replica of various
segments of the smart grid, are foreseen as able to model, simulate, and
control the respective segments. At the same time, knowledge-based digital
twins, coupled with AI, may also empower humans to understand aspects of the
system through natural language interaction in view of planning and policy
making. This paper is the first to assess and report on the potential of
Retrieval Augmented Generation (RAG) question answers related to household
electrical energy measurement aspects leveraging a knowledge-based energy
digital twin. Relying on the recently published electricity consumption
knowledge graph that actually represents a knowledge-based digital twin, we
study the capabilities of ChatGPT, Gemini and Llama in answering electricity
related questions. Furthermore, we compare the answers with the ones generated
through a RAG techniques that leverages an existing electricity knowledge-based
digital twin. Our findings illustrate that the RAG approach not only reduces
the incidence of incorrect information typically generated by LLMs but also
significantly improves the quality of the output by grounding responses in
verifiable data. This paper details our methodology, presents a comparative
analysis of responses with and without RAG, and discusses the implications of
our findings for future applications of AI in specialized sectors like energy
data analysis.",Carolina Fortuna
2024-06-03T17:07:46Z,http://arxiv.org/abs/2406.06572v2,Graph Neural Network Enhanced Retrieval for Question Answering of LLMs,"Retrieval augmented generation has revolutionized large language model (LLM)
outputs by providing factual supports. Nevertheless, it struggles to capture
all the necessary knowledge for complex reasoning questions. Existing retrieval
methods typically divide reference documents into passages, treating them in
isolation. These passages, however, are often interrelated, such as passages
that are contiguous or share the same keywords. Therefore, it is crucial to
recognize such relatedness for enhancing the retrieval process. In this paper,
we propose a novel retrieval method, called GNN-Ret, which leverages graph
neural networks (GNNs) to enhance retrieval by exploiting the relatedness
between passages. Specifically, we first construct a graph of passages by
connecting passages that are structure-related or keyword-related. A graph
neural network (GNN) is then leveraged to exploit the relationships between
passages and improve the retrieval of supporting passages. Furthermore, we
extend our method to handle multi-hop reasoning questions using a recurrent
graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates
the graphs of passages from previous steps, thereby enhancing the retrieval of
supporting passages. Extensive experiments on benchmark datasets demonstrate
that GNN-Ret achieves higher accuracy for question answering with a single
query of LLMs than strong baselines that require multiple queries, and RGNN-Ret
further improves accuracy and achieves state-of-the-art performance, with up to
10.4% accuracy improvement on the 2WikiMQA dataset.",Zijian Li
2024-06-04T08:34:19Z,http://arxiv.org/abs/2406.06577v1,"RAG-based Crowdsourcing Task Decomposition via Masked Contrastive
  Learning with Prompts","Crowdsourcing is a critical technology in social manufacturing, which
leverages an extensive and boundless reservoir of human resources to handle a
wide array of complex tasks. The successful execution of these complex tasks
relies on task decomposition (TD) and allocation, with the former being a
prerequisite for the latter. Recently, pre-trained language models (PLMs)-based
methods have garnered significant attention. However, they are constrained to
handling straightforward common-sense tasks due to their inherent restrictions
involving limited and difficult-to-update knowledge as well as the presence of
hallucinations. To address these issues, we propose a retrieval-augmented
generation-based crowdsourcing framework that reimagines TD as event detection
from the perspective of natural language understanding. However, the existing
detection methods fail to distinguish differences between event types and
always depend on heuristic rules and external semantic analyzing tools.
Therefore, we present a Prompt-Based Contrastive learning framework for TD
(PBCT), which incorporates a prompt-based trigger detector to overcome
dependence. Additionally, trigger-attentive sentinel and masked contrastive
learning are introduced to provide varying attention to trigger and contextual
features according to different event types. Experiment results demonstrate the
competitiveness of our method in both supervised and zero-shot detection. A
case study on printed circuit board manufacturing is showcased to validate its
adaptability to unknown professional domains.",Jing Yang
2024-05-09T18:15:12Z,http://arxiv.org/abs/2406.07561v1,"Artificial Intelligence as the New Hacker: Developing Agents for
  Offensive Security","In the vast domain of cybersecurity, the transition from reactive defense to
offensive has become critical in protecting digital infrastructures. This paper
explores the integration of Artificial Intelligence (AI) into offensive
cybersecurity, particularly through the development of an autonomous AI agent,
ReaperAI, designed to simulate and execute cyberattacks. Leveraging the
capabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI
demonstrates the potential to identify, exploit, and analyze security
vulnerabilities autonomously.
  This research outlines the core methodologies that can be utilized to
increase consistency and performance, including task-driven penetration testing
frameworks, AI-driven command generation, and advanced prompting techniques.
The AI agent operates within a structured environment using Python, enhanced by
Retrieval Augmented Generation (RAG) for contextual understanding and memory
retention. ReaperAI was tested on platforms including, Hack The Box, where it
successfully exploited known vulnerabilities, demonstrating its potential
power.
  However, the deployment of AI in offensive security presents significant
ethical and operational challenges. The agent's development process revealed
complexities in command execution, error handling, and maintaining ethical
constraints, highlighting areas for future enhancement.
  This study contributes to the discussion on AI's role in cybersecurity by
showcasing how AI can augment offensive security strategies. It also proposes
future research directions, including the refinement of AI interactions with
cybersecurity tools, enhancement of learning mechanisms, and the discussion of
ethical guidelines for AI in offensive roles. The findings advocate for a
unique approach to AI implementation in cybersecurity, emphasizing innovation.",Leroy Jacob Valencia
2024-06-13T16:10:19Z,http://arxiv.org/abs/2406.09272v3,"Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric
  Videos","Generating realistic audio for human actions is important for many
applications, such as creating sound effects for films or virtual reality
games. Existing approaches implicitly assume total correspondence between the
video and audio during training, yet many sounds happen off-screen and have
weak to no correspondence with the visuals -- resulting in uncontrolled ambient
sounds or hallucinations at test time. We propose a novel ambient-aware audio
generation model, AV-LDM. We devise a novel audio-conditioning mechanism to
learn to disentangle foreground action sounds from the ambient background
sounds in in-the-wild training videos. Given a novel silent video, our model
uses retrieval-augmented generation to create audio that matches the visual
content both semantically and temporally. We train and evaluate our model on
two in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we
introduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.
Our model outperforms an array of existing methods, allows controllable
generation of the ambient sound, and even shows promise for generalizing to
computer graphics game clips. Overall, our approach is the first to focus
video-to-audio generation faithfully on the observed visual content despite
training from uncurated clips with natural background sounds.",Changan Chen
2024-06-14T13:28:31Z,http://arxiv.org/abs/2406.10018v1,"STALL+: Boosting LLM-based Repository-level Code Completion with Static
  Analysis","Repository-level code completion is challenging as it involves complicated
contexts from multiple files in the repository. To date, researchers have
proposed two technical categories to enhance LLM-based repository-level code
completion, i.e., retrieval-augmented generation (RAG) and static analysis
integration. This work performs the first study on the static analysis
integration in LLM-based repository-level code completion by investigating both
the effectiveness and efficiency of static analysis integration strategies
across different phases of code completion. We first implement a framework
STALL+, which supports an extendable and customizable integration of multiple
static analysis strategies into the complete pipeline of LLM-based
repository-level code completion; and based on STALL+, we perform extensive
experiments by including different code LLMs on the latest repository-level
code completion benchmark CrossCodeEval. Our findings show that integrating
file-level dependencies in prompting phase performs the best while the
integration in post-processing phase performs the worse. Additionally, we
observe different improvements from static analysis between dynamic languages
and static languages, i.e., the best combination is prompting-phase with
decoding-phase integration for Java while the best combination is
prompting-phase with post-processing-phase integration for Python given the
limitations of statically analyzing dynamic languages. Additionally, we find
the complementarity between RAG and static analysis integration as well as
their cost-effectiveness after combination.",Junwei Liu
2024-06-14T16:00:29Z,http://arxiv.org/abs/2406.10149v2,"BABILong: Testing the Limits of LLMs with Long Context
  Reasoning-in-a-Haystack","In recent years, the input context sizes of large language models (LLMs) have
increased dramatically. However, existing evaluation methods have not kept
pace, failing to comprehensively assess the efficiency of models in handling
long contexts. To bridge this gap, we introduce the BABILong benchmark,
designed to test language models' ability to reason across facts distributed in
extremely long documents. BABILong includes a diverse set of 20 reasoning
tasks, including fact chaining, simple induction, deduction, counting, and
handling lists/sets. These tasks are challenging on their own, and even more
demanding when the required facts are scattered across long natural text. Our
evaluations show that popular LLMs effectively utilize only 10-20\% of the
context and their performance declines sharply with increased reasoning
complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
Generation methods achieve a modest 60\% accuracy on single-fact question
answering, independent of context length. Among context extension methods, the
highest performance is demonstrated by recurrent memory transformers after
fine-tuning, enabling the processing of lengths up to 50 million tokens. The
BABILong benchmark is extendable to any length to support the evaluation of new
upcoming models with increased capabilities, and we provide splits up to 10
million token lengths.",Yuri Kuratov
2024-06-11T01:27:00Z,http://arxiv.org/abs/2406.10261v1,"FoodSky: A Food-oriented Large Language Model that Passes the Chef and
  Dietetic Examination","Food is foundational to human life, serving not only as a source of
nourishment but also as a cornerstone of cultural identity and social
interaction. As the complexity of global dietary needs and preferences grows,
food intelligence is needed to enable food perception and reasoning for various
tasks, ranging from recipe generation and dietary recommendation to
diet-disease correlation discovery and understanding. Towards this goal, for
powerful capabilities across various domains and tasks in Large Language Models
(LLMs), we introduce Food-oriented LLM FoodSky to comprehend food data through
perception and reasoning. Considering the complexity and typicality of Chinese
cuisine, we first construct one comprehensive Chinese food corpus FoodEarth
from various authoritative sources, which can be leveraged by FoodSky to
achieve deep understanding of food-related data. We then propose Topic-based
Selective State Space Model (TS3M) and the Hierarchical Topic Retrieval
Augmented Generation (HTRAG) mechanism to enhance FoodSky in capturing
fine-grained food semantics and generating context-aware food-relevant text,
respectively. Our extensive evaluations demonstrate that FoodSky significantly
outperforms general-purpose LLMs in both chef and dietetic examinations, with
an accuracy of 67.2% and 66.4% on the Chinese National Chef Exam and the
National Dietetic Exam, respectively. FoodSky not only promises to enhance
culinary creativity and promote healthier eating patterns, but also sets a new
standard for domain-specific LLMs that address complex real-world issues in the
food domain. An online demonstration of FoodSky is available at
http://222.92.101.211:8200.",Pengfei Zhou
2024-06-15T17:07:31Z,http://arxiv.org/abs/2406.10690v3,"Automating Pharmacovigilance Evidence Generation: Using Large Language
  Models to Produce Context-Aware SQL","Objective: To enhance the efficiency and accuracy of information retrieval
from pharmacovigilance (PV) databases by employing Large Language Models (LLMs)
to convert natural language queries (NLQs) into Structured Query Language (SQL)
queries, leveraging a business context document.
  Materials and Methods: We utilized OpenAI's GPT-4 model within a
retrieval-augmented generation (RAG) framework, enriched with a business
context document, to transform NLQs into syntactically precise SQL queries.
Each NLQ was presented to the LLM randomly and independently to prevent
memorization. The study was conducted in three phases, varying query
complexity, and assessing the LLM's performance both with and without the
business context document.
  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing
from 8.3\% with the database schema alone to 78.3\% with the business context
document. This enhancement was consistent across low, medium, and high
complexity queries, indicating the critical role of contextual knowledge in
query generation.
  Discussion: The integration of a business context document markedly improved
the LLM's ability to generate accurate and contextually relevant SQL queries.
Performance achieved a maximum of 85\% when high complexity queries are
excluded, suggesting promise for routine deployment.
  Conclusion: This study presents a novel approach to employing LLMs for safety
data retrieval and analysis, demonstrating significant advancements in query
generation accuracy. The methodology offers a framework applicable to various
data-intensive domains, enhancing the accessibility and efficiency of
information retrieval for non-technical users.",Jeffery L. Painter
2024-06-17T02:25:45Z,http://arxiv.org/abs/2406.11147v2,"Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level
  RAG","Vulnerability detection is essential for software quality assurance. In
recent years, deep learning models (especially large language models) have
shown promise in vulnerability detection. In this work, we propose a novel
LLM-based vulnerability detection technique Vul-RAG, which leverages
knowledge-level retrieval-augmented generation (RAG) framework to detect
vulnerability for the given code in three phases. First, Vul-RAG constructs a
vulnerability knowledge base by extracting multi-dimension knowledge via LLMs
from existing CVE instances; second, for a given code snippet, Vul-RAG}
retrieves the relevant vulnerability knowledge from the constructed knowledge
base based on functional semantics; third, Vul-RAG leverages LLMs to check the
vulnerability of the given code snippet by reasoning the presence of
vulnerability causes and fixing solutions of the retrieved vulnerability
knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows
that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative
improvement in accuracy/pairwise-accuracy. In addition, our user study shows
that the vulnerability knowledge generated by Vul-RAG can serve as high-quality
explanations which can improve the manual detection accuracy from 0.60 to 0.77.",Xueying Du
2024-06-17T04:35:17Z,http://arxiv.org/abs/2406.11201v2,"Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large
  Language Models","Large Language Models (LLMs) have the unique capability to understand and
generate human-like text from input queries. When fine-tuned, these models show
enhanced performance on domain-specific queries. OpenAI highlights the process
of fine-tuning, stating: ""To fine-tune a model, you are required to provide at
least 10 examples. We typically see clear improvements from fine-tuning on 50
to 100 training examples, but the right number varies greatly based on the
exact use case."" This study extends this concept to the integration of LLMs
within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve
accuracy and relevance by leveraging external corpus data for information
retrieval. However, RAG's promise of delivering optimal responses often falls
short in complex query scenarios. This study aims to specifically examine the
effects of fine-tuning LLMs on their ability to extract and integrate
contextual data to enhance the performance of RAG systems across multiple
domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data
extraction and contextual understanding by comparing the accuracy and
completeness of fine-tuned models against baseline performances across datasets
from multiple domains. Our findings indicate that fine-tuning resulted in a
decline in performance compared to the baseline models, contrary to the
improvements observed in standalone LLM applications as suggested by OpenAI.
This study highlights the need for vigorous investigation and validation of
fine-tuned models for domain-specific tasks.",Scott Barnett
2024-06-17T09:25:10Z,http://arxiv.org/abs/2406.11357v2,"Refiner: Restructure Retrieval Content Efficiently to Advance
  Question-Answering Capabilities","Large Language Models (LLMs) are limited by their parametric knowledge,
leading to hallucinations in knowledge-extensive tasks. To address this,
Retrieval-Augmented Generation (RAG) incorporates external document chunks to
expand LLM knowledge. Furthermore, compressing information from document chunks
through extraction or summarization can improve LLM performance. Nonetheless,
LLMs still struggle to notice and utilize scattered key information, a problem
known as the ""lost-in-the-middle"" syndrome. Therefore, we typically need to
restructure the content for LLM to recognize the key information. We propose
$\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that
operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a
single decoder-only LLM to adaptively extract query-relevant contents verbatim
along with the necessary context, and section them based on their
interconnectedness, thereby highlights information distinction, and aligns
downstream LLMs with the original context effectively. Experiments show that a
trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to
downstream LLM in improving answer accuracy, and outperforms other
state-of-the-art advanced RAG and concurrent compressing approaches in various
single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5%
tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared
to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that
can be seamlessly integrated with RAG systems, facilitating its application
across diverse open-source frameworks.",Zhonghao Li
2024-05-25T13:38:15Z,http://arxiv.org/abs/2406.12881v1,Towards Unlocking Insights from Logbooks Using AI,"Electronic logbooks contain valuable information about activities and events
concerning their associated particle accelerator facilities. However, the
highly technical nature of logbook entries can hinder their usability and
automation. As natural language processing (NLP) continues advancing, it offers
opportunities to address various challenges that logbooks present. This work
explores jointly testing a tailored Retrieval Augmented Generation (RAG) model
for enhancing the usability of particle accelerator logbooks at institutes like
DESY, BESSY, Fermilab, BNL, SLAC, LBNL, and CERN. The RAG model uses a corpus
built on logbook contributions and aims to unlock insights from these logbooks
by leveraging retrieval over facility datasets, including discussion about
potential multimodal sources. Our goals are to increase the FAIR-ness
(findability, accessibility, interoperability, and reusability) of logbooks by
exploiting their information content to streamline everyday use, enable
macro-analysis for root cause analysis, and facilitate problem-solving
automation.",Antonin Sulc
2024-06-16T22:04:10Z,http://arxiv.org/abs/2406.12934v1,Current state of LLM Risks and AI Guardrails,"Large language models (LLMs) have become increasingly sophisticated, leading
to widespread deployment in sensitive applications where safety and reliability
are paramount. However, LLMs have inherent risks accompanying them, including
bias, potential for unsafe actions, dataset poisoning, lack of explainability,
hallucinations, and non-reproducibility. These risks necessitate the
development of ""guardrails"" to align LLMs with desired behaviors and mitigate
potential harm.
  This work explores the risks associated with deploying LLMs and evaluates
current approaches to implementing guardrails and model alignment techniques.
We examine intrinsic and extrinsic bias evaluation methods and discuss the
importance of fairness metrics for responsible AI development. The safety and
reliability of agentic LLMs (those capable of real-world actions) are explored,
emphasizing the need for testability, fail-safes, and situational awareness.
  Technical strategies for securing LLMs are presented, including a layered
protection model operating at external, secondary, and internal levels. System
prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to
minimize bias and protect privacy are highlighted.
  Effective guardrail design requires a deep understanding of the LLM's
intended use case, relevant regulations, and ethical considerations. Striking a
balance between competing requirements, such as accuracy and privacy, remains
an ongoing challenge. This work underscores the importance of continuous
research and development to ensure the safe and responsible use of LLMs in
real-world applications.",Suriya Ganesh Ayyamperumal
2024-06-19T20:13:42Z,http://arxiv.org/abs/2406.13805v1,"WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge
  Conflicts from Wikipedia","Retrieval-augmented generation (RAG) has emerged as a promising solution to
mitigate the limitations of large language models (LLMs), such as
hallucinations and outdated information. However, it remains unclear how LLMs
handle knowledge conflicts arising from different augmented retrieved passages,
especially when these passages originate from the same source and have equal
trustworthiness. In this work, we conduct a comprehensive evaluation of
LLM-generated answers to questions that have varying answers based on
contradictory passages from Wikipedia, a dataset widely regarded as a
high-quality pre-training resource for most LLMs. Specifically, we introduce
WikiContradict, a benchmark consisting of 253 high-quality, human-annotated
instances designed to assess LLM performance when augmented with retrieved
passages containing real-world knowledge conflicts. We benchmark a diverse
range of both closed and open-source LLMs under different QA scenarios,
including RAG with a single passage, and RAG with 2 contradictory passages.
Through rigorous human evaluations on a subset of WikiContradict instances
involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and
limitations of these models. For instance, when provided with two passages
containing contradictory facts, all models struggle to generate answers that
accurately reflect the conflicting nature of the context, especially for
implicit conflicts requiring reasoning. Since human evaluation is costly, we
also introduce an automated model that estimates LLM performance using a strong
open-source language model, achieving an F-score of 0.8. Using this automated
metric, we evaluate more than 1,500 answers from seven LLMs across all
WikiContradict instances. To facilitate future work, we release WikiContradict
on: https://ibm.biz/wikicontradict.",Yufang Hou
2024-06-20T16:59:52Z,http://arxiv.org/abs/2406.14497v1,CodeRAG-Bench: Can Retrieval Augment Code Generation?,"While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.",Zora Zhiruo Wang
2024-06-20T21:27:57Z,http://arxiv.org/abs/2406.14745v2,"Relation Extraction with Fine-Tuned Large Language Models in Retrieval
  Augmented Generation Frameworks","Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.",Sefika Efeoglu
2024-06-21T08:45:52Z,http://arxiv.org/abs/2406.14979v2,"Retrieve-Plan-Generation: An Iterative Planning and Answering Framework
  for Knowledge-Intensive LLM Generation","Despite the significant progress of large language models (LLMs) in various
tasks, they often produce factual errors due to their limited internal
knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with
external knowledge sources, offers a promising solution. However, these methods
can be misled by irrelevant paragraphs in retrieved documents. Due to the
inherent uncertainty in LLM generation, inputting the entire document may
introduce off-topic information, causing the model to deviate from the central
topic and affecting the relevance of the generated content. To address these
issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates
plan tokens to guide subsequent generation in the plan stage. In the answer
stage, the model selects relevant fine-grained paragraphs based on the plan and
uses them for further answer generation. This plan-answer process is repeated
iteratively until completion, enhancing generation relevance by focusing on
specific topics. To implement this framework efficiently, we utilize a simple
but effective multi-task prompt-tuning method, enabling the existing LLMs to
handle both planning and answering. We comprehensively compare RPG with
baselines across 5 knowledge-intensive generation tasks, demonstrating the
effectiveness of our approach.",Yuanjie Lyu
2024-06-21T17:23:21Z,http://arxiv.org/abs/2406.15319v3,LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,"In traditional RAG framework, the basic retrieval units are normally short.
The common retrievers like DPR normally work with 100-word Wikipedia
paragraphs. Such a design forces the retriever to search over a large corpus to
find the `needle' unit. In contrast, the readers only need to generate answers
from the short retrieved units. The imbalanced `heavy' retriever and `light'
reader design can lead to sub-optimal performance. The loss of contextual
information in the short, chunked units may increase the likelihood of
introducing hard negatives during the retrieval stage. Additionally, the reader
might not fully leverage the capabilities of recent advancements in LLMs. In
order to alleviate the imbalance, we propose a new framework LongRAG,
consisting of a `long retriever' and a `long reader'. In the two
Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire
Wikipedia corpus into 4K-token units by grouping related documents. By
increasing the unit size, we significantly reduce the total number of units.
This greatly reduces the burden on the retriever, resulting in strong retrieval
performance with only a few (less than 8) top units. Without requiring any
training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which
are on par with the (fully-trained) SoTA model. Furthermore, we test on two
non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes
each individual document as a single (long) unit rather than chunking them into
smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%
on MultiFieldQA-en. Our study offers insights into the future roadmap for
combining RAG with long-context LLMs.",Ziyan Jiang
2024-06-23T04:35:42Z,http://arxiv.org/abs/2406.16008v2,"Found in the Middle: Calibrating Positional Attention Bias Improves Long
  Context Utilization","Large language models (LLMs), even when specifically trained to process long
input contexts, struggle to capture relevant information located in the middle
of their input. This phenomenon has been known as the lost-in-the-middle
problem. In this work, we make three contributions. First, we set out to
understand the factors that cause this phenomenon. In doing so, we establish a
connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs
exhibit a U-shaped attention bias where the tokens at the beginning and at the
end of its input receive higher attention, regardless of their relevance.
Second, we mitigate this positional bias through a calibration mechanism,
found-in-the-middle, that allows the model to attend to contexts faithfully
according to their relevance, even though when they are in the middle. Third,
we show found-in-the-middle not only achieves better performance in locating
relevant information within a long context, but also eventually leads to
improved retrieval-augmented generation (RAG) performance across various tasks,
outperforming existing methods by up to 15 percentage points. These findings
open up future directions in understanding LLM attention bias and its potential
consequences.",Cheng-Yu Hsieh
2024-06-24T01:22:54Z,http://arxiv.org/abs/2406.16252v2,"Graph-Augmented LLMs for Personalized Health Insights: A Case Study in
  Sleep Analysis","Health monitoring systems have revolutionized modern healthcare by enabling
the continuous capture of physiological and behavioral data, essential for
preventive measures and early health intervention. While integrating this data
with Large Language Models (LLMs) has shown promise in delivering interactive
health advice, traditional methods like Retrieval-Augmented Generation (RAG)
and fine-tuning often fail to fully utilize the complex, multi-dimensional, and
temporally relevant data from wearable devices. These conventional approaches
typically provide limited actionable and personalized health insights due to
their inadequate capacity to dynamically integrate and interpret diverse health
data streams. In response, this paper introduces a graph-augmented LLM
framework designed to significantly enhance the personalization and clarity of
health insights. Utilizing a hierarchical graph structure, the framework
captures inter and intra-patient relationships, enriching LLM prompts with
dynamic feature importance scores derived from a Random Forest Model. The
effectiveness of this approach is demonstrated through a sleep analysis case
study involving 20 college students during the COVID-19 lockdown, highlighting
the potential of our model to generate actionable and personalized health
insights efficiently. We leverage another LLM to evaluate the insights for
relevance, comprehensiveness, actionability, and personalization, addressing
the critical need for models that process and interpret complex health data
effectively. Our findings show that augmenting prompts with our framework
yields significant improvements in all 4 criteria. Through our framework, we
can elicit well-crafted, more thoughtful responses tailored to a specific
patient.",Ajan Subramanian
2024-06-24T23:57:57Z,http://arxiv.org/abs/2406.17186v2,"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented
  Analysis Generation","Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.",Abe Bohan Hou
2024-06-25T09:42:56Z,http://arxiv.org/abs/2406.17419v2,"Leave No Document Behind: Benchmarking Long-Context LLMs with Extended
  Multi-Doc QA","Long-context modeling capabilities have garnered widespread attention,
leading to the emergence of Large Language Models (LLMs) with ultra-context
windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually
catching up. However, existing benchmarks employ irrelevant noise texts to
artificially extend the length of test cases, diverging from the real-world
scenarios of long-context applications. To bridge this gap, we propose a novel
long-context benchmark, Loong, aligning with realistic scenarios through
extended multi-document question answering (QA). Unlike typical document QA, in
Loong's test cases, each document is relevant to the final answer, ignoring any
document will lead to the failure of the answer. Furthermore, Loong introduces
four types of tasks with a range of context lengths: Spotlight Locating,
Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic
and comprehensive evaluation of long-context understanding. Extensive
experiments indicate that existing long-context language models still exhibit
considerable potential for enhancement. Retrieval augmented generation (RAG)
achieves poor performance, demonstrating that Loong can reliably assess the
model's long-context modeling capabilities.",Minzheng Wang
2024-06-25T15:43:20Z,http://arxiv.org/abs/2406.17651v5,"Software Model Evolution with Large Language Models: Experiments on
  Simulated, Public, and Industrial Datasets","Modeling structure and behavior of software systems plays a crucial role in
the industrial practice of software engineering. As with other software
engineering artifacts, software models are subject to evolution. Supporting
modelers in evolving software models with recommendations for model completions
is still an open problem, though. In this paper, we explore the potential of
large language models for this task. In particular, we propose an approach,
RAMC, leveraging large language models, model histories, and
retrieval-augmented generation for model completion. Through experiments on
three datasets, including an industrial application, one public open-source
community dataset, and one controlled collection of simulated model
repositories, we evaluate the potential of large language models for model
completion with RAMC. We found that large language models are indeed a
promising technology for supporting software model evolution (62.30%
semantically correct completions on real-world industrial data and up to 86.19%
type-correct completions). The general inference capabilities of large language
models are particularly useful when dealing with concepts for which there are
few, noisy, or no examples at all.",Christof Tinnes
2024-06-26T00:00:45Z,http://arxiv.org/abs/2406.17987v4,Multi-step Inference over Unstructured Data,"The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.",Aditya Kalyanpur
2024-06-27T07:56:44Z,http://arxiv.org/abs/2406.18966v3,"UniGen: A Unified Framework for Textual Dataset Generation Using Large
  Language Models","Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.",Siyuan Wu
2024-06-28T01:14:43Z,http://arxiv.org/abs/2406.19593v1,"SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs","Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.",Xin Su
2024-06-21T08:52:11Z,http://arxiv.org/abs/2407.00072v5,Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback,"RAG systems face limitations when semantic relevance alone does not guarantee
improved generation quality. This issue becomes particularly evident due to the
sensitivity of large language models (LLMs) to the ordering of few-shot
prompts, which can affect model performance. To address this challenge,
aligning LLM outputs with human preferences using structured feedback, such as
options to copy, regenerate, or dislike, offers a promising method for
improvement. This feedback is applied to the entire list of inputs rather than
giving specific ratings for individual documents, making it a Listwide Labels
Learning-to-Rank task.
  To address this task, we propose Pistis-RAG, a new RAG framework designed
with a content-centric approach to better align LLMs with human preferences.
Pistis-RAG effectively utilizes human feedback, enhancing content ranking and
generation quality. To validate our framework, we use public datasets to
simulate human feedback, allowing us to evaluate and refine our method
effectively. Experimental results indicate that Pistis-RAG improves alignment
with human preferences relative to the baseline RAG system, showing a 6.06%
increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy
metrics. These results highlight Pistis-RAG's effectiveness in overcoming the
limitations associated with traditional RAG approaches.",Yu Bai
2024-06-30T15:38:48Z,http://arxiv.org/abs/2407.00731v2,"Large Language Models Struggle in Token-Level Clinical Named Entity
  Recognition","Large Language Models (LLMs) have revolutionized various sectors, including
healthcare where they are employed in diverse applications. Their utility is
particularly significant in the context of rare diseases, where data scarcity,
complexity, and specificity pose considerable challenges. In the clinical
domain, Named Entity Recognition (NER) stands out as an essential task and it
plays a crucial role in extracting relevant information from clinical texts.
Despite the promise of LLMs, current research mostly concentrates on
document-level NER, identifying entities in a more general context across
entire documents, without extracting their precise location. Additionally,
efforts have been directed towards adapting ChatGPT for token-level NER.
However, there is a significant research gap when it comes to employing
token-level NER for clinical texts, especially with the use of local
open-source LLMs. This study aims to bridge this gap by investigating the
effectiveness of both proprietary and local LLMs in token-level clinical NER.
Essentially, we delve into the capabilities of these models through a series of
experiments involving zero-shot prompting, few-shot prompting,
retrieval-augmented generation (RAG), and instruction-fine-tuning. Our
exploration reveals the inherent challenges LLMs face in token-level NER,
particularly in the context of rare diseases, and suggests possible
improvements for their application in healthcare. This research contributes to
narrowing a significant gap in healthcare informatics and offers insights that
could lead to a more refined application of LLMs in the healthcare sector.",Qiuhao Lu
2024-07-01T05:28:40Z,http://arxiv.org/abs/2407.00978v2,"Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in
  Internet of Medical Things: A Diffusion-based Contract Approach","Secure data management and effective data sharing have become paramount in
the rapidly evolving healthcare landscape, especially with the growing
integration of the Internet of Medical Things (IoMT). The rise of generative
artificial intelligence has further elevated Multi-modal Large Language Models
(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.
MLLMs can support multi-modal inputs and generate diverse types of content by
leveraging large-scale training on vast amounts of multi-modal data. However,
critical challenges persist in developing medical MLLMs, including security and
freshness issues of healthcare data, affecting the output quality of MLLMs. To
this end, in this paper, we propose a hybrid Retrieval-Augmented Generation
(RAG)-empowered medical MLLM framework for healthcare data management. This
framework leverages a hierarchical cross-chain architecture to facilitate
secure data training. Moreover, it enhances the output quality of MLLMs through
hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG
results and incorporates these retrieval results as additional inputs to MLLMs.
Additionally, we employ age of information to indirectly evaluate the data
freshness impact of MLLMs and utilize contract theory to incentivize healthcare
data holders to share their fresh data, mitigating information asymmetry during
data sharing. Finally, we utilize a generative diffusion model-based deep
reinforcement learning algorithm to identify the optimal contract for efficient
data sharing. Numerical results demonstrate the effectiveness of the proposed
schemes, which achieve secure and efficient healthcare data management.",Cheng Su
2024-07-01T09:19:50Z,http://arxiv.org/abs/2407.01110v1,"SecGenAI: Enhancing Security of Cloud-based Generative AI Applications
  within Australian Critical Technologies of National Interest","The rapid advancement of Generative AI (GenAI) technologies offers
transformative opportunities within Australia's critical technologies of
national interest while introducing unique security challenges. This paper
presents SecGenAI, a comprehensive security framework for cloud-based GenAI
applications, with a focus on Retrieval-Augmented Generation (RAG) systems.
SecGenAI addresses functional, infrastructure, and governance requirements,
integrating end-to-end security analysis to generate specifications emphasizing
data privacy, secure deployment, and shared responsibility models. Aligned with
Australian Privacy Principles, AI Ethics Principles, and guidelines from the
Australian Cyber Security Centre and Digital Transformation Agency, SecGenAI
mitigates threats such as data leakage, adversarial attacks, and model
inversion. The framework's novel approach combines advanced machine learning
techniques with robust security measures, ensuring compliance with Australian
regulations while enhancing the reliability and trustworthiness of GenAI
systems. This research contributes to the field of intelligent systems by
providing actionable strategies for secure GenAI implementation in industry,
fostering innovation in AI applications, and safeguarding national interests.",Christoforus Yoga Haryanto
2024-07-01T11:07:23Z,http://arxiv.org/abs/2407.01178v1,$\text{Memory}^3$: Language Modeling with Explicit Memory,"The training and inference of large language models (LLMs) are together a
costly process that transports knowledge from raw data to meaningful
computation. Inspired by the memory hierarchy of the human brain, we reduce
this cost by equipping LLMs with explicit memory, a memory format cheaper than
model parameters and text retrieval-augmented generation (RAG). Conceptually,
with most of its knowledge externalized to explicit memories, the LLM can enjoy
a smaller parameter size, training cost, and inference cost, all proportional
to the amount of remaining ""abstract knowledge"". As a preliminary proof of
concept, we train from scratch a 2.4B LLM, which achieves better performance
than much larger LLMs as well as RAG models, and maintains higher decoding
speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is
the third form of memory in LLMs after implicit memory (model parameters) and
working memory (context key-values). We introduce a memory circuitry theory to
support the externalization of knowledge, and present novel techniques
including a memory sparsification mechanism that makes storage tractable and a
two-stage pretraining scheme that facilitates memory formation.",Hongkang Yang
2024-07-02T07:52:30Z,http://arxiv.org/abs/2407.02028v1,"Why does in-context learning fail sometimes? Evaluating in-context
  learning on open and closed questions","We measure the performance of in-context learning as a function of task
novelty and difficulty for open and closed questions. For that purpose, we
created a novel benchmark consisting of hard scientific questions, each paired
with a context of various relevancy. We show that counter-intuitively, a
context that is more aligned with the topic does not always help more than a
less relevant context. This effect is especially visible for open questions and
questions of high difficulty or novelty. This result reveals a fundamental
difference between the treatment of close-form and open-form questions by
large-language models and shows a need for a more robust evaluation of
in-context learning on the variety of different types of questions. It also
poses a new question of how to optimally select a context for large language
models, especially in the context of Retrieval Augmented Generation (RAG)
systems. Our results suggest that the answer to this question can be highly
application-dependent and might be contingent on factors including the format
of the question, the perceived difficulty level of the questions, and the
novelty or popularity of the information we seek.",Xiang Li
2024-07-03T01:28:51Z,http://arxiv.org/abs/2407.02742v1,"A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized
  Retrieval Augmentation","Natural Language to Code Generation has made significant progress in recent
years with the advent of Large Language Models(LLMs). While generation for
general-purpose languages like C, C++, and Python has improved significantly,
LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs
having a high number of custom function names. Additionally, constant updates
to function names add to the challenge as LLMs need to stay up-to-date. In this
paper, we present optimizations for using Retrieval Augmented Generation (or
RAG) with LLMs for DSL generation along with an ablation study comparing these
strategies. We generated a train as well as test dataset with a DSL to
represent automation tasks across roughly 700 APIs in public domain. We used
the training dataset to fine-tune a Codex model for this DSL. Our results
showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The
compilation rate, however, showed that both the models still got the syntax
wrong many times, with RAG-based method being 2 pts better. Conversely,
hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for
API parameter keys. We conclude that an optimized RAG model can match the
quality of fine-tuned models and offer advantages for new, unseen APIs.",Nastaran Bassamzadeh
2024-07-04T12:29:06Z,http://arxiv.org/abs/2407.03889v1,"Automated C/C++ Program Repair for High-Level Synthesis via Large
  Language Models","In High-Level Synthesis (HLS), converting a regular C/C++ program into its
HLS-compatible counterpart (HLS-C) still requires tremendous manual effort.
Various program scripts have been introduced to automate this process. But the
resulting codes usually contain many issues that should be manually repaired by
developers. Since Large Language Models (LLMs) have the ability to automate
code generation, they can also be used for automated program repair in HLS.
However, due to the limited training of LLMs considering hardware and software
simultaneously, hallucinations may occur during program repair using LLMs,
leading to compilation failures. Besides, using LLMs for iterative repair also
incurs a high cost. To address these challenges, we propose an LLM-driven
program repair framework that takes regular C/C++ code as input and
automatically generates its corresponding HLS-C code for synthesis while
minimizing human repair effort. To mitigate the hallucinations in LLMs and
enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is
introduced to guide the LLMs toward correct repair. In addition, we use LLMs to
create a static bit width optimization program to identify the optimized bit
widths for variables. Moreover, LLM-driven HLS optimization strategies are
introduced to add/tune pragmas in HLS-C programs for circuit optimization.
Experimental results demonstrate that the proposed LLM-driven automated
framework can achieve much higher repair pass rates in 24 real-world
applications compared with the traditional scripts and the direct application
of LLMs for program repair.",Kangwei Xu
2024-07-05T17:43:30Z,http://arxiv.org/abs/2407.04681v1,"Rethinking Visual Prompting for Multimodal Large Language Models with
  External Knowledge","In recent years, multimodal large language models (MLLMs) have made
significant strides by training on vast high-quality image-text datasets,
enabling them to generally understand images well. However, the inherent
difficulty in explicitly conveying fine-grained or spatially dense information
in text, such as masks, poses a challenge for MLLMs, limiting their ability to
answer questions requiring an understanding of detailed or localized visual
elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)
concept, this paper proposes a new visual prompt approach to integrate
fine-grained external knowledge, gleaned from specialized vision models (e.g.,
instance segmentation/OCR models), into MLLMs. This is a promising yet
underexplored direction for enhancing MLLMs' performance. Our approach diverges
from concurrent works, which transform external knowledge into additional text
prompts, necessitating the model to indirectly learn the correspondence between
visual content and text coordinates. Instead, we propose embedding fine-grained
knowledge information directly into a spatial embedding map as a visual prompt.
This design can be effortlessly incorporated into various MLLMs, such as LLaVA
and Mipha, considerably improving their visual understanding performance.
Through rigorous experiments, we demonstrate that our method can enhance MLLM
performance across nine benchmarks, amplifying their fine-grained context-aware
capabilities.",Yuanze Lin
2024-07-06T09:10:05Z,http://arxiv.org/abs/2407.05015v1,"How do you know that? Teaching Generative Language Models to Reference
  Answers to Biomedical Questions","Large language models (LLMs) have recently become the leading source of
answers for users' questions online. Despite their ability to offer eloquent
answers, their accuracy and reliability can pose a significant challenge. This
is especially true for sensitive domains such as biomedicine, where there is a
higher need for factually correct answers. This paper introduces a biomedical
retrieval-augmented generation (RAG) system designed to enhance the reliability
of generated responses. The system is based on a fine-tuned LLM for the
referenced question-answering, where retrieved relevant abstracts from PubMed
are passed to LLM's context as input through a prompt. Its output is an answer
based on PubMed abstracts, where each statement is referenced accordingly,
allowing the users to verify the answer. Our retrieval system achieves an
absolute improvement of 23% compared to the PubMed search engine. Based on the
manual evaluation on a small sample, our fine-tuned LLM component achieves
comparable results to GPT-4 Turbo in referencing relevant abstracts. We make
the dataset used to fine-tune the models and the fine-tuned models based on
Mistral-7B-instruct-v0.1 and v0.2 publicly available.",Bojana Bašaragin
2024-07-06T16:45:07Z,http://arxiv.org/abs/2407.05131v2,"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language
  Models","The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on medical VQA and report generation tasks across three
datasets, achieving an average improvement of 47.4% in factual accuracy. We
publicly release our benchmark and code in
https://github.com/richard-peng-xia/RULE.",Peng Xia
2024-07-08T13:07:50Z,http://arxiv.org/abs/2407.06245v2,"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open
  Radio Access Networks","Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.",Pranshav Gajjar
2024-07-09T15:59:28Z,http://arxiv.org/abs/2407.06985v4,"PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and
  Tuning Methods","In domain-specific applications, GPT-4, augmented with precise prompts or
Retrieval-Augmented Generation (RAG), shows notable potential but faces the
critical tri-lemma of performance, cost, and data privacy. High performance
requires sophisticated processing techniques, yet managing multiple agents
within a complex workflow often proves costly and challenging. To address this,
we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.
This systematizes domain-specific tasks by integrating precise question
decomposition, advanced information retrieval, comprehensive summarization, and
rigorous self-assessment. Given the concerns of cost and data privacy,
enterprises are shifting from proprietary models like GPT-4 to custom models,
striking a balance between cost, security, and performance. We developed
industrial practices leveraging online data and user feedback for efficient
model tuning. This study provides best practice guidelines for applying
multi-agent systems in domain-specific problem-solving and implementing
effective agent tuning strategies. Our empirical studies, particularly in the
financial question-answering domain, demonstrate that our approach achieves
95.0% of GPT-4's performance, while effectively managing costs and ensuring
data privacy.",Yiying Wang
2024-07-09T17:33:24Z,http://arxiv.org/abs/2407.07061v2,"Internet of Agents: Weaving a Web of Heterogeneous Agents for
  Collaborative Intelligence","The rapid advancement of large language models (LLMs) has paved the way for
the development of highly capable autonomous agents. However, existing
multi-agent frameworks often struggle with integrating diverse capable
third-party agents due to reliance on agents defined within their own
ecosystems. They also face challenges in simulating distributed environments,
as most frameworks are limited to single-device setups. Furthermore, these
frameworks often rely on hard-coded communication pipelines, limiting their
adaptability to dynamic task requirements. Inspired by the concept of the
Internet, we propose the Internet of Agents (IoA), a novel framework that
addresses these limitations by providing a flexible and scalable platform for
LLM-based multi-agent collaboration. IoA introduces an agent integration
protocol, an instant-messaging-like architecture design, and dynamic mechanisms
for agent teaming and conversation flow control. Through extensive experiments
on general assistant tasks, embodied AI tasks, and retrieval-augmented
generation benchmarks, we demonstrate that IoA consistently outperforms
state-of-the-art baselines, showcasing its ability to facilitate effective
collaboration among heterogeneous agents. IoA represents a step towards linking
diverse agents in an Internet-like environment, where agents can seamlessly
collaborate to achieve greater intelligence and capabilities. Our codebase has
been released at \url{https://github.com/OpenBMB/IoA}.",Weize Chen
2024-07-10T16:08:46Z,http://arxiv.org/abs/2407.07791v2,"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent
  Communities","The rapid adoption of large language models (LLMs) in multi-agent systems has
highlighted their impressive capabilities in various applications, such as
collaborative problem-solving and autonomous negotiation. However, the security
implications of these LLM-based multi-agent systems have not been thoroughly
investigated, particularly concerning the spread of manipulated knowledge. In
this paper, we investigate this critical issue by constructing a detailed
threat model and a comprehensive simulation environment that mirrors real-world
multi-agent deployments in a trusted platform. Subsequently, we propose a novel
two-stage attack method involving Persuasiveness Injection and Manipulated
Knowledge Injection to systematically explore the potential for manipulated
knowledge (i.e., counterfactual and toxic knowledge) spread without explicit
prompt manipulation.
  Our method leverages the inherent vulnerabilities of LLMs in handling world
knowledge, which can be exploited by attackers to unconsciously spread
fabricated information. Through extensive experiments, we demonstrate that our
attack method can successfully induce LLM-based agents to spread both
counterfactual and toxic knowledge without degrading their foundational
capabilities during agent communication. Furthermore, we show that these
manipulations can persist through popular retrieval-augmented generation
frameworks, where several benign agents store and retrieve manipulated chat
histories for future interactions. This persistence indicates that even after
the interaction has ended, the benign agents may continue to be influenced by
manipulated knowledge. Our findings reveal significant security risks in
LLM-based multi-agent systems, emphasizing the imperative need for robust
defenses against manipulated knowledge spread, such as introducing ``guardian''
agents and advanced fact-checking tools.",Tianjie Ju
2024-07-13T21:13:55Z,http://arxiv.org/abs/2407.10005v1,"Fine-grained Analysis of In-context Linear Estimation: Data,
  Architecture, and Beyond","Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through
gradient descent steps. However, the existing results on the optimization
landscape apply under stylized settings where task and feature vectors are
assumed to be IID and the attention weights are fully parameterized. In this
work, we develop a stronger characterization of the optimization and
generalization landscape of ICL through contributions on architectures,
low-rank parameterization, and correlated designs: (1) We study the landscape
of 1-layer linear attention and 1-layer H3, a state-space model. Under a
suitable correlated design assumption, we prove that both implement 1-step
preconditioned gradient descent. We show that thanks to its native convolution
filters, H3 also has the advantage of implementing sample weighting and
outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank
parameterized attention weights in terms of covariance spectrum. Through this,
we also shed light on how LoRA can adapt to a new distribution by capturing the
shift between task covariances. Experimental results corroborate our
theoretical findings. Overall, this work explores the optimization and risk
landscape of ICL in practically meaningful settings and contributes to a more
thorough understanding of its mechanics.",Yingcong Li
2024-07-13T22:45:46Z,http://arxiv.org/abs/2407.10021v1,"Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation","Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.",Kriti Bhattarai
2024-05-01T20:43:06Z,http://arxiv.org/abs/2407.10246v3,"CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science
  Education","The growing enrollments in computer science courses and increase in class
sizes necessitate scalable, automated tutoring solutions to adequately support
student learning. While Large Language Models (LLMs) like GPT-4 have
demonstrated potential in assisting students through question-answering,
educators express concerns over student overreliance, miscomprehension of
generated code, and the risk of inaccurate answers. Rather than banning these
tools outright, we advocate for a constructive approach that harnesses the
capabilities of AI while mitigating potential risks. This poster introduces
CourseAssist, a novel LLM-based tutoring system tailored for computer science
education. Unlike generic LLM systems, CourseAssist uses retrieval-augmented
generation, user intent classification, and question decomposition to align AI
responses with specific course materials and learning objectives, thereby
ensuring pedagogical appropriateness of LLMs in educational settings. We
evaluated CourseAssist against a baseline of GPT-4 using a dataset of 50
question-answer pairs from a programming languages course, focusing on the
criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation
results show that CourseAssist significantly outperforms the baseline,
demonstrating its potential to serve as an effective learning assistant. We
have also deployed CourseAssist in 6 computer science courses at a large public
R1 research university reaching over 500 students. Interviews with 20 student
users show that CourseAssist improves computer science instruction by
increasing the accessibility of course-specific tutoring help and shortening
the feedback loop on their programming assignments. Future work will include
extensive pilot testing at more universities and exploring better collaborative
relationships between students, educators, and AI that improve computer science
learning experiences.",Ty Feng
2024-07-15T12:35:00Z,http://arxiv.org/abs/2407.10670v1,"Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for
  Improved Quality and Efficiency in RAG Systems","Retrieval-augmented generation (RAG) techniques leverage the in-context
learning capabilities of large language models (LLMs) to produce more accurate
and relevant responses. Originating from the simple 'retrieve-then-read'
approach, the RAG framework has evolved into a highly flexible and modular
paradigm. A critical component, the Query Rewriter module, enhances knowledge
retrieval by generating a search-friendly query. This method aligns input
questions more closely with the knowledge base. Our research identifies
opportunities to enhance the Query Rewriter module to Query Rewriter+ by
generating multiple queries to overcome the Information Plateaus associated
with a single query and by rewriting questions to eliminate Ambiguity, thereby
clarifying the underlying intent. We also find that current RAG systems exhibit
issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge
Filter. These two modules are both based on the instruction-tuned Gemma-2B
model, which together enhance response quality. The final identified issue is
Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the
Retriever Trigger to solve this. The former supports the dynamic expansion of
the RAG system's knowledge base in a parameter-free manner, while the latter
optimizes the cost for accessing external knowledge, thereby improving resource
utilization and response efficiency. These four RAG modules synergistically
improve the response quality and efficiency of the RAG system. The
effectiveness of these modules has been validated through experiments and
ablation studies across six common QA datasets. The source code can be accessed
at https://github.com/Ancientshi/ERM4.",Yunxiao Shi
2024-06-24T12:09:34Z,http://arxiv.org/abs/2407.10994v1,"Panza: A Personalized Text Writing Assistant via Data Playback and Local
  Fine-Tuning","The availability of powerful open-source large language models (LLMs) opens
exciting use-cases, such as automated personal assistants that adapt to the
user's unique data and demands. Two key desiderata for such assistants are
personalization-in the sense that the assistant should reflect the user's own
style-and privacy-in the sense that users may prefer to always store their
personal data locally, on their own computing device. We present a new design
for such an automated assistant, for the specific use case of personal
assistant for email generation, which we call Panza. Specifically, Panza can be
both trained and inferenced locally on commodity hardware, and is personalized
to the user's writing style. Panza's personalization features are based on a
new technique called data playback, which allows us to fine-tune an LLM to
better reflect a user's writing style using limited data. We show that, by
combining efficient fine-tuning and inference methods, Panza can be executed
entirely locally using limited resources-specifically, it can be executed
within the same resources as a free Google Colab instance. Finally, our key
methodological contribution is a careful study of evaluation metrics, and of
how different choices of system components (e.g. the use of Retrieval-Augmented
Generation or different fine-tuning approaches) impact the system's
performance.",Armand Nicolicioiu
2024-07-16T08:21:02Z,http://arxiv.org/abs/2407.11485v1,Scientific QA System with Verifiable Answers,"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.",Adela Ljajić
2024-07-16T11:58:54Z,http://arxiv.org/abs/2407.11638v1,"A Comprehensive Evaluation of Large Language Models on Temporal Event
  Forecasting","Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.",He Chang
2024-07-17T17:59:47Z,http://arxiv.org/abs/2407.12784v1,"AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
  Bases","LLM agents have demonstrated remarkable performance across various
applications, primarily due to their advanced capabilities in reasoning,
utilizing external knowledge and tools, calling APIs, and executing actions to
interact with environments. Current agents typically utilize a memory module or
a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and
instances with similar embeddings from knowledge bases to inform task planning
and execution. However, the reliance on unverified knowledge bases raises
significant concerns about their safety and trustworthiness. To uncover such
vulnerabilities, we propose a novel red teaming approach AgentPoison, the first
backdoor attack targeting generic and RAG-based LLM agents by poisoning their
long-term memory or RAG knowledge base. In particular, we form the trigger
generation process as a constrained optimization to optimize backdoor triggers
by mapping the triggered instances to a unique embedding space, so as to ensure
that whenever a user instruction contains the optimized backdoor trigger, the
malicious demonstrations are retrieved from the poisoned memory or knowledge
base with high probability. In the meantime, benign instructions without the
trigger will still maintain normal performance. Unlike conventional backdoor
attacks, AgentPoison requires no additional model training or fine-tuning, and
the optimized backdoor trigger exhibits superior transferability, in-context
coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's
effectiveness in attacking three types of real-world LLM agents: RAG-based
autonomous driving agent, knowledge-intensive QA agent, and healthcare
EHRAgent. On each agent, AgentPoison achieves an average attack success rate
higher than 80% with minimal impact on benign performance (less than 1%) with a
poison rate less than 0.1%.",Zhaorun Chen
2024-07-04T15:10:51Z,http://arxiv.org/abs/2407.12843v4,"NutriBench: A Dataset for Evaluating Large Language Models on Nutrition
  Estimation from Meal Descriptions","Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html",Andong Hua
2024-07-15T17:40:15Z,http://arxiv.org/abs/2407.12873v1,Evaluation of RAG Metrics for Question Answering in the Telecom Domain,"Retrieval Augmented Generation (RAG) is widely used to enable Large Language
Models (LLMs) perform Question Answering (QA) tasks in various domains.
However, RAG based on open-source LLM for specialized domains has challenges of
evaluating generated responses. A popular framework in the literature is the
RAG Assessment (RAGAS), a publicly available library which uses LLMs for
evaluation. One disadvantage of RAGAS is the lack of details of derivation of
numerical value of the evaluation metrics. One of the outcomes of this work is
a modified version of this package for few metrics (faithfulness, context
relevance, answer relevance, answer correctness, answer similarity and factual
correctness) through which we provide the intermediate outputs of the prompts
by using any LLMs. Next, we analyse the expert evaluations of the output of the
modified RAGAS package and observe the challenges of using it in the telecom
domain. We also study the effect of the metrics under correct vs. wrong
retrieval and observe that few of the metrics have higher values for correct
retrieval. We also study for differences in metrics between base embeddings and
those domain adapted via pre-training and fine-tuning. Finally, we comment on
the suitability and challenges of using these metrics for in-the-wild telecom
QA task.",Sujoy Roychowdhury
2024-07-18T13:43:01Z,http://arxiv.org/abs/2407.13511v1,"Can Open-Source LLMs Compete with Commercial Models? Exploring the
  Few-Shot Performance of Current GPT Models in Biomedical Tasks","Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT
and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)
benchmarks across different domains. New competing Open-Source alternatives
like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while
often offering higher throughput and being less costly to use. Open-Source LLMs
can also be self-hosted, which makes them interesting for enterprise and
clinical use cases where sensitive data should not be processed by third
parties. We participated in the 12th BioASQ challenge, which is a retrieval
augmented generation (RAG) setting, and explored the performance of current GPT
models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning
(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional
relevant knowledge from Wikipedia added to the context-window of the LLM might
improve their performance. Mixtral 8x7b was competitive in the 10-shot setting,
both with and without fine-tuning, but failed to produce usable results in the
zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to
measurable performance gains. Our results indicate that the performance gap
between commercial and open-source models in RAG setups exists mainly in the
zero-shot setting and can be closed by simply collecting few-shot examples for
domain-specific use cases. The code needed to rerun these experiments is
available through GitHub.",Samy Ateia
2024-07-18T17:59:30Z,http://arxiv.org/abs/2407.13766v2,Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark,"Large Multimodal Models (LMMs) have made significant strides in visual
question-answering for single images. Recent advancements like long-context
LMMs have allowed them to ingest larger, or even multiple, images. However, the
ability to process a large number of visual tokens does not guarantee effective
retrieval and reasoning for multi-image question answering (MIQA), especially
in real-world applications like photo album searches or satellite imagery
analysis. In this work, we first assess the limitations of current benchmarks
for long-context LMMs. We address these limitations by introducing a new
vision-centric, long-context benchmark, ""Visual Haystacks (VHs)"". We
comprehensively evaluate both open-source and proprietary models on VHs, and
demonstrate that these models struggle when reasoning across potentially
unrelated images, perform poorly on cross-image reasoning, as well as exhibit
biases based on the placement of key information within the context window.
Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented
Generation), an open-source, lightweight visual-RAG framework that processes up
to 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of
contemporary models. MIRAGE demonstrates up to 13% performance improvement over
existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA
multi-image QA benchmark, and achieves competitive performance on single-image
QA with state-of-the-art LMMs.",Tsung-Han Wu
2024-07-20T01:02:27Z,http://arxiv.org/abs/2407.14717v2,Differential Privacy of Cross-Attention with Provable Guarantee,"Cross-attention has become a fundamental module nowadays in many important
artificial intelligence applications, e.g., retrieval-augmented generation
(RAG), system prompt, guided stable diffusion, and many more. Ensuring
cross-attention privacy is crucial and urgently needed because its key and
value matrices may contain sensitive information about model providers and
their users. In this work, we design a novel differential privacy (DP) data
structure to address the privacy security of cross-attention with a theoretical
guarantee. In detail, let $n$ be the input token length of system prompt/RAG
data, $d$ be the feature dimension, $0 < \alpha \le 1$ be the relative error
parameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the
maximum value of the value matrix, and $r,s,\epsilon_s$ be parameters of
polynomial kernel methods. Then, our data structure requires
$\widetilde{O}(ndr^2)$ memory consumption with $\widetilde{O}(nr^2)$
initialization time complexity and $\widetilde{O}(\alpha^{-1} r^2)$ query time
complexity for a single token query. In addition, our data structure can
guarantee that the process of answering user query satisfies $(\epsilon,
\delta)$-DP with $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w
r^2)$ additive error and $n^{-1} (\alpha + \epsilon_s)$ relative error between
our output and the true answer. Furthermore, our result is robust to adaptive
queries in which users can intentionally attack the cross-attention system. To
our knowledge, this is the first work to provide DP for cross-attention and is
promising to inspire more privacy algorithm design in large generative models
(LGMs).",Yingyu Liang
2024-07-14T00:42:39Z,http://arxiv.org/abs/2407.15718v1,Integrating AI Tutors in a Programming Course,"RAGMan is an LLM-powered tutoring system that can support a variety of
course-specific and homework-specific AI tutors. RAGMan leverages Retrieval
Augmented Generation (RAG), as well as strict instructions, to ensure the
alignment of the AI tutors' responses. By using RAGMan's AI tutors, students
receive assistance with their specific homework assignments without directly
obtaining solutions, while also having the ability to ask general
programming-related questions.
  RAGMan was deployed as an optional resource in an introductory programming
course with an enrollment of 455 students. It was configured as a set of five
homework-specific AI tutors. This paper describes the interactions the students
had with the AI tutors, the students' feedback, and a comparative grade
analysis. Overall, about half of the students engaged with the AI tutors, and
the vast majority of the interactions were legitimate homework questions. When
students posed questions within the intended scope, the AI tutors delivered
accurate responses 98% of the time. Within the students used AI tutors, 78%
reported that the tutors helped their learning. Beyond AI tutors' ability to
provide valuable suggestions, students reported appreciating them for fostering
a safe learning environment free from judgment.",Iris Ma
2024-07-24T06:06:07Z,http://arxiv.org/abs/2407.17023v2,DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models,"Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.",Sara Vera Marjanović
2024-07-26T07:05:54Z,http://arxiv.org/abs/2407.18553v2,REAPER: Reasoning based Retrieval Planning for Complex RAG Systems,"Complex dialog systems often use retrieved evidence to facilitate factual
responses. Such RAG (Retrieval Augmented Generation) systems retrieve from
massive heterogeneous data stores that are usually architected as multiple
indexes or APIs instead of a single monolithic source. For a given query,
relevant evidence needs to be retrieved from one or a small subset of possible
retrieval sources. Complex queries can even require multi-step retrieval. For
example, a conversational agent on a retail site answering customer questions
about past orders will need to retrieve the appropriate customer order first
and then the evidence relevant to the customer's question in the context of the
ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by
interleaving reasoning and retrieval steps. However, each reasoning step
directly adds to the latency of the system. For large models this latency cost
is significant -- in the order of multiple seconds. Multi-agent systems may
classify the query to a single Agent associated with a retrieval source, though
this means that a (small) classification model dictates the performance of a
large language model. In this work we present REAPER (REAsoning-based PlannER)
- an LLM based planner to generate retrieval plans in conversational systems.
We show significant gains in latency over Agent-based systems and are able to
scale easily to new and unseen use cases as compared to classification-based
planning. Though our method can be applied to any RAG system, we show our
results in the context of a conversational shopping assistant.",Ashutosh Joshi
2024-07-28T12:47:20Z,http://arxiv.org/abs/2407.19487v1,RLCoder: Reinforcement Learning for Repository-Level Code Completion,"Repository-level code completion aims to generate code for unfinished code
snippets within the context of a specified repository. Existing approaches
mainly rely on retrieval-augmented generation strategies due to limitations in
input sequence length. However, traditional lexical-based retrieval methods
like BM25 struggle to capture code semantics, while model-based retrieval
methods face challenges due to the lack of labeled data for training.
Therefore, we propose RLCoder, a novel reinforcement learning framework, which
can enable the retriever to learn to retrieve useful content for code
completion without the need for labeled data. Specifically, we iteratively
evaluate the usefulness of retrieved content based on the perplexity of the
target code when provided with the retrieved content as additional context, and
provide feedback to update the retriever parameters. This iterative process
enables the retriever to learn from its successes and failures, gradually
improving its ability to retrieve relevant and high-quality content.
Considering that not all situations require information beyond code files and
not all retrieved context is helpful for generation, we also introduce a stop
signal mechanism, allowing the retriever to decide when to retrieve and which
candidates to retain autonomously. Extensive experimental results demonstrate
that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval
and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover,
experiments show that our framework can generalize across different programming
languages and further improve previous methods like RepoCoder. We provide the
code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.",Yanlin Wang
2024-07-30T09:04:45Z,http://arxiv.org/abs/2407.20668v1,"Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion
  Prediction for Social Media Influencers","Predicting influencers' views and public sentiment on social media is crucial
for anticipating societal trends and guiding strategic responses. This study
introduces a novel computational framework to predict opinion leaders'
perspectives and the emotive reactions of the populace, addressing the inherent
challenges posed by the unstructured, context-sensitive, and heterogeneous
nature of online communication. Our research introduces an innovative module
that starts with the automatic 5W1H (Where, Who, When, What, Why, and How)
questions formulation engine, tailored to emerging news stories and trending
topics. We then build a total of 60 anonymous opinion leader agents in six
domains and realize the views generation based on an enhanced large language
model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we
synthesize the potential views of opinion leaders and predicted the emotional
responses to different events. The efficacy of our automated 5W1H module is
corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity.
The influencer agents exhibit a consistent performance, achieving an average
GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the
'Russia-Ukraine War' as a case study, our methodology accurately foresees key
influencers' perspectives and aligns emotional predictions with real-world
sentiment trends in various domains.",Qinglan Wei
2024-07-31T04:01:08Z,http://arxiv.org/abs/2407.21320v2,MetaOpenFOAM: an LLM-based multi-agent framework for CFD,"Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and so on. MetaOpenFOAM harnesses the power of
MetaGPT's assembly line paradigm, which assigns diverse roles to various
agents, efficiently breaking down complex CFD tasks into manageable subtasks.
Langchain further complements MetaOpenFOAM by integrating Retrieval-Augmented
Generation (RAG) technology, which enhances the framework's ability by
integrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a
benchmark for natural language-based CFD solver, consisting of eight CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The eight CFD
simulation tasks encompass a range of multidimensional flow problems, covering
compressible and incompressible flows with different physical processes. This
demonstrates the capability to automate CFD simulations using only natural
language input, iteratively correcting errors to achieve the desired
simulations. An ablation study was conducted to verify the necessity of each
component in the multi-agent system and the RAG technology. A sensitivity study
on the randomness of LLM showed that LLM with low randomness can obtain more
stable and accurate results. Additionally, MetaOpenFOAM owns the ability to
identify and modify key parameters in user requirements, and excels in
correcting bugs when failure match occur,which demonstrates the generalization
of MetaOpenFOAM.",Yuxuan Chen
2024-07-28T14:55:22Z,http://arxiv.org/abs/2408.01462v1,"Faculty Perspectives on the Potential of RAG in Computer Science Higher
  Education","The emergence of Large Language Models (LLMs) has significantly impacted the
field of Natural Language Processing and has transformed conversational tasks
across various domains because of their widespread integration in applications
and public access. The discussion surrounding the application of LLMs in
education has raised ethical concerns, particularly concerning plagiarism and
policy compliance. Despite the prowess of LLMs in conversational tasks, the
limitations of reliability and hallucinations exacerbate the need to guardrail
conversations, motivating our investigation of RAG in computer science higher
education. We developed Retrieval Augmented Generation (RAG) applications for
the two tasks of virtual teaching assistants and teaching aids. In our study,
we collected the ratings and opinions of faculty members in undergraduate and
graduate computer science university courses at various levels, using our
personalized RAG systems for each course. This study is the first to gather
faculty feedback on the application of LLM-based RAG in education. The
investigation revealed that while faculty members acknowledge the potential of
RAG systems as virtual teaching assistants and teaching aids, certain barriers
and features are suggested for their full-scale deployment. These findings
contribute to the ongoing discussion on the integration of advanced language
models in educational settings, highlighting the need for careful consideration
of ethical implications and the development of appropriate safeguards to ensure
responsible and effective implementation.",Sagnik Dakshit
2024-08-06T03:44:06Z,http://arxiv.org/abs/2408.02937v2,"A Real-Time Adaptive Multi-Stream GPU System for Online Approximate
  Nearest Neighborhood Search","In recent years, Approximate Nearest Neighbor Search (ANNS) has played a
pivotal role in modern search and recommendation systems, especially in
emerging LLM applications like Retrieval-Augmented Generation. There is a
growing exploration into harnessing the parallel computing capabilities of GPUs
to meet the substantial demands of ANNS. However, existing systems primarily
focus on offline scenarios, overlooking the distinct requirements of online
applications that necessitate real-time insertion of new vectors. This
limitation renders such systems inefficient for real-world scenarios. Moreover,
previous architectures struggled to effectively support real-time insertion due
to their reliance on serial execution streams. In this paper, we introduce a
novel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our
architecture achieves its objectives through three key advancements: 1) We
initially examined the real-time insertion mechanisms in existing GPU ANNS
systems and discovered their reliance on repetitive copying and memory
allocation, which significantly hinders real-time effectiveness on GPUs. As a
solution, we introduce a dynamic vector insertion algorithm based on memory
blocks, which includes in-place rearrangement. 2) To enable real-time vector
insertion in parallel, we introduce a multi-stream parallel execution mode,
which differs from existing systems that operate serially within a single
stream. Our system utilizes a dynamic resource pool, allowing multiple streams
to execute concurrently without additional execution blocking. 3) Through
extensive experiments and comparisons, our approach effectively handles varying
QPS levels across different datasets, reducing latency by up to 40%-80%. The
proposed system has also been deployed in real-world industrial search and
recommendation systems, serving hundreds of millions of users daily, and has
achieved good results.",Yiping Sun
2024-08-06T09:02:53Z,http://arxiv.org/abs/2408.03047v2,"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready
  Multimodal Conversational Agents","Multimodal conversational agents are highly desirable because they offer
natural and human-like interaction. However, there is a lack of comprehensive
end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive
integration of audio, video, and text with response times of 200-250ms,
challenges remain in balancing latency, accuracy, cost, and data privacy. To
better understand and quantify these issues, we developed OpenOmni, an
open-source, end-to-end pipeline benchmarking tool that integrates advanced
technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented
Generation, Large Language Models, along with the ability to integrate
customized models. OpenOmni supports local and cloud deployment, ensuring data
privacy and supporting latency and accuracy benchmarking. This flexible
framework allows researchers to customize the pipeline, focusing on real
bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can
significantly enhance applications like indoor assistance for visually impaired
individuals, advancing human-computer interaction. Our demonstration video is
available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via
https://openomni.ai4wa.com, code is available via
https://github.com/AI4WA/OpenOmniFramework.",Qiang Sun
2024-08-06T16:55:54Z,http://arxiv.org/abs/2408.03297v2,"KnowPO: Knowledge-aware Preference Optimization for Controllable
  Knowledge Selection in Retrieval-Augmented Language Models","By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors such
as contextual ignorance and contextual overinclusion. To this end, we propose a
Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at
achieving adaptive knowledge selection based on contextual relevance in real
retrieval scenarios. Concretely, we proposed a general paradigm for
constructing knowledge conflict datasets, which comprehensively cover various
error types and learn how to avoid these negative signals through preference
optimization methods. Simultaneously, we proposed a rewriting strategy and data
ratio optimization strategy to address preference imbalances. Experimental
results show that KnowPO outperforms previous methods for handling knowledge
conflicts by over 37\%, while also exhibiting robust generalization across
various out-of-distribution datasets.",Ruizhe Zhang
2024-08-07T23:22:58Z,http://arxiv.org/abs/2408.04125v2,Exploring RAG-based Vulnerability Augmentation with LLMs,"Detecting vulnerabilities is vital for software security, yet deep
learning-based vulnerability detectors (DLVD) face a data shortage, which
limits their effectiveness. Data augmentation can potentially alleviate the
data shortage, but augmenting vulnerable code is challenging and requires a
generative solution that maintains vulnerability. Previous works have only
focused on generating samples that contain single statements or specific types
of vulnerabilities. Recently, large language models (LLMs) have been used to
solve various code generation and comprehension tasks with inspiring results,
especially when fused with retrieval augmented generation (RAG). Therefore, we
propose VulScribeR, a novel LLM-based solution that leverages carefully curated
prompt templates to augment vulnerable datasets. More specifically, we explore
three strategies to augment both single and multi-statement vulnerabilities,
with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation
across three vulnerability datasets and DLVD models, using two LLMs, show that
our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling
(ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable
samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated
vulnerable samples. Our approach demonstrates its feasibility for large-scale
data augmentation by generating 1K samples at as cheap as US$ 1.88.",Seyed Shayan Daneshvar
2024-08-02T19:49:19Z,http://arxiv.org/abs/2408.04645v1,"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors
  for a Robotics Course","This study evaluates the performance of Large Language Models (LLMs) as an
Artificial Intelligence-based tutor for a university course. In particular,
different advanced techniques are utilized, such as prompt engineering,
Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the
different models and applied techniques using common similarity metrics like
BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of
helpfulness and trustworthiness. Our findings indicate that RAG combined with
prompt engineering significantly enhances model responses and produces better
factual answers. In the context of education, RAG appears as an ideal technique
as it is based on enriching the input of the model with additional information
and material which usually is already present for a university course.
Fine-tuning, on the other hand, can produce quite small, still strong expert
models, but poses the danger of overfitting. Our study further asks how we
measure performance of LLMs and how well current measurements represent
correctness or relevance? We find high correlation on similarity metrics and a
bias of most of these metrics towards shorter responses. Overall, our research
points to both the potential and challenges of integrating LLMs in educational
settings, suggesting a need for balanced training approaches and advanced
evaluation frameworks.",Sebastian Kahl
2024-08-08T22:18:01Z,http://arxiv.org/abs/2408.04775v1,"Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction","Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.",Reza Khanmohammadi
2024-08-09T05:20:05Z,http://arxiv.org/abs/2408.04870v5,ConfusedPilot: Confused Deputy Risks in RAG-based LLMs,"Retrieval augmented generation (RAG) is a process where a large language
model (LLM) retrieves useful information from a database and then generates the
responses. It is becoming popular in enterprise settings for daily business
operations. For example, Copilot for Microsoft 365 has accumulated millions of
businesses. However, the security implications of adopting such RAG-based
systems are unclear.
  In this paper, we introduce ConfusedPilot, a class of security
vulnerabilities of RAG systems that confuse Copilot and cause integrity and
confidentiality violations in its responses. First, we investigate a
vulnerability that embeds malicious text in the modified prompt in RAG,
corrupting the responses generated by the LLM. Second, we demonstrate a
vulnerability that leaks secret data, which leverages the caching mechanism
during retrieval. Third, we investigate how both vulnerabilities can be
exploited to propagate misinformation within the enterprise and ultimately
impact its operations, such as sales and manufacturing. We also discuss the
root cause of these attacks by investigating the architecture of a RAG-based
system. This study highlights the security vulnerabilities in today's RAG-based
systems and proposes design guidelines to secure future RAG-based systems.",Ayush RoyChowdhury
2024-08-09T12:26:05Z,http://arxiv.org/abs/2408.05025v2,"Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations
  in LLM-based Application Frameworks","Retrieval Augmented Generation (RAG) is a technique commonly used to equip
models with out of distribution knowledge. This process involves collecting,
indexing, retrieving, and providing information to an LLM for generating
responses. Despite its growing popularity due to its flexibility and low cost,
the security implications of RAG have not been extensively studied. The data
for such systems are often collected from public sources, providing an attacker
a gateway for indirect prompt injections to manipulate the responses of the
model. In this paper, we investigate the security of RAG systems against
end-to-end indirect prompt manipulations. First, we review existing RAG
framework pipelines, deriving a prototypical architecture and identifying
critical parameters. We then examine prior works searching for techniques that
attackers can use to perform indirect prompt manipulations. Finally, we
implemented Rag 'n Roll, a framework to determine the effectiveness of attacks
against end-to-end RAG applications. Our results show that existing attacks are
mostly optimized to boost the ranking of malicious documents during the
retrieval phase. However, a higher rank does not immediately translate into a
reliable attack. Most attacks, against various configurations, settle around a
40% success rate, which could rise to 60% when considering ambiguous answers as
successful attacks (those that include the expected benign one as well).
Additionally, when using unoptimized documents, attackers deploying two of them
(or more) for a target query can achieve similar results as those using
optimized ones. Finally, exploration of the configuration space of a RAG showed
limited impact in thwarting the attacks, where the most successful combination
severely undermines functionality.",Gianluca De Stefano
2024-08-09T15:53:55Z,http://arxiv.org/abs/2408.05141v3,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",Ye Yuan
2024-08-12T03:52:11Z,http://arxiv.org/abs/2408.05911v1,"A New Pipeline For Generating Instruction Dataset via RAG and Self
  Fine-Tuning","With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.",Chih-Wei Song
2024-08-12T06:16:37Z,http://arxiv.org/abs/2408.05933v1,"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case
  Study with Locally Deployed Ollama Models","With the growing demand for offline PDF chatbots in automotive industrial
production environments, optimizing the deployment of large language models
(LLMs) in local, low-performance settings has become increasingly important.
This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques
for processing complex automotive industry documents using locally deployed
Ollama models. Based on the Langchain framework, we propose a multi-dimensional
optimization approach for Ollama's local RAG implementation. Our method
addresses key challenges in automotive document processing, including
multi-column layouts and technical specifications. We introduce improvements in
PDF processing, retrieval mechanisms, and context compression, tailored to the
unique characteristics of automotive industry documents. Additionally, we
design custom classes supporting embedding pipelines and an agent supporting
self-RAG based on LangGraph best practices. To evaluate our approach, we
constructed a proprietary dataset comprising typical automotive industry
documents, including technical reports and corporate regulations. We compared
our optimized RAG model and self-RAG agent against a naive RAG baseline across
three datasets: our automotive industry dataset, QReCC, and CoQA. Results
demonstrate significant improvements in context precision, context recall,
answer relevancy, and faithfulness, with particularly notable performance on
the automotive industry dataset. Our optimization scheme provides an effective
solution for deploying local RAG systems in the automotive sector, addressing
the specific needs of PDF chatbots in industrial production environments. This
research has important implications for advancing information processing and
intelligent production in the automotive industry.",Fei Liu
2024-08-12T16:33:51Z,http://arxiv.org/abs/2408.06272v1,"A RAG-Based Question-Answering Solution for Cyber-Attack Investigation
  and Attribution","In the constantly evolving field of cybersecurity, it is imperative for
analysts to stay abreast of the latest attack trends and pertinent information
that aids in the investigation and attribution of cyber-attacks. In this work,
we introduce the first question-answering (QA) model and its application that
provides information to the cybersecurity experts about cyber-attacks
investigations and attribution. Our QA model is based on Retrieval Augmented
Generation (RAG) techniques together with a Large Language Model (LLM) and
provides answers to the users' queries based on either our knowledge base (KB)
that contains curated information about cyber-attacks investigations and
attribution or on outside resources provided by the users. We have tested and
evaluated our QA model with various types of questions, including KB-based,
metadata-based, specific documents from the KB, and external sources-based
questions. We compared the answers for KB-based questions with those from
OpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms
OpenAI's GPT models by providing the source of the answers and overcoming the
hallucination limitations of the GPT models, which is critical for cyber-attack
investigation and attribution. Additionally, our analysis showed that when the
RAG QA model is given few-shot examples rather than zero-shot instructions, it
generates better answers compared to cases where no examples are supplied in
addition to the query.",Sampath Rajapaksha
2024-08-15T04:29:33Z,http://arxiv.org/abs/2408.08335v1,Plan with Code: Comparing approaches for robust NL to DSL generation,"Planning in code is considered a more reliable approach for many
orchestration tasks. This is because code is more tractable than steps
generated via Natural Language and make it easy to support more complex
sequences by abstracting deterministic logic into functions. It also allows
spotting issues with incorrect function names with the help of parsing checks
that can be run on code. Progress in Code Generation methodologies, however,
remains limited to general-purpose languages like C, C++, and Python. LLMs
continue to face challenges with custom function names in Domain Specific
Languages or DSLs, leading to higher hallucination rates and syntax errors.
This is more common for custom function names, that are typically part of the
plan. Moreover, keeping LLMs up-to-date with newer function names is an issue.
This poses a challenge for scenarios like task planning over a large number of
APIs, since the plan is represented as a DSL having custom API names. In this
paper, we focus on workflow automation in RPA (Robotic Process Automation)
domain as a special case of task planning. We present optimizations for using
Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with
an ablation study comparing these strategies with a fine-tuned model. Our
results showed that the fine-tuned model scored the best on code similarity
metric. However, with our optimizations, RAG approach is able to match the
quality for in-domain API names in the test set. Additionally, it offers
significant advantage for out-of-domain or unseen API names, outperforming
Fine-Tuned model on similarity metric by 7 pts.",Nastaran Bassamzadeh
2024-08-16T20:55:21Z,http://arxiv.org/abs/2408.09017v1,Meta Knowledge for Retrieval Augmented Large Language Models,"Retrieval Augmented Generation (RAG) is a technique used to augment Large
Language Models (LLMs) with contextually relevant, time-critical, or
domain-specific information without altering the underlying model parameters.
However, constructing RAG systems that can effectively synthesize information
from large and diverse set of documents remains a significant challenge. We
introduce a novel data-centric RAG workflow for LLMs, transforming the
traditional retrieve-then-read system into a more advanced
prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher
domain expert-level understanding of the knowledge base. Our methodology relies
on generating metadata and synthetic Questions and Answers (QA) for each
document, as well as introducing the new concept of Meta Knowledge Summary (MK
Summary) for metadata-based clusters of documents. The proposed innovations
enable personalized user-query augmentation and in-depth information retrieval
across the knowledge base. Our research makes two significant contributions:
using LLMs as evaluators and employing new comparative performance metrics, we
demonstrate that (1) using augmented queries with synthetic question matching
significantly outperforms traditional RAG pipelines that rely on document
chunking (p < 0.01), and (2) meta knowledge-augmented queries additionally
significantly improve retrieval precision and recall, as well as the final
answers breadth, depth, relevancy, and specificity. Our methodology is
cost-effective, costing less than $20 per 2000 research papers using Claude 3
Haiku, and can be adapted with any fine-tuning of either the language or
embedding models to further enhance the performance of end-to-end RAG
pipelines.",Laurent Mombaerts
2024-08-20T17:49:51Z,http://arxiv.org/abs/2408.11043v1,"Reconciling Methodological Paradigms: Employing Large Language Models as
  Novice Qualitative Research Assistants in Talent Management Research","Qualitative data collection and analysis approaches, such as those employing
interviews and focus groups, provide rich insights into customer attitudes,
sentiment, and behavior. However, manually analyzing qualitative data requires
extensive time and effort to identify relevant topics and thematic insights.
This study proposes a novel approach to address this challenge by leveraging
Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for
analyzing interview transcripts. The novelty of this work lies in strategizing
the research inquiry as one that is augmented by an LLM that serves as a novice
research assistant. This research explores the mental model of LLMs to serve as
novice qualitative research assistants for researchers in the talent management
space. A RAG-based LLM approach is extended to enable topic modeling of
semi-structured interview data, showcasing the versatility of these models
beyond their traditional use in information retrieval and search. Our findings
demonstrate that the LLM-augmented RAG approach can successfully extract topics
of interest, with significant coverage compared to manually generated topics
from the same dataset. This establishes the viability of employing LLMs as
novice qualitative research assistants. Additionally, the study recommends that
researchers leveraging such models lean heavily on quality criteria used in
traditional qualitative research to ensure rigor and trustworthiness of their
approach. Finally, the paper presents key recommendations for industry
practitioners seeking to reconcile the use of LLMs with established qualitative
research paradigms, providing a roadmap for the effective integration of these
powerful, albeit novice, AI tools in the analysis of qualitative datasets
within talent",Sreyoshi Bhaduri
2024-08-20T20:47:27Z,http://arxiv.org/abs/2408.11189v1,Reading with Intent,"Retrieval augmented generation (RAG) systems augment how knowledge language
models are by integrating external information sources such as Wikipedia,
internal documents, scientific papers, or the open internet. RAG systems that
rely on the open internet as their knowledge source have to contend with the
complexities of human-generated content. Human communication extends much
deeper than just the words rendered as text. Intent, tonality, and connotation
can all change the meaning of what is being conveyed. Recent real-world
deployments of RAG systems have shown some difficulty in understanding these
nuances of human communication. One significant challenge for these systems
lies in processing sarcasm. Though the Large Language Models (LLMs) that make
up the backbone of these RAG systems are able to detect sarcasm, they currently
do not always use these detections for the subsequent processing of text. To
address these issues, in this paper, we synthetically generate sarcastic
passages from Natural Question's Wikipedia retrieval corpus. We then test the
impact of these passages on the performance of both the retriever and reader
portion of the RAG pipeline. We introduce a prompting system designed to
enhance the model's ability to interpret and generate responses in the presence
of sarcasm, thus improving overall system performance. Finally, we conduct
ablation studies to validate the effectiveness of our approach, demonstrating
improvements in handling sarcastic content within RAG systems.",Benjamin Reichman
2024-08-21T12:09:37Z,http://arxiv.org/abs/2408.11557v4,"A Quick, trustworthy spectral knowledge Q&A system leveraging
  retrieval-augmented generation on LLM","Large Language Model (LLM) has demonstrated significant success in a range of
natural language processing (NLP) tasks within general domain. The emergence of
LLM has introduced innovative methodologies across diverse fields, including
the natural sciences. Researchers aim to implement automated, concurrent
process driven by LLM to supplant conventional manual, repetitive and
labor-intensive work. In the domain of spectral analysis and detection, it is
imperative for researchers to autonomously acquire pertinent knowledge across
various research objects, which encompasses the spectroscopic techniques and
the chemometric methods that are employed in experiments and analysis.
Paradoxically, despite the recognition of spectroscopic detection as an
effective analytical method, the fundamental process of knowledge retrieval
remains both time-intensive and repetitive. In response to this challenge, we
first introduced the Spectral Detection and Analysis Based Paper(SDAAP)
dataset, which is the first open-source textual knowledge dataset for spectral
analysis and detection and contains annotated literature data as well as
corresponding knowledge instruction data. Subsequently, we also designed an
automated Q\&A framework based on the SDAAP dataset, which can retrieve
relevant knowledge and generate high-quality responses by extracting entities
in the input as retrieval parameters. It is worth noting that: within this
framework, LLM is only used as a tool to provide generalizability, while RAG
technique is used to accurately capture the source of the knowledge.This
approach not only improves the quality of the generated responses, but also
ensures the traceability of the knowledge. Experimental results show that our
framework generates responses with more reliable expertise compared to the
baseline.",Jiheng Liang
2024-08-22T09:37:40Z,http://arxiv.org/abs/2408.12249v1,LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction,"Large Language Models (LLMs) are increasingly adopted for applications in
healthcare, reaching the performance of domain experts on tasks such as
question answering and document summarisation. Despite their success on these
tasks, it is unclear how well LLMs perform on tasks that are traditionally
pursued in the biomedical domain, such as structured information extration. To
breach this gap, in this paper, we systematically benchmark LLM performance in
Medical Classification and Named Entity Recognition (NER) tasks. We aim to
disentangle the contribution of different factors to the performance,
particularly the impact of LLMs' task knowledge and reasoning capabilities,
their (parametric) domain knowledge, and addition of external knowledge. To
this end we evaluate various open LLMs -- including BioMistral and Llama-2
models -- on a diverse set of biomedical datasets, using standard prompting,
Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as
Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
Counter-intuitively, our results reveal that standard prompting consistently
outperforms more complex techniques across both tasks, laying bare the
limitations in the current application of CoT, self-consistency and RAG in the
biomedical domain. Our findings suggest that advanced prompting methods
developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are
not easily portable to biomedical tasks where precise structured outputs are
required. This highlights the need for more effective integration of external
knowledge and reasoning mechanisms in LLMs to enhance their performance in
real-world biomedical applications.",Aishik Nagar
2024-08-27T00:50:14Z,http://arxiv.org/abs/2408.14717v1,Text2SQL is Not Enough: Unifying AI and Databases with TAG,"AI systems that serve natural language questions over databases promise to
unlock tremendous value. Such systems would allow users to leverage the
powerful reasoning and knowledge capabilities of language models (LMs)
alongside the scalable computational power of data management systems. These
combined capabilities would empower users to ask arbitrary natural language
questions over custom data sources. However, existing methods and benchmarks
insufficiently explore this setting. Text2SQL methods focus solely on natural
language questions that can be expressed in relational algebra, representing a
small subset of the questions real users wish to ask. Likewise,
Retrieval-Augmented Generation (RAG) considers the limited subset of queries
that can be answered with point lookups to one or a few data records within the
database. We propose Table-Augmented Generation (TAG), a unified and
general-purpose paradigm for answering natural language questions over
databases. The TAG model represents a wide range of interactions between the LM
and database that have been previously unexplored and creates exciting research
opportunities for leveraging the world knowledge and reasoning capabilities of
LMs over data. We systematically develop benchmarks to study the TAG problem
and find that standard methods answer no more than 20% of queries correctly,
confirming the need for further research in this area. We release code for the
benchmark at https://github.com/TAG-Research/TAG-Bench.",Asim Biswal
2024-08-28T06:28:01Z,http://arxiv.org/abs/2408.15562v1,"Boosting Lossless Speculative Decoding via Feature Sampling and Partial
  Alignment Distillation","Lossless speculative decoding accelerates target large language model (LLM)
inference by employing a lightweight draft model for generating tree-structured
candidates, which are subsequently verified in parallel by the target LLM.
Currently, effective approaches leverage feature-level rather than token-level
autoregression within the draft model to facilitate more straightforward
predictions and enhanced knowledge distillation. In this paper, we reassess
these approaches and propose FSPAD (Feature Sampling and Partial Alignment
Distillation for Lossless Speculative Decoding), which introduces two
straightforward and effective components within the existing framework to boost
lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to
sample features of the target LLM in high-dimensional space before feeding them
into the draft model, due to the inherent uncertainty of the features
preventing the draft model from obtaining the specific token output by the
target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken
the draft model's connection between features and logits, aiming to reduce the
conflict between feature alignment and logit confidence during training. Our
experiments include both greedy and non-greedy decoding on the largest and
smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in
multi-turn conversation, translation, summarization, question answering,
mathematical reasoning, and retrieval-augmented generation. The results show
that FSPAD outperforms the state-of-the-art method across all the
aforementioned tasks and target LLMs.",Lujun Gui
2024-08-24T19:34:04Z,http://arxiv.org/abs/2409.00082v1,"Towards Human-Level Understanding of Complex Process Engineering
  Schematics: A Pedagogical, Introspective Multi-Agent Framework for
  Open-Domain Question Answering","In the chemical and process industries, Process Flow Diagrams (PFDs) and
Piping and Instrumentation Diagrams (P&IDs) are critical for design,
construction, and maintenance. Recent advancements in Generative AI, such as
Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in
understanding and interpreting process diagrams for Visual Question Answering
(VQA). However, proprietary models pose data privacy risks, and their
computational complexity prevents knowledge editing for domain-specific
customization on consumer hardware. To overcome these challenges, we propose a
secure, on-premises enterprise solution using a hierarchical, multi-agent
Retrieval Augmented Generation (RAG) framework for open-domain question
answering (ODQA) tasks, offering enhanced data privacy, explainability, and
cost-effectiveness. Our novel multi-agent framework employs introspective and
specialized sub-agents using open-source, small-scale multimodal models with
the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis,
integrating multiple information sources to provide accurate and contextually
relevant answers. Our approach, supported by iterative self-correction, aims to
deliver superior performance in ODQA tasks. We conducted rigorous experimental
studies, and the empirical results validated the proposed approach
effectiveness.",Sagar Srinivas Sakhinana
2024-08-26T08:17:42Z,http://arxiv.org/abs/2409.00090v1,Evaluating ChatGPT on Nuclear Domain-Specific Data,"This paper examines the application of ChatGPT, a large language model (LLM),
for question-and-answer (Q&A) tasks in the highly specialized field of nuclear
data. The primary focus is on evaluating ChatGPT's performance on a curated
test dataset, comparing the outcomes of a standalone LLM with those generated
through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their
recent advancements, are prone to generating incorrect or 'hallucinated'
information, which is a significant limitation in applications requiring high
accuracy and reliability. This study explores the potential of utilizing RAG in
LLMs, a method that integrates external knowledge bases and sophisticated
retrieval techniques to enhance the accuracy and relevance of generated
outputs. In this context, the paper evaluates ChatGPT's ability to answer
domain-specific questions, employing two methodologies: A) direct response from
the LLM, and B) response from the LLM within a RAG framework. The effectiveness
of these methods is assessed through a dual mechanism of human and LLM
evaluation, scoring the responses for correctness and other metrics. The
findings underscore the improvement in performance when incorporating a RAG
pipeline in an LLM, particularly in generating more accurate and contextually
appropriate responses for nuclear domain-specific queries. Additionally, the
paper highlights alternative approaches to further refine and improve the
quality of answers in such specialized domains.",Muhammad Anwar
2024-09-01T07:01:22Z,http://arxiv.org/abs/2409.00636v1,"A Learnable Agent Collaboration Network Framework for Personalized
  Multimodal AI Search Engine","Large language models (LLMs) and retrieval-augmented generation (RAG)
techniques have revolutionized traditional information access, enabling AI
agent to search and summarize information on behalf of users during dynamic
dialogues. Despite their potential, current AI search engines exhibit
considerable room for improvement in several critical areas. These areas
include the support for multimodal information, the delivery of personalized
responses, the capability to logically answer complex questions, and the
facilitation of more flexible interactions. This paper proposes a novel AI
Search Engine framework called the Agent Collaboration Network (ACN). The ACN
framework consists of multiple specialized agents working collaboratively, each
with distinct roles such as Account Manager, Solution Strategist, Information
Manager, and Content Creator. This framework integrates mechanisms for picture
content understanding, user profile tracking, and online evolution, enhancing
the AI search engine's response quality, personalization, and interactivity. A
highlight of the ACN is the introduction of a Reflective Forward Optimization
method (RFO), which supports the online synergistic adjustment among agents.
This feature endows the ACN with online learning capabilities, ensuring that
the system has strong interactive flexibility and can promptly adapt to user
feedback. This learning method may also serve as an optimization approach for
agent-based systems, potentially influencing other domains of agent
applications.",Yunxiao Shi
2024-09-02T23:28:15Z,http://arxiv.org/abs/2409.01495v1,The Compressor-Retriever Architecture for Language Model OS,"Recent advancements in large language models (LLMs) have significantly
enhanced their capacity to aggregate and process information across multiple
modalities, enabling them to perform a wide range of tasks such as multimodal
data querying, tool usage, web interactions, and handling long documents. These
capabilities pave the way for transforming LLMs from mere chatbots into
general-purpose agents capable of interacting with the real world. This paper
explores the concept of using a language model as the core component of an
operating system (OS), effectively acting as a CPU that processes data stored
in a context window, which functions as RAM. A key challenge in realizing such
an LM OS is managing the life-long context and ensuring statefulness across
sessions, a feature limited by the current session-based interaction paradigm
due to context window size limit. To address this, we introduce
compressor-retriever, a model-agnostic architecture designed for life-long
context management. Unlike other long-context solutions such as
retrieval-augmented generation, our approach exclusively uses the base model's
forward function to compress and retrieve context, ensuring end-to-end
differentiability. Preliminary experiments demonstrate the effectiveness of
this architecture in in-context learning tasks, marking a step towards the
development of a fully stateful LLM OS. Project repo available at:
https://github.com/gblackout/LM-OS",Yuan Yang
2024-09-03T02:50:04Z,http://arxiv.org/abs/2409.01556v2,"Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka
  Culture","This study introduces a comprehensive benchmark designed to evaluate the
performance of large language models (LLMs) in understanding and processing
cultural knowledge, with a specific focus on Hakka culture as a case study.
Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework
that systematically assesses LLMs across six cognitive domains: Remembering,
Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark
extends beyond traditional single-dimensional evaluations by providing a deeper
analysis of LLMs' abilities to handle culturally specific content, ranging from
basic recall of facts to higher-order cognitive tasks such as creative
synthesis. Additionally, the study integrates Retrieval-Augmented Generation
(RAG) technology to address the challenges of minority cultural knowledge
representation in LLMs, demonstrating how RAG enhances the models' performance
by dynamically incorporating relevant external information. The results
highlight the effectiveness of RAG in improving accuracy across all cognitive
domains, particularly in tasks requiring precise retrieval and application of
cultural knowledge. However, the findings also reveal the limitations of RAG in
creative tasks, underscoring the need for further optimization. This benchmark
provides a robust tool for evaluating and comparing LLMs in culturally diverse
contexts, offering valuable insights for future research and development in
AI-driven cultural knowledge preservation and dissemination.",Chen-Chi Chang
2024-09-04T00:10:36Z,http://arxiv.org/abs/2409.02343v1,"NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
  Retrieval","$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of $k$-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE's non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.",Sepanta Zeighami
2024-09-04T09:46:33Z,http://arxiv.org/abs/2409.02572v3,"Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models","Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within
the field of Digital Forensics (DF). It focuses on examining and analyzing
time-based digital artefacts, such as timestamps derived from event logs, file
metadata, and other relevant data, to correlate events linked to cyber
incidents and reconstruct their chronological sequence. Traditional tools often
struggle to efficiently handle the large volume and variety of data generated
during DF investigations and Incident Response (IR) processes. This paper
introduces a novel framework, GenDFIR, which combines Rule-Based Artificial
Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance
and automate the TA process. The proposed approach consists of two key stages:
(1) R-BAI is used to identify and select anomalous digital artefacts based on
predefined rules. (2) The selected artefacts are then transformed into
embeddings for processing by an LLM with the assistance of a
Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to
perform automated TA on the artefacts and predict potential incident outcomes.
To validate the framework, we evaluated its performance, efficiency, and
reliability. Several metrics were applied to simulated cyber incident
scenarios, which were presented as forensic case documents. Our findings
demonstrate the significant potential of integrating R-BAI and LLMs for TA.
This innovative approach underscores the power of Generative AI (GenAI),
particularly LLMs, and opens up new possibilities for advanced threat detection
and incident reconstruction, marking a significant advancement in the field.",Fatma Yasmine Loumachi
2024-09-04T13:49:19Z,http://arxiv.org/abs/2409.02711v1,"Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for
  PostNL","The developments in the field of generative AI has brought a lot of
opportunities for companies, for instance to improve efficiency in customer
service and automating tasks. PostNL, the biggest parcel and E-commerce
corporation of the Netherlands wants to use generative AI to enhance the
communication around track and trace of parcels. During the internship a
Minimal Viable Product (MVP) is created to showcase the value of using
generative AI technologies, to enhance parcel tracking, analyzing the parcel's
journey and being able to communicate about it in an easy to understand manner.
The primary goal was to develop an in-house LLM-based system, reducing
dependency on external platforms and establishing the feasibility of a
dedicated generative AI team within the company. This multi-agent LLM based
system aimed to construct parcel journey stories and identify logistical
disruptions with heightened efficiency and accuracy. The research involved
deploying a sophisticated AI-driven communication system, employing
Retrieval-Augmented Generation (RAG) for enhanced response precision, and
optimizing large language models (LLMs) tailored to domain specific tasks.
  The MVP successfully implemented a multi-agent open-source LLM system, called
SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of
user inquiries and improving internal knowledge handling. Results and
evaluation demonstrated technological innovation and feasibility, notably in
communication about the track and trace of a parcel, which exceeded initial
expectations. These advancements highlight the potential of AI-driven solutions
in logistics, suggesting many opportunities for further refinement and broader
implementation within PostNL operational framework.",Mohammad Reshadati
2024-09-04T16:43:14Z,http://arxiv.org/abs/2409.02864v3,Language Model Powered Digital Biology with BRAD,"Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, engineering, and every day life. However, integrating the
wide array of computational tools, databases, and scientific literature
continues to pose a challenge to biological research. LLMs are well-suited for
unstructured integration, efficient information retrieval, and automating
standard workflows and actions from these diverse resources. To harness these
capabilities in bioinformatics, we present a prototype Bioinformatics Retrieval
Augmented Digital assistant (BRAD). BRAD is a chatbot and agentic system that
integrates a variety of bioinformatics tools. The Python package implements an
AI \texttt{Agent} that is powered by LLMs and connects to a local file system,
online databases, and a user's software. The \texttt{Agent} is highly
configurable, enabling tasks such as Retrieval-Augmented Generation, searches
across bioinformatics databases, and the execution of software pipelines.
BRAD's coordinated integration of bioinformatics tools delivers a context-aware
and semi-autonomous system that extends beyond the capabilities of conventional
LLM-based chatbots. A graphical user interface (GUI) provides an intuitive
interface to the system.",Joshua Pickard
2024-09-05T17:14:23Z,http://arxiv.org/abs/2409.03708v2,RAG based Question-Answering for Contextual Response Prediction System,"Large Language Models (LLMs) have shown versatility in various Natural
Language Processing (NLP) tasks, including their potential as effective
question-answering systems. However, to provide precise and relevant
information in response to specific customer queries in industry settings, LLMs
require access to a comprehensive knowledge base to avoid hallucinations.
Retrieval Augmented Generation (RAG) emerges as a promising technique to
address this challenge. Yet, developing an accurate question-answering
framework for real-world applications using RAG entails several challenges: 1)
data availability issues, 2) evaluating the quality of generated content, and
3) the costly nature of human evaluation. In this paper, we introduce an
end-to-end framework that employs LLMs with RAG capabilities for industry use
cases. Given a customer query, the proposed system retrieves relevant knowledge
documents and leverages them, along with previous chat history, to generate
response suggestions for customer service agents in the contact centers of a
major retail company. Through comprehensive automated and human evaluations, we
show that this solution outperforms the current BERT-based algorithms in
accuracy and relevance. Our findings suggest that RAG-based LLMs can be an
excellent support to human customer service representatives by lightening their
workload.",Sriram Veturi
2024-08-16T21:59:59Z,http://arxiv.org/abs/2409.03759v1,VERA: Validation and Evaluation of Retrieval-Augmented Systems,"The increasing use of Retrieval-Augmented Generation (RAG) systems in various
applications necessitates stringent protocols to ensure RAG systems accuracy,
safety, and alignment with user intentions. In this paper, we introduce VERA
(Validation and Evaluation of Retrieval-Augmented Systems), a framework
designed to enhance the transparency and reliability of outputs from large
language models (LLMs) that utilize retrieved information. VERA improves the
way we evaluate RAG systems in two important ways: (1) it introduces a
cross-encoder based mechanism that encompasses a set of multidimensional
metrics into a single comprehensive ranking score, addressing the challenge of
prioritizing individual metrics, and (2) it employs Bootstrap statistics on
LLM-based metrics across the document repository to establish confidence
bounds, ensuring the repositorys topical coverage and improving the overall
reliability of retrieval systems. Through several use cases, we demonstrate how
VERA can strengthen decision-making processes and trust in AI applications. Our
findings not only contribute to the theoretical understanding of LLM-based RAG
evaluation metric but also promote the practical implementation of responsible
AI systems, marking a significant advancement in the development of reliable
and transparent generative AI technologies.",Tianyu Ding
2024-09-05T13:45:42Z,http://arxiv.org/abs/2409.04475v2,"Revolutionizing Database Q&A with Large Language Models: Comprehensive
  Benchmark and Evaluation","The development of Large Language Models (LLMs) has revolutionized QA across
various industries, including the database domain. However, there is still a
lack of a comprehensive benchmark to evaluate the capabilities of different
LLMs and their modular components in database QA. To this end, we introduce
DQABench, the first comprehensive database QA benchmark for LLMs. DQABench
features an innovative LLM-based method to automate the generation, cleaning,
and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in
English and Chinese, separately. These QA pairs cover a wide range of
database-related knowledge extracted from manuals, online communities, and
database instances. This inclusion allows for an additional assessment of LLMs'
Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)
capabilities in the database QA task. Furthermore, we propose a comprehensive
LLM-based database QA testbed DQATestbed. This testbed is highly modular and
scalable, with basic and advanced components such as Question Classification
Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,
DQABench provides a comprehensive evaluation pipeline that computes various
metrics throughout a standardized evaluation process to ensure the accuracy and
fairness of the evaluation. We use DQABench to evaluate the database QA
capabilities under the proposed testbed comprehensively. The evaluation reveals
findings like (i) the strengths and limitations of nine LLM-based QA bots and
(ii) the performance impact and potential improvements of various service
components (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the
future development of LLM-based database QA research.",Yihang Zheng
2024-09-09T20:52:25Z,http://arxiv.org/abs/2409.06062v1,Retrieval Augmented Correction of Named Entity Speech Recognition Errors,"In recent years, end-to-end automatic speech recognition (ASR) systems have
proven themselves remarkably accurate and performant, but these systems still
have a significant error rate for entity names which appear infrequently in
their training data. In parallel to the rise of end-to-end ASR systems, large
language models (LLMs) have proven to be a versatile tool for various natural
language processing (NLP) tasks. In NLP tasks where a database of relevant
knowledge is available, retrieval augmented generation (RAG) has achieved
impressive results when used with LLMs. In this work, we propose a RAG-like
technique for correcting speech recognition entity name errors. Our approach
uses a vector database to index a set of relevant entities. At runtime,
database queries are generated from possibly errorful textual ASR hypotheses,
and the entities retrieved using these queries are fed, along with the ASR
hypotheses, to an LLM which has been adapted to correct ASR errors. Overall,
our best system achieves 33%-39% relative word error rate reductions on
synthetic test sets focused on voice assistant queries of rare music entities
without regressing on the STOP test set, a publicly available voice assistant
test set covering many domains.",Ernest Pusateri
2024-09-10T12:12:09Z,http://arxiv.org/abs/2409.06450v1,"Multimodal Large Language Model Driven Scenario Testing for Autonomous
  Vehicles","The generation of corner cases has become increasingly crucial for
efficiently testing autonomous vehicles prior to road deployment. However,
existing methods struggle to accommodate diverse testing requirements and often
lack the ability to generalize to unseen situations, thereby reducing the
convenience and usability of the generated scenarios. A method that facilitates
easily controllable scenario generation for efficient autonomous vehicles (AV)
testing with realistic and challenging situations is greatly needed. To address
this, we proposed OmniTester: a multimodal Large Language Model (LLM) based
framework that fully leverages the extensive world knowledge and reasoning
capabilities of LLMs. OmniTester is designed to generate realistic and diverse
scenarios within a simulation environment, offering a robust solution for
testing and evaluating AVs. In addition to prompt engineering, we employ tools
from Simulation of Urban Mobility to simplify the complexity of codes generated
by LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a
self-improvement mechanism to enhance the LLM's understanding of scenarios,
thereby increasing its ability to produce more realistic scenes. In the
experiments, we demonstrated the controllability and realism of our approaches
in generating three types of challenging and complex scenarios. Additionally,
we showcased its effectiveness in reconstructing new scenarios described in
crash report, driven by the generalization capability of LLMs.",Qiujing Lu
2024-09-11T17:21:59Z,http://arxiv.org/abs/2409.07431v2,Synthetic continued pretraining,"Pretraining on large-scale, unstructured internet text enables language
models to acquire a significant amount of world knowledge. However, this
knowledge acquisition is data-inefficient--to learn a given fact, models must
be trained on hundreds to thousands of diverse representations of it. This
poses a challenge when adapting a pretrained model to a small corpus of
domain-specific documents, where each fact may appear rarely or only once. We
propose to bridge this gap with synthetic continued pretraining: using the
small domain-specific corpus to synthesize a large corpus more amenable to
learning, and then performing continued pretraining on the synthesized corpus.
We instantiate this proposal with EntiGraph, a synthetic data augmentation
algorithm that extracts salient entities from the source documents and then
generates diverse text by drawing connections between the sampled entities.
Synthetic continued pretraining with EntiGraph enables a language model to
answer questions and follow generic instructions related to the source
documents without access to them. If, instead, the source documents are
available at inference time, we show that the knowledge acquired through our
approach compounds with retrieval-augmented generation. To better understand
these results, we build a simple mathematical model of EntiGraph, and show how
synthetic data augmentation can ""rearrange"" knowledge to enable more
data-efficient learning.",Zitong Yang
2024-09-08T00:23:37Z,http://arxiv.org/abs/2409.07489v2,RAGent: Retrieval-based Access Control Policy Generation,"Manually generating access control policies from an organization's high-level
requirement specifications poses significant challenges. It requires laborious
efforts to sift through multiple documents containing such specifications and
translate their access requirements into access control policies. Also, the
complexities and ambiguities of these specifications often result in errors by
system administrators during the translation process, leading to data breaches.
However, the automated policy generation frameworks designed to help
administrators in this process are unreliable due to limitations, such as the
lack of domain adaptation. Therefore, to improve the reliability of access
control policy generation, we propose RAGent, a novel retrieval-based access
control policy generation framework based on language models. RAGent identifies
access requirements from high-level requirement specifications with an average
state-of-the-art F1 score of 87.9%. Through retrieval augmented generation,
RAGent then translates the identified access requirements into access control
policies with an F1 score of 77.9%. Unlike existing frameworks, RAGent
generates policies with complex components like purposes and conditions, in
addition to subjects, actions, and resources. Moreover, RAGent automatically
verifies the generated policies and iteratively refines them through a novel
verification-refinement mechanism, further improving the reliability of the
process by 3%, reaching the F1 score of 80.6%. We also introduce three
annotated datasets for developing access control policy generation frameworks
in the future, addressing the data scarcity of the domain.",Sakuna Harinda Jayasundara
2024-09-12T01:51:06Z,http://arxiv.org/abs/2409.07691v1,"Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,
  fine-tuning and deploying Rerankers for RAG","Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.",Gabriel de Souza P. Moreira
2024-09-12T08:25:33Z,http://arxiv.org/abs/2409.07829v1,"Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:
  A Case Study in WeChat","UI automation tests play a crucial role in ensuring the quality of mobile
applications. Despite the growing popularity of machine learning techniques to
generate these tests, they still face several challenges, such as the mismatch
of UI elements. The recent advances in Large Language Models (LLMs) have
addressed these issues by leveraging their semantic understanding capabilities.
However, a significant gap remains in applying these models to industrial-level
app testing, particularly in terms of cost optimization and knowledge
limitation. To address this, we introduce CAT to create cost-effective UI
automation tests for industry apps by combining machine learning and LLMs with
best practices. Given the task description, CAT employs Retrieval Augmented
Generation (RAG) to source examples of industrial app usage as the few-shot
learning context, assisting LLMs in generating the specific sequence of
actions. CAT then employs machine learning techniques, with LLMs serving as a
complementary optimizer, to map the target element on the UI screen. Our
evaluations on the WeChat testing dataset demonstrate the CAT's performance and
cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming
the state-of-the-art. We have also integrated our approach into the real-world
WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and
enhancing the developers' testing process.",Sidong Feng
2024-08-30T13:31:32Z,http://arxiv.org/abs/2409.09052v1,"OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in
  Computed Tomography","Multimodal large language models (MLLMs) have achieved significant success in
the general field of image processing. Their emerging task generalization and
freeform conversational capabilities can greatly facilitate medical diagnostic
assistance, helping patients better understand their conditions and enhancing
doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging
technique used to capture the internal mechanisms of a patient's condition and
is widely utilized. However, in past research, the complex textural features of
this imaging data have made accurate interpretation by algorithms challenging,
impeding the performance of general LLMs in diagnostic assistance. To address
this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is
trained on 120,000 CT images and diagnostic reports and includes a
Retrieval-Augmented Generation (RAG) module capable of effectively mitigating
model hallucinations. This module is informed by extensive medical literature,
textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT
images but also stores, understands, and reasons over medical knowledge and
language. In extensive experiments, OrthoDoc outperforms commercial models led
by GPT-4, demonstrating superior diagnostic capabilities and accuracy.
Specifically, OrthoDoc significantly surpasses existing models in the diagnosis
of common orthopedic conditions such as fractures, arthritis, and tumors.
Additionally, OrthoDoc exhibits robust generalization and stability when
handling rare and complex cases.",Youzhu Jin
2024-09-14T07:19:18Z,http://arxiv.org/abs/2409.09343v1,"Generative AI in Data Center Networking: Fundamentals, Perspectives, and
  Case Study","Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as
OpenAI's ChatGPT, is revolutionizing various fields. Central to this
transformation is Data Center Networking (DCN), which not only provides the
computational power necessary for GenAI training and inference but also
delivers GenAI-driven services to users. This article examines an interplay
between GenAI and DCNs, highlighting their symbiotic relationship and mutual
advancements. We begin by reviewing current challenges within DCNs and discuss
how GenAI contributes to enhancing DCN capabilities through innovations, such
as data augmentation, process automation, and domain transfer. We then focus on
analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining
insights that catalyze the evolution of DCNs to more effectively support GenAI
and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs,
we present a case study on full-lifecycle DCN digital twins. In this study, we
employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate
optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning
(DRL) for optimizing the RAG knowledge placement strategy. This approach not
only demonstrates the application of advanced GenAI methods within DCNs but
also positions the digital twin as a pivotal GenAI service operating on DCNs.
We anticipate that this article can promote further research into enhancing the
virtuous interaction between GenAI and DCNs.",Yinqiu Liu
2024-09-16T01:08:18Z,http://arxiv.org/abs/2409.09916v1,SFR-RAG: Towards Contextually Faithful LLMs,"Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.",Xuan-Phi Nguyen
2024-09-17T16:55:25Z,http://arxiv.org/abs/2409.11353v3,"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models","Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.",Mengfei Liang
2024-09-19T08:26:45Z,http://arxiv.org/abs/2409.12558v1,"RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval
  Augmented Dialogues","In real-world applications with Large Language Models (LLMs), external
retrieval mechanisms - such as Search-Augmented Generation (SAG), tool
utilization, and Retrieval-Augmented Generation (RAG) - are often employed to
enhance the quality of augmented generations in dialogues. These approaches
often come with multi-turn dialogue, where each interaction is enriched by
relevant information retrieved from external sources. Existing benchmarks
either assess LLMs' chat abilities in multi-turn dialogues or their use of
retrieval for augmented responses in single-turn settings. However, there is a
gap in evaluating LLMs' ability to leverage retrieval for more precise
responses across multiple turns. To address this limitation, we introduce
RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate
LLMs' capabilities in multi-turn dialogues following retrievals, essential for
their deployment in context-rich applications. RAD-Bench evaluates two key
abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are
measured using discriminative questions and retrieved contexts, and
corresponding reference answers, assessing how effectively LLMs integrate and
reason with context to maintain and enhance conversation quality over multiple
turns. Our evaluation results on commonly used LLMs reveal that model
performance deteriorates as additional layers of conditions or constraints are
applied across conversation turns, even when accurate retrieved contexts are
provided.",Tzu-Lin Kuo
2024-09-19T11:48:29Z,http://arxiv.org/abs/2409.12682v1,Retrieval-Augmented Test Generation: How Far Are We?,"Retrieval Augmented Generation (RAG) has shown notable advancements in
software engineering tasks. Despite its potential, RAG's application in unit
test generation remains under-explored. To bridge this gap, we take the
initiative to investigate the efficacy of RAG-based LLMs in test generation. As
RAGs can leverage various knowledge sources to enhance their performance, we
also explore the impact of different sources of RAGs' knowledge bases on unit
test generation to provide insights into their practical benefits and
limitations. Specifically, we examine RAG built upon three types of domain
knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.
Each source offers essential knowledge for creating tests from different
perspectives, i.e., API documentations provide official API usage guidelines,
GitHub issues offer resolutions of issues related to the APIs from the library
developers, and StackOverflow Q&As present community-driven solutions and best
practices. For our experiment, we focus on five widely used and typical
Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,
Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex
neural networks efficiently. We conducted experiments using the top 10% most
widely used APIs across these projects, involving a total of 188 APIs. We
investigate the effectiveness of four state-of-the-art LLMs (open and
closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1
405B. Additionally, we compare three prompting strategies in generating unit
test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an
API-level RAG on the three external sources. Finally, we compare the cost of
different sources of knowledge used for the RAG.",Jiho Shin
2024-09-19T14:36:00Z,http://arxiv.org/abs/2409.12812v2,"Towards Interactive and Learnable Cooperative Driving Automation: a
  Large Language Model-Driven Decision-Making Framework","At present, Connected Autonomous Vehicles (CAVs) have begun to open road
testing around the world, but their safety and efficiency performance in
complex scenarios is still not satisfactory. Cooperative driving leverages the
connectivity ability of CAVs to achieve synergies greater than the sum of their
parts, making it a promising approach to improving CAV performance in complex
scenarios. However, the lack of interaction and continuous learning ability
limits current cooperative driving to single-scenario applications and specific
Cooperative Driving Automation (CDA). To address these challenges, this paper
proposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative
driving framework, to achieve all-scenario and all-CDA. First, since Large
Language Models(LLMs) are not adept at handling mathematical calculations, an
environment module is introduced to update vehicle positions based on semantic
decisions, thus avoiding potential errors from direct LLM control of vehicle
positions. Second, based on the four levels of CDA defined by the SAE J3216
standard, we propose a Chain-of-Thought (COT) based reasoning module that
includes state perception, intent sharing, negotiation, and decision-making,
enhancing the stability of LLMs in multi-step reasoning tasks. Centralized
conflict resolution is then managed through a conflict coordinator in the
reasoning process. Finally, by introducing a memory module and employing
retrieval-augmented generation, CAVs are endowed with the ability to learn from
their past experiences. We validate the proposed CoDrivingLLM through ablation
experiments on the negotiation module, reasoning with different shots
experience, and comparison with other cooperative driving methods.",Shiyu Fang
2024-09-03T15:30:57Z,http://arxiv.org/abs/2409.13695v1,You Only Use Reactive Attention Slice For Long Context Retrieval,"Supporting longer context for Large Language Models (LLM) is a promising
direction to advance LLMs. As training a model for a longer context window is
computationally expensive, many alternative solutions, such as Retrieval
Augmented Generation (RAG), have been used. However, most existing RAG methods
adopt embedding-based retrieval that falls short on long contexts.
  To address such challenges, we propose an attention-based retrieval
technique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a
novel retrieval heuristic called reaction score to rank the relevance of each
sentence in the input context with the query sentence. Intuitively, we measure
how the per-token attention score ""reacts"" to the query and greedily retrieves
the most reactive sentences. Internally, YOURA generates a token-indexed vector
(called reaction vector) for the whole input context. To map each sentence to
the token-indexed vector, we propose an Embedding-Agnostic Sentence Yield
(EASY), a best-effort token wiggling algorithm.
  We evaluate our retrieval technique on three open-source pre-trained LLM
models across six LongBench QA datasets. Our technique achieves up to 30% vLLM
inference throughput improvement for serving long-context queries with a nearly
identical quality score to the simple yet effective truncate-middle approach.",Yun Joon Soh
2024-09-05T02:34:05Z,http://arxiv.org/abs/2409.13699v1,Vietnamese Legal Information Retrieval in Question-Answering System,"In the modern era of rapidly increasing data volumes, accurately retrieving
and recommending relevant documents has become crucial in enhancing the
reliability of Question Answering (QA) systems. Recently, Retrieval Augmented
Generation (RAG) has gained significant recognition for enhancing the
capabilities of large language models (LLMs) by mitigating hallucination issues
in QA systems, which is particularly beneficial in the legal domain. Various
methods, such as semantic search using dense vector embeddings or a combination
of multiple techniques to improve results before feeding them to LLMs, have
been proposed. However, these methods often fall short when applied to the
Vietnamese language due to several challenges, namely inefficient Vietnamese
data processing leading to excessive token length or overly simplistic ensemble
techniques that lead to instability and limited improvement. Moreover, a
critical issue often overlooked is the ordering of final relevant documents
which are used as reference to ensure the accuracy of the answers provided by
LLMs. In this report, we introduce our three main modifications taken to
address these challenges. First, we explore various practical approaches to
data processing to overcome the limitations of the embedding model.
Additionally, we enhance Reciprocal Rank Fusion by normalizing order to combine
results from keyword and vector searches effectively. We also meticulously
re-rank the source pieces of information used by LLMs with Active Retrieval to
improve user experience when refining the information generated. In our
opinion, this technique can also be considered as a new re-ranking method that
might be used in place of the traditional cross encoder. Finally, we integrate
these techniques into a comprehensive QA system, significantly improving its
performance and reliability",Thiem Nguyen Ba
2024-09-22T16:20:00Z,http://arxiv.org/abs/2409.14516v1,"Beyond Words: Evaluating Large Language Models in Transportation
  Planning","The resurgence and rapid advancement of Generative Artificial Intelligence
(GenAI) in 2023 has catalyzed transformative shifts across numerous industry
sectors, including urban transportation and logistics. This study investigates
the evaluation of Large Language Models (LLMs), specifically GPT-4 and
Phi-3-mini, to enhance transportation planning. The study assesses the
performance and spatial comprehension of these models through a
transportation-informed evaluation framework that includes general geospatial
skills, general transportation domain skills, and real-world transportation
problem-solving. Utilizing a mixed-methods approach, the research encompasses
an evaluation of the LLMs' general Geographic Information System (GIS) skills,
general transportation domain knowledge as well as abilities to support human
decision-making in the real-world transportation planning scenarios of
congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy
and reliability across various GIS and transportation-specific tasks compared
to Phi-3-mini, highlighting its potential as a robust tool for transportation
planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical
scenarios, suggesting its utility in resource-constrained environments. The
findings underscore the transformative potential of GenAI technologies in urban
transportation planning. Future work could explore the application of newer
LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a
broader set of real-world transportation planning and operations challenges, to
deepen the integration of advanced AI models in transportation management
practices.",Shaowei Ying
2024-09-23T16:16:08Z,http://arxiv.org/abs/2409.15163v1,"Lessons Learned on Information Retrieval in Electronic Health Records: A
  Comparison of Embedding Models and Pooling Strategies","Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.",Skatje Myers
2024-09-14T02:34:26Z,http://arxiv.org/abs/2409.15355v4,Block-Attention for Efficient RAG,"We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.",East Sun
2024-09-18T16:10:47Z,http://arxiv.org/abs/2409.15364v1,VERA: Validation and Enhancement for Retrieval Augmented systems,"Large language models (LLMs) exhibit remarkable capabilities but often
produce inaccurate responses, as they rely solely on their embedded knowledge.
Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external
information retrieval system, supplying additional context along with the query
to mitigate inaccuracies for a particular context. However, accuracy issues
still remain, as the model may rely on irrelevant documents or extrapolate
incorrectly from its training knowledge. To assess and improve the performance
of both the retrieval system and the LLM in a RAG framework, we propose
\textbf{VERA} (\textbf{V}alidation and \textbf{E}nhancement for
\textbf{R}etrieval \textbf{A}ugmented systems), a system designed to: 1)
Evaluate and enhance the retrieved context before response generation, and 2)
Evaluate and refine the LLM-generated response to ensure precision and minimize
errors. VERA employs an evaluator-cum-enhancer LLM that first checks if
external retrieval is necessary, evaluates the relevance and redundancy of the
retrieved context, and refines it to eliminate non-essential information.
Post-response generation, VERA splits the response into atomic statements,
assesses their relevance to the query, and ensures adherence to the context.
Our experiments demonstrate VERA's remarkable efficacy not only in improving
the performance of smaller open-source models, but also larger state-of-the art
models. These enhancements underscore VERA's potential to produce accurate and
relevant responses, advancing the state-of-the-art in retrieval-augmented
language modeling. VERA's robust methodology, combining multiple evaluation and
refinement steps, effectively mitigates hallucinations and improves retrieval
and response processes, making it a valuable tool for applications demanding
high accuracy and reliability in information generation. .",Nitin Aravind Birur
2024-09-26T21:44:11Z,http://arxiv.org/abs/2409.18313v4,"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and
  Generation","There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhouse of large-scale
non-parametric knowledge, however existing techniques do not directly transfer
to the embodied domain, which is multimodal, data is highly correlated, and
perception requires abstraction.
  To address these challenges, we introduce Embodied-RAG, a framework that
enhances the foundational model of an embodied agent with a non-parametric
memory system capable of autonomously constructing hierarchical knowledge for
both navigation and language generation. Embodied-RAG handles a full range of
spatial and semantic resolutions across diverse environments and query types,
whether for a specific object or a holistic description of ambiance. At its
core, Embodied-RAG's memory is structured as a semantic forest, storing
language descriptions at varying levels of detail. This hierarchical
organization allows the system to efficiently generate context-sensitive
outputs across different robotic platforms. We demonstrate that Embodied-RAG
effectively bridges RAG to the robotics domain, successfully handling over 200
explanation and navigation queries across 19 environments, highlighting its
promise for general-purpose non-parametric system for embodied agents.",Quanting Xie
2024-09-21T13:44:34Z,http://arxiv.org/abs/2409.19006v2,"Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent
  Framework for Intellectual Property Management and Analysis","Patents are the currency of innovation, and like any currency, they need to
be managed and protected (Gavin Potenza). Patents, as legal documents that
secure intellectual property rights, play a critical role in technological
innovation. The growing complexity of patent documents and the surge in patent
applications have created a need for automated solutions in patent analysis. In
this work, we present PatExpert, an autonomous multi-agent conversational
framework designed to streamline and optimize patent-related tasks. The
framework consists of a metaagent that coordinates task-specific expert agents
for various patent-related tasks and a critique agent for error handling and
feedback provision. The meta-agent orchestrates specialized expert agents, each
fine-tuned for specific tasks such as patent classification, acceptance, claim
generation, abstractive summarization, multi-patent analysis, and scientific
hypothesis generation. For multi-patent analysis, the framework incorporates
advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance
response accuracy and relevance by combining semantic similarity with knowledge
graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and
Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and
provide iterative feedback. The framework also prioritizes explainability,
ensuring transparent justifications for decisions made during patent analysis.
Its comprehensive capabilities make it a valuable tool for automating complex
patent workflows, enhancing efficiency, accuracy, and compliance in
patent-related tasks. Empirical evidence demonstrates significant improvements
in patent processing tasks, concluding that the framework offers a robust
solution for automating and optimizing patent analysis.",Sakhinana Sagar Srinivas
2024-09-24T23:33:07Z,http://arxiv.org/abs/2409.19019v1,RAGProbe: An Automated Approach for Evaluating RAG Applications,"Retrieval Augmented Generation (RAG) is increasingly being used when building
Generative AI applications. Evaluating these applications and RAG pipelines is
mostly done manually, via a trial and error process. Automating evaluation of
RAG pipelines requires overcoming challenges such as context misunderstanding,
wrong format, incorrect specificity, and missing content. Prior works therefore
focused on improving evaluation metrics as well as enhancing components within
the pipeline using available question and answer datasets. However, they have
not focused on 1) providing a schema for capturing different types of
question-answer pairs or 2) creating a set of templates for generating
question-answer pairs that can support automation of RAG pipeline evaluation.
In this paper, we present a technique for generating variations in
question-answer pairs to trigger failures in RAG pipelines. We validate 5
open-source RAG pipelines using 3 datasets. Our approach revealed the highest
failure rates when prompts combine multiple questions: 91% for questions when
spanning multiple documents and 78% for questions from a single document;
indicating a need for developers to prioritise handling these combined
questions. 60% failure rate was observed in academic domain dataset and 53% and
62% failure rates were observed in open-domain datasets. Our automated approach
outperforms the existing state-of-the-art methods, by increasing the failure
rate by 51% on average per dataset. Our work presents an automated approach for
continuously monitoring the health of RAG pipelines, which can be integrated
into existing CI/CD pipelines, allowing for improved quality.",Shangeetha Sivasothy
2024-09-30T10:48:20Z,http://arxiv.org/abs/2409.20181v2,"Reference Trustable Decoding: A Training-Free Augmentation Paradigm for
  Large Language Models","Large language models (LLMs) have rapidly advanced and demonstrated
impressive capabilities. In-Context Learning (ICL) and Parameter-Efficient
Fine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to
downstream tasks. ICL typically constructs a few-shot learning scenario, either
manually or by setting up a Retrieval-Augmented Generation (RAG) system,
helping models quickly grasp domain knowledge or question-answering patterns
without changing model parameters. However, this approach involves trade-offs,
such as slower inference speed and increased space occupancy. PEFT assists the
model in adapting to tasks through minimal parameter modifications, but the
training process still demands high hardware requirements, even with a small
number of parameters involved. To address these challenges, we propose
Reference Trustable Decoding (RTD), a paradigm that allows models to quickly
adapt to new tasks without fine-tuning, maintaining low inference costs. RTD
constructs a reference datastore from the provided training examples and
optimizes the LLM's final vocabulary distribution by flexibly selecting
suitable references based on the input, resulting in more trustable responses
and enabling the model to adapt to downstream tasks at a low cost. Experimental
evaluations on various LLMs using different benchmarks demonstrate that RTD
establishes a new paradigm for augmenting models to downstream tasks.
Furthermore, our method exhibits strong orthogonality with traditional methods,
allowing for concurrent usage. Our code can be found at
https://github.com/ShiLuohe/ReferenceTrustableDecoding",Luohe Shi
2024-10-02T04:29:08Z,http://arxiv.org/abs/2410.01231v1,"Revisiting the Index Construction of Proximity Graph-Based Approximate
  Nearest Neighbor Search","Proximity graphs (PG) have gained increasing popularity as the
state-of-the-art (SOTA) solutions to $k$-approximate nearest neighbor ($k$-ANN)
search on high-dimensional data, which serves as a fundamental function in
various fields, e.g. information retrieval and retrieval-augmented
generation~(RAG). Although PG-based approaches have the best $k$-ANN search
performance, their index construction cost is superlinear to the number of
points, since they have to identify close neighbors for each point to establish
the edges. Such superlinear cost substantially limits their scalability in the
era of big data. Hence, the goal of this paper is to accelerate the
construction of PG-based methods without compromising their $k$-ANN search
performance.
  To achieve this goal, two mainstream categories of PG are revisited: relative
neighborhood graph (RNG) and navigable small world graph (NSWG). By revisiting
their construction process, we find the issues of construction efficiency. To
address these issues, we propose a new construction framework with a novel
pruning strategy for edge selection, which accelerates RNG construction while
keeping its $k$-ANN search performance. Then, we integrate this framework into
NSWG construction to enhance both the construction efficiency and $k$-ANN
search performance. Moreover, extensive experiments are conducted to validate
our construction framework for both RNG and NSWG. The results demonstrate that
it significantly reduces the PG construction cost, achieving up to 5.6x speedup
while not compromising the $k$-ANN search performance.",Shuo Yang
2024-10-02T11:26:02Z,http://arxiv.org/abs/2410.01428v1,"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with
  Retrieval-Augmentation for Solving Challenging Tasks","State-of-the-art large language models (LLMs) exhibit impressive
problem-solving capabilities but may struggle with complex reasoning and
factual correctness. Existing methods harness the strengths of chain-of-thought
and retrieval-augmented generation (RAG) to decompose a complex problem into
simpler steps and apply retrieval to improve factual correctness. These methods
work well on straightforward reasoning tasks but often falter on challenging
tasks such as competitive programming and mathematics, due to frequent
reasoning errors and irrelevant knowledge retrieval. To address this, we
introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a
novel framework that leverages fine-tuned critic models to guide both reasoning
and retrieval processes through planning. CR-Planner solves a problem by
iteratively selecting and executing sub-goals. Initially, it identifies the
most promising sub-goal from reasoning, query generation, and retrieval, guided
by rewards given by a critic model named sub-goal critic. It then executes this
sub-goal through sampling and selecting the optimal output based on evaluations
from another critic model named execution critic. This iterative process,
informed by retrieved information and critic models, enables CR-Planner to
effectively navigate the solution space towards the final answer. We employ
Monte Carlo Tree Search to collect the data for training the critic models,
allowing for a systematic exploration of action sequences and their long-term
impacts. We validate CR-Planner on challenging domain-knowledge-intensive and
reasoning-heavy tasks, including competitive programming, theorem-driven math
reasoning, and complex domain retrieval problems. Our experiments demonstrate
that CR-Planner significantly outperforms baselines, highlighting its
effectiveness in addressing challenging problems by improving both reasoning
and retrieval.",Xingxuan Li
2024-10-02T17:37:18Z,http://arxiv.org/abs/2410.01782v1,"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large
  Language Models","Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/",Shayekh Bin Islam
2024-09-27T23:05:02Z,http://arxiv.org/abs/2410.01841v1,A GEN AI Framework for Medical Note Generation,"The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.",Hui Yi Leong
2024-10-03T12:24:18Z,http://arxiv.org/abs/2410.02429v2,"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language
  Models","Large Language Models (LLMs) have demonstrated remarkable capabilities across
textual and visual domains but often generate outputs that violate physical
laws, revealing a gap in their understanding of the physical world. Inspired by
human cognition, where perception is fundamental to reasoning, we explore
augmenting LLMs with enhanced perception abilities using Internet of Things
(IoT) sensor data and pertinent knowledge for IoT task reasoning in the
physical world. In this work, we systematically study LLMs capability to
address real-world IoT tasks by augmenting their perception and knowledge base,
and then propose a unified framework, IoT-LLM, to enhance such capability. In
IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats
amenable to LLMs, activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions, and expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning. To evaluate the performance, We design a new benchmark
with five real-world IoT tasks with different data types and reasoning
difficulties and provide the benchmarking results on six open-source and
close-source LLMs. Experimental results demonstrate the limitations of existing
LLMs with naive textual inputs that cannot perform these tasks effectively. We
show that IoT-LLM significantly enhances the performance of IoT tasks reasoning
of LLM, such as GPT-4, achieving an average improvement of 65% across various
tasks against previous methods. The results also showcase LLMs ability to
comprehend IoT data and the physical law behind data by providing a reasoning
process. Limitations of our work are claimed to inspire future research in this
new era.",Tuo An
2024-10-04T00:08:46Z,http://arxiv.org/abs/2410.03049v1,"Scalable Frame-based Construction of Sociocultural NormBases for
  Socially-Aware Dialogues","Sociocultural norms serve as guiding principles for personal conduct in
social interactions, emphasizing respect, cooperation, and appropriate
behavior, which is able to benefit tasks including conversational information
retrieval, contextual information retrieval and retrieval-enhanced machine
learning. We propose a scalable approach for constructing a Sociocultural Norm
(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We
construct a comprehensive and publicly accessible Chinese Sociocultural
NormBase. Our approach utilizes socially aware dialogues, enriched with
contextual frames, as the primary data source to constrain the generating
process and reduce the hallucinations. This enables extracting of high-quality
and nuanced natural-language norm statements, leveraging the pragmatic
implications of utterances with respect to the situation. As real dialogue
annotated with gold frames are not readily available, we propose using
synthetic data. Our empirical results show: (i) the quality of the SCNs derived
from synthetic data is comparable to that from real dialogues annotated with
gold frames, and (ii) the quality of the SCNs extracted from real data,
annotated with either silver (predicted) or gold frames, surpasses that without
the frame annotations. We further show the effectiveness of the extracted SCNs
in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple
downstream dialogue tasks.",Shilin Qu
2024-10-04T18:00:28Z,http://arxiv.org/abs/2410.03829v1,"Misinformation with Legal Consequences (MisLC): A New Task Towards
  Harnessing Societal Harm of Misinformation","Misinformation, defined as false or inaccurate information, can result in
significant societal harm when it is spread with malicious or even innocuous
intent. The rapid online information exchange necessitates advanced detection
mechanisms to mitigate misinformation-induced harm. Existing research, however,
has predominantly focused on assessing veracity, overlooking the legal
implications and social consequences of misinformation. In this work, we take a
novel angle to consolidate the definition of misinformation detection using
legal issues as a measurement of societal ramifications, aiming to bring
interdisciplinary efforts to tackle misinformation and its consequence. We
introduce a new task: Misinformation with Legal Consequence (MisLC), which
leverages definitions from a wide range of legal domains covering 4 broader
legal topics and 11 fine-grained legal issues, including hate speech, election
laws, and privacy regulations. For this task, we advocate a two-step dataset
curation approach that utilizes crowd-sourced checkworthiness and expert
evaluations of misinformation. We provide insights about the MisLC task through
empirical evidence, from the problem definition to experiments and expert
involvement. While the latest large language models and retrieval-augmented
generation are effective baselines for the task, we find they are still far
from replicating expert performance.",Chu Fei Luo
2024-10-06T18:46:28Z,http://arxiv.org/abs/2410.04585v1,"Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community
  Retrieval","Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.",Pengcheng Jiang
2024-10-07T07:02:09Z,http://arxiv.org/abs/2410.04790v1,"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA","In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.",Xinyu Wang
2024-10-08T07:28:17Z,http://arxiv.org/abs/2410.05752v1,"Exploring the Meaningfulness of Nearest Neighbor Search in
  High-Dimensional Space","Dense high dimensional vectors are becoming increasingly vital in fields such
as computer vision, machine learning, and large language models (LLMs), serving
as standard representations for multimodal data. Now the dimensionality of
these vector can exceed several thousands easily. Despite the nearest neighbor
search (NNS) over these dense high dimensional vectors have been widely used
for retrieval augmented generation (RAG) and many other applications, the
effectiveness of NNS in such a high-dimensional space remains uncertain, given
the possible challenge caused by the ""curse of dimensionality."" To address
above question, in this paper, we conduct extensive NNS studies with different
distance functions, such as $L_1$ distance, $L_2$ distance and
angular-distance, across diverse embedding datasets, of varied types,
dimensionality and modality. Our aim is to investigate factors influencing the
meaningfulness of NNS. Our experiments reveal that high-dimensional text
embeddings exhibit increased resilience as dimensionality rises to higher
levels when compared to random vectors. This resilience suggests that text
embeddings are less affected to the ""curse of dimensionality,"" resulting in
more meaningful NNS outcomes for practical use. Additionally, the choice of
distance function has minimal impact on the relevance of NNS. Our study shows
the effectiveness of the embedding-based data representation method and can
offer opportunity for further optimization of dense vector-related
applications.",Zhonghan Chen
2024-10-08T12:30:07Z,http://arxiv.org/abs/2410.05983v1,Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,"Retrieval-augmented generation (RAG) empowers large language models (LLMs) to
utilize external knowledge sources. The increasing capacity of LLMs to process
longer input sequences opens up avenues for providing more retrieved
information, to potentially enhance the quality of generated outputs. It is
plausible to assume that a larger retrieval set would contain more relevant
information (higher recall), that might result in improved performance.
However, our empirical findings demonstrate that for many long-context LLMs,
the quality of generated output initially improves first, but then subsequently
declines as the number of retrieved passages increases. This paper investigates
this phenomenon, identifying the detrimental impact of retrieved ""hard
negatives"" as a key contributor. To mitigate this and enhance the robustness of
long-context LLM-based RAG, we propose both training-free and training-based
approaches. We first showcase the effectiveness of retrieval reordering as a
simple yet powerful training-free optimization. Furthermore, we explore
training-based methods, specifically RAG-specific implicit LLM fine-tuning and
RAG-oriented fine-tuning with intermediate reasoning, demonstrating their
capacity for substantial performance gains. Finally, we conduct a systematic
analysis of design choices for these training-based methods, including data
distribution, retriever selection, and training context length.",Bowen Jin
2024-10-09T17:59:58Z,http://arxiv.org/abs/2410.07176v1,"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for Large Language Models","Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.",Fei Wang
2024-10-10T17:55:02Z,http://arxiv.org/abs/2410.08182v1,"MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal
  Models","Existing multimodal retrieval benchmarks primarily focus on evaluating
whether models can retrieve and utilize external textual knowledge for question
answering. However, there are scenarios where retrieving visual information is
either more beneficial or easier to access than textual data. In this paper, we
introduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in
which we systematically identify and categorize scenarios where visually
augmented knowledge is better than textual knowledge, for instance, more images
from varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353
human-annotated multiple-choice questions across 9 distinct scenarios. With
MRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large
vision-language models (LVLMs). Our results show that all LVLMs exhibit greater
improvements when augmented with images compared to textual knowledge,
confirming that MRAG-Bench is vision-centric. Additionally, we conduct
extensive analysis with MRAG-Bench, which offers valuable insights into
retrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces
challenges in effectively leveraging retrieved knowledge, achieving only a
5.82% improvement with ground-truth information, in contrast to a 33.16%
improvement observed in human participants. These findings highlight the
importance of MRAG-Bench in encouraging the community to enhance LVLMs' ability
to utilize retrieved visual knowledge more effectively.",Wenbo Hu
2024-10-10T18:21:00Z,http://arxiv.org/abs/2410.08289v1,"Increasing the Difficulty of Automatically Generated Questions via
  Reinforcement Learning with Synthetic Preference","As the cultural heritage sector increasingly adopts technologies like
Retrieval-Augmented Generation (RAG) to provide more personalised search
experiences and enable conversations with collections data, the demand for
specialised evaluation datasets has grown. While end-to-end system testing is
essential, it's equally important to assess individual components. We target
the final, answering task, which is well-suited to Machine Reading
Comprehension (MRC). Although existing MRC datasets address general domains,
they lack the specificity needed for cultural heritage information.
Unfortunately, the manual creation of such datasets is prohibitively expensive
for most heritage institutions. This paper presents a cost-effective approach
for generating domain-specific MRC datasets with increased difficulty using
Reinforcement Learning from Human Feedback (RLHF) from synthetic preference
data. Our method leverages the performance of existing question-answering
models on a subset of SQuAD to create a difficulty metric, assuming that more
challenging questions are answered correctly less frequently. This research
contributes: (1) A methodology for increasing question difficulty using PPO and
synthetic data; (2) Empirical evidence of the method's effectiveness, including
human evaluation; (3) An in-depth error analysis and study of emergent
phenomena; and (4) An open-source codebase and set of three llama-2-chat
adapters for reproducibility and adaptation.",William Thorne
2024-10-11T13:36:13Z,http://arxiv.org/abs/2410.08801v1,"A Methodology for Evaluating RAG Systems: A Case Study On Configuration
  Dependency Validation","Retrieval-augmented generation (RAG) is an umbrella of different components,
design decisions, and domain-specific adaptations to enhance the capabilities
of large language models and counter their limitations regarding hallucination
and outdated and missing knowledge. Since it is unclear which design decisions
lead to a satisfactory performance, developing RAG systems is often
experimental and needs to follow a systematic and sound methodology to gain
sound and reliable results. However, there is currently no generally accepted
methodology for RAG evaluation despite a growing interest in this technology.
In this paper, we propose a first blueprint of a methodology for a sound and
reliable evaluation of RAG systems and demonstrate its applicability on a
real-world software engineering research task: the validation of configuration
dependencies across software technologies. In summary, we make two novel
contributions: (i) A novel, reusable methodological design for evaluating RAG
systems, including a demonstration that represents a guideline, and (ii) a RAG
system, which has been developed following this methodology, that achieves the
highest accuracy in the field of dependency validation. For the blueprint's
demonstration, the key insights are the crucial role of choosing appropriate
baselines and metrics, the necessity for systematic RAG refinements derived
from qualitative failure analysis, as well as the reporting practices of key
design decision to foster replication and evaluation.",Sebastian Simon
2024-10-11T17:57:06Z,http://arxiv.org/abs/2410.09141v1,ACER: Automatic Language Model Context Extension via Retrieval,"Long-context modeling is one of the critical capabilities of language AI for
digesting and reasoning over complex information pieces. In practice,
long-context capabilities are typically built into a pre-trained language
model~(LM) through a carefully designed context extension stage, with the goal
of producing generalist long-context capabilities. In our preliminary
experiments, however, we discovered that the current open-weight generalist
long-context models are still lacking in practical long-context processing
tasks. While this means perfectly effective long-context modeling demands
task-specific data, the cost can be prohibitive. In this paper, we draw
inspiration from how humans process a large body of information: a lossy
\textbf{retrieval} stage ranks a large set of documents while the reader ends
up reading deeply only the top candidates. We build an \textbf{automatic} data
synthesis pipeline that mimics this process using short-context LMs. The
short-context LMs are further tuned using these self-generated data to obtain
task-specific long-context capabilities. Similar to how pre-training learns
from imperfect data, we hypothesize and further demonstrate that the
short-context model can bootstrap over the synthetic data, outperforming not
only long-context generalist models but also the retrieval and read pipeline
used to synthesize the training data in real-world tasks such as long-context
retrieval augmented generation.",Luyu Gao
2024-10-12T22:31:01Z,http://arxiv.org/abs/2410.09662v1,"Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and
  Nays!","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge bases, achieving state-of-the-art results in
various coding tasks. The core of RAG is retrieving demonstration examples,
which is essential to balance effectiveness (generation quality) and efficiency
(retrieval time) for optimal performance. However, the high-dimensional nature
of code representations and large knowledge bases often create efficiency
bottlenecks, which are overlooked in previous research. This paper
systematically evaluates the efficiency-effectiveness trade-off of retrievers
across three coding tasks: Program Synthesis, Commit Message Generation, and
Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)
and four dense retrievers, including one exhaustive dense retriever (SBERT's
Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).
Our findings show that while BM25 excels in effectiveness, it suffers in
efficiency as the knowledge base grows beyond 1000 entries. In large-scale
retrieval, efficiency differences become more pronounced, with approximate
dense retrievers offering the greatest gains. For instance, in Commit
Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in
RougeL compared with BM25. Our results also show that increasing the number of
demonstrations in the prompt doesn't always improve the effectiveness and can
increase latency and lead to incorrect outputs. Our findings provide valuable
insights for practitioners aiming to build efficient and effective RAG systems
for coding tasks.",Pengfei He
2024-10-13T16:28:38Z,http://arxiv.org/abs/2410.09908v1,"Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble
  for Zero-shot Learning","Foundation models have become a cornerstone in deep learning, with techniques
like Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models.
Similarly, methods such as Retrieval-Augmented Generation (RAG), which leverage
vectorized databases, have further improved model performance by grounding
outputs in external information. While these approaches have demonstrated
notable success, they often require extensive training or labeled data, which
can limit their adaptability in resource-constrained environments. To address
these challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new
method that creates a vectorized database of LoRAs, enabling efficient
retrieval and application of model adaptations to new tasks. RPE minimizes the
need for extensive training and eliminates the requirement for labeled data,
making it particularly effective for zero-shot learning. Additionally, RPE is
well-suited for privacy-sensitive domains like healthcare, as it modifies model
parameters without accessing raw data. When applied to tasks such as medical
report generation and image segmentation, RPE not only proved effective but
also surpassed supervised fine-tuning methods in certain cases, highlighting
its potential to enhance both computational efficiency and privacy in deep
learning applications.",Pengfei Jin
2024-10-14T17:59:58Z,http://arxiv.org/abs/2410.10817v1,When Does Perceptual Alignment Benefit Vision Representations?,"Humans judge perceptual similarity according to diverse visual attributes,
including scene layout, subject location, and camera pose. Existing vision
models understand a wide range of semantic abstractions but improperly weigh
these attributes and thus make inferences misaligned with human perception.
While vision representations have previously benefited from alignment in
contexts like image generation, the utility of perceptually aligned
representations in more general-purpose settings remains unclear. Here, we
investigate how aligning vision model representations to human perceptual
judgments impacts their usability across diverse computer vision tasks. We
finetune state-of-the-art models on human similarity judgments for image
triplets and evaluate them across standard vision benchmarks. We find that
aligning models to perceptual judgments yields representations that improve
upon the original backbones across many downstream tasks, including counting,
segmentation, depth estimation, instance retrieval, and retrieval-augmented
generation. In addition, we find that performance is widely preserved on other
tasks, including specialized out-of-distribution domains such as in medical
imaging and 3D environment frames. Our results suggest that injecting an
inductive bias about human perceptual knowledge into vision models can
contribute to better representations.",Shobhita Sundaram
2024-10-15T19:04:13Z,http://arxiv.org/abs/2410.11996v1,"Holistic Reasoning with Long-Context LMs: A Benchmark for Database
  Operations on Massive Textual Data","The rapid increase in textual information means we need more efficient
methods to sift through, organize, and understand it all. While
retrieval-augmented generation (RAG) models excel in accessing information from
large document collections, they struggle with complex tasks that require
aggregation and reasoning over information spanning across multiple
documents--what we call holistic reasoning. Long-context language models
(LCLMs) have great potential for managing large-scale documents, but their
holistic reasoning capabilities remain unclear. In this work, we introduce
HoloBench, a novel framework that brings database reasoning operations into
text-based contexts, making it easier to systematically evaluate how LCLMs
handle holistic reasoning across large documents. Our approach adjusts key
factors such as context length, information density, distribution of
information, and query complexity to evaluate LCLMs comprehensively. Our
experiments show that the amount of information in the context has a bigger
influence on LCLM performance than the actual context length. Furthermore, the
complexity of queries affects performance more than the amount of information,
particularly for different types of queries. Interestingly, queries that
involve finding maximum or minimum values are easier for LCLMs and are less
affected by context length, even though they pose challenges for RAG systems.
However, tasks requiring the aggregation of multiple pieces of information show
a noticeable drop in accuracy as context length increases. Additionally, we
find that while grouping relevant information generally improves performance,
the optimal positioning varies across models. Our findings surface both the
advancements and the ongoing challenges in achieving a holistic understanding
of long contexts.",Seiji Maekawa
2024-10-16T08:55:49Z,http://arxiv.org/abs/2410.12380v1,"Evaluation of Attribution Bias in Retrieval-Augmented Large Language
  Models","Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.",Amin Abolghasemi
2024-10-16T17:59:32Z,http://arxiv.org/abs/2410.12788v2,"Meta-Chunking: Learning Efficient Text Segmentation via Logical
  Perception","Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances
performance and speed, and precisely identifies the boundaries of text chunks
by analyzing the characteristics of context perplexity distribution.
Additionally, considering the inherent complexity of different texts, we
propose a strategy that combines PPL Chunking with dynamic merging to achieve a
balance between fine-grained and coarse-grained text chunking. Experiments
conducted on eleven datasets demonstrate that Meta-Chunking can more
efficiently improve the performance of single-hop and multi-hop question
answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it
outperforms similarity chunking by 1.32 while only consuming 45.8% of the time.
Furthermore, through the analysis of models of various scales and types, we
observed that PPL Chunking exhibits notable flexibility and adaptability. Our
code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",Jihao Zhao
2024-10-11T19:16:03Z,http://arxiv.org/abs/2410.12858v1,"Large Language Models for Medical OSCE Assessment: A Novel Approach to
  Transcript Analysis","Grading Objective Structured Clinical Examinations (OSCEs) is a
time-consuming and expensive process, traditionally requiring extensive manual
effort from human experts. In this study, we explore the potential of Large
Language Models (LLMs) to assess skills related to medical student
communication. We analyzed 2,027 video-recorded OSCE examinations from the
University of Texas Southwestern Medical Center (UTSW), spanning four years
(2019-2022), and several different medical cases or ""stations."" Specifically,
our focus was on evaluating students' ability to summarize patients' medical
history: we targeted the rubric item 'did the student summarize the patients'
medical history?' from the communication skills rubric. After transcribing
speech audio captured by OSCE videos using Whisper-v3, we studied the
performance of various LLM-based approaches for grading students on this
summarization task based on their examination transcripts. Using various
frontier-level open-source and proprietary LLMs, we evaluated different
techniques such as zero-shot chain-of-thought prompting, retrieval augmented
generation, and multi-model ensemble methods. Our results show that frontier
LLM models like GPT-4 achieved remarkable alignment with human graders,
demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential
for LLM-based OSCE grading to augment the current grading process. Open-source
models also showed promising results, suggesting potential for widespread,
cost-effective deployment. Further, we present a failure analysis identifying
conditions where LLM grading may be less reliable in this context and recommend
best practices for deploying LLMs in medical education settings.",Ameer Hamza Shakur
2024-10-16T23:03:27Z,http://arxiv.org/abs/2410.13085v1,"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language
  Models","Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.",Peng Xia
2024-10-17T04:00:29Z,http://arxiv.org/abs/2410.13198v1,"Failing Forward: Improving Generative Error Correction for ASR with
  Synthetic Data and Retrieval Augmentation","Generative Error Correction (GEC) has emerged as a powerful post-processing
method to enhance the performance of Automatic Speech Recognition (ASR)
systems. However, we show that GEC models struggle to generalize beyond the
specific types of errors encountered during training, limiting their ability to
correct new, unseen errors at test time, particularly in out-of-domain (OOD)
scenarios. This phenomenon amplifies with named entities (NEs), where, in
addition to insufficient contextual information or knowledge about the NEs,
novel NEs keep emerging. To address these issues, we propose DARAG (Data- and
Retrieval-Augmented Generative Error Correction), a novel approach designed to
improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC
training dataset with synthetic data generated by prompting LLMs and
text-to-speech models, thereby simulating additional errors from which the
model can learn. For OOD scenarios, we simulate test-time errors from new
domains similarly and in an unsupervised fashion. Additionally, to better
handle named entities, we introduce retrieval-augmented correction by
augmenting the input with entities retrieved from a database. Our approach is
simple, scalable, and both domain- and language-agnostic. We experiment on
multiple datasets and settings, showing that DARAG outperforms all our
baselines, achieving 8\% -- 30\% relative WER improvements in ID and 10\% --
33\% improvements in OOD settings.",Sreyan Ghosh
2024-10-17T08:37:25Z,http://arxiv.org/abs/2410.13326v1,"Comparing the Utility, Preference, and Performance of Course Material
  Search Functionality and Retrieval-Augmented Generation Large Language Model
  (RAG-LLM) AI Chatbots in Information-Seeking Tasks","Providing sufficient support for students requires substantial resources,
especially considering the growing enrollment numbers. Students need help in a
variety of tasks, ranging from information-seeking to requiring support with
course assignments. To explore the utility of recent large language models
(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that
augments the answers that are produced with information from the course
materials. To study the effect of the LLM-powered AI chatbot, we conducted a
lab-based user study (N=14), in which the participants worked on tasks from a
web software development course. The participants were divided into two groups,
where one of the groups first had access to the chatbot and then to a more
traditional search functionality, while another group started with the search
functionality and was then given the chatbot. We assessed the participants'
performance and perceptions towards the chatbot and the search functionality
and explored their preferences towards the support functionalities. Our
findings highlight that both support mechanisms are seen as useful and that
support mechanisms work well for specific tasks, while less so for other tasks.
We also observe that students tended to prefer the second support mechanism
more, where students who were first given the chatbot tended to prefer the
search functionality and vice versa.",Leonardo Pasquarelli
2024-10-17T13:33:12Z,http://arxiv.org/abs/2410.13542v1,LLM-based Unit Test Generation via Property Retrieval,"Automated unit test generation has been widely studied, with Large Language
Models (LLMs) recently showing significant potential. Moreover, in the context
of unit test generation, these tools prioritize high code coverage, often at
the expense of practical usability, correctness, and maintainability. In
response, we propose Property-Based Retrieval Augmentation, a novel mechanism
that extends LLM-based Retrieval-Augmented Generation (RAG) beyond basic
vector, text similarity, and graph-based methods. Our approach considers
task-specific context and introduces a tailored property retrieval mechanism.
Specifically, in the unit test generation task, we account for the unique
structure of unit tests by dividing the test generation process into Given,
When, and Then phases. When generating tests for a focal method, we not only
retrieve general context for the code under test but also consider
task-specific context such as pre-existing tests of other methods, which can
provide valuable insights for any of the Given, When, and Then phases. This
forms property relationships between focal method and other methods, thereby
expanding the scope of retrieval beyond traditional RAG. We implement this
approach in a tool called APT, which sequentially performs preprocessing,
property retrieval, and unit test generation, using an iterative strategy where
newly generated tests guide the creation of subsequent ones. We evaluated APT
on 12 open-source projects with 1515 methods, and the results demonstrate that
APT consistently outperforms existing tools in terms of correctness,
completeness, and maintainability of the generated tests. Moreover, we
introduce a novel code-context-aware retrieval mechanism for LLMs beyond
general context, offering valuable insights and potential applications for
other code-related tasks.",Zhe Zhang
2024-10-18T06:51:13Z,http://arxiv.org/abs/2410.14209v1,"Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents","In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.",Zihan Liu
2024-10-18T16:44:22Z,http://arxiv.org/abs/2410.14594v2,"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases","Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).",Elias Lumer
2024-10-19T07:08:40Z,http://arxiv.org/abs/2410.15016v1,"Transit Pulse: Utilizing Social Media as a Source for Customer Feedback
  and Information Extraction with Large Language Model","Users of the transit system flood social networks daily with messages that
contain valuable insights crucial for improving service quality. These posts
help transit agencies quickly identify emerging issues. Parsing topics and
sentiments is key to gaining comprehensive insights to foster service
excellence. However, the volume of messages makes manual analysis impractical,
and standard NLP techniques like Term Frequency-Inverse Document Frequency
(TF-IDF) fall short in nuanced interpretation. Traditional sentiment analysis
separates topics and sentiments before integrating them, often missing the
interaction between them. This incremental approach complicates classification
and reduces analytical productivity. To address these challenges, we propose a
novel approach to extracting and analyzing transit-related information,
including sentiment and sarcasm detection, identification of unusual system
problems, and location data from social media. Our method employs Large
Language Models (LLM), specifically Llama 3, for a streamlined analysis free
from pre-established topic labels. To enhance the model's domain-specific
knowledge, we utilize Retrieval-Augmented Generation (RAG), integrating
external knowledge sources into the information extraction pipeline. We
validated our method through extensive experiments comparing its performance
with traditional NLP approaches on user tweet data from the real world transit
system. Our results demonstrate the potential of LLMs to transform social media
data analysis in the public transit domain, providing actionable insights and
enhancing transit agencies' responsiveness by extracting a broader range of
information.",Jiahao Wang
2024-10-19T16:46:21Z,http://arxiv.org/abs/2410.15154v1,"MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation
  and Rigorous Verification","Large Language Models (LLMs) have shown considerable promise in code
generation. However, the automation sector, especially in motion control,
continues to rely heavily on manual programming due to the complexity of tasks
and critical safety considerations. In this domain, incorrect code execution
can pose risks to both machinery and personnel, necessitating specialized
expertise. To address these challenges, we introduce MCCoder, an LLM-powered
system designed to generate code that addresses complex motion control tasks,
with integrated soft-motion data verification. MCCoder enhances code generation
through multitask decomposition, hybrid retrieval-augmented generation (RAG),
and self-correction with a private motion library. Moreover, it supports data
verification by logging detailed trajectory data and providing simulations and
plots, allowing users to assess the accuracy of the generated code and
bolstering confidence in LLM-based programming. To ensure robust validation, we
propose MCEVAL, an evaluation dataset with metrics tailored to motion control
tasks of varying difficulties. Experiments indicate that MCCoder improves
performance by 11.61% overall and by 66.12% on complex tasks in MCEVAL dataset
compared with base models with naive RAG. This system and dataset aim to
facilitate the application of code generation in automation settings with
strict safety requirements. MCCoder is publicly available at
https://github.com/MCCodeAI/MCCoder.",Yin Li
2024-10-20T04:24:16Z,http://arxiv.org/abs/2410.15277v1,"BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via
  Compression","Retrieval-augmented generation (RAG) can supplement large language models
(LLMs) by integrating external knowledge. However, as the number of retrieved
documents increases, the input length to LLMs grows linearly, causing a
dramatic increase in latency and a degradation in long-context understanding.
This is particularly serious for multi-hop questions that require a chain of
reasoning across documents. To accelerate inference, reduce costs, and minimize
distractions, this paper presents BRIEF (Bridging Retrieval and Inference
through Evidence Fusion), a lightweight approach that performs query-aware
multi-hop reasoning by compressing retrieved documents into highly dense
textual summaries to integrate into in-context learning. To enable learning
compression for multi-hop reasoning, we curate synthetic data by extracting
atomic proposition expressions that encapsulate distinct factoids from the
source documents to compose synthetic summaries. Based on our synthetic data
built entirely by open-source models, BRIEF generates more concise summaries
and enables a range of LLMs to achieve exceptional open-domain question
answering (QA) performance. For example, on HotpotQA, BRIEF improves the
compression rate by 2 times compared to the state-of-the-art baseline, while
outperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It
also generates more concise summaries than proprietary GPT-3.5, while
demonstrating nearly identical QA performance.",Yuankai Li
2024-10-20T14:31:05Z,http://arxiv.org/abs/2410.15403v2,"MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis
  and Knowledge-based Departmental Consultation","We present MMDS, a system capable of recognizing medical images and patient
facial details, and providing professional medical diagnoses. The system
consists of two core components:The first component is the analysis of medical
images and videos. We trained a specialized multimodal medical model capable of
interpreting medical images and accurately analyzing patients' facial emotions
and facial paralysis conditions. The model achieved an accuracy of 72.59% on
the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in
recognizing the ""happy"" emotion. In facial paralysis recognition, the model
reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on
this model, we developed a parser for analyzing facial movement videos of
patients with facial paralysis, achieving precise grading of the paralysis
severity. In tests on 30 videos of facial paralysis patients, the system
demonstrated a grading accuracy of 83.3%.The second component is the generation
of professional medical responses. We employed a large language model,
integrated with a medical knowledge base, to generate professional diagnoses
based on the analysis of medical images or videos. The core innovation lies in
our development of a department-specific knowledge base routing management
mechanism, in which the large language model categorizes data by medical
departments and, during the retrieval process, determines the appropriate
knowledge base to query. This significantly improves retrieval accuracy in the
RAG (retrieval-augmented generation) process.",Yi Ren
2024-10-21T09:22:29Z,http://arxiv.org/abs/2410.15805v1,"RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for
  IT Operations and Maintenance","With the ever-increasing demands on Question Answering (QA) systems for IT
operations and maintenance, an efficient and supervised fine-tunable framework
is necessary to ensure the data security, private deployment and continuous
upgrading. Although Large Language Models (LLMs) have notably improved the
open-domain QA's performance, how to efficiently handle enterprise-exclusive
corpora and build domain-specific QA systems are still less-studied for
industrial applications. In this paper, we propose a general and comprehensive
framework based on Retrieval Augmented Generation (RAG) and facilitate the
whole business process of establishing QA systems for IT operations and
maintenance. In accordance with the prevailing RAG method, our proposed
framework, named with RAG4ITOps, composes of two major stages: (1) Models
Fine-tuning \& Data Vectorization, and (2) Online QA System Process. At the
Stage 1, we leverage a contrastive learning method with two negative sampling
strategies to fine-tune the embedding model, and design the instruction
templates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method.
At the Stage 2, an efficient process of QA system is built for serving. We
collect enterprise-exclusive corpora from the domain of cloud computing, and
the extensive experiments show that our method achieves superior results than
counterparts on two kinds of QA tasks. Our experiment also provide a case for
applying the RAG4ITOps to real-world enterprise-level applications.",Tianyang Zhang
2024-10-17T22:04:32Z,http://arxiv.org/abs/2410.16322v1,"SouLLMate: An Application Enhancing Diverse Mental Health Support with
  Adaptive LLMs, Prompt Engineering, and RAG Techniques","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide diverse, accessible, stigma-free, personalized, and
real-time mental health support through cutting-edge AI technologies. It makes
the following contributions: (1) Conducting an extensive survey of recent
mental health support methods to identify prevalent functionalities and unmet
needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates
LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt
engineering, and domain knowledge. This system offers advanced features such as
Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for
personalized profile uploads and Conversational Information Extraction. (3)
Developing novel evaluation approaches for preliminary assessments and risk
detection via professionally annotated interview data and real-life suicide
tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive
Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to
enhance model performance and usability through context-sensitive response
adjustments, semantic coherence evaluations, and enhanced accuracy of
long-context reasoning in language models. This study contributes to advancing
mental health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-22T00:30:08Z,http://arxiv.org/abs/2410.16592v1,ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding,"The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.",Andrew Kan
2024-10-22T00:47:54Z,http://arxiv.org/abs/2410.16597v1,"Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency","Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.",Prafulla Kumar Choubey
2024-10-22T12:56:04Z,http://arxiv.org/abs/2410.16977v1,"IPL: Leveraging Multimodal Large Language Models for Intelligent Product
  Listing","Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,
Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are
mainly targeting individual sellers who usually lack sufficient experience in
e-commerce. Individual sellers often struggle to compose proper descriptions
for selling products. With the recent advancement of Multimodal Large Language
Models (MLLMs), we attempt to integrate such state-of-the-art generative AI
technologies into the product listing process. To this end, we develop IPL, an
Intelligent Product Listing tool tailored to generate descriptions using
various product attributes such as category, brand, color, condition, etc. IPL
enables users to compose product descriptions by merely uploading photos of the
selling product. More importantly, it can imitate the content style of our C2C
platform Xianyu. This is achieved by employing domain-specific instruction
tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation
(RAG) process. A comprehensive empirical evaluation demonstrates that the
underlying model of IPL significantly outperforms the base model in
domain-specific tasks while producing less hallucination. IPL has been
successfully deployed in our production system, where 72% of users have their
published product listings based on the generated content, and those product
listings are shown to have a quality score 5.6% higher than those without AI
assistance.",Kang Chen
2024-10-08T19:42:00Z,http://arxiv.org/abs/2410.18107v1,In-Context Code-Text Learning for Bimodal Software Engineering,"Bimodal software analysis initially appeared to be within reach with the
advent of large language models. Unfortunately, the complex interplay of
natural language text and code in software engineering, presents unique
challenges that prevent pretrained models to generalize to a variety of tasks.
We postulate that in-context learning for the code-text bimodality is a
promising avenue. This paper thus introduces a comprehensive study of
in-context code-text learning, focusing on leveraging pretrained CodeLLAMA
models.
  We consider a diverse dataset encompassing 23 software engineering tasks,
which we transform in an in-context learning format. To effectively extract
informative features, we propose a configurable prompt template. Our proposed
pipeline, InCTRL, then unifies prompt learning across various software
engineering tasks. Extensive evaluation on the study datasets demonstrates the
superiority of INCTRL-models in few-shot performance, surpassing
state-of-the-art models including the support model, CodeLLAMA. Typically, we
observe that applied to the CodeLLAMA model, INCTRL brings improvements in
terms of precision (at least about 12\%) and recall (up to 93.88\%) on various
tasks. For example, on the task of program repair, INCTRL improves the BLEU
score of CodeLLAMA by 85 points, while for clone detection, INCTRL achieves an
improvement of 69 percentage points. Moreover, INCTRL-models offer
state-of-the-art performance when using retrieval-augmented generation on
individual downstream tasks. Finally, we qualitatively analyze the benefits of
INCTRL over CodeLLAMA and open-source all models for broader impact.
  We make our code and dataset publicly available at: \begin{center}
  {\url{https://anonymous.4open.science/r/inctrl-B65B}} \end{center}",Xunzhu Tang
2024-10-24T14:47:25Z,http://arxiv.org/abs/2410.18792v2,An LLM Agent for Automatic Geospatial Data Analysis,"Large language models (LLMs) are being used in data science code generation
tasks, but they often struggle with complex sequential tasks, leading to
logical errors. Their application to geospatial data processing is particularly
challenging due to difficulties in incorporating complex data structures and
spatial constraints, effectively utilizing diverse function calls, and the
tendency to hallucinate less-used geospatial libraries. To tackle these
problems, we introduce GeoAgent, a new interactive framework designed to help
LLMs handle geospatial data processing more effectively. GeoAgent pioneers the
integration of a code interpreter, static analysis, and Retrieval-Augmented
Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,
offering a novel approach to geospatial data processing. In addition, we
contribute a new benchmark specifically designed to evaluate the LLM-based
approach in geospatial tasks. This benchmark leverages a variety of Python
libraries and includes both single-turn and multi-turn tasks such as data
acquisition, data analysis, and visualization. By offering a comprehensive
evaluation among diverse geospatial contexts, this benchmark sets a new
standard for developing LLM-based approaches in geospatial data analysis tasks.
Our findings suggest that relying solely on knowledge of LLM is insufficient
for accurate geospatial task programming, which requires coherent multi-step
processes and multiple function calls. Compared to the baseline LLMs, the
proposed GeoAgent has demonstrated superior performance, yielding notable
improvements in function calls and task completion. In addition, these results
offer valuable insights for the future development of LLM agents in automatic
geospatial data analysis task programming.",Yuxing Chen
2024-10-15T16:37:18Z,http://arxiv.org/abs/2410.19790v1,"Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP
  Technical Specifications","This paper proposes a Question-Answering (QA) system for the telecom domain
using 3rd Generation Partnership Project (3GPP) technical documents. Alongside,
a hybrid dataset, Telco-DPR, which consists of a curated 3GPP corpus in a
hybrid format, combining text and tables, is presented. Additionally, the
dataset includes a set of synthetic question/answer pairs designed to evaluate
the retrieval performance of QA systems on this type of data. The retrieval
models, including the sparse model, Best Matching 25 (BM25), as well as dense
models, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval
(DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank
(MRR). The results show that DHR, a retriever model utilising hierarchical
passage selection through fine-tuning at both the document and passage levels,
outperforms traditional methods in retrieving relevant technical information,
achieving a Top-10 accuracy of 86.2%. Additionally, the Retriever-Augmented
Generation (RAG) technique, used in the proposed QA system, is evaluated to
demonstrate the benefits of using the hybrid dataset and the DHR. The proposed
QA system, using the developed RAG model and the Generative Pretrained
Transformer (GPT)-4, achieves a 14% improvement in answer accuracy, when
compared to a previous benchmark on the same dataset.",Thaina Saraiva
2024-10-26T15:42:50Z,http://arxiv.org/abs/2410.20204v1,"Generative AI in Health Economics and Outcomes Research: A Taxonomy of
  Key Definitions and Emerging Applications, an ISPOR Working Group Report","Objective: This article offers a taxonomy of generative artificial
intelligence (AI) for health economics and outcomes research (HEOR), explores
its emerging applications, and outlines methods to enhance the accuracy and
reliability of AI-generated outputs. Methods: The review defines foundational
generative AI concepts and highlights current HEOR applications, including
systematic literature reviews, health economic modeling, real-world evidence
generation, and dossier development. Approaches such as prompt engineering
(zero-shot, few-shot, chain-of-thought, persona pattern prompting),
retrieval-augmented generation, model fine-tuning, and the use of
domain-specific models are introduced to improve AI accuracy and reliability.
Results: Generative AI shows significant potential in HEOR, enhancing
efficiency, productivity, and offering novel solutions to complex challenges.
Foundation models are promising in automating complex tasks, though challenges
remain in scientific reliability, bias, interpretability, and workflow
integration. The article discusses strategies to improve the accuracy of these
AI tools. Conclusion: Generative AI could transform HEOR by increasing
efficiency and accuracy across various applications. However, its full
potential can only be realized by building HEOR expertise and addressing the
limitations of current AI technologies. As AI evolves, ongoing research and
innovation will shape its future role in the field.",Rachael Fleurence
2024-10-26T19:48:47Z,http://arxiv.org/abs/2410.20263v1,"EfficientEQA: An Efficient Approach for Open Vocabulary Embodied
  Question Answering","Embodied Question Answering (EQA) is an essential yet challenging task for
robotic home assistants. Recent studies have shown that large vision-language
models (VLMs) can be effectively utilized for EQA, but existing works either
focus on video-based question answering without embodied exploration or rely on
closed-form choice sets. In real-world scenarios, a robotic agent must
efficiently explore and accurately answer questions in open-vocabulary
settings. To address these challenges, we propose a novel framework called
EfficientEQA for open-vocabulary EQA, which enables efficient exploration and
accurate answering. In EfficientEQA, the robot actively explores unknown
environments using Semantic-Value-Weighted Frontier Exploration, a strategy
that prioritizes exploration based on semantic importance provided by
calibrated confidence from black-box VLMs to quickly gather relevant
information. To generate accurate answers, we employ Retrieval-Augmented
Generation (RAG), which utilizes BLIP to retrieve useful images from
accumulated observations and VLM reasoning to produce responses without relying
on predefined answer choices. Additionally, we detect observations that are
highly relevant to the question as outliers, allowing the robot to determine
when it has sufficient information to stop exploring and provide an answer.
Experimental results demonstrate the effectiveness of our approach, showing an
improvement in answering accuracy by over 15% and efficiency, measured in
running steps, by over 20% compared to state-of-the-art methods.",Kai Cheng
2024-10-27T16:23:26Z,http://arxiv.org/abs/2410.21330v1,LLM Robustness Against Misinformation in Biomedical Question Answering,"The retrieval-augmented generation (RAG) approach is used to reduce the
confabulation of large language models (LLMs) for question answering by
retrieving and providing additional context coming from external knowledge
sources (e.g., by adding the context to the prompt). However, injecting
incorrect information can mislead the LLM to generate an incorrect answer.
  In this paper, we evaluate the effectiveness and robustness of four LLMs
against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in
answering biomedical questions. We assess the answer accuracy on yes-no and
free-form questions in three scenarios: vanilla LLM answers (no context is
provided), ""perfect"" augmented generation (correct context is provided), and
prompt-injection attacks (incorrect context is provided). Our results show that
Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla
(0.651) and ""perfect"" RAG (0.802) scenarios. However, the accuracy gap between
the models almost disappears with ""perfect"" RAG, suggesting its potential to
mitigate the LLM's size-related effectiveness differences.
  We further evaluate the ability of the LLMs to generate malicious context on
one hand and the LLM's robustness against prompt-injection attacks on the other
hand, using metrics such as attack success rate (ASR), accuracy under attack,
and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2,
GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is
injected in the target model's prompt. Interestingly, Llama is shown to be the
most effective adversary, causing accuracy drops of up to 0.48 for vanilla
answers and 0.63 for ""perfect"" RAG across target models. Our analysis reveals
that robustness rankings vary depending on the evaluation measure, highlighting
the complexity of assessing LLM resilience to adversarial attacks.",Alexander Bondarenko
2024-10-29T17:55:00Z,http://arxiv.org/abs/2410.22316v1,Understanding Synthetic Context Extension via Retrieval Heads,"Long-context LLMs are increasingly in demand for applications such as
retrieval-augmented generation. To defray the cost of pretraining LLMs over
long contexts, recent work takes an approach of synthetic context extension:
fine-tuning LLMs with synthetically generated long-context data in a
post-training stage. However, it remains unclear how and why this synthetic
context extension imparts abilities for downstream long-context tasks. In this
paper, we investigate fine-tuning on synthetic data for three long-context
tasks that require retrieval and reasoning. We vary the realism of ""needle""
concepts to be retrieved and diversity of the surrounding ""haystack"" context,
from using LLMs to construct synthetic documents to using templated relations
and creating symbolic datasets. We find that models trained on synthetic data
fall short of the real data, but surprisingly, the mismatch can be interpreted
and even predicted in terms of a special set of attention heads that are
responsible for retrieval over long context: retrieval heads (Wu et al., 2024).
The retrieval heads learned on synthetic data are mostly subsets of the
retrieval heads learned on real data, and there is a strong correlation between
the recall of heads learned and the downstream performance of a model.
Furthermore, with attention knockout and activation patching, we
mechanistically show that retrieval heads are necessary and explain model
performance, although they are not totally sufficient. Our results shed light
on how to interpret synthetic data fine-tuning performance and how to approach
creating better data for learning real-world capabilities over long contexts.",Xinyu Zhao
2024-10-30T13:22:22Z,http://arxiv.org/abs/2410.22996v1,"Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A
  Knowledge Graph Generation Approach","A well structured collection of the various Quantum Cascade Laser (QCL)
design and working properties data provides a platform to analyze and
understand the relationships between these properties. By analyzing these
relationships, we can gain insights into how different design features impact
laser performance properties such as the working temperature. Most of these QCL
properties are captured in scientific text. There is therefore need for
efficient methodologies that can be utilized to extract QCL properties from
text and generate a semantically enriched and interlinked platform where the
properties can be analyzed to uncover hidden relations. There is also the need
to maintain provenance and reference information on which these properties are
based. Semantic Web technologies such as Ontologies and Knowledge Graphs have
proven capability in providing interlinked data platforms for knowledge
representation in various domains. In this paper, we propose an approach for
generating a QCL properties Knowledge Graph (KG) from text for semantic
enrichment of the properties. The approach is based on the QCL ontology and a
Retrieval Augmented Generation (RAG) enabled information extraction pipeline
based on GPT 4-Turbo language model. The properties of interest include:
working temperature, laser design type, lasing frequency, laser optical power
and the heterostructure. The experimental results demonstrate the feasibility
and effectiveness of this approach for efficiently extracting QCL properties
from unstructured text and generating a QCL properties Knowledge Graph, which
has potential applications in semantic enrichment and analysis of QCL data.",Deperias Kerre
2024-10-30T14:08:50Z,http://arxiv.org/abs/2410.23041v1,Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval,"As LLMs exhibit a high degree of human-like capability, increasing attention
has been paid to role-playing research areas in which responses generated by
LLMs are expected to mimic human replies. This has promoted the exploration of
role-playing agents in various applications, such as chatbots that can engage
in natural conversations with users and virtual assistants that can provide
personalized support and guidance. The crucial factor in the role-playing task
is the effective utilization of character memory, which stores characters'
profiles, experiences, and historical dialogues. Retrieval Augmented Generation
(RAG) technology is used to access the related memory to enhance the response
generation of role-playing agents. Most existing studies retrieve related
information based on the semantic similarity of memory to maintain characters'
personalized traits, and few attempts have been made to incorporate the
emotional factor in the retrieval argument generation (RAG) of LLMs. Inspired
by the Mood-Dependent Memory theory, which indicates that people recall an
event better if they somehow reinstate during recall the original emotion they
experienced during learning, we propose a novel emotion-aware memory retrieval
framework, termed Emotional RAG, which recalls the related memory with
consideration of emotional state in role-playing agents. Specifically, we
design two kinds of retrieval strategies, i.e., combination strategy and
sequential strategy, to incorporate both memory semantic and emotional states
during the retrieval process. Extensive experiments on three representative
role-playing datasets demonstrate that our Emotional RAG framework outperforms
the method without considering the emotional factor in maintaining the
personalities of role-playing agents. This provides evidence to further
reinforce the Mood-Dependent Memory theory in psychology.",Le Huang
2024-10-31T14:22:20Z,http://arxiv.org/abs/2410.23968v1,"EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable
  Robot Task Planning","Recent advances in Large Language Models (LLMs) have helped facilitate
exciting progress for robotic planning in real, open-world environments. 3D
scene graphs (3DSGs) offer a promising environment representation for grounding
such LLM-based planners as they are compact and semantically rich. However, as
the robot's environment scales (e.g., number of entities tracked) and the
complexity of scene graph information increases (e.g., maintaining more
attributes), providing the 3DSG as-is to an LLM-based planner quickly becomes
infeasible due to input token count limits and attentional biases present in
LLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods
that retrieve query-relevant document chunks for LLM question and answering, we
adapt the paradigm for our embodied domain. Specifically, we propose a 3D scene
subgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based
planner with for executing natural language robotic tasks. Notably, our
retrieved subgraphs adapt to changes in the environment as well as changes in
task-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's
ability to significantly reduce input token counts (by an order of magnitude)
and planning time (up to 70% reduction in average time per planning step) while
improving success rates on AI2Thor simulated household tasks with a single-arm,
mobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with
a manipulator to highlight the performance benefits for robot deployment at the
edge in real environments.",Meghan Booker
2024-11-01T01:51:31Z,http://arxiv.org/abs/2411.00304v1,"Unified Generative and Discriminative Training for Multi-modal Large
  Language Models","In recent times, Vision-Language Models (VLMs) have been trained under two
predominant paradigms. Generative training has enabled Multimodal Large
Language Models (MLLMs) to tackle various complex tasks, yet issues such as
hallucinations and weak object discrimination persist. Discriminative training,
exemplified by models like CLIP, excels in zero-shot image-text classification
and retrieval, yet struggles with complex scenarios requiring fine-grained
semantic differentiation. This paper addresses these challenges by proposing a
unified approach that integrates the strengths of both paradigms. Considering
interleaved image-text sequences as the general format of input samples, we
introduce a structure-induced training strategy that imposes semantic
relationships between input samples and the MLLM's hidden state. This approach
enhances the MLLM's ability to capture global semantics and distinguish
fine-grained semantics. By leveraging dynamic sequence alignment within the
Dynamic Time Warping framework and integrating a novel kernel for fine-grained
semantic differentiation, our method effectively balances generative and
discriminative tasks. Extensive experiments demonstrate the effectiveness of
our approach, achieving state-of-the-art results in multiple generative tasks,
especially those requiring cognitive and discrimination abilities.
Additionally, our method surpasses discriminative benchmarks in interleaved and
fine-grained retrieval tasks. By employing a retrieval-augmented generation
strategy, our approach further enhances performance in some generative tasks
within one model, offering a promising direction for future research in
vision-language modeling.",Wei Chow
2024-11-03T15:25:47Z,http://arxiv.org/abs/2411.01606v2,"DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with
  Large Language Models","The rise of Large Language Models (LLMs) has streamlined frontend interface
creation through tools like Vercel's V0, yet surfaced challenges in design
quality (e.g., accessibility, and usability). Current solutions, often limited
by their focus, generalisability, or data dependency, fall short in addressing
these complexities. Moreover, none of them examine the quality of LLM-generated
UI design. In this work, we introduce DesignRepair, a novel dual-stream design
guideline-aware system to examine and repair the UI design quality issues from
both code aspect and rendered page aspect. We utilised the mature and popular
Material Design as our knowledge base to guide this process. Specifically, we
first constructed a comprehensive knowledge base encoding Google's Material
Design principles into low-level component knowledge base and high-level system
design knowledge base. After that, DesignRepair employs a LLM for the
extraction of key components and utilizes the Playwright tool for precise page
analysis, aligning these with the established knowledge bases. Finally, we
integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4
to holistically refine and repair frontend code through a strategic divide and
conquer approach. Our extensive evaluations validated the efficacy and utility
of our approach, demonstrating significant enhancements in adherence to design
guidelines, accessibility, and user experience metrics.",Mingyue Yuan
2024-11-04T05:25:39Z,http://arxiv.org/abs/2411.01807v1,Can Language Models Enable In-Context Database?,"Large language models (LLMs) are emerging as few-shot learners capable of
handling a variety of tasks, including comprehension, planning, reasoning,
question answering, arithmetic calculations, and more. At the core of these
capabilities is LLMs' proficiency in representing and understanding structural
or semi-structural data, such as tables and graphs. Numerous studies have
demonstrated that reasoning on tabular data or graphs is not only feasible for
LLMs but also gives a promising research direction which treats these data as
in-context data. The lightweight and human readable characteristics of
in-context database can potentially make it an alternative for the traditional
database in typical RAG (Retrieval Augmented Generation) settings. However,
almost all current work focuses on static in-context data, which does not allow
dynamic update. In this paper, to enable dynamic database update, delta
encoding of database is proposed. We explore how data stored in traditional
RDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD
(Create, Read, Update and Delete) operations on in-context databases. A
benchmark named InConDB is presented and extensive experiments are conducted to
show the performance of different language models in enabling in-context
database by varying the database encoding method, prompting method, operation
type and input data distribution, revealing both the proficiency and
limitations.",Yu Pan
2024-10-18T05:23:39Z,http://arxiv.org/abs/2411.02404v1,"Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative
  Mining","Ranking consistently emerges as a primary focus in information retrieval
research. Retrieval and ranking models serve as the foundation for numerous
applications, including web search, open domain QA, enterprise domain QA, and
text-based recommender systems. Typically, these models undergo training on
triplets consisting of binary relevance assignments, comprising one positive
and one negative passage. However, their utilization involves a context where a
significantly more nuanced understanding of relevance is necessary, especially
when re-ranking a large pool of potentially relevant passages. Although
collecting positive examples through user feedback like impressions or clicks
is straightforward, identifying suitable negative pairs from a vast pool of
possibly millions or even billions of documents possess a greater challenge.
Generating a substantial number of negative pairs is often necessary to
maintain the high quality of the model. Several approaches have been suggested
in literature to tackle the issue of selecting suitable negative pairs from an
extensive corpus. This study focuses on explaining the crucial role of hard
negatives in the training process of cross-encoder models, specifically aiming
to explain the performance gains observed with hard negative sampling compared
to random sampling. We have developed a robust hard negative mining technique
for efficient training of cross-encoder re-rank models on an enterprise dataset
which has domain specific context. We provide a novel perspective to enhance
retrieval models, ultimately influencing the performance of advanced LLM
systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action
Agents (ReAct). The proposed approach demonstrates that learning both
similarity and dissimilarity simultaneously with cross-encoders improves
performance of retrieval systems.",Hansa Meghwani
2024-11-04T22:45:52Z,http://arxiv.org/abs/2411.02657v1,"Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare
  Disease Knowledge","Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.",Karthik Soman
2024-11-05T09:58:36Z,http://arxiv.org/abs/2411.02959v1,"HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge
  in RAG Systems","Retrieval-Augmented Generation (RAG) has been shown to improve knowledge
capabilities and alleviate the hallucination problem of LLMs. The Web is a
major source of external knowledge used in RAG systems, and many commercial
systems such as ChatGPT and Perplexity have used Web search engines as their
major retrieval systems. Typically, such RAG systems retrieve search results,
download HTML sources of the results, and then extract plain texts from the
HTML sources. Plain text documents or chunks are fed into the LLMs to augment
the generation. However, much of the structural and semantic information
inherent in HTML, such as headings and table structures, is lost during this
plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,
which uses HTML instead of plain text as the format of retrieved knowledge in
RAG. We believe HTML is better than plain text in modeling knowledge in
external documents, and most LLMs possess robust capacities to understand HTML.
However, utilizing HTML presents new challenges. HTML contains additional
content such as tags, JavaScript, and CSS specifications, which bring extra
input tokens and noise to the RAG system. To address this issue, we propose
HTML cleaning, compression, and pruning strategies, to shorten the HTML while
minimizing the loss of information. Specifically, we design a two-step
block-tree-based pruning method that prunes useless HTML blocks and keeps only
the relevant part of the HTML. Experiments on six QA datasets confirm the
superiority of using HTML in RAG systems.",Jiejun Tan
2024-11-04T00:01:34Z,http://arxiv.org/abs/2411.03349v1,RuAG: Learned-rule-augmented Generation for Large Language Models,"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have
gained attention for their ability to enhance LLMs' reasoning by incorporating
external knowledge but suffer from limited contextual window size, leading to
insufficient information injection. To this end, we propose a novel framework,
RuAG, to automatically distill large volumes of offline data into interpretable
first-order logic rules, which are injected into LLMs to boost their reasoning
capabilities. Our method begins by formulating the search process relying on
LLMs' commonsense, where LLMs automatically define head and body predicates.
Then, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational
searching space and efficiently discover logic rules from data. The resulting
logic rules are translated into natural language, allowing targeted knowledge
injection and seamless integration into LLM prompts for LLM's downstream task
reasoning. We evaluate our framework on public and private industrial tasks,
including natural language processing, time-series, decision-making, and
industrial tasks, demonstrating its effectiveness in enhancing LLM's capability
over diverse tasks.",Yudi Zhang
2024-11-07T07:07:34Z,http://arxiv.org/abs/2411.04476v1,"LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation
  Combining Hierarchical Agents and RAG","The increasing use of smart devices has emphasized the critical role of
maintenance in production activities. Interactive Electronic Technical Manuals
(IETMs) are vital tools that support the maintenance of smart equipment.
However, traditional IETMs face challenges such as transitioning from Graphical
User Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing
complex logical relationships. Additionally, they must meet the current demands
for higher intelligence. This paper proposes a Maintenance Scheme Generation
Method based on Large Language Models (LLM-R). The proposed method includes
several key innovations: We propose the Low Rank Adaptation-Knowledge Retention
(LORA-KR) loss technology to proportionally adjust mixed maintenance data for
fine-tuning the LLM. This method prevents knowledge conflicts caused by mixed
data, improving the model's adaptability and reasoning ability in specific
maintenance domains, Besides, Hierarchical Task-Based Agent and
Instruction-level Retrieval-Augmented Generation (RAG) technologies are adopted
to optimize the generation steps and mitigate the phenomenon of hallucination
caused by the model's Inability to access contextual information. This
enhancement improves the model's flexibility and accuracy in handling known or
unknown maintenance objects and maintenance scheme scenarios. To validate the
proposed method's effectiveness in maintenance tasks, a maintenance scheme
dataset was constructed using objects from different fields. The experimental
results show that the accuracy of the maintenance schemes generated by the
proposed method reached 91.59%, indicating which improvement enhances the
intelligence of maintenance schemes and introduces novel technical approaches
for equipment maintenance.",Laifa Tao
2024-11-07T18:29:38Z,http://arxiv.org/abs/2411.04952v1,"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page
  Multi-document Understanding","Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.",Jaemin Cho
2024-11-07T21:10:39Z,http://arxiv.org/abs/2411.05185v1,PentestAgent: Incorporating LLM Agents to Automated Penetration Testing,"Penetration testing is a critical technique for identifying security
vulnerabilities, traditionally performed manually by skilled security
specialists. This complex process involves gathering information about the
target system, identifying entry points, exploiting the system, and reporting
findings. Despite its effectiveness, manual penetration testing is
time-consuming and expensive, often requiring significant expertise and
resources that many organizations cannot afford. While automated penetration
testing methods have been proposed, they often fall short in real-world
applications due to limitations in flexibility, adaptability, and
implementation.
  Recent advancements in large language models (LLMs) offer new opportunities
for enhancing penetration testing through increased intelligence and
automation. However, current LLM-based approaches still face significant
challenges, including limited penetration testing knowledge and a lack of
comprehensive automation capabilities. To address these gaps, we propose
PentestAgent, a novel LLM-based automated penetration testing framework that
leverages the power of LLMs and various LLM-based techniques like Retrieval
Augmented Generation (RAG) to enhance penetration testing knowledge and
automate various tasks. Our framework leverages multi-agent collaboration to
automate intelligence gathering, vulnerability analysis, and exploitation
stages, reducing manual intervention. We evaluate PentestAgent using a
comprehensive benchmark, demonstrating superior performance in task completion
and overall efficiency. This work significantly advances the practical
applicability of automated penetration testing systems.",Xiangmin Shen
2024-11-08T09:40:53Z,http://arxiv.org/abs/2411.05442v1,"IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge
  Delivery","In the rapidly evolving landscape of cyber security, intelligent chatbots are
gaining prominence. Artificial Intelligence, Machine Learning, and Natural
Language Processing empower these chatbots to handle user inquiries and deliver
threat intelligence. This helps cyber security knowledge readily available to
both professionals and the public. Traditional rule-based chatbots often lack
flexibility and struggle to adapt to user interactions. In contrast, Large
Language Model-based chatbots offer contextually relevant information across
multiple domains and adapt to evolving conversational contexts. In this work,
we develop IntellBot, an advanced cyber security Chatbot built on top of
cutting-edge technologies like Large Language Models and Langchain alongside a
Retrieval-Augmented Generation model to deliver superior capabilities. This
chatbot gathers information from diverse data sources to create a comprehensive
knowledge base covering known vulnerabilities, recent cyber attacks, and
emerging threats. It delivers tailored responses, serving as a primary hub for
cyber security insights. By providing instant access to relevant information
and resources, this IntellBot enhances threat intelligence, incident response,
and overall security posture, saving time and empowering users with knowledge
of cyber security best practices. Moreover, we analyzed the performance of our
copilot using a two-stage evaluation strategy. We achieved BERT score above 0.8
by indirect approach and a cosine similarity score ranging from 0.8 to 1, which
affirms the accuracy of our copilot. Additionally, we utilized RAGAS to
evaluate the RAG model, and all evaluation metrics consistently produced scores
above 0.77, highlighting the efficacy of our system.",Dincy R. Arikkat
2024-11-09T15:12:28Z,http://arxiv.org/abs/2411.06207v1,"Exploring Knowledge Boundaries in Large Language Models for Retrieval
  Judgment","Large Language Models (LLMs) are increasingly recognized for their practical
applications. However, these models often encounter challenges in dynamically
changing knowledge, as well as in managing unknown static knowledge.
Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a
significant impact on LLMs. Actually, we find that the impact of RAG on the
question answering capabilities of LLMs can be categorized into three groups:
beneficial, neutral, and harmful. By minimizing retrieval requests that yield
neutral or harmful results, we can effectively reduce both time and
computational costs, while also improving the overall performance of LLMs. This
insight motivates us to differentiate between types of questions using certain
metrics as indicators, to decrease the retrieval ratio without compromising
performance. In our work, we propose a method that is able to identify
different types of questions from this view by training a Knowledge Boundary
Model (KBM). Experiments conducted on 11 English and Chinese datasets
illustrate that the KBM effectively delineates the knowledge boundary,
significantly decreasing the proportion of retrievals required for optimal
end-to-end performance. Specifically, we evaluate the effectiveness of KBM in
three complex scenarios: dynamic knowledge, long-tail static knowledge, and
multi-hop problems, as well as its functionality as an external LLM plug-in.",Zhen Zhang
2024-11-11T14:25:37Z,http://arxiv.org/abs/2411.07021v2,Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation,"Retrieval-augmented generation (RAG) has shown impressive capability in
providing reliable answer predictions and addressing hallucination problems. A
typical RAG implementation uses powerful retrieval models to extract external
information and large language models (LLMs) to generate answers. In contrast,
recent LLM-based retrieval has gained attention for its substantial
improvements in information retrieval (IR) due to the LLMs' semantic
understanding capability. However, directly applying LLM to RAG systems
presents challenges. This may cause feature locality problems as massive
parametric knowledge can hinder effective usage of global information across
the corpus; for example, an LLM-based retriever often inputs document summaries
instead of full documents. Moreover, various pre-trained tasks in LLMs
introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning
architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever
is constructed by integrating LoRA-based representation learning to tackle
feature locality issues. To enhance retrieval performance, we develop two
patterns (invariant and variant patterns) and an invariance loss to reduce LLM
variance. In the generation stage, a refined fine-tuning method is employed to
improve LLM accuracy in generating answers based on retrieved information.
Experimental results show that Invar-RAG significantly outperforms existing
baselines across three open-domain question answering (ODQA) datasets. Code is
available in the Supplementary Material for reproducibility.",Ziwei Liu
2024-11-11T16:12:11Z,http://arxiv.org/abs/2411.07091v1,"Impact of LLM-based Review Comment Generation in Practice: A Mixed
  Open-/Closed-source User Study","We conduct a large-scale empirical user study in a live setup to evaluate the
acceptance of LLM-generated comments and their impact on the review process.
This user study was performed in two organizations, Mozilla (which has its
codebase available as open source) and Ubisoft (fully closed-source). Inside
their usual review environment, participants were given access to RevMate, an
LLM-based assistive tool suggesting generated review comments using an
off-the-shelf LLM with Retrieval Augmented Generation to provide extra code and
review context, combined with LLM-as-a-Judge, to auto-evaluate the generated
comments and discard irrelevant cases. Based on more than 587 patch reviews
provided by RevMate, we observed that 8.1% and 7.2%, respectively, of
LLM-generated comments were accepted by reviewers in each organization, while
14.6% and 20.5% other comments were still marked as valuable as review or
development tips. Refactoring-related comments are more likely to be accepted
than Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra
time spent by reviewers to inspect generated comments or edit accepted ones
(36/119), yielding an overall median of 43s per patch, is reasonable. The
accepted generated comments are as likely to yield future revisions of the
revised patch as human-written comments (74% vs 73% at chunk-level).",Doriane Olewicki
2024-11-12T10:12:12Z,http://arxiv.org/abs/2411.07688v1,"Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with
  ImageRAG","Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000
$\times$ 100,000 pixels or more) poses a significant challenge for current
Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize
the UHR image to standard input image size, the extensive spatial and
contextual information that UHR images contain will be neglected. Otherwise,
the original size of these images often exceeds the token limits of standard
RSMLLMs, making it difficult to process the entire image and capture long-range
dependencies to answer the query based on the abundant visual context. In this
paper, we introduce ImageRAG for RS, a training-free framework to address the
complexities of analyzing UHR remote sensing imagery. By transforming UHR
remote sensing image analysis task to image's long context selection task, we
design an innovative image contextual retrieval mechanism based on the
Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's
core innovation lies in its ability to selectively retrieve and focus on the
most relevant portions of the UHR image as visual contexts that pertain to a
given query. Fast path and slow path are proposed in this framework to handle
this task efficiently and effectively. ImageRAG allows RSMLLMs to manage
extensive context and spatial information from UHR RSI, ensuring the analysis
is both accurate and efficient.",Zilun Zhang
2024-10-29T07:25:30Z,http://arxiv.org/abs/2411.08041v1,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured
and unstructured data is a major challenge in many real-world applications.
Pattern matching and querying represent fundamental tasks in modern data
analytics that leverage this curated knowledge. The development of such
applications necessitates overcoming several research challenges, including
data extraction, named entity recognition, data modeling, and designing query
interfaces. Moreover, the explainability of these functionalities is critical
for their broader adoption.
  The emergence of Large Language Models (LLMs) has accelerated the development
lifecycle of new capabilities. Nonetheless, there is an ongoing need for
domain-specific tools tailored to user activities. The creation of digital
assistants has gained considerable traction in recent years, with LLMs offering
a promising avenue to develop such assistants utilizing domain-specific
knowledge and assumptions.
  In this context, we introduce an advanced query and reasoning system,
GraphAide, which constructs a knowledge graph (KG) from diverse sources and
allows to query and reason over the resulting KG. GraphAide harnesses both the
KG and LLMs to rapidly develop domain-specific digital assistants. It
integrates design patterns from retrieval augmented generation (RAG) and the
semantic web to create an agentic LLM application. GraphAide underscores the
potential for streamlined and efficient development of specialized digital
assistants, thereby enhancing their applicability across various domains.",Sumit Purohit
2024-11-12T19:55:07Z,http://arxiv.org/abs/2411.08148v1,"Adaptive Meta-Learning for Robust Deepfake Detection: A Multi-Agent
  Framework to Data Drift and Model Generalization","Pioneering advancements in artificial intelligence, especially in genAI, have
enabled significant possibilities for content creation, but also led to
widespread misinformation and false content. The growing sophistication and
realism of deepfakes is raising concerns about privacy invasion, identity
theft, and has societal, business impacts, including reputational damage and
financial loss. Many deepfake detectors have been developed to tackle this
problem. Nevertheless, as for every AI model, the deepfake detectors face the
wrath of lack of considerable generalization to unseen scenarios and
cross-domain deepfakes. Besides, adversarial robustness is another critical
challenge, as detectors drastically underperform to the slightest imperceptible
change. Most state-of-the-art detectors are trained on static datasets and lack
the ability to adapt to emerging deepfake attack trends. These three crucial
challenges though hold paramount importance for reliability in practise,
particularly in the deepfake domain, are also the problems with any other AI
application. This paper proposes an adversarial meta-learning algorithm using
task-specific adaptive sample synthesis and consistency regularization, in a
refinement phase. By focussing on the classifier's strengths and weaknesses, it
boosts both robustness and generalization of the model. Additionally, the paper
introduces a hierarchical multi-agent retrieval-augmented generation workflow
with a sample synthesis module to dynamically adapt the model to new data
trends by generating custom deepfake samples. The paper further presents a
framework integrating the meta-learning algorithm with the hierarchical
multi-agent workflow, offering a holistic solution for enhancing
generalization, robustness, and adaptability. Experimental results demonstrate
the model's consistent performance across various datasets, outperforming the
models in comparison.",Dinesh Srivasthav P
2024-11-14T08:12:36Z,http://arxiv.org/abs/2411.09269v1,"Harnessing multiple LLMs for Information Retrieval: A case study on Deep
  Learning methodologies in Biodiversity publications","Deep Learning (DL) techniques are increasingly applied in scientific studies
across various domains to address complex research questions. However, the
methodological details of these DL models are often hidden in the unstructured
text. As a result, critical information about how these models are designed,
trained, and evaluated is challenging to access and comprehend. To address this
issue, in this work, we use five different open-source Large Language Models
(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,
and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)
approach to extract and process DL methodological details from scientific
publications automatically. We built a voting classifier from the outputs of
five LLMs to accurately report DL methodological information. We tested our
approach using biodiversity publications, building upon our previous research.
To validate our pipeline, we employed two datasets of DL-related biodiversity
publications: a curated set of 100 publications from our prior work and a set
of 364 publications from the Ecological Informatics journal. Our results
demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of
DL methodological information, achieving an accuracy of 69.5% (417 out of 600
comparisons) based solely on textual content from publications. This
performance was assessed against human annotators who had access to code,
figures, tables, and other supplementary information. Although demonstrated in
biodiversity, our methodology is not limited to this field; it can be applied
across other scientific domains where detailed methodological reporting is
essential for advancing knowledge and ensuring reproducibility. This study
presents a scalable and reliable approach for automating information
extraction, facilitating better reproducibility and knowledge transfer across
studies.",Vamsi Krishna Kommineni
2024-11-14T17:25:43Z,http://arxiv.org/abs/2411.09607v1,"Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the
  AutoNuggetizer Framework","This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on ""refactoring"" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.",Ronak Pradeep
2024-11-16T20:18:57Z,http://arxiv.org/abs/2411.10878v1,"Empowering Meta-Analysis: Leveraging Large Language Models for
  Scientific Synthesis","This study investigates the automation of meta-analysis in scientific
documents using large language models (LLMs). Meta-analysis is a robust
statistical method that synthesizes the findings of multiple studies support
articles to provide a comprehensive understanding. We know that a meta-article
provides a structured analysis of several articles. However, conducting
meta-analysis by hand is labor-intensive, time-consuming, and susceptible to
human error, highlighting the need for automated pipelines to streamline the
process. Our research introduces a novel approach that fine-tunes the LLM on
extensive scientific datasets to address challenges in big data handling and
structured data extraction. We automate and optimize the meta-analysis process
by integrating Retrieval Augmented Generation (RAG). Tailored through prompt
engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for
fine-tuning on large contextual datasets, LLMs efficiently generate structured
meta-analysis content. Human evaluation then assesses relevance and provides
information on model performance in key metrics. This research demonstrates
that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs
generating 87.6% relevant meta-analysis abstracts. The relevance of the
context, based on human evaluation, shows a reduction in irrelevancy from 4.56%
to 1.9%. These experiments were conducted in a low-resource environment,
highlighting the study's contribution to enhancing the efficiency and
reliability of meta-analysis automation.",Jawad Ibn Ahad
2024-11-17T00:09:04Z,http://arxiv.org/abs/2411.10918v1,"LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems
  Anomaly Detection","Modern industrial infrastructures rely heavily on Cyber-Physical Systems
(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic
effects. To reduce these risks, anomaly detection methods based on physical
invariants have been developed. However, these methods often require
domain-specific expertise to manually define invariants, making them costly and
difficult to scale. To address this limitation, we propose a novel approach to
extract physical invariants from CPS testbeds for anomaly detection. Our
insight is that CPS design documentation often contains semantically rich
descriptions of physical procedures, which can profile inter-correlated
dynamics among system components. Leveraging the built-in physics and
engineering knowledge of recent generative AI models, we aim to automate this
traditionally manual process, improving scalability and reducing costs. This
work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)
workflow with a customized prompting system tailored for CPS documentation,
enabling accurate extraction of semantic information and inference of physical
invariants from complex, multimodal content. Then, rather than directly
applying the inferred invariants for anomaly detection, we introduce an
innovative statistics-based learning approach that integrates these invariants
into the training dataset. This method addresses limitations such as
hallucination and concept drift, enhancing the reliability of the model. We
evaluate our approach on real-world public CPS security dataset which contains
86 data points and 58 attacking cases. The results show that our approach
achieves a high precision of 0.923, accurately detecting anomalies while
minimizing false alarms.",Danial Abshari
2024-11-17T10:26:25Z,http://arxiv.org/abs/2411.11033v1,"REACCEPT: Automated Co-evolution of Production and Test Code Based on
  Dynamic Validation and Large Language Models","Synchronizing production and test code, known as PT co-evolution, is critical
for software quality in the software development lifecycle. Existing methods
for automatic PT co-evolution either utilize predefined heuristic rules or rely
on simple application of machine learning techniques. Due to the limitations of
underlying techniques, existing methods either only partially automate PT
co-evolution (e.g., only automate obsolete test code identification) or result
in low accuracy.
  In this paper, we propose REACCEPT, a novel approach that leverages large
language models and dynamic validation to fully automate PT co-evolution (i.e.,
capable of both identifying and updating obsolete test cases). REACCEPT relies
on experience-based prompt template generation, dynamic validation, and
retrieval-augmented generation techniques to accomplish automated PT
co-evolution. To evaluate REACCEPT's effectiveness, we extensive experiments
with a dataset of 537 Java projects and compared REACCEPT's performance with
several state-of-the-art methods. Results show that REACCEPT achieved an update
accuracy of 60.16% on correctly identified obsolete test code, surpassing the
state-of-the-art technique CEPROT by 90%. This confirms that REACCEPT can
effectively assist developers in maintaining test code, improving overall
software quality and reducing maintenance effort.",Jianlei Chi
2024-11-17T14:45:52Z,http://arxiv.org/abs/2411.11090v1,"ForPKG-1.0: A Framework for Constructing Forestry Policy Knowledge Graph
  and Application Analysis","A policy knowledge graph can provide decision support for tasks such as
project compliance, policy analysis, and intelligent question answering, and
can also serve as an external knowledge base to assist the reasoning process of
related large language models. Although there have been many related works on
knowledge graphs, there is currently a lack of research on the construction
methods of policy knowledge graphs. This paper, focusing on the forestry field,
designs a complete policy knowledge graph construction framework, including:
firstly, proposing a fine-grained forestry policy domain ontology; then,
proposing an unsupervised policy information extraction method, and finally,
constructing a complete forestry policy knowledge graph. The experimental
results show that the proposed ontology has good expressiveness and
extensibility, and the policy information extraction method proposed in this
paper achieves better results than other unsupervised methods. Furthermore, by
analyzing the application of the knowledge graph in the
retrieval-augmented-generation task of the large language models, the practical
application value of the knowledge graph in the era of large language models is
confirmed. The knowledge graph resource will be released on an open-source
platform and can serve as the basic knowledge base for forestry policy-related
intelligent systems. It can also be used for academic research. In addition,
this study can provide reference and guidance for the construction of policy
knowledge graphs in other fields.",Jingyun Sun
2024-11-18T06:33:05Z,http://arxiv.org/abs/2411.11323v1,"SayComply: Grounding Field Robotic Tasks in Operational Compliance
  through Retrieval-Based Language Models","This paper addresses the problem of task planning for robots that must comply
with operational manuals in real-world settings. Task planning under these
constraints is essential for enabling autonomous robot operation in domains
that require adherence to domain-specific knowledge. Current methods for
generating robot goals and plans rely on common sense knowledge encoded in
large language models. However, these models lack grounding of robot plans to
domain-specific knowledge and are not easily transferable between multiple
sites or customers with different compliance needs. In this work, we present
SayComply, which enables grounding robotic task planning with operational
compliance using retrieval-based language models. We design a hierarchical
database of operational, environment, and robot embodiment manuals and
procedures to enable efficient retrieval of the relevant context under the
limited context length of the LLMs. We then design a task planner using a
tree-based retrieval augmented generation (RAG) technique to generate robot
tasks that follow user instructions while simultaneously complying with the
domain knowledge in the database. We demonstrate the benefits of our approach
through simulations and hardware experiments in real-world scenarios that
require precise context retrieval across various types of context,
outperforming the standard RAG method. Our approach bridges the gap in
deploying robots that consistently adhere to operational protocols, offering a
scalable and edge-deployable solution for ensuring compliance across varied and
complex real-world environments. Project website: saycomply.github.io.",Muhammad Fadhil Ginting
2024-11-17T23:20:37Z,http://arxiv.org/abs/2411.11913v1,"On-Board Vision-Language Models for Personalized Autonomous Vehicle
  Motion Control: System Design and Real-World Validation","Personalized driving refers to an autonomous vehicle's ability to adapt its
driving behavior or control strategies to match individual users' preferences
and driving styles while maintaining safety and comfort standards. However,
existing works either fail to capture every individual preference precisely or
become computationally inefficient as the user base expands. Vision-Language
Models (VLMs) offer promising solutions to this front through their natural
language understanding and scene reasoning capabilities. In this work, we
propose a lightweight yet effective on-board VLM framework that provides
low-latency personalized driving performance while maintaining strong reasoning
capabilities. Our solution incorporates a Retrieval-Augmented Generation
(RAG)-based memory module that enables continuous learning of individual
driving preferences through human feedback. Through comprehensive real-world
vehicle deployment and experiments, our system has demonstrated the ability to
provide safe, comfortable, and personalized driving experiences across various
scenarios and significantly reduce takeover rates by up to 76.9%. To the best
of our knowledge, this work represents the first end-to-end VLM-based motion
control system in real-world autonomous vehicles.",Can Cui
2024-11-18T21:43:52Z,http://arxiv.org/abs/2411.12078v1,Molecule Generation with Fragment Retrieval Augmentation,"Fragment-based drug discovery, in which molecular fragments are assembled
into new molecules with desirable biochemical properties, has achieved great
success. However, many fragment-based molecule generation methods show limited
exploration beyond the existing fragments in the database as they only
reassemble or slightly modify the given ones. To tackle this problem, we
propose a new fragment-based molecule generation framework with retrieval
augmentation, namely Fragment Retrieval-Augmented Generation (f-RAG). f-RAG is
based on a pre-trained molecular generative model that proposes additional
fragments from input fragments to complete and generate a new molecule. Given a
fragment vocabulary, f-RAG retrieves two types of fragments: (1) hard
fragments, which serve as building blocks that will be explicitly included in
the newly generated molecule, and (2) soft fragments, which serve as reference
to guide the generation of new fragments through a trainable fragment injection
module. To extrapolate beyond the existing fragments, f-RAG updates the
fragment vocabulary with generated fragments via an iterative refinement
process which is further enhanced with post-hoc genetic fragment modification.
f-RAG can achieve an improved exploration-exploitation trade-off by maintaining
a pool of fragments and expanding it with novel and high-quality fragments
through a strong generative prior.",Seul Lee
2024-11-19T12:17:43Z,http://arxiv.org/abs/2411.12449v2,Neon: News Entity-Interaction Extraction for Enhanced Question Answering,"Capturing fresh information in near real-time and using it to augment
existing large language models (LLMs) is essential to generate up-to-date,
grounded, and reliable output. This problem becomes particularly challenging
when LLMs are used for informational tasks in rapidly evolving fields, such as
Web search related to recent or unfolding events involving entities, where
generating temporally relevant responses requires access to up-to-the-hour news
sources. However, the information modeled by the parametric memory of LLMs is
often outdated, and Web results from prototypical retrieval systems may fail to
capture the latest relevant information and struggle to handle conflicting
reports in evolving news. To address this challenge, we present the NEON
framework, designed to extract emerging entity interactions -- such as events
or activities -- as described in news articles. NEON constructs an
entity-centric timestamped knowledge graph that captures such interactions,
thereby facilitating enhanced QA capabilities related to news events. Our
framework innovates by integrating open Information Extraction (openIE) style
tuples into LLMs to enable in-context retrieval-augmented generation. This
integration demonstrates substantial improvements in QA performance when
tackling temporal, entity-centric search queries. Through NEON, LLMs can
deliver more accurate, reliable, and up-to-date responses.",Sneha Singhania
2024-11-20T07:44:34Z,http://arxiv.org/abs/2411.13093v3,Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension,"Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.",Yongdong Luo
2024-11-20T10:17:09Z,http://arxiv.org/abs/2411.13173v2,"Writing Style Matters: An Examination of Bias and Fairness in
  Information Retrieval Systems","The rapid advancement of Language Model technologies has opened new
opportunities, but also introduced new challenges related to bias and fairness.
This paper explores the uncharted territory of potential biases in
state-of-the-art universal text embedding models towards specific document and
query writing styles within Information Retrieval (IR) systems. Our
investigation reveals that different embedding models exhibit different
preferences of document writing style, while more informal and emotive styles
are less favored by most embedding models. In terms of query writing styles,
many embedding models tend to match the style of the query with the style of
the retrieved documents, but some show a consistent preference for specific
styles. Text embedding models fine-tuned on synthetic data generated by LLMs
display a consistent preference for certain style of generated data. These
biases in text embedding based IR systems can inadvertently silence or
marginalize certain communication styles, thereby posing a significant threat
to fairness in information retrieval. Finally, we also compare the answer
styles of Retrieval Augmented Generation (RAG) systems based on different LLMs
and find out that most text embedding models are biased towards LLM's answer
styles when used as evaluation metrics for answer correctness. This study sheds
light on the critical issue of writing style based bias in IR systems, offering
valuable insights for the development of more fair and robust models.",Hongliu Cao
2024-11-17T07:32:46Z,http://arxiv.org/abs/2411.13584v1,"AddrLLM: Address Rewriting via Large Language Model on Nationwide
  Logistics Data","Textual description of a physical location, commonly known as an address,
plays an important role in location-based services(LBS) such as on-demand
delivery and navigation. However, the prevalence of abnormal addresses, those
containing inaccuracies that fail to pinpoint a location, have led to
significant costs. Address rewriting has emerged as a solution to rectify these
abnormal addresses. Despite the critical need, existing address rewriting
methods are limited, typically tailored to correct specific error types, or
frequently require retraining to process new address data effectively. In this
study, we introduce AddrLLM, an innovative framework for address rewriting that
is built upon a retrieval augmented large language model. AddrLLM overcomes
aforementioned limitations through a meticulously designed Supervised
Fine-Tuning module, an Address-centric Retrieval Augmented Generation module
and a Bias-free Objective Alignment module. To the best of our knowledge, this
study pioneers the application of LLM-based address rewriting approach to solve
the issue of abnormal addresses. Through comprehensive offline testing with
real-world data on a national scale and subsequent online deployment, AddrLLM
has demonstrated superior performance in integration with existing logistics
system. It has significantly decreased the rate of parcel re-routing by
approximately 43\%, underscoring its exceptional efficacy in real-world
applications.",Qinchen Yang
2024-11-21T15:28:52Z,http://arxiv.org/abs/2411.14219v1,"Towards Context-Rich Automated Biodiversity Assessments: Deriving
  AI-Powered Insights from Camera Trap Data","Camera traps offer enormous new opportunities in ecological studies, but
current automated image analysis methods often lack the contextual richness
needed to support impactful conservation outcomes. Here we present an
integrated approach that combines deep learning-based vision and language
models to improve ecological reporting using data from camera traps. We
introduce a two-stage system: YOLOv10-X to localise and classify species
(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read
YOLOv10-X binding box labels to identify species, overcoming its limitation
with hard to classify objects in images. Additionally, Phi-3.5 detects broader
variables, such as vegetation type, and time of day, providing rich ecological
and environmental context to YOLO's species detection output. When combined,
this output is processed by the model's natural language system to answer
complex queries, and retrieval-augmented generation (RAG) is employed to enrich
responses with external information, like species weight and IUCN status
(information that cannot be obtained through direct visual analysis). This
information is used to automatically generate structured reports, providing
biodiversity stakeholders with deeper insights into, for example, species
abundance, distribution, animal behaviour, and habitat selection. Our approach
delivers contextually rich narratives that aid in wildlife management
decisions. By providing contextually rich insights, our approach not only
reduces manual effort but also supports timely decision-making in conservation,
potentially shifting efforts from reactive to proactive management.",Paul Fergus
2024-11-21T16:28:32Z,http://arxiv.org/abs/2411.14272v1,"Efficient Aspect-Based Summarization of Climate Change Reports with
  Small Language Models","The use of Natural Language Processing (NLP) for helping decision-makers with
Climate Change action has recently been highlighted as a use case aligning with
a broader drive towards NLP technologies for social good. In this context,
Aspect-Based Summarization (ABS) systems that extract and summarize relevant
information are particularly useful as they provide stakeholders with a
convenient way of finding relevant information in expert-curated reports. In
this work, we release a new dataset for ABS of Climate Change reports and we
employ different Large Language Models (LLMs) and so-called Small Language
Models (SLMs) to tackle this problem in an unsupervised way. Considering the
problem at hand, we also show how SLMs are not significantly worse for the
problem while leading to reduced carbon footprint; we do so by applying for the
first time an existing framework considering both energy efficiency and task
performance to the evaluation of zero-shot generative models for ABS. Overall,
our results show that modern language models, both big and small, can
effectively tackle ABS for Climate Change reports but more research is needed
when we frame the problem as a Retrieval Augmented Generation (RAG) problem and
our work and dataset will help foster efforts in this direction.",Iacopo Ghinassi
2024-11-22T08:21:03Z,http://arxiv.org/abs/2411.14790v3,KBAlign: Efficient Self Adaptation on Specific Knowledge Bases,"Humans can utilize techniques to quickly acquire knowledge from specific
materials in advance, such as creating self-assessment questions, enabling us
to achieving related tasks more efficiently. In contrast, large language models
(LLMs) usually relies on retrieval-augmented generation to exploit knowledge
materials in an instant manner, or requires external signals such as human
preference data and stronger LLM annotations to conduct knowledge adaptation.
To unleash the self-learning potential of LLMs, we propose KBAlign, an approach
designed for efficient adaptation to downstream tasks involving knowledge
bases. Our method utilizes iterative training with self-annotated data such as
Q&A pairs and revision suggestions, enabling the model to grasp the knowledge
content efficiently. Experimental results on multiple datasets demonstrate the
effectiveness of our approach, significantly boosting model performance in
downstream tasks that require specific knowledge at a low cost. Notably, our
approach achieves over 90% of the performance improvement that can be obtained
by using GPT-4-turbo annotation, while relying entirely on self-supervision. We
release our experimental data, models, and process analyses to the community
for further exploration (https://github.com/thunlp/KBAlign).",Zheni Zeng
2024-11-22T16:15:50Z,http://arxiv.org/abs/2411.15041v1,"mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for
  Knowledge-Based VQA","Advanced Multimodal Large Language Models (MLLMs) struggle with recent
Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their
limited and frozen knowledge scope, often leading to ambiguous and inaccurate
responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally
introduced to provide MLLMs with comprehensive and up-to-date knowledge,
effectively expanding the knowledge scope. However, current mRAG methods have
inherent drawbacks, including: 1) Performing retrieval even when external
knowledge is not needed. 2) Lacking of identification of evidence that supports
the query. 3) Increasing model complexity due to additional information
filtering modules or rules. To address these shortcomings, we propose a novel
generalized framework called \textbf{m}ultimodal
\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration
(mR$^2$AG), which achieves adaptive retrieval and useful information
localization to enable answers through two easy-to-implement reflection
operations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection
is designed to distinguish different user queries and avoids redundant
retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in
locating beneficial evidence of the retrieved content and generating answers
accordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM
with efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset
(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,
GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while
maintaining the exceptional capabilities of base MLLMs across a wide range of
Visual-dependent tasks.",Tao Zhang
2024-11-20T04:47:42Z,http://arxiv.org/abs/2411.15203v1,"Multimodal large language model for wheat breeding: a new exploration of
  smart breeding","UAV remote sensing technology has become a key technology in crop breeding,
which can achieve high-throughput and non-destructive collection of crop
phenotyping data. However, the multidisciplinary nature of breeding has brought
technical barriers and efficiency challenges to knowledge mining. Therefore, it
is important to develop a smart breeding goal tool to mine cross-domain
multimodal data. Based on different pre-trained open-source multimodal large
language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used
supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and
reinforcement learning from human feedback (RLHF) technologies to inject
cross-domain knowledge into MLLMs, thereby constructing multiple multimodal
large language models for wheat breeding (WBLMs). The above WBLMs were
evaluated using the newly created evaluation benchmark in this study. The
results showed that the WBLM constructed using SFT, RAG and RLHF technologies
and InternVL2-8B has leading performance. Then, subsequent experiments were
conducted using the WBLM. Ablation experiments indicated that the combination
of SFT, RAG, and RLHF technologies can improve the overall generation
performance, enhance the generated quality, balance the timeliness and
adaptability of the generated answer, and reduce hallucinations and biases. The
WBLM performed best in wheat yield prediction using cross-domain data (remote
sensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of
0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate
professional decision support answers for phenotyping estimation, environmental
stress assessment, target germplasm screening, cultivation technique
recommendation, and seed price query tasks.",Guofeng Yang
2024-11-24T03:56:43Z,http://arxiv.org/abs/2411.15700v1,"RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large
  Language Models on Dietary Supplements","\textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.",Zaifu Zhan
2024-11-25T15:35:51Z,http://arxiv.org/abs/2411.16495v2,"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous
  Knowledge Reasoning","Recent advancements in large language models (LLMs) have led to significant
improvements in various natural language processing tasks, but it is still
challenging for LLMs to perform knowledge-intensive complex question answering
due to LLMs' inefficacy in reasoning planning and the hallucination problem. A
typical solution is to employ retrieval-augmented generation (RAG) coupled with
chain-of-thought (CoT) reasoning, which decomposes complex questions into
chain-like sub-questions and applies iterative RAG at each sub-question.
However, prior works exhibit sub-optimal reasoning planning and overlook
dynamic knowledge retrieval from heterogeneous sources. In this paper, we
propose AtomR, a novel heterogeneous knowledge reasoning framework that
conducts multi-source reasoning at the atomic level. Drawing inspiration from
the graph modeling of knowledge, AtomR leverages large language models (LLMs)
to decompose complex questions into combinations of three atomic knowledge
operators, significantly enhancing the reasoning process at both the planning
and execution stages. We also introduce BlendQA, a novel evaluation benchmark
tailored to assess complex heterogeneous knowledge reasoning. Experiments show
that AtomR significantly outperforms state-of-the-art baselines across three
single-source and two multi-source reasoning benchmarks, with notable
performance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",Amy Xin
2024-11-23T18:14:42Z,http://arxiv.org/abs/2411.16740v3,"Document Haystacks: Vision-Language Reasoning Over Piles of 1000+
  Documents","Large multimodal models (LMMs) have achieved impressive progress in
vision-language understanding, yet they face limitations in real-world
applications requiring complex reasoning over a large number of images.
Existing benchmarks for multi-image question-answering are limited in scope,
each question is paired with only up to 30 images, which does not fully capture
the demands of large-scale retrieval tasks encountered in the real-world
usages. To reduce these gaps, we introduce two document haystack benchmarks,
dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on
large-scale visual document retrieval and understanding. Additionally, we
propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)
framework that leverages a suite of multimodal vision encoders, each optimized
for specific strengths, and a dedicated question-document relevance module.
V-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the
challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,
compared to the previous best baseline models. Additionally, integrating V-RAG
with LMMs enables them to efficiently operate across thousands of images,
yielding significant improvements on our DocHaystack and InfoHaystack
benchmarks. Our code and datasets are available at
https://github.com/Vision-CAIR/dochaystacks",Jun Chen
2024-11-28T06:29:46Z,http://arxiv.org/abs/2411.18948v1,"Knowledge Database or Poison Base? Detecting RAG Poisoning Attack
  through LLM Activations","As Large Language Models (LLMs) are progressively deployed across diverse
fields and real-world applications, ensuring the security and robustness of
LLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a
cutting-edge approach designed to address the limitations of large language
models (LLMs). By retrieving information from the relevant knowledge database,
RAG enriches the input to LLMs, enabling them to produce responses that are
more accurate and contextually appropriate. It is worth noting that the
knowledge database, being sourced from publicly available channels such as
Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves
injecting malicious texts into the knowledge database, ultimately leading to
the generation of the attacker's target response (also called poisoned
response). However, there are currently limited methods available for detecting
such poisoning attacks. We aim to bridge the gap in this work. Particularly, we
introduce RevPRAG, a flexible and automated detection pipeline that leverages
the activations of LLMs for poisoned response detection. Our investigation
uncovers distinct patterns in LLMs' activations when generating correct
responses versus poisoned responses. Our results on multiple benchmark datasets
and RAG architectures show our approach could achieve 98% true positive rate,
while maintaining false positive rates close to 1%. We also evaluate recent
backdoor detection methods specifically designed for LLMs and applicable for
identifying poisoned responses in RAG. The results demonstrate that our
approach significantly surpasses them.",Xue Tan
2024-11-28T11:24:43Z,http://arxiv.org/abs/2411.19064v1,"Way to Specialist: Closing Loop Between Specialized LLM and Evolving
  Domain Knowledge Graph","Large language models (LLMs) have demonstrated exceptional performance across
a wide variety of domains. Nonetheless, generalist LLMs continue to fall short
in reasoning tasks necessitating specialized knowledge. Prior investigations
into specialized LLMs focused on domain-specific training, which entails
substantial efforts in domain data acquisition and model parameter fine-tuning.
To address these challenges, this paper proposes the Way-to-Specialist (WTS)
framework, which synergizes retrieval-augmented generation with knowledge
graphs (KGs) to enhance the specialized capability of LLMs in the absence of
specialized training. In distinction to existing paradigms that merely utilize
external knowledge from general KGs or static domain KGs to prompt LLM for
enhanced domain-specific reasoning, WTS proposes an innovative
""LLM$\circlearrowright$KG"" paradigm, which achieves bidirectional enhancement
between specialized LLM and domain knowledge graph (DKG). The proposed paradigm
encompasses two closely coupled components: the DKG-Augmented LLM and the
LLM-Assisted DKG Evolution. The former retrieves question-relevant domain
knowledge from DKG and uses it to prompt LLM to enhance the reasoning
capability for domain-specific tasks; the latter leverages LLM to generate new
domain knowledge from processed tasks and use it to evolve DKG. WTS closes the
loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling
continuous improvement in the domain specialization as it progressively answers
and learns from domain-specific questions. We validate the performance of WTS
on 6 datasets spanning 5 domains. The experimental results show that WTS
surpasses the previous SOTA in 4 specialized domains and achieves a maximum
performance improvement of 11.3%.",Yutong Zhang
2024-11-28T15:53:27Z,http://arxiv.org/abs/2411.19229v2,Habit Coach: Customising RAG-based chatbots to support behavior change,"This paper presents the iterative development of Habit Coach, a GPT-based
chatbot designed to support users in habit change through personalized
interaction. Employing a user-centered design approach, we developed the
chatbot using a Retrieval-Augmented Generation (RAG) system, which enables
behavior personalization without retraining the underlying language model
(GPT-4). The system leverages document retrieval and specialized prompts to
tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and
narrative therapy techniques. A key challenge in the development process was
the difficulty of translating declarative knowledge into effective interaction
behaviors. In the initial phase, the chatbot was provided with declarative
knowledge about CBT via reference textbooks and high-level conversational
goals. However, this approach resulted in imprecise and inefficient behavior,
as the GPT model struggled to convert static information into dynamic and
contextually appropriate interactions. This highlighted the limitations of
relying solely on declarative knowledge to guide chatbot behavior, particularly
in nuanced, therapeutic conversations. Over four iterations, we addressed this
issue by gradually transitioning towards procedural knowledge, refining the
chatbot's interaction strategies, and improving its overall effectiveness. In
the final evaluation, 5 participants engaged with the chatbot over five
consecutive days, receiving individualized CBT interventions. The Self-Report
Habit Index (SRHI) was used to measure habit strength before and after the
intervention, revealing a reduction in habit strength post-intervention. These
results underscore the importance of procedural knowledge in driving effective,
personalized behavior change support in RAG-based systems.",Arian Fooroogh Mand Arabi
2024-11-29T05:31:04Z,http://arxiv.org/abs/2411.19478v1,"Zero-Indexing Internet Search Augmented Generation for Large Language
  Models","Retrieval augmented generation has emerged as an effective method to enhance
large language model performance. This approach typically relies on an internal
retrieval module that uses various indexing mechanisms to manage a static
pre-processed corpus. However, such a paradigm often falls short when it is
necessary to integrate the most up-to-date information that has not been
updated into the corpus during generative inference time. In this paper, we
explore an alternative approach that leverages standard search engine APIs to
dynamically integrate the latest online information (without maintaining any
index for any fixed corpus), thereby improving the quality of generated
content. We design a collaborative LLM-based paradigm, where we include: (i) a
parser-LLM that determines if the Internet augmented generation is demanded and
extracts the search keywords if so with a single inference; (ii) a mixed
ranking strategy that re-ranks the retrieved HTML files to eliminate bias
introduced from the search engine API; and (iii) an extractor-LLM that can
accurately and efficiently extract relevant information from the fresh content
in each HTML file. We conduct extensive empirical studies to evaluate the
performance of this Internet search augmented generation paradigm. The
experimental results demonstrate that our method generates content with
significantly improved quality. Our system has been successfully deployed in a
production environment to serve 01.AI's generative inference requests.",Guangxin He
2024-11-29T07:57:32Z,http://arxiv.org/abs/2411.19528v1,"RAGDiffusion: Faithful Cloth Generation via External Knowledge
  Assimilation","Standard clothing asset generation involves creating forward-facing flat-lay
garment images displayed on a clear background by extracting clothing
information from diverse real-world contexts, which presents significant
challenges due to highly standardized sampling distributions and precise
structural requirements in the generated images. Existing models have limited
spatial perception and often exhibit structural hallucinations in this
high-specification generative task. To address this issue, we propose a novel
Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance
structure determinacy and mitigate hallucinations by assimilating external
knowledge from LLM and databases. RAGDiffusion consists of two core processes:
(1) Retrieval-based structure aggregation, which employs contrastive learning
and a Structure Locally Linear Embedding (SLLE) to derive global structure and
spatial landmarks, providing both soft and hard guidance to counteract
structural ambiguities; and (2) Omni-level faithful garment generation, which
introduces a three-level alignment that ensures fidelity in structural,
pattern, and decoding components within the diffusing. Extensive experiments on
challenging real-world datasets demonstrate that RAGDiffusion synthesizes
structurally and detail-faithful clothing assets with significant performance
improvements, representing a pioneering effort in high-specification faithful
generation with RAG to confront intrinsic hallucinations and enhance fidelity.",Xianfeng Tan
2024-11-29T08:34:07Z,http://arxiv.org/abs/2411.19539v1,Knowledge Management for Automobile Failure Analysis Using Graph RAG,"This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.",Yuta Ojima
2024-11-29T09:07:21Z,http://arxiv.org/abs/2411.19554v1,"Unimib Assistant: designing a student-friendly RAG-based chatbot for all
  their needs","Natural language processing skills of Large Language Models (LLMs) are
unprecedented, having wide diffusion and application in different tasks. This
pilot study focuses on specializing ChatGPT behavior through a
Retrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs
feature. The purpose of our chatbot, called Unimib Assistant, is to provide
information and solutions to the specific needs of University of Milano-Bicocca
(Unimib) students through a question-answering approach. We provided the system
with a prompt highlighting its specific purpose and behavior, as well as
university-related documents and links obtained from an initial need-finding
phase, interviewing six students. After a preliminary customization phase, a
qualitative usability test was conducted with six other students to identify
the strengths and weaknesses of the chatbot, with the goal of improving it in a
subsequent redesign phase. While the chatbot was appreciated for its
user-friendly experience, perceived general reliability, well-structured
responses, and conversational tone, several significant technical and
functional limitations emerged. In particular, the satisfaction and overall
experience of the users was impaired by the system's inability to always
provide fully accurate information. Moreover, it would often neglect to report
relevant information even if present in the materials uploaded and prompt
given. Furthermore, it sometimes generated unclickable links, undermining its
trustworthiness, since providing the source of information was an important
aspect for our users. Further in-depth studies and feedback from other users as
well as implementation iterations are planned to refine our Unimib Assistant.",Chiara Antico
2024-11-29T13:57:07Z,http://arxiv.org/abs/2411.19710v1,"Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating
  RAG Systems","Retrieval Augmented Generation (RAG) systems are a widespread application of
Large Language Models (LLMs) in the industry. While many tools exist empowering
developers to build their own systems, measuring their performance locally,
with datasets reflective of the system's use cases, is a technological
challenge. Solutions to this problem range from non-specific and cheap (most
public datasets) to specific and costly (generating data from local documents).
In this paper, we show that using public question and answer (Q&A) datasets to
assess retrieval performance can lead to non-optimal systems design, and that
common tools for RAG dataset generation can lead to unbalanced data. We propose
solutions to these issues based on the characterization of RAG datasets through
labels and through label-targeted data generation. Finally, we show that
fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that
these observations are invaluable to the know-your-data step of RAG systems
development.",Rafael Teixeira de Lima
2024-11-29T20:13:56Z,http://arxiv.org/abs/2412.00239v1,Generating a Low-code Complete Workflow via Task Decomposition and RAG,"AI technologies are moving rapidly from research to production. With the
popularity of Foundation Models (FMs) that generate text, images, and video,
AI-based systems are increasing their complexity. Compared to traditional
AI-based software, systems employing FMs, or GenAI-based systems, are more
difficult to design due to their scale and versatility. This makes it necessary
to document best practices, known as design patterns in software engineering,
that can be used across GenAI applications. Our first contribution is to
formalize two techniques, Task Decomposition and Retrieval-Augmented Generation
(RAG), as design patterns for GenAI-based systems. We discuss their trade-offs
in terms of software quality attributes and comment on alternative approaches.
We recommend to AI practitioners to consider these techniques not only from a
scientific perspective but also from the standpoint of desired engineering
properties such as flexibility, maintainability, safety, and security. As a
second contribution, we describe our industry experience applying Task
Decomposition and RAG to build a complex real-world GenAI application for
enterprise users: Workflow Generation. The task of generating workflows entails
generating a specific plan using data from the system environment, taking as
input a user requirement. As these two patterns affect the entire AI
development cycle, we explain how they impacted the dataset creation, model
training, model evaluation, and deployment phases.",Orlando Marquez Ayala
2024-11-30T14:32:48Z,http://arxiv.org/abs/2412.00495v1,"Rethinking Strategic Mechanism Design In The Age Of Large Language
  Models: New Directions For Communication Systems","This paper explores the application of large language models (LLMs) in
designing strategic mechanisms -- including auctions, contracts, and games --
for specific purposes in communication networks. Traditionally, strategic
mechanism design in telecommunications has relied on human expertise to craft
solutions based on game theory, auction theory, and contract theory. However,
the evolving landscape of telecom networks, characterized by increasing
abstraction, emerging use cases, and novel value creation opportunities, calls
for more adaptive and efficient approaches. We propose leveraging LLMs to
automate or semi-automate the process of strategic mechanism design, from
intent specification to final formulation. This paradigm shift introduces both
semi-automated and fully-automated design pipelines, raising crucial questions
about faithfulness to intents, incentive compatibility, algorithmic stability,
and the balance between human oversight and artificial intelligence (AI)
autonomy. The paper discusses potential frameworks, such as retrieval-augmented
generation (RAG)-based systems, to implement LLM-driven mechanism design in
communication networks contexts. We examine key challenges, including LLM
limitations in capturing domain-specific constraints, ensuring strategy
proofness, and integrating with evolving telecom standards. By providing an
in-depth analysis of the synergies and tensions between LLMs and strategic
mechanism design within the IoT ecosystem, this work aims to stimulate
discussion on the future of AI-driven information economic mechanisms in
telecommunications and their potential to address complex, dynamic network
management scenarios.",Ismail Lotfi
2024-11-30T23:11:44Z,http://arxiv.org/abs/2412.00608v3,"Leveraging LLM for Automated Ontology Extraction and Knowledge Graph
  Generation","Extracting relevant and structured knowledge from large, complex technical
documents within the Reliability and Maintainability (RAM) domain is
labor-intensive and prone to errors. Our work addresses this challenge by
presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge
Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through
an interactive user interface guided by our adaptive iterative Chain of Thought
(CoT) algorithm to ensure that the ontology extraction process and, thus, KG
generation align with user-specific requirements. Although KG generation
follows a clear, structured path based on the confirmed ontology, there is no
universally correct ontology as it is inherently based on the user's
preferences. OntoKGen recommends an ontology grounded in best practices,
minimizing user effort and providing valuable insights that may have been
overlooked, all while giving the user complete control over the final ontology.
Having generated the KG based on the confirmed ontology, OntoKGen enables
seamless integration into schemeless, non-relational databases like Neo4j. This
integration allows for flexible storage and retrieval of knowledge from
diverse, unstructured sources, facilitating advanced querying, analysis, and
decision-making. Moreover, the generated KG serves as a robust foundation for
future integration into Retrieval Augmented Generation (RAG) systems, offering
enhanced capabilities for developing domain-specific intelligent applications.",Mohammad Sadeq Abolhasani
2024-12-01T15:19:23Z,http://arxiv.org/abs/2412.00846v1,"Improving Multimodal LLMs Ability In Geometry Problem Solving,
  Reasoning, And Multistep Scoring","This paper presents GPSM4K, a comprehensive geometry multimodal dataset
tailored to augment the problem-solving capabilities of Large Vision Language
Models (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs
manually extracted from mathematics textbooks spanning grades 7-12 and is
further augmented to 5340 problems, consisting of both numerical and
theorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which
feature only objective-type questions, GPSM4K offers detailed step-by-step
solutions in a consistent format, facilitating a comprehensive evaluation of
problem-solving approaches. This dataset serves as an excellent benchmark for
assessing the geometric reasoning capabilities of LVLMs. Evaluation of our test
set shows that there is scope for improvement needed in open-source language
models in geometry problem-solving. Finetuning on our training set increases
the geometry problem-solving capabilities of models. Further, We also evaluate
the effectiveness of techniques such as image captioning and Retrieval
Augmentation generation (RAG) on model performance. We leveraged LLM to
automate the task of final answer evaluation by providing ground truth and
predicted solutions. This research will help to assess and improve the
geometric reasoning capabilities of LVLMs.",Avinash Anand
2024-12-03T21:00:10Z,http://arxiv.org/abs/2412.02835v1,"CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural
  Networks","We present CAISSON, a novel hierarchical approach to Retrieval-Augmented
Generation (RAG) that transforms traditional single-vector search into a
multi-view clustering framework. At its core, CAISSON leverages dual
Self-Organizing Maps (SOMs) to create complementary organizational views of the
document space, where each view captures different aspects of document
relationships through specialized embeddings. The first view processes combined
text and metadata embeddings, while the second operates on metadata enriched
with concept embeddings, enabling a comprehensive multi-view analysis that
captures both fine-grained semantic relationships and high-level conceptual
patterns. This dual-view approach enables more nuanced document discovery by
combining evidence from different organizational perspectives. To evaluate
CAISSON, we develop SynFAQA, a framework for generating synthetic financial
analyst notes and question-answer pairs that systematically tests different
aspects of information retrieval capabilities. Drawing on HotPotQA's
methodology for constructing multi-step reasoning questions, SynFAQA generates
controlled test cases where each question is paired with the set of notes
containing its ground-truth answer, progressing from simple single-entity
queries to complex multi-hop retrieval tasks involving multiple entities and
concepts. Our experimental results demonstrate substantial improvements over
both basic and enhanced RAG implementations, particularly for complex
multi-entity queries, while maintaining practical response times suitable for
interactive applications.",Igor Halperin
2024-12-04T03:02:46Z,http://arxiv.org/abs/2412.02987v1,"Advancing Conversational Psychotherapy: Integrating Privacy,
  Dual-Memory, and Domain Expertise with Large Language Models","Mental health has increasingly become a global issue that reveals the
limitations of traditional conversational psychotherapy, constrained by
location, time, expense, and privacy concerns. In response to these challenges,
we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed
to democratize access to psychotherapy. SoulSpeak improves upon the
capabilities of standard LLM-enabled chatbots by incorporating a novel
dual-memory component that combines short-term and long-term context via
Retrieval Augmented Generation (RAG) to offer personalized responses while
ensuring the preservation of user privacy and intimacy through a dedicated
privacy module. In addition, it leverages a counseling chat dataset of
therapist-client interactions and various prompting techniques to align the
generated responses with psychotherapeutic methods. We introduce two fine-tuned
BERT models to evaluate the system against existing LLMs and human therapists:
the Conversational Psychotherapy Preference Model (CPPM) to simulate human
preference among responses and another to assess response relevance to user
input. CPPM is useful for training and evaluating psychotherapy-focused
language models independent from SoulSpeak, helping with the constrained
resources available for psychotherapy. Furthermore, the effectiveness of the
dual-memory component and the robustness of the privacy module are also
examined. Our findings highlight the potential and challenge of enhancing
mental health care by offering an alternative that combines the expertise of
traditional therapy with the advantages of LLMs, providing a promising way to
address the accessibility and personalization gap in current mental health
services.",XiuYu Zhang
2024-12-04T18:26:13Z,http://arxiv.org/abs/2412.03531v1,"A Review on Scientific Knowledge Extraction using Large Language Models
  in Biomedical Sciences","The rapid advancement of large language models (LLMs) has opened new
boundaries in the extraction and synthesis of medical knowledge, particularly
within evidence synthesis. This paper reviews the state-of-the-art applications
of LLMs in the biomedical domain, exploring their effectiveness in automating
complex tasks such as evidence synthesis and data extraction from a biomedical
corpus of documents. While LLMs demonstrate remarkable potential, significant
challenges remain, including issues related to hallucinations, contextual
understanding, and the ability to generalize across diverse medical tasks. We
highlight critical gaps in the current research literature, particularly the
need for unified benchmarks to standardize evaluations and ensure reliability
in real-world applications. In addition, we propose directions for future
research, emphasizing the integration of state-of-the-art techniques such as
retrieval-augmented generation (RAG) to enhance LLM performance in evidence
synthesis. By addressing these challenges and utilizing the strengths of LLMs,
we aim to improve access to medical literature and facilitate meaningful
discoveries in healthcare.",Gabriel Lino Garcia
2024-12-06T03:02:58Z,http://arxiv.org/abs/2412.04741v1,"Question Answering for Decisionmaking in Green Building Design: A
  Multimodal Data Reasoning Method Driven by Large Language Models","In recent years, the critical role of green buildings in addressing energy
consumption and environmental issues has become widely acknowledged. Research
indicates that over 40% of potential energy savings can be achieved during the
early design stage. Therefore, decision-making in green building design (DGBD),
which is based on modeling and performance simulation, is crucial for reducing
building energy costs. However, the field of green building encompasses a broad
range of specialized knowledge, which involves significant learning costs and
results in low decision-making efficiency. Many studies have already applied
artificial intelligence (AI) methods to this field. Based on previous research,
this study innovatively integrates large language models with DGBD, creating
GreenQA, a question answering framework for multimodal data reasoning.
Utilizing Retrieval Augmented Generation, Chain of Thought, and Function Call
methods, GreenQA enables multimodal question answering, including weather data
analysis and visualization, retrieval of green building cases, and knowledge
query. Additionally, this study conducted a user survey using the GreenQA web
platform. The results showed that 96% of users believed the platform helped
improve design efficiency. This study not only effectively supports DGBD but
also provides inspiration for AI-assisted design.",Yihui Li
2024-12-06T17:07:27Z,http://arxiv.org/abs/2412.05187v1,SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot,"Surgical interventions, particularly in neurology, represent complex and
high-stakes scenarios that impose substantial cognitive burdens on surgical
teams. Although deliberate education and practice can enhance cognitive
capabilities, surgical training opportunities remain limited due to patient
safety concerns. To address these cognitive challenges in surgical training and
operation, we propose SurgBox, an agent-driven sandbox framework to
systematically enhance the cognitive capabilities of surgeons in immersive
surgical simulations. Specifically, our SurgBox leverages large language models
(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically
replicate various surgical roles, enabling realistic training environments for
deliberate practice. In particular, we devise Surgery Copilot, an AI-driven
assistant to actively coordinate the surgical information stream and support
clinical decision-making, thereby diminishing the cognitive workload of
surgical teams during surgery. By incorporating a novel Long-Short Memory
mechanism, our Surgery Copilot can effectively balance immediate procedural
assistance with comprehensive surgical knowledge. Extensive experiments using
real neurosurgical procedure records validate our SurgBox framework in both
enhancing surgical cognitive capabilities and supporting clinical
decision-making. By providing an integrated solution for training and
operational support to address cognitive challenges, our SurgBox framework
advances surgical education and practice, potentially transforming surgical
outcomes and healthcare quality. The code is available at
https://github.com/franciszchen/SurgBox.",Jinlin Wu
2024-12-06T17:35:52Z,http://arxiv.org/abs/2412.05206v1,"ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented
  Argumentation with LLM Judges","Computational argumentation, which involves generating answers or summaries
for controversial topics like abortion bans and vaccination, has become
increasingly important in today's polarized environment. Sophisticated LLM
capabilities offer the potential to provide nuanced, evidence-based answers to
such questions through Retrieval-Augmented Argumentation (RAArg), leveraging
real-world evidence for high-quality, grounded arguments. However, evaluating
RAArg remains challenging, as human evaluation is costly and difficult for
complex, lengthy answers on complicated topics. At the same time, re-using
existing argumentation datasets is no longer sufficient, as they lack long,
complex arguments and realistic evidence from potentially misleading sources,
limiting holistic evaluation of retrieval effectiveness and argument quality.
To address these gaps, we investigate automated evaluation methods using
multiple fine-grained LLM judges, providing better and more interpretable
assessments than traditional single-score metrics and even previously reported
human crowdsourcing. To validate the proposed techniques, we introduce ConQRet,
a new benchmark featuring long and complex human-authored arguments on debated
topics, grounded in real-world websites, allowing an exhaustive evaluation
across retrieval effectiveness, argument quality, and groundedness. We validate
our LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed
LLM Judges and the ConQRet benchmark can enable rapid progress in computational
argumentation and can be naturally extended to other complex
retrieval-augmented generation tasks.",Kaustubh D. Dhole
2024-12-07T08:50:24Z,http://arxiv.org/abs/2412.05587v2,"GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on
  the Google Earth Engine Platform Powered by Large Language Models","As the scale and complexity of spatiotemporal data continue to grow rapidly,
the use of geospatial modeling on the Google Earth Engine (GEE) platform
presents dual challenges: improving the coding efficiency of domain experts and
enhancing the coding capabilities of interdisciplinary users. To address these
challenges and improve the performance of large language models (LLMs) in
geospatial code generation tasks, we propose a framework for building a
geospatial operator knowledge base tailored to the GEE JavaScript API. This
framework consists of an operator syntax knowledge table, an operator
relationship frequency table, an operator frequent pattern knowledge table, and
an operator relationship chain knowledge table. By leveraging Abstract Syntax
Tree (AST) techniques and frequent itemset mining, we systematically extract
operator knowledge from 185,236 real GEE scripts and syntax documentation,
forming a structured knowledge base. Experimental results demonstrate that the
framework achieves over 90% accuracy, recall, and F1 score in operator
knowledge extraction. When integrated with the Retrieval-Augmented Generation
(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge
base improves performance by 20-30%. Ablation studies further quantify the
necessity of each knowledge table in the knowledge base construction. This work
provides robust support for the advancement and application of geospatial code
modeling techniques, offering an innovative approach to constructing
domain-specific knowledge bases that enhance the code generation capabilities
of LLMs, and fostering the deeper integration of generative AI technologies
within the field of geoinformatics.",Shuyang Hou
2024-12-08T23:00:06Z,http://arxiv.org/abs/2412.06099v1,DECO: Life-Cycle Management of Enterprise-Grade Chatbots,"Software engineers frequently grapple with the challenge of accessing
disparate documentation and telemetry data, including Troubleshooting Guides
(TSGs), incident reports, code repositories, and various internal tools
developed by multiple stakeholders. While on-call duties are inevitable,
incident resolution becomes even more daunting due to the obscurity of legacy
sources and the pressures of strict time constraints. To enhance the efficiency
of on-call engineers (OCEs) and streamline their daily workflows, we introduced
DECO -- a comprehensive framework for developing, deploying, and managing
enterprise-grade chatbots tailored to improve productivity in engineering
routines. This paper details the design and implementation of the DECO
framework, emphasizing its innovative NL2SearchQuery functionality and a
hierarchical planner. These features support efficient and customized
retrieval-augmented-generation (RAG) algorithms that not only extract relevant
information from diverse sources but also select the most pertinent toolkits in
response to user queries. This enables the addressing of complex technical
questions and provides seamless, automated access to internal resources.
Additionally, DECO incorporates a robust mechanism for converting unstructured
incident logs into user-friendly, structured guides, effectively bridging the
documentation gap. Feedback from users underscores DECO's pivotal role in
simplifying complex engineering tasks, accelerating incident resolution, and
bolstering organizational productivity. Since its launch in September 2023,
DECO has demonstrated its effectiveness through extensive engagement, with tens
of thousands of interactions from hundreds of active users across multiple
organizations within the company.",Yiwen Zhu
2024-12-09T04:56:43Z,http://arxiv.org/abs/2412.06206v1,SiReRAG: Indexing Similar and Related Information for Multihop Reasoning,"Indexing is an important step towards strong performance in
retrieval-augmented generation (RAG) systems. However, existing methods
organize data based on either semantic similarity (similarity) or related
information (relatedness), but do not cover both perspectives comprehensively.
Our analysis reveals that modeling only one perspective results in insufficient
knowledge synthesis, leading to suboptimal performance on complex tasks
requiring multihop reasoning. In this paper, we propose SiReRAG, a novel RAG
indexing approach that explicitly considers both similar and related
information. On the similarity side, we follow existing work and explore some
variances to construct a similarity tree based on recursive summarization. On
the relatedness side, SiReRAG extracts propositions and entities from texts,
groups propositions via shared entities, and generates recursive summaries to
construct a relatedness tree. We index and flatten both similarity and
relatedness trees into a unified retrieval pool. Our experiments demonstrate
that SiReRAG consistently outperforms state-of-the-art indexing methods on
three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an
average 1.9% improvement in F1 scores. As a reasonably efficient solution,
SiReRAG enhances existing reranking methods significantly, with up to 7.8%
improvement in average F1 scores.",Nan Zhang
2024-12-09T18:59:46Z,http://arxiv.org/abs/2412.06786v1,"Retrieving Semantics from the Deep: an RAG Solution for Gesture
  Synthesis","Non-verbal communication often comprises of semantically rich gestures that
help convey the meaning of an utterance. Producing such semantic co-speech
gestures has been a major challenge for the existing neural systems that can
generate rhythmic beat gestures, but struggle to produce semantically
meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based
gesture generation approach that leverages Retrieval Augmented Generation (RAG)
to produce natural-looking and semantically rich gestures. Our neuro-explicit
gesture generation approach is designed to produce semantic gestures grounded
in interpretable linguistic knowledge. We achieve this by using explicit domain
knowledge to retrieve exemplar motions from a database of co-speech gestures.
Once retrieved, we then inject these semantic exemplar gestures into our
diffusion-based gesture generation pipeline using DDIM inversion and retrieval
guidance at the inference time without any need of training. Further, we
propose a control paradigm for guidance, that allows the users to modulate the
amount of influence each retrieval insertion has over the generated sequence.
Our comparative evaluations demonstrate the validity of our approach against
recent gesture generation approaches. The reader is urged to explore the
results on our project page.",M. Hamza Mughal
2024-12-06T21:17:47Z,http://arxiv.org/abs/2412.06827v1,"Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning
  with Human-AI Feedback","Large Language Models (LLMs) have demonstrated strong capabilities in
text-based tasks but struggle with the complex reasoning required for physics
problems, particularly in advanced arithmetic and conceptual understanding.
While some research has explored ways to enhance LLMs in physics education
using techniques such as prompt engineering and Retrieval Augmentation
Generation (RAG), not enough effort has been made in addressing their
limitations in physics reasoning. This paper presents a novel approach to
improving LLM performance on physics questions using Reinforcement Learning
with Human and Artificial Intelligence Feedback (RLHAIF). We evaluate several
reinforcement learning methods, including Proximal Policy Optimization (PPO),
Direct Preference Optimization (DPO), and Remax optimization. These methods are
chosen to investigate RL policy performance with different settings on the
PhyQA dataset, which includes challenging physics problems from high school
textbooks. Our RLHAIF model, tested on leading LLMs like LLaMA2 and Mistral,
achieved superior results, notably with the MISTRAL-PPO model, demonstrating
marked improvements in reasoning and accuracy. It achieved high scores, with a
58.67 METEOR score and a 0.74 Reasoning score, making it a strong example for
future physics reasoning research in this area.",Avinash Anand
2024-12-07T01:32:13Z,http://arxiv.org/abs/2412.06832v1,"SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to
  Question Answering","Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to
generalize to new information by decoupling reasoning capabilities from static
knowledge bases. Traditional RAG enhancements have explored vertical scaling --
assigning subtasks to specialized modules -- and horizontal scaling --
replicating tasks across multiple agents -- to improve performance. However,
real-world applications impose diverse Service Level Agreements (SLAs) and
Quality of Service (QoS) requirements, involving trade-offs among objectives
such as reducing cost, ensuring answer quality, and adhering to specific
operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG
tailored for real-world Question Answering (QA) applications. By integrating
task-specific non-functional requirements -- such as answer quality, cost, and
latency -- into the system, we enable dynamic reconfiguration to meet diverse
SLAs. Our method maps these Service Level Objectives (SLOs) to system-level
parameters, allowing the generation of optimal results within specified
resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic
re-orchestration of a multi-agent RAG system can effectively manage the
trade-off between answer quality and cost. By adjusting the system based on
query intent and operational conditions, we systematically balance performance
and resource utilization. This approach allows the system to meet SLOs for
various query types, showcasing its practicality for real-world applications.",Michael Iannelli
2024-12-10T14:39:51Z,http://arxiv.org/abs/2412.07548v1,"Automatic Database Configuration Debugging using Retrieval-Augmented
  Language Models","Database management system (DBMS) configuration debugging, e.g., diagnosing
poorly configured DBMS knobs and generating troubleshooting recommendations, is
crucial in optimizing DBMS performance. However, the configuration debugging
process is tedious and, sometimes challenging, even for seasoned database
administrators (DBAs) with sufficient experience in DBMS configurations and
good understandings of the DBMS internals (e.g., MySQL or Oracle). To address
this difficulty, we propose Andromeda, a framework that utilizes large language
models (LLMs) to enable automatic DBMS configuration debugging. Andromeda
serves as a natural surrogate of DBAs to answer a wide range of natural
language (NL) questions on DBMS configuration issues, and to generate
diagnostic suggestions to fix these issues. Nevertheless, directly prompting
LLMs with these professional questions may result in overly generic and often
unsatisfying answers. To this end, we propose a retrieval-augmented generation
(RAG) strategy that effectively provides matched domain-specific contexts for
the question from multiple sources. They come from related historical
questions, troubleshooting manuals and DBMS telemetries, which significantly
improve the performance of configuration debugging. To support the RAG
strategy, we develop a document retrieval mechanism addressing heterogeneous
documents and design an effective method for telemetry analysis. Extensive
experiments on real-world DBMS configuration debugging datasets show that
Andromeda significantly outperforms existing solutions.",Sibei Chen
2024-12-10T16:05:56Z,http://arxiv.org/abs/2412.07626v1,"OmniDocBench: Benchmarking Diverse PDF Document Parsing with
  Comprehensive Annotations","Document content extraction is crucial in computer vision, especially for
meeting the high-quality data needs of large language models (LLMs) and
retrieval-augmented generation (RAG) technologies. However, current document
parsing methods suffer from significant limitations in terms of diversity and
comprehensive evaluation. To address these challenges, we introduce
OmniDocBench, a novel multi-source benchmark designed to advance automated
document content extraction. OmniDocBench includes a meticulously curated and
annotated high-quality evaluation dataset comprising nine diverse document
types, such as academic papers, textbooks, slides, among others. Our benchmark
provides a flexible and comprehensive evaluation framework with 19 layout
category labels and 14 attribute labels, enabling multi-level assessments
across entire datasets, individual modules, or specific data types. Using
OmniDocBench, we perform an exhaustive comparative analysis of existing modular
pipelines and multimodal end-to-end methods, highlighting their limitations in
handling document diversity and ensuring fair evaluation. OmniDocBench
establishes a robust, diverse, and fair evaluation standard for the document
content extraction field, offering crucial insights for future advancements and
fostering the development of document parsing technologies. The codes and
dataset is available in https://github.com/opendatalab/OmniDocBench.",Linke Ouyang
2024-12-10T17:20:47Z,http://arxiv.org/abs/2412.07687v1,"Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions","The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.",Anant Prakash Awasthi
2024-12-10T18:17:02Z,http://arxiv.org/abs/2412.07724v2,Granite Guardian,"We introduce the Granite Guardian models, a suite of safeguards designed to
provide risk detection for prompts and responses, enabling safe and responsible
use in combination with any large language model (LLM). These models offer
comprehensive coverage across multiple risk dimensions, including social bias,
profanity, violence, sexual content, unethical behavior, jailbreaking, and
hallucination-related risks such as context relevance, groundedness, and answer
relevance for retrieval-augmented generation (RAG). Trained on a unique dataset
combining human annotations from diverse sources and synthetic data, Granite
Guardian models address risks typically overlooked by traditional risk
detection models, such as jailbreaks and RAG-specific issues. With AUC scores
of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks
respectively, Granite Guardian is the most generalizable and competitive model
available in the space. Released as open-source, Granite Guardian aims to
promote responsible AI development across the community.
  https://github.com/ibm-granite/granite-guardian",Inkit Padhi
2024-12-11T03:00:24Z,http://arxiv.org/abs/2412.08054v1,Federated In-Context LLM Agent Learning,"Large Language Models (LLMs) have revolutionized intelligent services by
enabling logical reasoning, tool use, and interaction with external systems as
agents. The advancement of LLMs is frequently hindered by the scarcity of
high-quality data, much of which is inherently sensitive. Federated learning
(FL) offers a potential solution by facilitating the collaborative training of
distributed LLMs while safeguarding private data. However, FL frameworks face
significant bandwidth and computational demands, along with challenges from
heterogeneous data distributions. The emerging in-context learning capability
of LLMs offers a promising approach by aggregating natural language rather than
bulky model parameters. Yet, this method risks privacy leakage, as it
necessitates the collection and presentation of data samples from various
clients during aggregation. In this paper, we propose a novel
privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,
which to our best knowledge for the first work unleashes the power of
in-context learning to train diverse LLM agents through FL. In our design,
knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums
Generation (KCG) module are transmitted between clients and the server instead
of model parameters in previous FL methods. Apart from that, an incredible
Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)
module is designed and we incorporate the aggregated global knowledge
compendium as a teacher to teach LLM agents the usage of tools. We conducted
extensive experiments and the results show that FICAL has competitive
performance compared to other SOTA baselines with a significant communication
cost decrease of $\mathbf{3.33\times10^5}$ times.",Panlong Wu
2024-12-13T07:51:32Z,http://arxiv.org/abs/2412.09936v1,"CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven
  Visual Language Models","The obesity phenomenon, known as the heavy issue, is a leading cause of
preventable chronic diseases worldwide. Traditional calorie estimation tools
often rely on specific data formats or complex pipelines, limiting their
practicality in real-world scenarios. Recently, vision-language models (VLMs)
have excelled in understanding real-world contexts and enabling conversational
interactions, making them ideal for downstream tasks such as ingredient
analysis. However, applying VLMs to calorie estimation requires domain-specific
data and alignment strategies. To this end, we curated CalData, a 330K
image-text pair dataset tailored for ingredient recognition and calorie
estimation, combining a large-scale recipe dataset with detailed nutritional
instructions for robust vision-language training. Built upon this dataset, we
present CaLoRAify, a novel VLM framework aligning ingredient recognition and
calorie estimation via training with visual-text pairs. During inference, users
only need a single monocular food image to estimate calories while retaining
the flexibility of agent-based conversational interaction. With Low-rank
Adaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our
system enhances the performance of foundational VLMs in the vertical domain of
calorie estimation. Our code and data are fully open-sourced at
https://github.com/KennyYao2001/16824-CaLORAify.",Dongyu Yao
2024-12-13T21:28:17Z,http://arxiv.org/abs/2412.10571v3,"Evidence Contextualization and Counterfactual Attribution for
  Conversational QA over Heterogeneous Data with RAG Systems","Retrieval Augmented Generation (RAG) works as a backbone for interacting with
an enterprise's own data via Conversational Question Answering (ConvQA). In a
RAG system, a retriever fetches passages from a collection in response to a
question, which are then included in the prompt of a large language model (LLM)
for generating a natural language (NL) answer. However, several RAG systems
today suffer from two shortcomings: (i) retrieved passages usually contain
their raw text and lack appropriate document context, negatively impacting both
retrieval and answering quality; and (ii) attribution strategies that explain
answer generation typically rely only on similarity between the answer and the
retrieved passages, thereby only generating plausible but not causal
explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies
the above concerns by: (i) contextualizing evidence with source metadata and
surrounding text; and (ii) computing counterfactual attribution, a causal
explanation approach where the contribution of an evidence to an answer is
determined by the similarity of the original response to the answer obtained by
removing that evidence. To evaluate our proposals, we release a new benchmark
ConfQuestions: it has 300 hand-created conversational questions, each in
English and German, coupled with ground truth URLs, completed questions, and
answers from 215 public Confluence pages. These documents are typical of
enterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE
on ConfQuestions show the viability of our ideas: contextualization improves
RAG performance, and counterfactual explanations outperform standard
attribution.",Rishiraj Saha Roy
2024-12-15T04:51:30Z,http://arxiv.org/abs/2412.11050v1,"RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous
  Driving with Vision-Language Models","Understanding and addressing corner cases is essential for ensuring the
safety and reliability of autonomous driving systems. Vision-Language Models
(VLMs) play a crucial role in enhancing scenario comprehension, yet they face
significant challenges, such as hallucination and insufficient real-world
grounding, which compromise their performance in critical driving scenarios. In
this work, we propose RAC3, a novel framework designed to improve VLMs' ability
to handle corner cases effectively. The framework integrates
Retrieval-Augmented Generation (RAG) to mitigate hallucination by dynamically
incorporating context-specific external knowledge. A cornerstone of RAC3 is its
cross-modal alignment fine-tuning, which utilizes contrastive learning to embed
image-text pairs into a unified semantic space, enabling robust retrieval of
similar scenarios. We evaluate RAC3 through extensive experiments using a
curated dataset of corner case scenarios, demonstrating its ability to enhance
semantic alignment, improve hallucination mitigation, and achieve superior
performance metrics, such as Cosine Similarity and ROUGE-L scores. For example,
for the LLaVA-v1.6-34B VLM, the cosine similarity between the generated text
and the reference text has increased by 5.22\%. The F1-score in ROUGE-L has
increased by 39.91\%, the Precision has increased by 55.80\%, and the Recall
has increased by 13.74\%. This work underscores the potential of
retrieval-augmented VLMs to advance the robustness and safety of autonomous
driving in complex environments.",Yujin Wang
2024-12-17T10:36:52Z,http://arxiv.org/abs/2412.12775v1,RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service,"Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.",Yihang Cheng
2024-12-17T15:38:42Z,http://arxiv.org/abs/2412.13018v1,"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in
  Financial Domain","As a typical and practical application of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) techniques have gained extensive
attention, particularly in vertical domains where LLMs may lack domain-specific
knowledge. In this paper, we introduce an omnidirectional and automatic RAG
benchmark, OmniEval, in the financial domain. Our benchmark is characterized by
its multi-dimensional evaluation framework, including (1) a matrix-based RAG
scenario evaluation system that categorizes queries into five task classes and
16 financial topics, leading to a structured assessment of diverse query
scenarios; (2) a multi-dimensional evaluation data generation approach, which
combines GPT-4-based automatic generation and human annotation, achieving an
87.47\% acceptance ratio in human evaluations on generated instances; (3) a
multi-stage evaluation system that evaluates both retrieval and generation
performance, result in a comprehensive evaluation on the RAG pipeline; and (4)
robust evaluation metrics derived from rule-based and LLM-based ones, enhancing
the reliability of assessments through manual annotations and supervised
fine-tuning of an LLM evaluator. Our experiments demonstrate the
comprehensiveness of OmniEval, which includes extensive test datasets and
highlights the performance variations of RAG systems across diverse topics and
tasks, revealing significant opportunities for RAG models to improve their
capabilities in vertical domains. We open source the code of our benchmark in
\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",Shuting Wang
2024-12-18T11:28:05Z,http://arxiv.org/abs/2412.13746v1,"RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented
  Generation for Preference Alignment","Despite the significant progress made by existing retrieval augmented
language models (RALMs) in providing trustworthy responses and grounding in
reliable sources, they often overlook effective alignment with human
preferences. In the alignment process, reward models (RMs) act as a crucial
proxy for human values to guide optimization. However, it remains unclear how
to evaluate and select a reliable RM for preference alignment in RALMs. To this
end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG
settings. First, we design four crucial and challenging RAG-specific scenarios
to assess RMs, including multi-hop reasoning, fine-grained citation,
appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG
subsets, six retrievers, and 24 RALMs to increase the diversity of data
sources. Finally, we adopt an LLM-as-a-judge approach to improve preference
annotation efficiency and effectiveness, exhibiting a strong correlation with
human annotations. Based on the RAG-RewardBench, we conduct a comprehensive
evaluation of 45 RMs and uncover their limitations in RAG scenarios.
Additionally, we also reveal that existing trained RALMs show almost no
improvement in preference alignment, highlighting the need for a shift towards
preference-aligned training.We release our benchmark and code publicly at
https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",Zhuoran Jin
2024-12-18T12:11:39Z,http://arxiv.org/abs/2412.13774v1,Designing an LLM-Based Copilot for Manufacturing Equipment Selection,"Effective decision-making in automation equipment selection is critical for
reducing ramp-up time and maintaining production quality, especially in the
face of increasing product variation and market demands. However, limited
expertise and resource constraints often result in inefficiencies during the
ramp-up phase when new products are integrated into production lines. Existing
methods often lack structured and tailored solutions to support automation
engineers in reducing ramp-up time, leading to compromises in quality. This
research investigates whether large-language models (LLMs), combined with
Retrieval-Augmented Generation (RAG), can assist in streamlining equipment
selection in ramp-up planning. We propose a factual-driven copilot integrating
LLMs with structured and semi-structured knowledge retrieval for three
component types (robots, feeders and vision systems), providing a guided and
traceable state-machine process for decision-making in automation equipment
selection. The system was demonstrated to an industrial partner, who tested it
on three internal use-cases. Their feedback affirmed its capability to provide
logical and actionable recommendations for automation equipment. More
specifically, among 22 equipment prompts analyzed, 19 involved selecting the
correct equipment while considering most requirements, and in 6 cases, all
requirements were fully met.",Jonas Werheid
2024-12-18T12:45:55Z,http://arxiv.org/abs/2412.13799v1,"Enhancing Rhetorical Figure Annotation: An Ontology-Based Web
  Application with RAG Integration","Rhetorical figures play an important role in our communication. They are used
to convey subtle, implicit meaning, or to emphasize statements. We notice them
in hate speech, fake news, and propaganda. By improving the systems for
computational detection of rhetorical figures, we can also improve tasks such
as hate speech and fake news detection, sentiment analysis, opinion mining, or
argument mining. Unfortunately, there is a lack of annotated data, as well as
qualified annotators that would help us build large corpora to train machine
learning models for the detection of rhetorical figures. The situation is
particularly difficult in languages other than English, and for rhetorical
figures other than metaphor, sarcasm, and irony. To overcome this issue, we
develop a web application called ""Find your Figure"" that facilitates the
identification and annotation of German rhetorical figures. The application is
based on the German Rhetorical ontology GRhOOT which we have specially adapted
for this purpose. In addition, we improve the user experience with Retrieval
Augmented Generation (RAG). In this paper, we present the restructuring of the
ontology, the development of the web application, and the built-in RAG
pipeline. We also identify the optimal RAG settings for our application. Our
approach is one of the first to practically use rhetorical ontologies in
combination with RAG and shows promising results.",Ramona Kühn
2024-12-14T17:08:34Z,http://arxiv.org/abs/2412.15247v1,"Streamlining Systematic Reviews: A Novel Application of Large Language
  Models","Systematic reviews (SRs) are essential for evidence-based guidelines but are
often limited by the time-consuming nature of literature screening. We propose
and evaluate an in-house system based on Large Language Models (LLMs) for
automating both title/abstract and full-text screening, addressing a critical
gap in the literature. Using a completed SR on Vitamin D and falls (14,439
articles), the LLM-based system employed prompt engineering for title/abstract
screening and Retrieval-Augmented Generation (RAG) for full-text screening. The
system achieved an article exclusion rate (AER) of 99.5%, specificity of 99.6%,
a false negative rate (FNR) of 0%, and a negative predictive value (NPV) of
100%. After screening, only 78 articles required manual review, including all
20 identified by traditional methods, reducing manual screening time by 95.5%.
For comparison, Rayyan, a commercial tool for title/abstract screening,
achieved an AER of 72.1% and FNR of 5% when including articles Rayyan
considered as undecided or likely to include. Lowering Rayyan's inclusion
thresholds improved FNR to 0% but increased screening time. By addressing both
screening phases, the LLM-based system significantly outperformed Rayyan and
traditional methods, reducing total screening time to 25.5 hours while
maintaining high accuracy. These findings highlight the transformative
potential of LLMs in SR workflows by offering a scalable, efficient, and
accurate solution, particularly for the full-text screening phase, which has
lacked automation tools.",Fouad Trad
2024-12-19T22:51:56Z,http://arxiv.org/abs/2412.15443v1,"SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic
  Retrieval","Retrieval-Augmented Generation (RAG) systems have become pivotal in
leveraging vast corpora to generate informed and contextually relevant
responses, notably reducing hallucinations in Large Language Models. Despite
significant advancements, these systems struggle to efficiently process and
retrieve information from large datasets while maintaining a comprehensive
understanding of the context. This paper introduces SKETCH, a novel methodology
that enhances the RAG retrieval process by integrating semantic text retrieval
with knowledge graphs, thereby merging structured and unstructured data for a
more holistic comprehension. SKETCH, demonstrates substantial improvements in
retrieval performance and maintains superior context integrity compared to
traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,
NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline
approaches on key RAGAS metrics such as answer_relevancy, faithfulness,
context_precision and context_recall. Notably, on the Italian Cuisine dataset,
SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,
representing the highest performance across all evaluated metrics. These
results highlight SKETCH's capability in delivering more accurate and
contextually relevant responses, setting new benchmarks for future retrieval
systems.",Aakash Mahalingam
2024-12-21T13:19:15Z,http://arxiv.org/abs/2412.16615v1,"Large Language Model Can Be a Foundation for Hidden Rationale-Based
  Retrieval","Despite the recent advancement in Retrieval-Augmented Generation (RAG)
systems, most retrieval methodologies are often developed for factual
retrieval, which assumes query and positive documents are semantically similar.
In this paper, we instead propose and study a more challenging type of
retrieval task, called hidden rationale retrieval, in which query and document
are not similar but can be inferred by reasoning chains, logic relationships,
or empirical experiences. To address such problems, an instruction-tuned Large
language model (LLM) with a cross-encoder architecture could be a reasonable
choice. To further strengthen pioneering LLM-based retrievers, we design a
special instruction that transforms the retrieval task into a generative task
by prompting LLM to answer a binary-choice question. The model can be
fine-tuned with direct preference optimization (DPO). The framework is also
optimized for computational efficiency with no performance degradation. We name
this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned
performance superiority on Emotional Support Conversation (ESC), compared with
previous retrieval works. Our study suggests the potential to employ LLM as a
foundation for a wider scope of retrieval tasks. Our codes, models, and
datasets are available on https://github.com/flyfree5/LaHoRe.",Luo Ji
2024-12-22T20:03:35Z,http://arxiv.org/abs/2412.17146v1,LLM Agent for Fire Dynamics Simulations,"Significant advances have been achieved in leveraging foundation models, such
as large language models (LLMs), to accelerate complex scientific workflows. In
this work we introduce FoamPilot, a proof-of-concept LLM agent designed to
enhance the usability of FireFOAM, a specialized solver for fire dynamics and
fire suppression simulations built using OpenFOAM, a popular open-source
toolbox for computational fluid dynamics (CFD). FoamPilot provides three core
functionalities: code insight, case configuration and simulation evaluation.
Code insight is an alternative to traditional keyword searching leveraging
retrieval-augmented generation (RAG) and aims to enable efficient navigation
and summarization of the FireFOAM source code for developers and experienced
users. For case configuration, the agent interprets user requests in natural
language and aims to modify existing simulation setups accordingly to support
intermediate users. FoamPilot's job execution functionality seeks to manage the
submission and execution of simulations in high-performance computing (HPC)
environments and provide preliminary analysis of simulation results to support
less experienced users. Promising results were achieved for each functionality,
particularly for simple tasks, and opportunities were identified for
significant further improvement for more complex tasks. The integration of
these functionalities into a single LLM agent is a step aimed at accelerating
the simulation workflow for engineers and scientists employing FireFOAM for
complex simulations critical for improving fire safety.",Leidong Xu
2024-12-24T02:21:09Z,http://arxiv.org/abs/2412.18100v1,EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent,"The rapid growth of scientific techniques and knowledge is reflected in the
exponential increase in new patents filed annually. While these patents drive
innovation, they also present significant burden for researchers and engineers,
especially newcomers. To avoid the tedious work of navigating a vast and
complex landscape to identify trends and breakthroughs, researchers urgently
need efficient tools to summarize, evaluate, and contextualize patents,
revealing their innovative contributions and underlying scientific
principles.To address this need, we present EvoPat, a multi-LLM-based patent
agent designed to assist users in analyzing patents through Retrieval-Augmented
Generation (RAG) and advanced search strategies. EvoPat leverages multiple
Large Language Models (LLMs), each performing specialized roles such as
planning, identifying innovations, and conducting comparative evaluations. The
system integrates data from local databases, including patents, literature,
product catalogous, and company repositories, and online searches to provide
up-to-date insights. The ability to collect information not included in
original database automatically is also implemented. Through extensive testing
in the natural language processing (NLP) domain, we demonstrate that EvoPat
outperforms GPT-4 in tasks such as patent summarization, comparative analysis,
and technical evaluation. EvoPat represents a significant step toward creating
AI-powered tools that empower researchers and engineers to efficiently navigate
the complexities of the patent landscape.",Suyuan Wang
2023-04-28T10:15:25Z,http://arxiv.org/abs/2304.14732v7,"Search-in-the-Chain: Interactively Enhancing Large Language Models with
  Search for Knowledge-intensive Tasks","Making the content generated by Large Language Model (LLM), accurate,
credible and traceable is crucial, especially in complex knowledge-intensive
tasks that require multi-step reasoning and each step needs knowledge to solve.
Retrieval-augmented generation is good potential to solve this problem.
However, where and how to introduce Information Retrieval (IR) to LLM is a big
challenge. Previous work has the problems that wrong knowledge retrieved by IR
misleads the LLM and interaction between IR and LLM breaks the reasoning chain
of LLM. This paper proposes a novel framework named
\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the reasoning chain named
Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer
pair. Second, IR verifies the answer of each node of CoQ. It corrects the
answer that is not consistent with the retrieved information when IR gives high
confidence, which improves the credibility. Third, LLM can indicate its missing
knowledge in CoQ and rely on IR to provide this knowledge to LLM. These
operations improve the accuracy in terms of reasoning and knowledge. Finally,
SearChain generates the reasoning process and marks references to supporting
documents for each reasoning step, which improves traceability. Interaction
with IR in SearChain forms a novel reasoning path based on a tree, which
enables LLM to dynamically modify the direction of reasoning. Experiments show
that SearChain outperforms state-of-the-art baselines on complex
knowledge-intensive tasks including multi-hop Q\&A, slot filling, fact
checking, and long-form Q\&A.",Shicheng Xu
2023-10-18T18:00:11Z,http://arxiv.org/abs/2310.12214v6,"InferDPT: Privacy-Preserving Inference for Black-box Large Language
  Model","Large language models (LLMs), like ChatGPT, have greatly simplified text
generation tasks. However, they have also raised concerns about privacy risks
such as data leakage and unauthorized data collection. Existing solutions for
privacy-preserving inference face practical challenges related to computation
time and communication costs. In this paper, we propose InferDPT, the first
practical framework for the privacy-preserving Inference of black-box LLMs,
implementing Differential Privacy in Text generation. InferDPT comprises two
key modules: the ""perturbation module"" utilizes the exponential mechanism to
generate a perturbed prompt, facilitating privacy-preserving inference with
black-box LLMs, and the ""extraction module"", inspired by knowledge distillation
and retrieval-augmented generation, extracts coherent and consistent text from
the perturbed generation result, ensuring successful text generation
completion. To address privacy concerns related to previous exponential
mechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,
a novel differential privacy mechanism integrated into the perturbation module
of InferDPT, which introduces the concept of ""RANdom adjacency"" for TEXT
perturbation within the prompt. Experimental results across three datasets
demonstrate that the text generation quality of InferDPT is comparable to that
of non-private GPT-4, and RANTEXT surpasses existing state-of-the-art
mechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and
utility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves
an average privacy protection rate exceeding 90% against embedding revision
attacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher
than that of CUSTEXT+.",Meng Tong
2023-11-29T03:07:00Z,http://arxiv.org/abs/2311.17330v2,"Biomedical knowledge graph-optimized prompt generation for large
  language models","Large Language Models (LLMs) are being adopted at an unprecedented rate, yet
still face challenges in knowledge-intensive domains like biomedicine.
Solutions such as pre-training and domain-specific fine-tuning add substantial
computational overhead, requiring further domain expertise. Here, we introduce
a token-optimized and robust Knowledge Graph-based Retrieval Augmented
Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)
with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful
biomedical text rooted in established knowledge. Compared to the existing RAG
technique for Knowledge Graphs, the proposed method utilizes minimal graph
schema for context extraction and uses embedding methods for context pruning.
This optimization in context extraction results in more than 50% reduction in
token consumption without compromising the accuracy, making a cost-effective
and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced
the performance of LLMs across diverse biomedical prompts by generating
responses rooted in established knowledge, accompanied by accurate provenance
and statistical evidence (if available) to substantiate the claims. Further
benchmarking on human curated datasets, such as biomedical true/false and
multiple-choice questions (MCQ), showed a remarkable 71% boost in the
performance of the Llama-2 model on the challenging MCQ dataset, demonstrating
the framework's capacity to empower open-source models with fewer parameters
for domain specific questions. Furthermore, KG-RAG enhanced the performance of
proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM in a token
optimized fashion, thus enhancing the adaptability of general-purpose LLMs to
tackle domain-specific questions in a cost-effective fashion.",Karthik Soman
2023-12-24T23:01:00Z,http://arxiv.org/abs/2312.15561v5,"README: Bridging Medical Jargon and Lay Understanding for Patient
  Education through Data-Centric NLP","The advancement in healthcare has shifted focus toward patient-centric
approaches, particularly in self-care and patient education, facilitated by
access to Electronic Health Records (EHR). However, medical jargon in EHRs
poses significant challenges in patient comprehension. To address this, we
introduce a new task of automatically generating lay definitions, aiming to
simplify complex medical terms into patient-friendly lay language. We first
created the README dataset, an extensive collection of over 50,000 unique
(medical term, lay definition) pairs and 300,000 mentions, each offering
context-aware lay definitions manually annotated by domain experts. We have
also engineered a data-centric Human-AI pipeline that synergizes data
filtering, augmentation, and selection to improve data quality. We then used
README as the training data for models and leveraged a Retrieval-Augmented
Generation method to reduce hallucinations and improve the quality of model
outputs. Our extensive automatic and human evaluations demonstrate that
open-source mobile-friendly models, when fine-tuned with high-quality data, are
capable of matching or even surpassing the performance of state-of-the-art
closed-source large language models like ChatGPT. This research represents a
significant stride in closing the knowledge gap in patient education and
advancing patient-centric healthcare solutions.",Zonghai Yao
2023-12-25T02:32:05Z,http://arxiv.org/abs/2312.15591v5,Privacy-Preserved Neural Graph Databases,"In the era of large language models (LLMs), efficient and accurate data
retrieval has become increasingly crucial for the use of domain-specific or
private data in the retrieval augmented generation (RAG). Neural graph
databases (NGDBs) have emerged as a powerful paradigm that combines the
strengths of graph databases (GDBs) and neural networks to enable efficient
storage, retrieval, and analysis of graph-structured data which can be
adaptively trained with LLMs. The usage of neural embedding storage and Complex
neural logical Query Answering (CQA) provides NGDBs with generalization
ability. When the graph is incomplete, by extracting latent patterns and
representations, neural graph databases can fill gaps in the graph structure,
revealing hidden relationships and enabling accurate query answering.
Nevertheless, this capability comes with inherent trade-offs, as it introduces
additional privacy risks to the domain-specific or private databases. Malicious
attackers can infer more sensitive information in the database using
well-designed queries such as from the answer sets of where Turing Award
winners born before 1950 and after 1940 lived, the living places of Turing
Award winner Hinton are probably exposed, although the living places may have
been deleted in the training stage due to the privacy concerns. In this work,
we propose a privacy-preserved neural graph database (P-NGDB) framework to
alleviate the risks of privacy leakage in NGDBs. We introduce adversarial
training techniques in the training stage to enforce the NGDBs to generate
indistinguishable answers when queried with private information, enhancing the
difficulty of inferring sensitive information through combinations of multiple
innocuous queries.",Qi Hu
2023-12-29T03:23:23Z,http://arxiv.org/abs/2312.17449v2,"DB-GPT: Empowering Database Interactions with Private Large Language
  Models","The recent breakthroughs in large language models (LLMs) are positioned to
transition many areas of software. Database technologies particularly have an
important entanglement with LLMs as efficient and intuitive database
interactions are paramount. In this paper, we present DB-GPT, a revolutionary
and production-ready project that integrates LLMs with traditional database
systems to enhance user experience and accessibility. DB-GPT is designed to
understand natural language queries, provide context-aware responses, and
generate complex SQL queries with high accuracy, making it an indispensable
tool for users ranging from novice to expert. The core innovation in DB-GPT
lies in its private LLM technology, which is fine-tuned on domain-specific
corpora to maintain user privacy and ensure data security while offering the
benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which
includes a novel retrieval augmented generation (RAG) knowledge system, an
adaptive learning mechanism to continuously improve performance based on user
feedback and a service-oriented multi-model framework (SMMF) with powerful
data-driven agents. Our extensive experiments and user studies confirm that
DB-GPT represents a paradigm shift in database interactions, offering a more
natural, efficient, and secure way to engage with data repositories. The paper
concludes with a discussion of the implications of DB-GPT framework on the
future of human-database interaction and outlines potential avenues for further
enhancements and applications in the field. The project code is available at
https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by
installing it with the instructions
https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute
video at https://www.youtube.com/watch?v=KYs4nTDzEhk.",Siqiao Xue
2024-01-02T17:56:30Z,http://arxiv.org/abs/2401.01313v3,"A Comprehensive Survey of Hallucination Mitigation Techniques in Large
  Language Models","As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.",S. M Towhidul Islam Tonmoy
2024-01-05T15:09:57Z,http://arxiv.org/abs/2401.02851v2,"Natural Language Programming in Medicine: Administering Evidence Based
  Clinical Workflows with Autonomous Agents Powered by Generative Large
  Language Models","Generative Large Language Models (LLMs) hold significant promise in
healthcare, demonstrating capabilities such as passing medical licensing exams
and providing clinical knowledge. However, their current use as information
retrieval tools is limited by challenges like data staleness, resource demands,
and occasional generation of incorrect information. This study assessed the
potential of LLMs to function as autonomous agents in a simulated tertiary care
medical center, using real-world clinical cases across multiple specialties.
Both proprietary and open-source LLMs were evaluated, with Retrieval Augmented
Generation (RAG) enhancing contextual relevance. Proprietary models,
particularly GPT-4, generally outperformed open-source models, showing improved
guideline adherence and more accurate responses with RAG. The manual evaluation
by expert clinicians was crucial in validating models' outputs, underscoring
the importance of human oversight in LLM operation. Further, the study
emphasizes Natural Language Programming (NLP) as the appropriate paradigm for
modifying model behavior, allowing for precise adjustments through tailored
prompts and real-world interactions. This approach highlights the potential of
LLMs to significantly enhance and supplement clinical decision-making, while
also emphasizing the value of continuous expert involvement and the flexibility
of NLP to ensure their reliability and effectiveness in healthcare settings.",Akhil Vaid
2024-01-10T02:57:20Z,http://arxiv.org/abs/2401.06800v1,Reinforcement Learning for Optimizing RAG for Domain Chatbots,"With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.",Mandar Kulkarni
2024-01-16T14:44:47Z,http://arxiv.org/abs/2401.08406v3,"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on
  Agriculture","There are two common ways in which developers are incorporating proprietary
and domain-specific data when building applications of Large Language Models
(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the
prompt with the external data, while fine-Tuning incorporates the additional
knowledge into the model itself. However, the pros and cons of both approaches
are not well understood. In this paper, we propose a pipeline for fine-tuning
and RAG, and present the tradeoffs of both for multiple popular LLMs, including
Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,
including extracting information from PDFs, generating questions and answers,
using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We
propose metrics to assess the performance of different stages of the RAG and
fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.
Agriculture as an industry has not seen much penetration of AI, and we study a
potentially disruptive application - what if we could provide location-specific
insights to a farmer? Our results show the effectiveness of our dataset
generation pipeline in capturing geographic-specific knowledge, and the
quantitative and qualitative benefits of RAG and fine-tuning. We see an
accuracy increase of over 6 p.p. when fine-tuning the model and this is
cumulative with RAG, which increases accuracy by 5 p.p. further. In one
particular experiment, we also demonstrate that the fine-tuned model leverages
information from across geographies to answer specific questions, increasing
answer similarity from 47% to 72%. Overall, the results point to how systems
built using LLMs can be adapted to respond and incorporate knowledge across a
dimension that is critical for a specific industry, paving the way for further
applications of LLMs in other industrial domains.",Angels Balaguer
2024-01-27T02:29:42Z,http://arxiv.org/abs/2401.15269v3,"Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models","Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.",Minbyul Jeong
2024-01-27T10:50:11Z,http://arxiv.org/abs/2401.15378v4,"A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM","Challenges exist in learning and understanding religions, such as the
complexity and depth of religious doctrines and teachings. Chatbots as
question-answering systems can help in solving these challenges. LLM chatbots
use NLP techniques to establish connections between topics and accurately
respond to complex questions. These capabilities make it perfect for
enlightenment on religion as a question-answering chatbot. However, LLMs also
tend to generate false information, known as hallucination. Also, the chatbots'
responses can include content that insults personal religious beliefs,
interfaith conflicts, and controversial or sensitive topics. It must avoid such
cases without promoting hate speech or offending certain groups of people or
their beliefs. This study uses a vector database-based Retrieval Augmented
Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our
question-answering system is called ""MufassirQAS"". We created a database
consisting of several open-access books that include Turkish context. These
books contain Turkish translations and interpretations of Islam. This database
is utilized to answer religion-related questions and ensure our answers are
trustworthy. The relevant part of the dataset, which LLM also uses, is
presented along with the answer. We have put careful effort into creating
system prompts that give instructions to prevent harmful, offensive, or
disrespectful responses to respect people's values and provide reliable
results. The system answers and shares additional information, such as the page
number from the respective book and the articles referenced for obtaining the
information. MufassirQAS and ChatGPT are also tested with sensitive questions.
We got better performance with our system. Study and enhancements are still in
progress. Results and future works are given.",Ahmet Yusuf Alan
2024-01-29T16:03:29Z,http://arxiv.org/abs/2402.01741v2,"Development and Testing of a Novel Large Language Model-Based Clinical
  Decision Support Systems for Medication Safety in 12 Clinical Specialties","Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) framework as a Clinical Decision Support Systems (CDSS) to
support safe medication prescription.
  Objective: To evaluate the efficacy of LLM-based CDSS in correctly
identifying medication errors in different patient case vignettes from diverse
medical and surgical sub-disciplines, against a human expert panel derived
ground truth. We compared performance for under 2 different CDSS practical
healthcare integration modalities: LLM-based CDSS alone (fully autonomous mode)
vs junior pharmacist + LLM-based CDSS (co-pilot, assistive mode).
  Design, Setting, and Participants: Utilizing a RAG model with
state-of-the-art medically-related LLMs (GPT-4, Gemini Pro 1.0 and Med-PaLM 2),
this study used 61 prescribing error scenarios embedded into 23 complex
clinical vignettes across 12 different medical and surgical specialties. A
multidisciplinary expert panel assessed these cases for Drug-Related Problems
(DRPs) using the PCNE classification and graded severity / potential for harm
using revised NCC MERP medication error index. We compared.
  Results RAG-LLM performed better compared to LLM alone. When employed in a
co-pilot mode, accuracy, recall, and F1 scores were optimized, indicating
effectiveness in identifying moderate to severe DRPs. The accuracy of DRP
detection with RAG-LLM improved in several categories but at the expense of
lower precision.
  Conclusions This study established that a RAG-LLM based CDSS significantly
boosts the accuracy of medication error identification when used alongside
junior pharmacists (co-pilot), with notable improvements in detecting severe
DRPs. This study also illuminates the comparative performance of current
state-of-the-art LLMs in RAG-based CDSS systems.",Jasmine Chiat Ling Ong
2024-01-30T00:21:41Z,http://arxiv.org/abs/2402.01748v2,"Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems","Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.",Shengzhe Xu
2024-02-20T02:16:16Z,http://arxiv.org/abs/2402.12659v2,FinBen: A Holistic Financial Benchmark for Large Language Models,"LLMs have transformed NLP and shown promise in various fields, yet their
potential in finance is underexplored due to a lack of comprehensive evaluation
benchmarks, the rapid development of LLMs, and the complexity of financial
tasks. In this paper, we introduce FinBen, the first extensive open-source
evaluation benchmark, including 36 datasets spanning 24 financial tasks,
covering seven critical aspects: information extraction (IE), textual analysis,
question answering (QA), text generation, risk management, forecasting, and
decision-making. FinBen offers several key innovations: a broader range of
tasks and datasets, the first evaluation of stock trading, novel agent and
Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source
evaluation datasets for text summarization, question answering, and stock
trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT,
and the latest Gemini, reveals several key findings: While LLMs excel in IE and
textual analysis, they struggle with advanced reasoning and complex tasks like
text generation and forecasting. GPT-4 excels in IE and stock trading, while
Gemini is better at text generation and forecasting. Instruction-tuned LLMs
improve textual analysis but offer limited benefits for complex tasks such as
QA. FinBen has been used to host the first financial LLMs shared task at the
FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel
solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation
in financial LLMs. All datasets, results, and codes are released for the
research community: https://github.com/The-FinAI/PIXIU.",Qianqian Xie
2024-03-11T16:12:34Z,http://arxiv.org/abs/2403.06857v1,"Development of a Reliable and Accessible Caregiving Language Model
  (CaLM)","Unlike professional caregivers, family caregivers often assume this role
without formal preparation or training. Because of this, there is an urgent
need to enhance the capacity of family caregivers to provide quality care.
Large language models can potentially be used as a foundation technology for
supporting caregivers as educational tools or as adjunct to care. This study
aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a
caregiving knowledge base, develop an accessible CaLM using a small FM that
requires fewer computing resources, and evaluate the performance of the model
compared to a large FM. We developed CaLM using the Retrieval Augmented
Generation (RAG) framework combined with FM fine-tuning for improving the
quality of FM answers by grounding the model on a caregiving knowledge base. We
used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B
parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving
knowledge base by gathering various types of documents from the Internet. In
this study, we focused on caregivers of individuals with Alzheimer's Disease
Related Dementias. We evaluated the models' performance using the benchmark
metrics commonly used in evaluating language models and their reliability to
provide accurate references with the answers. The RAG framework improved the
performance of all FMs used in this study across all measures. As expected, the
large FM performed better than small FMs across all metrics. The most
interesting result is that small fine-tuned FMs with RAG performed
significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2
small FM performed better than GPT 3.5 (even with RAG) in returning references
with the answers. The study shows that reliable and accessible CaLM can be
developed by using small FMs with a knowledge base specific to the caregiving
domain.",Bambang Parmanto
2024-03-21T13:05:18Z,http://arxiv.org/abs/2403.14374v1,FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,"Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.",Yuren Mao
2024-03-25T21:37:30Z,http://arxiv.org/abs/2403.17209v4,"Generation of Asset Administration Shell with Large Language Model
  Agents: Toward Semantic Interoperability in Digital Twins in the Context of
  Industry 4.0","This research introduces a novel approach for achieving semantic
interoperability in digital twins and assisting the creation of Asset
Administration Shell (AAS) as digital twin model within the context of Industry
4.0. The foundational idea of our research is that the communication based on
semantics and the generation of meaningful textual data are directly linked,
and we posit that these processes are equivalent if the exchanged information
can be serialized in text form. Based on this, we construct a ""semantic node""
data structure in our research to capture the semantic essence of textual data.
Then, a system powered by large language models is designed and implemented to
process the ""semantic node"" and generate standardized digital twin models from
raw textual data collected from datasheets describing technical assets. Our
evaluation demonstrates an effective generation rate of 62-79%, indicating a
substantial proportion of the information from the source text can be
translated error-free to the target digital twin instance model with the
generative capability of large language models. This result has a direct
application in the context of Industry 4.0, and the designed system is
implemented as a data model generation tool for reducing the manual effort in
creating AAS model. In our evaluation, a comparative analysis of different LLMs
and an in-depth ablation study of Retrieval-Augmented Generation (RAG)
mechanisms provide insights into the effectiveness of LLM systems for
interpreting technical concepts and translating data. Our findings emphasize
LLMs' capability to automate AAS instance creation and contribute to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are presented
on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.",Yuchen Xia
2024-03-28T03:09:42Z,http://arxiv.org/abs/2403.19113v1,FACTOID: FACtual enTailment fOr hallucInation Detection,"The widespread adoption of Large Language Models (LLMs) has facilitated
numerous benefits. However, hallucination is a significant concern. In
response, Retrieval Augmented Generation (RAG) has emerged as a highly
promising paradigm to improve LLM outputs by grounding them in factual
information. RAG relies on textual entailment (TE) or similar methods to check
if the text produced by LLMs is supported or contradicted, compared to
retrieved documents. This paper argues that conventional TE methods are
inadequate for spotting hallucinations in content generated by LLMs. For
instance, consider a prompt about the 'USA's stance on the Ukraine war''. The
AI-generated text states, ...U.S. President Barack Obama says the U.S. will not
put troops in Ukraine...'' However, during the war the U.S. president is Joe
Biden which contradicts factual reality. Moreover, current TE systems are
unable to accurately annotate the given text and identify the exact portion
that is contradicted. To address this, we introduces a new type of TE called
``Factual Entailment (FE).'', aims to detect factual inaccuracies in content
generated by LLMs while also highlighting the specific text segment that
contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation
Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL)
framework for FE, incorporating state-of-the-art (SoTA) long text embeddings
such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The
proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy
on the FACTOID benchmark compared to SoTA TE methods. As FE automatically
detects hallucinations, we assessed 15 modern LLMs and ranked them using our
proposed Auto Hallucination Vulnerability Index (HVI_auto). This index
quantifies and offers a comparative scale to evaluate and rank LLMs according
to their hallucinations.",Vipula Rawte
2024-04-16T00:43:03Z,http://arxiv.org/abs/2404.10198v2,"ClashEval: Quantifying the tug-of-war between an LLM's internal prior
  and external evidence","Retrieval augmented generation (RAG) is frequently used to mitigate
hallucinations and provide up-to-date knowledge for large language models
(LLMs). However, given that document retrieval is an imprecise task and
sometimes results in erroneous or even harmful content being presented in
context, this raises the question of how LLMs handle retrieved information: If
the provided content is incorrect, does the model know to ignore it, or does it
recapitulate the error? Conversely, when the model's initial response is
incorrect, does it always know to use the retrieved information to correct
itself, or does it insist on its wrong prior response? To answer this, we
curate a dataset of over 1200 questions across six domains (e.g., drug dosages,
Olympic records, locations) along with content relevant to answering each
question. We further apply precise perturbations to the answers in the content
that range from subtle to blatant errors. We benchmark six top-performing LLMs,
including GPT-4o, on this dataset and find that LLMs are susceptible to
adopting incorrect retrieved content, overriding their own correct prior
knowledge over 60% of the time. However, the more unrealistic the retrieved
content is (i.e. more deviated from truth), the less likely the model is to
adopt it. Also, the less confident a model is in its initial response (via
measuring token probabilities), the more likely it is to adopt the information
in the retrieved content. We exploit this finding and demonstrate simple
methods for improving model accuracy where there is conflicting retrieved
content. Our results highlight a difficult task and benchmark for LLMs --
namely, their ability to correctly discern when it is wrong in light of correct
retrieved content and to reject cases when the provided content is incorrect.",Kevin Wu
2024-04-18T16:38:02Z,http://arxiv.org/abs/2404.12309v2,iRAG: Advancing RAG for Videos with an Incremental Approach,"Retrieval-augmented generation (RAG) systems combine the strengths of
language generation and information retrieval to power many real-world
applications like chatbots. Use of RAG for understanding of videos is appealing
but there are two critical limitations. One-time, upfront conversion of all
content in large corpus of videos into text descriptions entails high
processing times. Also, not all information in the rich video data is typically
captured in the text descriptions. Since user queries are not known apriori,
developing a system for video to text conversion and interactive querying of
video data is challenging.
  To address these limitations, we propose an incremental RAG system called
iRAG, which augments RAG with a novel incremental workflow to enable
interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG
quickly indexes large repositories of videos, and in the incremental workflow,
it uses the index to opportunistically extract more details from select
portions of the videos to retrieve context relevant to an interactive user
query. Such an incremental workflow avoids long video to text conversion times,
and overcomes information loss issues due to conversion of video to text, by
doing on-demand query-specific extraction of details in video data. This
ensures high quality of responses to interactive user queries that are often
not known apriori. To the best of our knowledge, iRAG is the first system to
augment RAG with an incremental workflow to support efficient interactive
querying of a large corpus of videos. Experimental results on real-world
datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring
that latency and quality of responses to interactive user queries is comparable
to responses from a traditional RAG where all video data is converted to text
upfront before any user querying.",Md Adnan Arefeen
2024-04-22T09:25:05Z,http://arxiv.org/abs/2404.14464v1,"Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for
  Multi-hop Question Answering","Multi-hop question answering is a knowledge-intensive complex problem. Large
Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason
complex problems step by step, and retrieval-augmentation can effectively
alleviate factual errors caused by outdated and unknown knowledge in LLMs.
Recent works have introduced retrieval-augmentation in the CoT reasoning to
solve multi-hop question answering. However, these chain methods have the
following problems: 1) Retrieved irrelevant paragraphs may mislead the
reasoning; 2) An error in the chain structure may lead to a cascade of errors.
  In this paper, we propose a dynamic retrieval framework called Tree of
Reviews (ToR), where the root node is the question, and the other nodes are
paragraphs from retrieval, extending different reasoning paths from the root
node to other nodes. Our framework dynamically decides to initiate a new
search, reject, or accept based on the paragraphs on the reasoning paths.
Compared to related work, we introduce a tree structure to handle each
retrieved paragraph separately, alleviating the misleading effect of irrelevant
paragraphs on the reasoning path; the diversity of reasoning path extension
reduces the impact of a single reasoning error on the whole. We conducted
experiments on three different multi-hop question answering datasets. The
results show that compared to the baseline methods, ToR achieves
state-of-the-art performance in both retrieval and response generation. In
addition, we propose two tree-based search optimization strategies, pruning and
effective expansion, to reduce time overhead and increase the diversity of path
extension. We will release our code.",Li Jiapeng
2024-05-01T05:39:07Z,http://arxiv.org/abs/2405.00330v1,"Integrating A.I. in Higher Education: Protocol for a Pilot Study with
  'SAMCares: An Adaptive Learning Hub'","Learning never ends, and there is no age limit to grow yourself. However, the
educational landscape may face challenges in effectively catering to students'
inclusion and diverse learning needs. These students should have access to
state-of-the-art methods for lecture delivery, online resources, and technology
needs. However, with all the diverse learning sources, it becomes harder for
students to comprehend a large amount of knowledge in a short period of time.
Traditional assistive technologies and learning aids often lack the dynamic
adaptability required for individualized education plans. Large Language Models
(LLM) have been used in language translation, text summarization, and content
generation applications. With rapid growth in AI over the past years,
AI-powered chatbots and virtual assistants have been developed. This research
aims to bridge this gap by introducing an innovative study buddy we will be
calling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in
our case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation
(RAG) to offer real-time, context-aware, and adaptive educational support. The
context of the model will be limited to the knowledge base of Sam Houston State
University (SHSU) course notes. The LLM component enables a chat-like
environment to interact with it to meet the unique learning requirements of
each student. For this, we will build a custom web-based GUI. At the same time,
RAG enhances real-time information retrieval and text generation, in turn
providing more accurate and context-specific assistance. An option to upload
additional study materials in the web GUI is added in case additional knowledge
support is required. The system's efficacy will be evaluated through controlled
trials and iterative feedback mechanisms.",Syed Hasib Akhter Faruqui
2024-05-01T11:06:31Z,http://arxiv.org/abs/2405.00449v1,"RAG-based Explainable Prediction of Road Users Behaviors for Automated
  Driving using Knowledge Graphs and Large Language Models","Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.",Mohamed Manzour Hussien
2024-05-03T16:38:51Z,http://arxiv.org/abs/2405.02228v2,"REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific
  Sentences using Public and Proprietary LLMs","Automatic citation generation for sentences in a document or report is
paramount for intelligence analysts, cybersecurity, news agencies, and
education personnel. In this research, we investigate whether large language
models (LLMs) are capable of generating references based on two forms of
sentence queries: (a) Direct Queries, LLMs are asked to provide author names of
the given research article, and (b) Indirect Queries, LLMs are asked to provide
the title of a mentioned article when given a sentence from a different
article. To demonstrate where LLM stands in this task, we introduce a large
dataset called REASONS comprising abstracts of the 12 most popular domains of
scientific research on arXiv. From around 20K research articles, we make the
following deductions on public and proprietary LLMs: (a) State-of-the-art,
often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass
percentage (PP) to minimize the hallucination rate (HR). When tested with
Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant
metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented
generation (RAG) using Mistral demonstrates consistent and robust citation
support on indirect queries and matched performance to GPT-3.5 and GPT-4. The
HR across all domains and models decreased by an average of 41.93%, and the PP
was reduced to 0% in most cases. In terms of generation quality, the average F1
Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with
adversarial samples showed that LLMs, including the Advance RAG Mistral,
struggle to understand context, but the extent of this issue was small in
Mistral and GPT-4-Preview. Our study contributes valuable insights into the
reliability of RAG for automated citation generation tasks.",Deepa Tilwani
2024-05-03T02:48:55Z,http://arxiv.org/abs/2405.02355v3,"CodeGRAG: Bridging the Gap between Natural Language and Programming
  Language via Graphical Retrieval Augmented Generation","Utilizing large language models to generate codes has shown promising meaning
in software development revolution. Despite the intelligence shown by the
general large language models, their specificity in code generation can still
be improved due to the syntactic gap and mismatched vocabulary existing among
natural language and different programming languages. In this paper, we propose
CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance
the performance of LLMs. CodeGRAG builds the graphical view of code blocks
based on the control flow and data flow of them to fill the gap between
programming languages and natural language, which can facilitate natural
language based LLMs for better understanding of code syntax and serve as a
bridge among different programming languages. To take the extracted structural
knowledge into the foundation models, we propose 1) a hard meta-graph prompt
template to transform the challenging graphical representation into informative
knowledge for tuning-free models and 2) a soft prompting technique that injects
the domain knowledge of programming languages into the model parameters via
finetuning the models with the help of a pretrained GNN expert model. Various
experiments and ablations are done on four datasets including both the C++ and
python languages to validate the hard meta-graph prompt, the soft prompting
technique, and the effectiveness of the objectives for pretrained GNN expert.
CodeGRAG improves the code generation ability of LLMs and can even offer
performance gain for cross-lingual code generation. Code is available at
https://anonymous.4open.science/r/Code-5970/.",Kounianhua Du
2024-05-10T02:48:45Z,http://arxiv.org/abs/2405.06211v3,"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language
  Models","As one of the most advanced techniques in AI, Retrieval-Augmented Generation
(RAG) can offer reliable and up-to-date external knowledge, providing huge
convenience for numerous tasks. Particularly in the era of AI-Generated Content
(AIGC), the powerful capacity of retrieval in providing additional knowledge
enables RAG to assist existing generative AI in producing high-quality outputs.
Recently, Large Language Models (LLMs) have demonstrated revolutionary
abilities in language understanding and generation, while still facing inherent
limitations, such as hallucinations and out-of-date internal knowledge. Given
the powerful abilities of RAG in providing the latest and helpful auxiliary
information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged
to harness external and authoritative knowledge bases, rather than solely
relying on the model's internal knowledge, to augment the generation quality of
LLMs. In this survey, we comprehensively review existing research studies in
RA-LLMs, covering three primary technical perspectives: architectures, training
strategies, and applications. As the preliminary knowledge, we briefly
introduce the foundations and recent advances of LLMs. Then, to illustrate the
practical significance of RAG for LLMs, we systematically review mainstream
relevant work by their architectures, training strategies, and application
areas, detailing specifically the challenges of each and the corresponding
capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss
current limitations and several promising directions for future research.
Updated information about this survey can be found at
https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",Wenqi Fan
2024-05-18T22:43:44Z,http://arxiv.org/abs/2405.11407v2,Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?,"Advancements in deep learning have generated a large-scale interest in the
development of foundational deep learning models. The development of Large
Language Models (LLM) has evolved as a transformative paradigm in
conversational tasks, which has led to its integration and extension even in
the critical domain of healthcare. With LLMs becoming widely popular and their
public access through open-source models and integration with other
applications, there is a need to investigate their potential and limitations.
One such crucial task where LLMs are applied but require a deeper understanding
is that of self-diagnosis of medical conditions based on bias-validating
symptoms in the interest of public health. The widespread integration of Gemini
with Google search and GPT-4.0 with Bing search has led to a shift in the trend
of self-diagnosis using search engines to conversational LLM models. Owing to
the critical nature of the task, it is prudent to investigate and understand
the potential and limitations of public LLMs in the task of self-diagnosis. In
this study, we prepare a prompt engineered dataset of 10000 samples and test
the performance on the general task of self-diagnosis. We compared the
performance of both the state-of-the-art GPT-4.0 and the fee Gemini model on
the task of self-diagnosis and recorded contrasting accuracies of 63.07% and
6.01%, respectively. We also discuss the challenges, limitations, and potential
of both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future
research and towards the broader impact of general public knowledge.
Furthermore, we demonstrate the potential and improvement in performance for
the task of self-diagnosis using Retrieval Augmented Generation.",Nikil Sharan Prabahar Balasubramanian
2024-05-23T13:32:07Z,http://arxiv.org/abs/2405.14554v2,"SearchLVLMs: A Plug-and-Play Framework for Augmenting Large
  Vision-Language Models by Searching Up-to-Date Internet Knowledge","Large vision-language models (LVLMs) are ignorant of the up-to-date
knowledge, such as LLaVA series, because they cannot be updated frequently due
to the large amount of resources required, and therefore fail in many cases.
For example, if a LVLM was released on January 2024, and it wouldn't know the
singer of the theme song for the new Detective Conan movie, which wasn't
released until April 2024. To solve the problem, a promising solution motivated
by retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date
knowledge via internet search during inference, i.e., internet-augmented
generation (IAG), which is already integrated in some closed-source commercial
LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain
a mystery. In this paper, we propose a plug-and-play framework, for augmenting
existing LVLMs in handling visual question answering (VQA) about up-to-date
knowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to
effectively and efficiently find the most helpful content from the websites
returned by a search engine to prompt LVLMs with up-to-date knowledge. To train
the model and evaluate our framework's performance, we propose a pipeline to
automatically generate news-related VQA samples to construct a dataset, dubbed
UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness
of website/content for VQA samples to construct the training set. Experimental
results demonstrate the effectiveness of our framework, outperforming GPT-4V by
about 25% in accuracy.",Chuanhao Li
2024-05-24T17:34:32Z,http://arxiv.org/abs/2405.15739v3,"Large Language Models Reflect Human Citation Patterns with a Heightened
  Citation Bias","Citation practices are crucial in shaping the structure of scientific
knowledge, yet they are often influenced by contemporary norms and biases. The
emergence of Large Language Models (LLMs) introduces a new dynamic to these
practices. Interestingly, the characteristics and potential biases of
references recommended by LLMs that entirely rely on their parametric
knowledge, and not on search or retrieval-augmented generation, remain
unexplored. Here, we analyze these characteristics in an experiment using a
dataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge
cut-off date. In our experiment, LLMs are tasked with suggesting scholarly
references for the anonymized in-text citations within these papers. Our
findings reveal a remarkable similarity between human and LLM citation
patterns, but with a more pronounced high citation bias, which persists even
after controlling for publication year, title length, number of authors, and
venue. The results hold for both GPT-4, and the more capable models GPT-4o and
Claude 3.5 where the papers are part of the training data. Additionally, we
observe a large consistency between the characteristics of LLM's existing and
non-existent generated references, indicating the model's internalization of
citation patterns. By analyzing citation graphs, we show that the references
recommended are embedded in the relevant citation context, suggesting an even
deeper conceptual internalization of the citation networks. While LLMs can aid
in citation generation, they may also amplify existing biases, such as the
Matthew effect, and introduce new ones, potentially skewing scientific
knowledge dissemination.",Andres Algaba
2024-05-30T17:56:05Z,http://arxiv.org/abs/2405.20362v1,"Hallucination-Free? Assessing the Reliability of Leading AI Legal
  Research Tools","Legal practice has witnessed a sharp rise in products incorporating
artificial intelligence (AI). Such tools are designed to assist with a wide
range of core legal tasks, from search and summarization of caselaw to document
drafting. But the large language models used in these tools are prone to
""hallucinate,"" or make up false information, making their use risky in
high-stakes domains. Recently, certain legal research providers have touted
methods such as retrieval-augmented generation (RAG) as ""eliminating""
(Casetext, 2023) or ""avoid[ing]"" hallucinations (Thomson Reuters, 2023), or
guaranteeing ""hallucination-free"" legal citations (LexisNexis, 2023). Because
of the closed nature of these systems, systematically assessing these claims is
challenging. In this article, we design and report on the first preregistered
empirical evaluation of AI-driven legal research tools. We demonstrate that the
providers' claims are overstated. While hallucinations are reduced relative to
general-purpose chatbots (GPT-4), we find that the AI research tools made by
LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and
Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also
document substantial differences between systems in responsiveness and
accuracy. Our article makes four key contributions. It is the first to assess
and report the performance of RAG-based proprietary legal AI tools. Second, it
introduces a comprehensive, preregistered dataset for identifying and
understanding vulnerabilities in these systems. Third, it proposes a clear
typology for differentiating between hallucinations and accurate legal
responses. Last, it provides evidence to inform the responsibilities of legal
professionals in supervising and verifying AI outputs, which remains a central
open question for the responsible integration of AI into law.",Varun Magesh
2024-06-07T08:43:07Z,http://arxiv.org/abs/2406.04744v2,CRAG -- Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.",Xiao Yang
2024-06-26T03:32:35Z,http://arxiv.org/abs/2406.18039v1,"Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model
  with Three Types of Knowledge","Liver cancer has a high incidence rate, but primary healthcare settings often
lack experienced doctors. Advances in large models and AI technologies offer
potential assistance. This work aims to address limitations in liver cancer
diagnosis models, such as poor understanding of medical images, insufficient
consideration of liver blood vessels, and ensuring accurate medical
information. We propose a specialized diagnostic assistant to improve the
diagnostic capabilities of less experienced doctors. Our framework combines
large and small models, using optimized small models for precise patient image
perception. Specifically, a segmentation network iteratively removes ambiguous
pixels for liver tumor segmentation, and a multi-scale, multi-level
differential network segments liver vessels. Features from these segmentations
and medical records form a patient's personalized knowledge base. For
diagnosis, Chain of Thought (COT) technology designs prompts mimicking
experienced doctors' thought patterns, and Retrieval-Augmented Generation (RAG)
technology provides answers based on reliable domain knowledge and trusted
cases. Our small model methods improve liver tumor and vessel segmentation
performance, resulting in more accurate information extraction. The large model
component scores over 1 point higher on a 10-point scale in evaluations by
doctors compared to control methods. Our method enhances semantic perception of
medical images, improves classification of ambiguous pixels, and optimizes
small object perception. It considers blood vessel positions for specific
treatments and improves response credibility and interpretability by mimicking
experienced doctors' thought processes using reliable resources. This approach
has been recognized by doctors and benefits liver cancer auxiliary diagnosis.",Xuzhou Wu
2024-06-26T12:51:37Z,http://arxiv.org/abs/2406.18312v4,AI-native Memory: A Pathway from LLMs Towards AGI,"Large language models (LLMs) have demonstrated the world with the sparks of
artificial general intelligence (AGI). One opinion, especially from some
startups working on LLMs, argues that an LLM with nearly unlimited context
length can realize AGI. However, they might be too optimistic about the
long-context capability of (existing) LLMs -- (1) Recent literature has shown
that their effective context length is significantly smaller than their claimed
context length; and (2) Our reasoning-in-a-haystack experiments further
demonstrate that simultaneously finding the relevant information from a long
context and conducting (simple) reasoning is nearly impossible. In this paper,
we envision a pathway from LLMs to AGI through the integration of
\emph{memory}. We believe that AGI should be a system where LLMs serve as core
processors. In addition to raw data, the memory in this system would store a
large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw
data, this approach not only connects semantically related information closer,
but also simplifies complex inferences at the time of querying. As an
intermediate stage, the memory will likely be in the form of natural language
descriptions, which can be directly consumed by users too. Ultimately, every
agent/person should have its own large personal model, a deep neural network
model (thus \emph{AI-native}) that parameterizes and compresses all types of
memory, even the ones cannot be described by natural languages. Finally, we
discuss the significant potential of AI-native memory as the transformative
infrastructure for (proactive) engagement, personalization, distribution, and
social in the AGI era, as well as the incurred privacy and security challenges
with preliminary solutions.",Jingbo Shang
2024-06-29T15:23:28Z,http://arxiv.org/abs/2407.00466v1,"BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for
  Biomedical Science","Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,
draws increasing attention, where one common approach is to build a copilot
agent driven by Large Language Models (LLMs). However, to evaluate such
systems, people either rely on direct Question-Answering (QA) to the LLM
itself, or in a biomedical experimental manner. How to precisely benchmark
biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of
scientists, understanding the literature, and introduce BioKGBench. In contrast
to traditional evaluation benchmark that only focuses on factual QA, where the
LLMs are known to have hallucination issues, we first disentangle
""Understanding Literature"" into two atomic abilities, i) ""Understanding"" the
unstructured text from research papers by performing scientific claim
verification, and ii) Ability to interact with structured Knowledge-Graph
Question-Answering (KGQA) as a form of ""Literature"" grounding. We then
formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based
Retrieval-Augmented Generation (RAG) to identify the factual errors of existing
large-scale knowledge graph databases. We collect over two thousand data for
two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios
and biomedical ones, have either failed or inferior performance on our
benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors
which provide scenarios for agents to make discoveries and demonstrate the
effectiveness of our approach. The code and data are available at
https://github.com/westlake-autolab/BioKGBench.",Xinna Lin
2024-07-05T12:42:31Z,http://arxiv.org/abs/2407.04472v3,"EventChat: Implementation and user-centric evaluation of a large
  language model-driven conversational recommender system for exploring leisure
  events in an SME context","Large language models (LLMs) present an enormous evolution in the strategic
potential of conversational recommender systems (CRS). Yet to date, research
has predominantly focused upon technical frameworks to implement LLM-driven
CRS, rather than end-user evaluations or strategic implications for firms,
particularly from the perspective of a small to medium enterprises (SME) that
makeup the bedrock of the global economy. In the current paper, we detail the
design of an LLM-driven CRS in an SME setting, and its subsequent performance
in the field using both objective system metrics and subjective user
evaluations. While doing so, we additionally outline a short-form revised
ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly
evolving field. Our results reveal good system performance from a user
experience perspective (85.5% recommendation accuracy) but underscore latency,
cost, and quality issues challenging business viability. Notably, with a median
cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and
response time emerge as crucial areas for achieving a more user-friendly and
economically viable LLM-driven CRS for SME settings. One major driver of these
costs is the use of an advanced LLM as a ranker within the retrieval-augmented
generation (RAG) technique. Our results additionally indicate that relying
solely on approaches such as Prompt-based learning with ChatGPT as the
underlying LLM makes it challenging to achieve satisfying quality in a
production environment. Strategic considerations for SMEs deploying an
LLM-driven CRS are outlined, particularly considering trade-offs in the current
technical landscape.",Hannes Kunstmann
2024-07-10T02:33:09Z,http://arxiv.org/abs/2407.07321v2,"Examining Long-Context Large Language Models for Environmental Review
  Document Comprehension","As LLMs become increasingly ubiquitous, researchers have tried various
techniques to augment the knowledge provided to these models. Long context and
retrieval-augmented generation (RAG) are two such methods that have recently
gained popularity. In this work, we examine the benefits of both of these
techniques by utilizing question answering (QA) task in a niche domain. While
the effectiveness of LLM-based QA systems has already been established at an
acceptable level in popular domains such as trivia and literature, it has not
often been established in niche domains that traditionally require specialized
expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance
of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and
Mistral -- when answering questions originating from Environmental Impact
Statements prepared by U.S. federal government agencies in accordance with the
National Environmental Environmental Act (NEPA). We specifically measure the
ability of LLMs to understand the nuances of legal, technical, and
compliance-related information present in NEPA documents in different
contextual scenarios. We test the LLMs' internal prior NEPA knowledge by
providing questions without any context, as well as assess how LLMs synthesize
the contextual information present in long NEPA documents to facilitate the
question/answering task. We compare the performance of the models in handling
different types of questions (e.g., problem-solving, divergent, etc.). Our
results suggest that RAG powered models significantly outperform those provided
with only the PDF context in terms of answer accuracy, regardless of the choice
of the LLM. Our further analysis reveals that many models perform better
answering closed type questions (Yes/No) than divergent and problem-solving
questions.",Hung Phan
2024-06-05T03:32:06Z,http://arxiv.org/abs/2407.11987v1,SlicerChat: Building a Local Chatbot for 3D Slicer,"3D Slicer is a powerful platform for 3D data visualization and analysis, but
has a significant learning curve for new users. Generative AI applications,
such as ChatGPT, have emerged as a potential method of bridging the gap between
various sources of documentation using natural language. The limited exposure
of LLM services to 3D Slicer documentation, however, means that ChatGPT and
related services tend to suffer from significant hallucination. The objective
of this project is to build a chatbot architecture, called SlicerChat, that is
optimized to answer 3D Slicer related questions and able to run locally using
an open-source model. The core research questions explored in this work revolve
around the answer quality and speed differences due to fine-tuning, model size,
and the type of domain knowledge included in the prompt. A prototype SlicerChat
system was built as a custom extension in 3D Slicer based on the Code-Llama
Instruct architecture. Models of size 1.1B, 7B and 13B were fine-tuned using
Low rank Adaptation, and various sources of 3D Slicer documentation were
compiled for use in a Retrieval Augmented Generation paradigm. Testing
combinations of fine-tuning and model sizes on a benchmark dataset of five 3D
Slicer questions revealed that fine-tuning had no impact on model performance
or speed compared to the base architecture, and that larger models performed
better with a significant speed decrease. Experiments with adding 3D Slicer
documentation to the prompt showed that Python sample code and Markdown
documentation were the most useful information to include, but that adding 3D
Slicer scene data and questions taken from Discourse also improved model
performance. In conclusion, this project shows the potential for integrating a
high quality, local chatbot directly into 3D Slicer to help new users and
experienced developers alike to more efficiently use the software.",Colton Barr
2024-07-19T17:35:47Z,http://arxiv.org/abs/2407.14482v2,"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities","In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K
context window, designed to bridge the gap between open-source LLMs and leading
proprietary models (e.g., GPT-4-Turbo) in long-context understanding and
retrieval-augmented generation (RAG) capabilities. These two capabilities are
essential for LLMs to process large volumes of information that cannot fit into
a single prompt and are complementary to each other, depending on the
downstream tasks and computational budgets. We present a detailed continued
training recipe to extend the context window of Llama3-70B-base from 8K to 128K
tokens, along with a three-stage instruction tuning process to enhance the
model's instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
outperforms most existing state-of-the-art models, including
GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on
ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only
a 4K context window, showing the strong long context capability across varying
sequence lengths. We further provide extensive comparisons between direct
long-context and RAG solutions using the same state-of-the-art long-context
LLMs. Interestingly, we find that the performance of strong long-context LLMs
using RAG improves when retrieving a larger number of chunks. With a large set
of top-k chunks, RAG consistently outperforms direct long-context solution
using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B
and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To
advance research in this field, we open-sourced the model weights, training
data, and the evaluation setup for the for the community:
https://chatqa2-project.github.io/",Peng Xu
2024-07-19T18:08:39Z,http://arxiv.org/abs/2407.14609v1,"Adversarial Databases Improve Success in Retrieval-based Large Language
  Models","Open-source LLMs have shown great potential as fine-tuned chatbots, and
demonstrate robust abilities in reasoning and surpass many existing benchmarks.
Retrieval-Augmented Generation (RAG) is a technique for improving the
performance of LLMs on tasks that the models weren't explicitly trained on, by
leveraging external knowledge databases. Numerous studies have demonstrated the
effectiveness of RAG to more successfully accomplish downstream tasks when
using vector datasets that consist of relevant background information. It has
been implicitly assumed by those in the field that if adversarial background
information is utilized in this context, that the success of using a RAG-based
approach would be nonexistent or even negatively impact the results. To address
this assumption, we tested several open-source LLMs on the ability of RAG to
improve their success in answering multiple-choice questions (MCQ) in the
medical subspecialty field of Nephrology. Unlike previous studies, we examined
the effect of RAG in utilizing both relevant and adversarial background
databases. We set up several open-source LLMs, including Llama 3, Phi-3,
Mixtral 8x7b, Zephyr$\beta$, and Gemma 7B Instruct, in a zero-shot RAG
pipeline. As adversarial sources of information, text from the Bible and a
Random Words generated database were used for comparison. Our data show that
most of the open-source LLMs improve their multiple-choice test-taking success
as expected when incorporating relevant information vector databases.
Surprisingly however, adversarial Bible text significantly improved the success
of many LLMs and even random word text improved test taking ability of some of
the models. In summary, our results demonstrate for the first time the
countertintuitive ability of adversarial information datasets to improve the
RAG-based LLM success.",Sean Wu
2024-07-26T11:00:08Z,http://arxiv.org/abs/2408.00804v1,"ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large
  Language Model","The field of integrated circuit (IC) design is highly specialized, presenting
significant barriers to entry and research and development challenges. Although
large language models (LLMs) have achieved remarkable success in various
domains, existing LLMs often fail to meet the specific needs of students,
engineers, and researchers. Consequently, the potential of LLMs in the IC
design domain remains largely unexplored. To address these issues, we introduce
ChipExpert, the first open-source, instructional LLM specifically tailored for
the IC design field. ChipExpert is trained on one of the current best
open-source base model (Llama-3 8B). The entire training process encompasses
several key stages, including data preparation, continue pre-training,
instruction-guided supervised fine-tuning, preference alignment, and
evaluation. In the data preparation stage, we construct multiple high-quality
custom datasets through manual selection and data synthesis techniques. In the
subsequent two stages, ChipExpert acquires a vast amount of IC design knowledge
and learns how to respond to user queries professionally. ChipExpert also
undergoes an alignment phase, using Direct Preference Optimization, to achieve
a high standard of ethical performance. Finally, to mitigate the hallucinations
of ChipExpert, we have developed a Retrieval-Augmented Generation (RAG) system,
based on the IC design knowledge base. We also released the first IC design
benchmark ChipICD-Bench, to evaluate the capabilities of LLMs across multiple
IC design sub-domains. Through comprehensive experiments conducted on this
benchmark, ChipExpert demonstrated a high level of expertise in IC design
knowledge Question-and-Answer tasks.",Ning Xu
2024-08-02T21:54:13Z,http://arxiv.org/abs/2408.01585v3,"LibreLog: Accurate and Efficient Unsupervised Log Parsing Using
  Open-Source Large Language Models","Log parsing is a critical step that transforms unstructured log data into
structured formats, facilitating subsequent log-based analysis. Traditional
syntax-based log parsers are efficient and effective, but they often experience
decreased accuracy when processing logs that deviate from the predefined rules.
Recently, large language models (LLM) based log parsers have shown superior
parsing accuracy. However, existing LLM-based parsers face three main
challenges: 1)time-consuming and labor-intensive manual labeling for
fine-tuning or in-context learning, 2)increased parsing costs due to the vast
volume of log data and limited context size of LLMs, and 3)privacy risks from
using commercial models like ChatGPT with sensitive log information. To
overcome these limitations, this paper introduces LibreLog, an unsupervised log
parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance
privacy and reduce operational costs while achieving state-of-the-art parsing
accuracy. LibreLog first groups logs with similar static text but varying
dynamic variables using a fixed-depth grouping tree. It then parses logs within
these groups using three components: i)similarity scoring-based retrieval
augmented generation: selects diverse logs within each group based on Jaccard
similarity, helping the LLM distinguish between static text and dynamic
variables; ii)self-reflection: iteratively query LLMs to refine log templates
to improve parsing accuracy; and iii) log template memory: stores parsed
templates to reduce LLM queries for improved parsing efficiency. Our evaluation
on LogHub-2.0 shows that LibreLog achieves 25% higher parsing accuracy and
processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers.
In short, LibreLog addresses privacy and cost concerns of using commercial LLMs
while achieving state-of-the-arts parsing efficiency and accuracy.",Zeyang Ma
2024-08-06T02:09:35Z,http://arxiv.org/abs/2408.02900v1,"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular
  Annotations for Medicine","This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal
dataset for medicine, covering over 25 million images across 10 modalities,
with multigranular annotations for more than 65 diseases. These enriched
annotations encompass both global textual information, such as disease/lesion
type, modality, region-specific descriptions, and inter-regional relationships,
as well as detailed local annotations for regions of interest (ROIs), including
bounding boxes, segmentation masks. Unlike existing approach which is limited
by the availability of image-text pairs, we have developed the first automated
pipeline that scales up multimodal data by generating multigranular visual and
texual annotations (in the form of image-ROI-description triplets) without the
need for any paired text descriptions. Specifically, data from over 90
different sources have been collected, preprocessed, and grounded using
domain-specific expert models to identify ROIs related to abnormal regions. We
then build a comprehensive knowledge base and prompt multimodal large language
models to perform retrieval-augmented generation with the identified ROIs as
guidance, resulting in multigranular texual descriptions. Compared to existing
datasets, MedTrinity-25M provides the most enriched annotations, supporting a
comprehensive range of multimodal tasks such as captioning and report
generation, as well as vision-centric tasks like classification and
segmentation. Pretraining on MedTrinity-25M, our model achieves
state-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal
large language models and other representative SoTA approaches. This dataset
can also be utilized to support large-scale pre-training of multimodal medical
AI models, contributing to the development of future foundation models in the
medical domain.",Yunfei Xie
2024-08-06T14:53:25Z,http://arxiv.org/abs/2408.04665v1,"LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations","The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
literature text has been challenging but crucial for the logical design of new
MOFs with desirable functionality. The recent advent of large language models
(LLMs) provides disruptively new solution to this long-standing problem and
latest researches have reported over 90% F1 in extracting correct conditions
from MOFs literature. We argue in this paper that most existing synthesis
extraction practices with LLMs stay with the primitive zero-shot learning,
which could lead to downgraded extraction and application performance due to
the lack of specialized knowledge. This work pioneers and optimizes the
few-shot in-context learning paradigm for LLM extraction of material synthesis
conditions. First, we propose a human-AI joint data curation process to secure
high-quality ground-truth demonstrations for few-shot learning. Second, we
apply a BM25 algorithm based on the retrieval-augmented generation (RAG)
technique to adaptively select few-shot demonstrations for each MOF's
extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the
proposed few-shot method achieves much higher average F1 performance (0.93 vs.
0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
fully automatic evaluation that are more objective than the previous human
evaluation. The proposed method is further validated through real-world
material experiments: compared with the baseline zero-shot LLM, the proposed
few-shot approach increases the MOFs structural inference performance (R^2) by
29.4% in average.",Lei Shi
2024-08-15T21:09:09Z,http://arxiv.org/abs/2408.08422v1,"Assessing and Enhancing Large Language Models in Rare Disease
  Question-answering","Despite the impressive capabilities of Large Language Models (LLMs) in
general medical domains, questions remain about their performance in diagnosing
rare diseases. To answer this question, we aim to assess the diagnostic
performance of LLMs in rare diseases, and explore methods to enhance their
effectiveness in this area. In this work, we introduce a rare disease
question-answering (ReDis-QA) dataset to evaluate the performance of LLMs in
diagnosing rare diseases. Specifically, we collected 1360 high-quality
question-answer pairs within the ReDis-QA dataset, covering 205 rare diseases.
Additionally, we annotated meta-data for each question, facilitating the
extraction of subsets specific to any given disease and its property. Based on
the ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that
diagnosing rare diseases remains a significant challenge for these models.
  To facilitate retrieval augmentation generation for rare disease diagnosis,
we collect the first rare diseases corpus (ReCOP), sourced from the National
Organization for Rare Disorders (NORD) database. Specifically, we split the
report of each rare disease into multiple chunks, each representing a different
property of the disease, including their overview, symptoms, causes, effects,
related disorders, diagnosis, and standard therapies. This structure ensures
that the information within each chunk aligns consistently with a question.
Experiment results demonstrate that ReCOP can effectively improve the accuracy
of LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly
guides LLMs to generate trustworthy answers and explanations that can be traced
back to existing literature.",Guanchu Wang
2024-08-21T13:34:29Z,http://arxiv.org/abs/2408.11609v2,Xinyu: An Efficient LLM-based System for Commentary Generation,"Commentary provides readers with a deep understanding of events by presenting
diverse arguments and evidence. However, creating commentary is a
time-consuming task, even for skilled commentators. Large language models
(LLMs) have simplified the process of natural language generation, but their
direct application in commentary creation still faces challenges due to unique
task requirements. These requirements can be categorized into two levels: 1)
fundamental requirements, which include creating well-structured and logically
consistent narratives, and 2) advanced requirements, which involve generating
quality arguments and providing convincing evidence. In this paper, we
introduce Xinyu, an efficient LLM-based system designed to assist commentators
in generating Chinese commentaries. To meet the fundamental requirements, we
deconstruct the generation process into sequential steps, proposing targeted
strategies and supervised fine-tuning (SFT) for each step. To address the
advanced requirements, we present an argument ranking model for arguments and
establish a comprehensive evidence database that includes up-to-date events and
classic books, thereby strengthening the substantiation of the evidence with
retrieval augmented generation (RAG) technology. To evaluate the generated
commentaries more fairly, corresponding to the two-level requirements, we
introduce a comprehensive evaluation metric that considers five distinct
perspectives in commentary generation. Our experiments confirm the
effectiveness of our proposed system. We also observe a significant increase in
the efficiency of commentators in real-world scenarios, with the average time
spent on creating a commentary dropping from 4 hours to 20 minutes.
Importantly, such an increase in efficiency does not compromise the quality of
the commentaries.",Yiquan Wu
2024-08-25T13:36:22Z,http://arxiv.org/abs/2408.13833v1,"Biomedical Large Languages Models Seem not to be Superior to Generalist
  Models on Unseen Medical Data","Large language models (LLMs) have shown potential in biomedical applications,
leading to efforts to fine-tune them on domain-specific data. However, the
effectiveness of this approach remains unclear. This study evaluates the
performance of biomedically fine-tuned LLMs against their general-purpose
counterparts on a variety of clinical tasks. We evaluated their performance on
clinical case challenges from the New England Journal of Medicine (NEJM) and
the Journal of the American Medical Association (JAMA) and on several clinical
tasks (e.g., information extraction, document summarization, and clinical
coding). Using benchmarks specifically chosen to be likely outside the
fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly
perform inferior to their general-purpose counterparts, especially on tasks not
focused on medical knowledge. While larger models showed similar performance on
case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA
cases), smaller biomedical models showed more pronounced underperformance
(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).
Similar trends were observed across the CLUE (Clinical Language Understanding
Evaluation) benchmark tasks, with general-purpose models often performing
better on text generation, question answering, and coding tasks. Our results
suggest that fine-tuning LLMs to biomedical data may not provide the expected
benefits and may potentially lead to reduced performance, challenging
prevailing assumptions about domain-specific adaptation of LLMs and
highlighting the need for more rigorous evaluation frameworks in healthcare AI.
Alternative approaches, such as retrieval-augmented generation, may be more
effective in enhancing the biomedical capabilities of LLMs without compromising
their general knowledge.",Felix J. Dorfner
2024-09-06T14:58:30Z,http://arxiv.org/abs/2409.13709v1,"Column Vocabulary Association (CVA): semantic interpretation of dataless
  tables","Traditional Semantic Table Interpretation (STI) methods rely primarily on the
underlying table data to create semantic annotations. This year's SemTab
challenge introduced the ``Metadata to KG'' track, which focuses on performing
STI by using only metadata information, without access to the underlying data.
In response to this new challenge, we introduce a new term: Column Vocabulary
Association (CVA). This term refers to the task of semantic annotation of
column headers solely based on metadata information. In this study, we evaluate
the performance of various methods in executing the CVA task, including a Large
Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as
well as a more traditional similarity approach with SemanticBERT. Our
methodology uses a zero-shot setting, with no pretraining or examples passed to
the Large Language Models (LLMs), as we aim to avoid a domain-specific setting.
  We investigate a total of 7 different LLMs, of which three commercial GPT
models (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source
models (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate
this models with RAG systems, and we explore how variations in temperature
settings affect performances. Moreover, we continue our investigation by
performing the CVA task utilizing SemanticBERT, analyzing how various metadata
information influence its performance.
  Initial findings indicate that LLMs generally perform well at temperatures
below 1.0, achieving an accuracy of 100\% in certain cases. Nevertheless, our
investigation also reveal that the nature of the data significantly influences
CVA task outcomes. In fact, in cases where the input data and glossary are
related (for example by being created by the same organizations) traditional
methods appear to surpass the performance of LLMs.",Margherita Martorana
2024-09-10T02:00:28Z,http://arxiv.org/abs/2409.13731v3,"KAG: Boosting LLMs in Professional Domains via Knowledge Augmented
  Generation","The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.",Lei Liang
2024-09-23T17:22:09Z,http://arxiv.org/abs/2409.15228v3,"A Comprehensive Framework for Evaluating API-oriented Code Generation in
  Large Language Models","Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as
powerful tools for code generation, significantly enhancing productivity and
accelerating software development. However, existing benchmarks primarily focus
on general code generation without considering API-oriented code generation,
i.e., generating code that invokes APIs from specific libraries. Given the
growing demand for API-oriented code generation, there is a pressing need for a
systematic and automated approach to evaluate LLM on API-oriented code
generation. To address this gap, we propose AutoAPIEval, a lightweight and
automated framework designed to evaluate the capabilities of LLMs in
API-oriented code generation. Our framework works with any library that
provides API documentation and focuses on two unit tasks: API recommendation
and code example generation, along with four metrics to evaluate the generated
APIs and code examples, such as the proportion of incorrect API recommendations
for Task 1, and the proportion of code examples where no specific API is
invoked and uncompilable/unexecutable code examples for Task 2. In addition, we
conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)
and Java Runtime Environment 8 to demonstrate the framework's effectiveness.
Our findings reveal substantial variability in LLM performance across tasks,
with ChatGPT adhering better to instructions, while sharing similar
effectiveness in code example generation with its counterparts (i.e., MagiCoder
and DeekSeek Coder). We also identify key factors associated with code quality,
such as API popularity and model confidence, and build classifiers that achieve
high accuracy in detecting incorrect API recommendations and erroneous code
examples. Retrieval-augmented generation enhances the quality of code generated
by LLMs, though its effectiveness varies across different LLMs.",Yixi Wu
2024-09-26T17:30:28Z,http://arxiv.org/abs/2409.18164v2,Data-Prep-Kit: getting your data ready for LLM application development,"Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).",David Wood
2024-09-27T17:17:15Z,http://arxiv.org/abs/2409.18924v2,"AIPatient: Simulating Patients with EHRs and LLM Powered Agentic
  Workflow","Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.",Huizi Yu
2024-10-03T16:25:37Z,http://arxiv.org/abs/2410.02642v1,Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers,"Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.",Shijie Chen
2024-09-28T18:52:16Z,http://arxiv.org/abs/2410.03721v1,"Thematic Analysis with Open-Source Generative AI and Machine Learning: A
  New Method for Inductive Qualitative Codebook Development","This paper aims to answer one central question: to what extent can
open-source generative text models be used in a workflow to approximate
thematic analysis in social science research? To answer this question, we
present the Generative AI-enabled Theme Organization and Structuring (GATOS)
workflow, which uses open-source machine learning techniques, natural language
processing tools, and generative text models to facilitate thematic analysis.
To establish validity of the method, we present three case studies applying the
GATOS workflow, leveraging these models and techniques to inductively create
codebooks similar to traditional procedures using thematic analysis.
Specifically, we investigate the extent to which a workflow comprising
open-source models and tools can inductively produce codebooks that approach
the known space of themes and sub-themes. To address the challenge of gleaning
insights from these texts, we combine open-source generative text models,
retrieval-augmented generation, and prompt engineering to identify codes and
themes in large volumes of text, i.e., generate a qualitative codebook. The
process mimics an inductive coding process that researchers might use in
traditional thematic analysis by reading text one unit of analysis at a time,
considering existing codes already in the codebook, and then deciding whether
or not to generate a new code based on whether the extant codebook provides
adequate thematic coverage. We demonstrate this workflow using three synthetic
datasets from hypothetical organizational research settings: a study of
teammate feedback in teamwork settings, a study of organizational cultures of
ethical behavior, and a study of employee perspectives about returning to their
offices after the pandemic. We show that the GATOS workflow is able to identify
themes in the text that were used to generate the original synthetic datasets.",Andrew Katz
2024-10-09T04:36:47Z,http://arxiv.org/abs/2410.06542v1,"MedImageInsight: An Open-Source Embedding Model for General Domain
  Medical Imaging","In this work, we present MedImageInsight, an open-source medical imaging
embedding model. MedImageInsight is trained on medical images with associated
text and labels across a diverse collection of domains, including X-Ray, CT,
MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and
mammography. Rigorous evaluations demonstrate MedImageInsight's ability to
achieve state-of-the-art (SOTA) or human expert level performance across
classification, image-image search, and fine-tuning tasks. Specifically, on
public datasets, MedImageInsight achieves SOTA in CT 3D medical image
retrieval, as well as SOTA in disease classification and search for chest
X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves
human expert performance in bone age estimation (on both public and partner
data), as well as AUC above 0.9 in most other domains. When paired with a text
decoder, MedImageInsight achieves near SOTA level single image report findings
generation with less than 10\% the parameters of other models. Compared to
fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight
outperforms in clinical metrics, but underperforms on lexical metrics where
GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight
can generate ROC curves, adjust sensitivity and specificity based on clinical
need, and provide evidence-based decision support through image-image search
(which can also enable retrieval augmented generation). In an independent
clinical evaluation of image-image search in chest X-ray, MedImageInsight
outperformed every other publicly available foundation model evaluated by large
margins (over 6 points AUC), and significantly outperformed other models in
terms of AI fairness (across age and gender). We hope releasing MedImageInsight
will help enhance collective progress in medical imaging AI research and
development.",Noel C. F. Codella
2024-10-11T00:34:20Z,http://arxiv.org/abs/2410.08431v1,"oRetrieval Augmented Generation for 10 Large Language Models and its
  Generalizability in Assessing Medical Fitness","Large Language Models (LLMs) show potential for medical applications but
often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)
allows customization with domain-specific information, making it suitable for
healthcare. This study evaluates the accuracy, consistency, and safety of RAG
models in determining fitness for surgery and providing preoperative
instructions. We developed LLM-RAG models using 35 local and 23 international
preoperative guidelines and tested them against human-generated responses. A
total of 3,682 responses were evaluated. Clinical documents were processed
using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were
assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects
of preoperative instructions. Established guidelines and expert judgment were
used to determine correct responses, with human-generated answers serving as
comparisons. The LLM-RAG models generated responses within 20 seconds,
significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model
achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no
hallucinations and producing correct instructions comparable to clinicians.
Results were consistent across both local and international guidelines. This
study demonstrates the potential of LLM-RAG models for preoperative healthcare
tasks, highlighting their efficiency, scalability, and reliability.",Yu He Ke
2024-10-19T21:50:11Z,http://arxiv.org/abs/2410.15222v1,"AutoFLUKA: A Large Language Model Based Framework for Automating Monte
  Carlo Simulations in FLUKA","Monte Carlo (MC) simulations, particularly using FLUKA, are essential for
replicating real-world scenarios across scientific and engineering fields.
Despite the robustness and versatility, FLUKA faces significant limitations in
automation and integration with external post-processing tools, leading to
workflows with a steep learning curve, which are time-consuming and prone to
human errors. Traditional methods involving the use of shell and Python
scripts, MATLAB, and Microsoft Excel require extensive manual intervention and
lack flexibility, adding complexity to evolving scenarios. This study explores
the potential of Large Language Models (LLMs) and AI agents to address these
limitations. AI agents, integrate natural language processing with autonomous
reasoning for decision-making and adaptive planning, making them ideal for
automation. We introduce AutoFLUKA, an AI agent application developed using the
LangChain Python Framework to automate typical MC simulation workflows in
FLUKA. AutoFLUKA can modify FLUKA input files, execute simulations, and
efficiently process results for visualization, significantly reducing human
labor and error. Our case studies demonstrate that AutoFLUKA can handle both
generalized and domain-specific cases, such as Microdosimetry, with an
streamlined automated workflow, showcasing its scalability and flexibility. The
study also highlights the potential of Retrieval Augmentation Generation (RAG)
tools to act as virtual assistants for FLUKA, further improving user
experience, time and efficiency. In conclusion, AutoFLUKA represents a
significant advancement in automating MC simulation workflows, offering a
robust solution to the inherent limitations. This innovation not only saves
time and resources but also opens new paradigms for research and development in
high energy physics, medical physics, nuclear engineering space and
environmental science.",Zavier Ndum Ndum
2024-10-21T03:51:54Z,http://arxiv.org/abs/2410.15621v1,"DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on
  Commercial DRAM-PIMs","Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic
similarity search in large datasets, has become a fundamental component of
critical applications such as information retrieval and retrieval-augmented
generation (RAG). However, ANNS is a well-known I/O-intensive algorithm with a
low compute-to-I/O ratio, often requiring massive storage due to the large
volume of high-dimensional data. This leads to I/O bottlenecks on CPUs and
memory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)
architecture, which offers high bandwidth, large-capacity memory, and the
ability to perform efficient computation in or near the data, presents a
promising solution for ANNS. In this work, we investigate the use of commercial
DRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS
engine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM
exhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup
tables (LUTs) to replace more multiplications with I/O operations. We then
systematically tune ANNS to search optimized configurations with lower
computational load, aligning the compute-to-I/O ratio of ANNS with that of
DRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS
algorithm, we further explore implementation optimizations to fully utilize the
two thousand parallel processing units with private local memory in DRAM-PIMs.
To address the load imbalance caused by ANNS requests distributed across
different clusters of large datasets, we propose a load-balancing strategy that
combines static data layout optimization with dynamic runtime request
scheduling. Experimental results on representative datasets show that DRIM-ANN
achieves an average performance speedup of 2.92x compared to a 32-thread CPU
counterpart.",Mingkai Chen
2024-10-21T17:34:39Z,http://arxiv.org/abs/2410.16229v2,Building A Coding Assistant via the Retrieval-Augmented Language Model,"Pretrained language models have shown strong effectiveness in code-related
tasks, such as code retrieval, code generation, code summarization, and code
completion tasks. In this paper, we propose COde assistaNt viA
retrieval-augmeNted language model (CONAN), which aims to build a code
assistant by mimicking the knowledge-seeking behaviors of humans during coding.
Specifically, it consists of a code structure aware retriever (CONAN-R) and a
dual-view code representation-based retrieval-augmented generation model
(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and
Masked Entity Prediction tasks to make language models code structure-aware and
learn effective representations for code snippets and documentation. Then
CONAN-G designs a dual-view code representation mechanism for implementing a
retrieval-augmented code generation model. CONAN-G regards the code
documentation descriptions as prompts, which help language models better
understand the code semantics. Our experiments show that CONAN achieves
convincing performance on different code generation tasks and significantly
outperforms previous retrieval augmented code generation models. Our further
analyses show that CONAN learns tailored representations for both code snippets
and documentation by aligning code-documentation data pairs and capturing
structural semantics by masking and predicting entities in the code data.
Additionally, the retrieved code snippets and documentation provide necessary
information from both program language and natural language to assist the code
generation process. CONAN can also be used as an assistant for Large Language
Models (LLMs), providing LLMs with external knowledge in shorter code document
lengths to improve their effectiveness on various code tasks. It shows the
ability of CONAN to extract necessary information and help filter out the noise
from retrieved code documents.",Xinze Li
2024-10-21T18:08:42Z,http://arxiv.org/abs/2410.16397v1,"Towards a Reliable Offline Personal AI Assistant for Long Duration
  Spaceflight","As humanity prepares for new missions to the Moon and Mars, astronauts will
need to operate with greater autonomy, given the communication delays that make
real-time support from Earth difficult. For instance, messages between Mars and
Earth can take up to 24 minutes, making quick responses impossible. This
limitation poses a challenge for astronauts who must rely on in-situ tools to
access the large volume of data from spacecraft sensors, rovers, and
satellites, data that is often fragmented and difficult to use. To bridge this
gap, systems like the Mars Exploration Telemetry-Driven Information System
(METIS) are being developed. METIS is an AI assistant designed to handle
routine tasks, monitor spacecraft systems, and detect anomalies, all while
reducing the reliance on mission control. Current Generative Pretrained
Transformer (GPT) Models, while powerful, struggle in safety-critical
environments. They can generate plausible but incorrect responses, a phenomenon
known as ""hallucination,"" which could endanger astronauts. To overcome these
limitations, this paper proposes enhancing systems like METIS by integrating
GPTs, Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and
Augmented Reality (AR). The idea is to allow astronauts to interact with their
data more intuitively, using natural language queries and visualizing real-time
information through AR. KGs will be used to easily access live telemetry and
multimodal data, ensuring that astronauts have the right information at the
right time. By combining AI, KGs, and AR, this new system will empower
astronauts to work more autonomously, safely, and efficiently during future
space missions.",Oliver Bensch
2024-10-28T19:35:47Z,http://arxiv.org/abs/2410.21480v1,"AiSciVision: A Framework for Specializing Large Multimodal Models in
  Scientific Image Classification","Trust and interpretability are crucial for the use of Artificial Intelligence
(AI) in scientific research, but current models often operate as black boxes
offering limited transparency and justifications for their outputs. We
introduce AiSciVision, a framework that specializes Large Multimodal Models
(LMMs) into interactive research partners and classification models for image
classification tasks in niche scientific domains. Our framework uses two key
components: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2)
domain-specific tools utilized in an agentic workflow. To classify a target
image, AiSciVision first retrieves the most similar positive and negative
labeled images as context for the LMM. Then the LMM agent actively selects and
applies tools to manipulate and inspect the target image over multiple rounds,
refining its analysis before making a final prediction. These VisRAG and
tooling components are designed to mirror the processes of domain experts, as
humans often compare new data to similar examples and use specialized tools to
manipulate and inspect images before arriving at a conclusion. Each inference
produces both a prediction and a natural language transcript detailing the
reasoning and tool usage that led to the prediction. We evaluate AiSciVision on
three real-world scientific image classification datasets: detecting the
presence of aquaculture ponds, diseased eelgrass, and solar panels. Across
these datasets, our method outperforms fully supervised models in low and
full-labeled data settings. AiSciVision is actively deployed in real-world use,
specifically for aquaculture research, through a dedicated web application that
displays and allows the expert users to converse with the transcripts. This
work represents a crucial step toward AI systems that are both interpretable
and effective, advancing their use in scientific research and scientific
discovery.",Brendan Hogan
2024-11-11T22:22:21Z,http://arxiv.org/abs/2411.07404v1,Controllable Context Sensitivity and the Knob Behind It,"When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.",Julian Minder
2024-11-23T08:18:55Z,http://arxiv.org/abs/2411.15490v1,"Improving Factuality of 3D Brain MRI Report Generation with Paired
  Image-domain Retrieval and Text-domain Augmentation","Acute ischemic stroke (AIS) requires time-critical management, with hours of
delayed intervention leading to an irreversible disability of the patient.
Since diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)
plays a crucial role in the detection of AIS, automated prediction of AIS from
DWI has been a research topic of clinical importance. While text radiology
reports contain the most relevant clinical information from the image findings,
the difficulty of mapping across different modalities has limited the
factuality of conventional direct DWI-to-report generation methods. Here, we
propose paired image-domain retrieval and text-domain augmentation (PIRTA), a
cross-modal retrieval-augmented generation (RAG) framework for providing
clinician-interpretative AIS radiology reports with improved factuality. PIRTA
mitigates the need for learning cross-modal mapping, which poses difficulty in
image-to-text generation, by casting the cross-modal mapping problem as an
in-domain retrieval of similar DWI images that have paired ground-truth text
radiology reports. By exploiting the retrieved radiology reports to augment the
report generation process of the query image, we show by experiments with
extensive in-house and public datasets that PIRTA can accurately retrieve
relevant reports from 3D DWI images. This approach enables the generation of
radiology reports with significantly higher accuracy compared to direct
image-to-text generation using state-of-the-art multimodal language models.",Junhyeok Lee
2024-01-30T18:58:43Z,http://arxiv.org/abs/2401.17268v1,Weaver: Foundation Models for Creative Writing,"This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.",Tiannan Wang
2024-06-29T22:39:20Z,http://arxiv.org/abs/2407.00541v1,"Answering real-world clinical questions using large language model based
  systems","Evidence to guide healthcare decisions is often limited by a lack of relevant
and trustworthy literature as well as difficulty in contextualizing existing
research for a specific patient. Large language models (LLMs) could potentially
address both challenges by either summarizing published literature or
generating new studies based on real-world data (RWD). We evaluated the ability
of five LLM-based systems in answering 50 clinical questions and had nine
independent physicians review the responses for relevance, reliability, and
actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus,
Gemini Pro 1.5) rarely produced answers that were deemed relevant and
evidence-based (2% - 10%). In contrast, retrieval augmented generation
(RAG)-based and agentic LLM systems produced relevant and evidence-based
answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic
ChatRWD was able to answer novel questions compared to other LLMs (65% vs.
0-9%). These results suggest that while general-purpose LLMs should not be used
as-is, a purpose-built system for evidence summarization based on RAG and one
for generating novel evidence working synergistically would improve
availability of pertinent evidence for patient care.",Yen Sia Low
