Published Date,Link,Title,Summary,First Author
2024-07-26T03:45:30Z,http://arxiv.org/abs/2407.21059v1,"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks","Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.",Yunfan Gao
2024-09-03T07:17:41Z,http://arxiv.org/abs/2409.01666v1,In Defense of RAG in the Era of Long-Context Language Models,"Overcoming the limited context limitations in early-generation LLMs,
retrieval-augmented generation (RAG) has been a reliable solution for
context-based answer generation in the past. Recently, the emergence of
long-context LLMs allows the models to incorporate much longer text sequences,
making RAG less attractive. Recent studies show that long-context LLMs
significantly outperform RAG in long-context applications. Unlike the existing
works favoring the long-context LLM over RAG, we argue that the extremely long
context in LLMs suffers from a diminished focus on relevant information and
leads to potential degradation in answer quality. This paper revisits the RAG
in long-context answer generation. We propose an order-preserve
retrieval-augmented generation (OP-RAG) mechanism, which significantly improves
the performance of RAG for long-context question-answer applications. With
OP-RAG, as the number of retrieved chunks increases, the answer quality
initially rises, and then declines, forming an inverted U-shaped curve. There
exist sweet points where OP-RAG could achieve higher answer quality with much
less tokens than long-context LLM taking the whole context as input. Extensive
experiments on public benchmark demonstrate the superiority of our OP-RAG.",Tan Yu
2024-10-27T00:42:21Z,http://arxiv.org/abs/2410.20299v1,"EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge
  Update","Large Language Models are revolutionizing Web, mobile, and Web of Things
systems, driving intelligent and scalable solutions. However, as
Retrieval-Augmented Generation (RAG) systems expand, they encounter significant
challenges related to scalability, including increased delay and communication
overhead. To address these issues, we propose EACO-RAG, an edge-assisted
distributed RAG system that leverages adaptive knowledge updates and inter-node
collaboration. By distributing vector datasets across edge nodes and optimizing
retrieval processes, EACO-RAG significantly reduces delay and resource
consumption while enhancing response accuracy. The system employs a multi-armed
bandit framework with safe online Bayesian methods to balance performance and
cost. Extensive experimental evaluation demonstrates that EACO-RAG outperforms
traditional centralized RAG systems in both response time and resource
efficiency. EACO-RAG effectively reduces delay and resource expenditure to
levels comparable to, or even lower than, those of local RAG systems, while
significantly improving accuracy. This study presents the first systematic
exploration of edge-assisted distributed RAG architectures, providing a
scalable and cost-effective solution for large-scale distributed environments.",Jiaxing Li
2024-01-27T11:41:48Z,http://arxiv.org/abs/2401.15391v1,"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
  Queries","Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.",Yixuan Tang
2024-10-09T17:59:58Z,http://arxiv.org/abs/2410.07176v1,"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for Large Language Models","Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.",Fei Wang
2024-01-11T12:04:11Z,http://arxiv.org/abs/2401.05856v1,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",Scott Barnett
2024-03-31T12:01:34Z,http://arxiv.org/abs/2404.00657v1,Observations on Building RAG Systems for Technical Documents,"Retrieval augmented generation (RAG) for technical documents creates
challenges as embeddings do not often capture domain information. We review
prior art for important factors affecting RAG and perform experiments to
highlight best practices and potential challenges to build RAG systems for
technical documents.",Sumit Soman
2024-08-05T15:16:24Z,http://arxiv.org/abs/2408.02545v1,"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation","Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.",Daniel Fleischer
2024-10-04T15:54:49Z,http://arxiv.org/abs/2410.03537v1,Ward: Provable RAG Dataset Inference via LLM Watermarks,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",Nikola JovanoviÄ‡
2024-10-28T05:35:04Z,http://arxiv.org/abs/2410.20753v1,Plan$\times$RAG: Planning-guided Retrieval Augmented Generation,"We introduce Planning-guided Retrieval Augmented Generation
(Plan$\times$RAG), a novel framework that augments the
\emph{retrieve-then-reason} paradigm of existing RAG frameworks to
\emph{plan-then-retrieve}. Plan$\times$RAG formulates a reasoning plan as a
directed acyclic graph (DAG), decomposing queries into interrelated atomic
sub-queries. Answer generation follows the DAG structure, allowing significant
gains in efficiency through parallelized retrieval and generation. While
state-of-the-art RAG solutions require extensive data generation and
fine-tuning of language models (LMs), Plan$\times$RAG incorporates frozen LMs
as plug-and-play experts to generate high-quality answers. Compared to existing
RAG solutions, Plan$\times$RAG demonstrates significant improvements in
reducing hallucinations and bolstering attribution due to its structured
sub-query decomposition. Overall, Plan$\times$RAG offers a new perspective on
integrating external knowledge in LMs while ensuring attribution by design,
contributing towards more reliable LM-based systems.",Prakhar Verma
2024-12-13T20:39:30Z,http://arxiv.org/abs/2412.10543v1,RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation,"RAG (Retrieval Augmented Generation) allows LLMs (large language models) to
generate better responses with external knowledge, but using more external
knowledge often improves generation quality at the expense of response delay.
Prior work either reduces the response delay (through better scheduling of RAG
queries) or strives to maximize quality (which involves tuning the RAG
workflow), but they fall short in optimizing the tradeoff between the delay and
quality of RAG responses. This paper presents RAGServe, the first RAG system
that jointly schedules queries and adapts the key RAG configurations of each
query, such as the number of retrieved text chunks and synthesis methods, in
order to balance quality optimization and response delay reduction. Using 4
popular RAG-QA datasets, we show that compared with the state-of-the-art RAG
optimization schemes, RAGServe reduces the generation latency by
$1.64-2.54\times$ without sacrificing generation quality.",Siddhant Ray
2024-06-25T20:23:15Z,http://arxiv.org/abs/2407.11005v1,"RAGBench: Explainable Benchmark for Retrieval-Augmented Generation
  Systems","Retrieval-Augmented Generation (RAG) has become a standard architectural
pattern for incorporating domain-specific knowledge into user-facing chat
applications powered by Large Language Models (LLMs). RAG systems are
characterized by (1) a document retriever that queries a domain-specific corpus
for context information relevant to an input query, and (2) an LLM that
generates a response based on the provided query and context. However,
comprehensive evaluation of RAG systems remains a challenge due to the lack of
unified evaluation criteria and annotated datasets. In response, we introduce
RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k
examples. It covers five unique industry-specific domains and various RAG task
types. RAGBench examples are sourced from industry corpora such as user
manuals, making it particularly relevant for industry applications. Further, we
formalize the TRACe evaluation framework: a set of explainable and actionable
RAG evaluation metrics applicable across all RAG domains. We release the
labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.
RAGBench explainable labels facilitate holistic evaluation of RAG systems,
enabling actionable feedback for continuous improvement of production
applications. Thorough extensive benchmarking, we find that LLM-based RAG
evaluation methods struggle to compete with a finetuned RoBERTa model on the
RAG evaluation task. We identify areas where existing approaches fall short and
propose the adoption of RAGBench with TRACe towards advancing the state of RAG
evaluation systems.",Robert Friel
2024-09-17T23:10:04Z,http://arxiv.org/abs/2409.11598v2,"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented
  Generation","Many language models now enhance their responses with retrieval capabilities,
leading to the widespread adoption of retrieval-augmented generation (RAG)
systems. However, despite retrieval being a core component of RAG, much of the
research in this area overlooks the extensive body of work on fair ranking,
neglecting the importance of considering all stakeholders involved. This paper
presents the first systematic evaluation of RAG systems integrated with fair
rankings. We focus specifically on measuring the fair exposure of each relevant
item across the rankings utilized by RAG systems (i.e., item-side fairness),
aiming to promote equitable growth for relevant item providers. To gain a deep
understanding of the relationship between item-fairness, ranking quality, and
generation quality in the context of RAG, we analyze nine different RAG systems
that incorporate fair rankings across seven distinct datasets. Our findings
indicate that RAG systems with fair rankings can maintain a high level of
generation quality and, in many cases, even outperform traditional RAG systems,
despite the general trend of a tradeoff between ensuring fairness and
maintaining system-effectiveness. We believe our insights lay the groundwork
for responsible and equitable RAG systems and open new avenues for future
research. We publicly release our codebase and dataset at
https://github.com/kimdanny/Fair-RAG.",To Eun Kim
2024-10-12T16:30:51Z,http://arxiv.org/abs/2410.09584v1,"Toward General Instruction-Following Alignment for Retrieval-Augmented
  Generation","Following natural instructions is crucial for the effective application of
Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in
Large Language Models (LLMs), research on assessing and improving
instruction-following (IF) alignment within the RAG domain remains limited. To
address this issue, we propose VIF-RAG, the first automated, scalable, and
verifiable synthetic pipeline for instruction-following alignment in RAG
systems. We start by manually crafting a minimal set of atomic instructions
(<100) and developing combination rules to synthesize and verify complex
instructions for a seed set. We then use supervised models for instruction
rewriting while simultaneously generating code to automate the verification of
instruction quality via a Python executor. Finally, we integrate these
instructions with extensive RAG and general data samples, scaling up to a
high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further
bridge the gap in instruction-following auto-evaluation for RAG systems, we
introduce FollowRAG Benchmark, which includes approximately 3K test samples,
covering 22 categories of general instruction constraints and four
knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG
can seamlessly integrate with different RAG benchmarks. Using FollowRAG and
eight widely-used IF and foundational abilities benchmarks for LLMs, we
demonstrate that VIF-RAG markedly enhances LLM performance across a broad range
of general instruction constraints while effectively leveraging its
capabilities in RAG scenarios. Further analysis offers practical insights for
achieving IF alignment in RAG systems. Our code and datasets are released at
https://FollowRAG.github.io.",Guanting Dong
2024-10-17T12:53:29Z,http://arxiv.org/abs/2410.13509v1,"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable
  Data Rewards","Retrieval-Augmented Generation (RAG) has proven its effectiveness in
mitigating hallucinations in Large Language Models (LLMs) by retrieving
knowledge from external resources. To adapt LLMs for RAG pipelines, current
approaches use instruction tuning to optimize LLMs, improving their ability to
utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses
on equipping LLMs to handle diverse RAG tasks using different instructions.
However, it trains RAG modules to overfit training signals and overlooks the
varying data preferences among agents within the RAG system. In this paper, we
propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG
systems by aligning data preferences between different RAG modules. DDR works
by collecting the rewards to optimize each agent with a rollout method. This
method prompts agents to sample some potential responses as perturbations,
evaluates the impact of these perturbations on the whole RAG system, and
subsequently optimizes the agent to produce outputs that improve the
performance of the RAG system. Our experiments on various knowledge-intensive
tasks demonstrate that DDR significantly outperforms the SFT method,
particularly for LLMs with smaller-scale parameters that depend more on the
retrieved knowledge. Additionally, DDR exhibits a stronger capability to align
the data preference between RAG modules. The DDR method makes generation module
more effective in extracting key information from documents and mitigating
conflicts between parametric memory and external knowledge. All codes are
available at https://github.com/OpenMatch/RAG-DDR.",Xinze Li
2024-12-19T04:18:51Z,http://arxiv.org/abs/2412.14510v1,PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization,"The emergence of Retrieval-augmented generation (RAG) has alleviated the
issues of outdated and hallucinatory content in the generation of large
language models (LLMs), yet it still reveals numerous limitations. When a
general-purpose LLM serves as the RAG generator, it often suffers from
inadequate response informativeness, response robustness, and citation quality.
Past approaches to tackle these limitations, either by incorporating additional
steps beyond generating responses or optimizing the generator through
supervised fine-tuning (SFT), still failed to align with the RAG requirement
thoroughly. Consequently, optimizing the RAG generator from multiple preference
perspectives while maintaining its end-to-end LLM form remains a challenge. To
bridge this gap, we propose Multiple Perspective Preference Alignment for
Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator
of RAG systems to align with RAG requirements comprehensively. Specifically, we
construct high-quality instruction fine-tuning data and multi-perspective
preference data by sampling varied quality responses from the generator across
different prompt documents quality scenarios. Subsequently, we optimize the
generator using SFT and Direct Preference Optimization (DPO). Extensive
experiments conducted on four question-answer datasets across three LLMs
demonstrate that PA-RAG can significantly enhance the performance of RAG
generators. Our code and datasets are available at
https://github.com/wujwyi/PA-RAG.",Jiayi Wu
2024-01-20T14:59:43Z,http://arxiv.org/abs/2401.11246v1,"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented
  Generation in Niche Domains, Exemplified by Korean Medicine","We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.",Bongsu Kang
2024-02-23T18:35:15Z,http://arxiv.org/abs/2402.16893v1,"The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)","Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.",Shenglai Zeng
2024-04-02T17:00:11Z,http://arxiv.org/abs/2404.02103v1,"CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions
  for RAG systems","Retrieval Augmented Generation (RAG) has become a popular application for
large language models. It is preferable that successful RAG systems provide
accurate answers that are supported by being grounded in a passage without any
hallucinations. While considerable work is required for building a full RAG
pipeline, being able to benchmark performance is also necessary. We present
ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG
pipeline. ClapNQ includes long answers with grounded gold passages from Natural
Questions (NQ) and a corpus to perform either retrieval, generation, or the
full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full
passage, and cohesive, with multiple pieces of the passage that are not
contiguous. RAG models must adapt to these properties to be successful at
ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight
areas where there is still significant room for improvement in grounded RAG.
CLAPNQ is publicly available at https://github.com/primeqa/clapnq",Sara Rosenthal
2024-05-22T12:12:40Z,http://arxiv.org/abs/2405.13576v1,"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation
  Research","With the advent of Large Language Models (LLMs), the potential of Retrieval
Augmented Generation (RAG) techniques have garnered considerable research
attention. Numerous novel algorithms and models have been introduced to enhance
various aspects of RAG systems. However, the absence of a standardized
framework for implementation, coupled with the inherently intricate RAG
process, makes it challenging and time-consuming for researchers to compare and
evaluate these approaches in a consistent environment. Existing RAG toolkits
like LangChain and LlamaIndex, while available, are often heavy and unwieldy,
failing to meet the personalized needs of researchers. In response to this
challenge, we propose FlashRAG, an efficient and modular open-source toolkit
designed to assist researchers in reproducing existing RAG methods and in
developing their own RAG algorithms within a unified framework. Our toolkit
implements 12 advanced RAG methods and has gathered and organized 32 benchmark
datasets. Our toolkit has various features, including customizable modular
framework, rich collection of pre-implemented RAG works, comprehensive
datasets, efficient auxiliary pre-processing scripts, and extensive and
standard evaluation metrics. Our toolkit and resources are available at
https://github.com/RUC-NLPIR/FlashRAG.",Jiajie Jin
2024-06-19T04:53:48Z,http://arxiv.org/abs/2406.13213v2,"Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database
  Filtering with LLM-Extracted Metadata","The retrieval-augmented generation (RAG) enables retrieval of relevant
information from an external knowledge source and allows large language models
(LLMs) to answer queries over previously unseen document collections. However,
it was demonstrated that traditional RAG applications perform poorly in
answering multi-hop questions, which require retrieving and reasoning over
multiple elements of supporting evidence. We introduce a new method called
Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to
improve the RAG selection of the relevant documents from various sources,
relevant to the question. While database filtering is specific to a set of
questions from a particular domain and format, we found out that Multi-Meta-RAG
greatly improves the results on the MultiHop-RAG benchmark. The code is
available at https://github.com/mxpoliakov/Multi-Meta-RAG.",Mykhailo Poliakov
2024-07-29T13:26:43Z,http://arxiv.org/abs/2407.19994v3,"A Study on the Implementation Method of an Agent-Based Advanced RAG
  System Using Graph","This study aims to improve knowledge-based question-answering (QA) systems by
overcoming the limitations of existing Retrieval-Augmented Generation (RAG)
models and implementing an advanced RAG system based on Graph technology to
develop high-quality generative AI services. While existing RAG models
demonstrate high accuracy and fluency by utilizing retrieved information, they
may suffer from accuracy degradation as they generate responses using
pre-loaded knowledge without reprocessing. Additionally, they cannot
incorporate real-time data after the RAG configuration stage, leading to issues
with contextual understanding and biased information. To address these
limitations, this study implemented an enhanced RAG system utilizing Graph
technology. This system is designed to efficiently search and utilize
information. Specifically, it employs LangGraph to evaluate the reliability of
retrieved information and synthesizes diverse data to generate more accurate
and enhanced responses. Furthermore, the study provides a detailed explanation
of the system's operation, key implementation steps, and examples through
implementation code and validation results, thereby enhancing the understanding
of advanced RAG technology. This approach offers practical guidelines for
implementing advanced RAG systems in corporate services, making it a valuable
resource for practical application.",Cheonsu Jeong
2024-10-03T15:26:50Z,http://arxiv.org/abs/2410.03780v1,Reward-RAG: Enhancing RAG with Reward Driven Supervision,"In this paper, we introduce Reward-RAG, a novel approach designed to enhance
the Retrieval-Augmented Generation (RAG) model through Reward-Driven
Supervision. Unlike previous RAG methodologies, which focus on training
language models (LMs) to utilize external knowledge retrieved from external
sources, our method adapts retrieval information to specific domains by
employing CriticGPT to train a dedicated reward model. This reward model
generates synthesized datasets for fine-tuning the RAG encoder, aligning its
outputs more closely with human preferences. The versatility of our approach
allows it to be effectively applied across various domains through
domain-specific fine-tuning. We evaluate Reward-RAG on publicly available
benchmarks from multiple domains, comparing it to state-of-the-art methods. Our
experimental results demonstrate significant improvements in performance,
highlighting the effectiveness of Reward-RAG in improving the relevance and
quality of generated responses. These findings underscore the potential of
integrating reward models with RAG to achieve superior outcomes in natural
language generation tasks.",Thang Nguyen
2024-10-03T22:29:47Z,http://arxiv.org/abs/2410.12837v1,"A Comprehensive Survey of Retrieval-Augmented Generation (RAG):
  Evolution, Current Landscape and Future Directions","This paper presents a comprehensive study of Retrieval-Augmented Generation
(RAG), tracing its evolution from foundational concepts to the current state of
the art. RAG combines retrieval mechanisms with generative language models to
enhance the accuracy of outputs, addressing key limitations of LLMs. The study
explores the basic architecture of RAG, focusing on how retrieval and
generation are integrated to handle knowledge-intensive tasks. A detailed
review of the significant technological advancements in RAG is provided,
including key innovations in retrieval-augmented language models and
applications across various domains such as question-answering, summarization,
and knowledge-based tasks. Recent research breakthroughs are discussed,
highlighting novel methods for improving retrieval efficiency. Furthermore, the
paper examines ongoing challenges such as scalability, bias, and ethical
concerns in deployment. Future research directions are proposed, focusing on
improving the robustness of RAG models, expanding the scope of application of
RAG models, and addressing societal implications. This survey aims to serve as
a foundational resource for researchers and practitioners in understanding the
potential of RAG and its trajectory in natural language processing.",Shailja Gupta
2024-11-29T04:25:31Z,http://arxiv.org/abs/2411.19463v1,"Towards Understanding Retrieval Accuracy and Prompt Quality in RAG
  Systems","Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the
capability of large language models (LLMs) and has demonstrated promising
efficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show
superior performance, they face unique challenges in stability and reliability.
Their complexity hinders developers' efforts to design, maintain, and optimize
effective RAG systems. Therefore, it is crucial to understand how RAG's
performance is impacted by its design. In this work, we conduct an early
exploratory study toward a better understanding of the mechanism of RAG
systems, covering three code datasets, three QA datasets, and two LLMs. We
focus on four design factors: retrieval document type, retrieval recall,
document selection, and prompt techniques. Our study uncovers how each factor
impacts system correctness and confidence, providing valuable insights for
developing an accurate and reliable RAG system. Based on these findings, we
present nine actionable guidelines for detecting defects and optimizing the
performance of RAG systems. We hope our early exploration can inspire further
advancements in engineering, improving and maintaining LLM-driven intelligent
software systems for greater efficiency and reliability.",Shengming Zhao
2021-06-22T03:17:59Z,http://arxiv.org/abs/2106.11517v1,"Fine-tune the Entire RAG Architecture (including DPR retriever) for
  Question-Answering","In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.",Shamane Siriwardhana
2024-10-28T09:55:52Z,http://arxiv.org/abs/2410.20878v1,"AutoRAG: Automated Framework for optimization of Retrieval Augmented
  Generation Pipeline","Using LLMs (Large Language Models) in conjunction with external documents has
made RAG (Retrieval-Augmented Generation) an essential technology. Numerous
techniques and modules for RAG are being researched, but their performance can
vary across different datasets. Finding RAG modules that perform well on
specific datasets is challenging. In this paper, we propose the AutoRAG
framework, which automatically identifies suitable RAG modules for a given
dataset. AutoRAG explores and approximates the optimal combination of RAG
modules for the dataset. Additionally, we share the results of optimizing a
dataset using AutoRAG. All experimental results and data are publicly available
and can be accessed through our GitHub repository
https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",Dongkyu Kim
2024-04-22T07:49:36Z,http://arxiv.org/abs/2404.13948v2,"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by
  Simulating Documents in the Wild via Low-level Perturbations","The robustness of recent Large Language Models (LLMs) has become increasingly
crucial as their applicability expands across various domains and real-world
applications. Retrieval-Augmented Generation (RAG) is a promising solution for
addressing the limitations of LLMs, yet existing studies on the robustness of
RAG often overlook the interconnected relationships between RAG components or
the potential threats prevalent in real-world databases, such as minor textual
errors. In this work, we investigate two underexplored aspects when assessing
the robustness of RAG: 1) vulnerability to noisy documents through low-level
perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we
introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}),
which targets these aspects. Specifically, GARAG is designed to reveal
vulnerabilities within each component and test the overall system functionality
against noisy documents. We validate RAG robustness by applying our
\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and
LLMs. The experimental results show that GARAG consistently achieves high
attack success rates. Also, it significantly devastates the performance of each
component and their synergy, highlighting the substantial risk that minor
textual inaccuracies pose in disrupting RAG systems in the real world.",Sukmin Cho
2024-06-03T02:56:14Z,http://arxiv.org/abs/2406.00944v2,A Theory for Token-Level Harmonization in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.",Shicheng Xu
2024-08-19T18:30:18Z,http://arxiv.org/abs/2408.10343v1,"LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the
  Legal Domain","Retrieval-Augmented Generation (RAG) systems are showing promising potential,
and are becoming increasingly relevant in AI-powered legal applications.
Existing benchmarks, such as LegalBench, assess the generative capabilities of
Large Language Models (LLMs) in the legal domain, but there is a critical gap
in evaluating the retrieval component of RAG systems. To address this, we
introduce LegalBench-RAG, the first benchmark specifically designed to evaluate
the retrieval step of RAG pipelines within the legal space. LegalBench-RAG
emphasizes precise retrieval by focusing on extracting minimal, highly relevant
text segments from legal documents. These highly relevant snippets are
preferred over retrieving document IDs, or large sequences of imprecise chunks,
both of which can exceed context window limitations. Long context windows cost
more to process, induce higher latency, and lead LLMs to forget or hallucinate
information. Additionally, precise results allow LLMs to generate citations for
the end user. The LegalBench-RAG benchmark is constructed by retracing the
context used in LegalBench queries back to their original locations within the
legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus
of over 79M characters, entirely human-annotated by legal experts. We also
introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and
experimentation. By providing a dedicated benchmark for legal retrieval,
LegalBench-RAG serves as a critical tool for companies and researchers focused
on enhancing the accuracy and performance of RAG systems in the legal domain.
The LegalBench-RAG dataset is publicly available at
https://github.com/zeroentropy-cc/legalbenchrag.",Nicholas Pipitone
2024-10-16T23:03:27Z,http://arxiv.org/abs/2410.13085v1,"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language
  Models","Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.",Peng Xia
2024-12-03T17:23:47Z,http://arxiv.org/abs/2412.02592v1,"OCR Hinders RAG: Evaluating the Cascading Impact of OCR on
  Retrieval-Augmented Generation","Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge to reduce hallucinations and incorporate
up-to-date information without retraining. As an essential part of RAG,
external knowledge bases are commonly built by extracting structured data from
unstructured PDF documents using Optical Character Recognition (OCR). However,
given the imperfect prediction of OCR and the inherent non-uniform
representation of structured data, knowledge bases inevitably contain various
OCR noises. In this paper, we introduce OHRBench, the first benchmark for
understanding the cascading impact of OCR on RAG systems. OHRBench includes 350
carefully selected unstructured PDF documents from six real-world RAG
application domains, along with Q&As derived from multimodal elements in
documents, challenging existing OCR solutions used for RAG To better understand
OCR's impact on RAG systems, we identify two primary types of OCR noise:
Semantic Noise and Formatting Noise and apply perturbation to generate a set of
structured data with varying degrees of each OCR noise. Using OHRBench, we
first conduct a comprehensive evaluation of current OCR solutions and reveal
that none is competent for constructing high-quality knowledge bases for RAG
systems. We then systematically evaluate the impact of these two noise types
and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the
potential of employing Vision-Language Models (VLMs) without OCR in RAG
systems. Code: https://github.com/opendatalab/OHR-Bench",Junyuan Zhang
2024-01-26T08:23:29Z,http://arxiv.org/abs/2402.01717v1,"From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical
  Regulatory Compliance Process","Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.",Jaewoong Kim
2024-03-14T02:26:31Z,http://arxiv.org/abs/2403.09040v2,"RAGGED: Towards Informed Design of Retrieval Augmented Generation
  Systems","Retrieval-augmented generation (RAG) can significantly improve the
performance of language models (LMs) by providing additional context for tasks
such as document-based question answering (DBQA). However, the effectiveness of
RAG is highly dependent on its configuration. To systematically find the
optimal configuration, we introduce RAGGED, a framework for analyzing RAG
configurations across various DBQA tasks. Using the framework, we discover
distinct LM behaviors in response to varying context quantities, context
qualities, and retrievers. For instance, while some models are robust to noisy
contexts, monotonically performing better with more contexts, others are more
noise-sensitive and can effectively use only a few contexts before declining in
performance. This framework also provides a deeper analysis of these
differences by evaluating the LMs' sensitivity to signal and noise under
specific context quality conditions. Using RAGGED, researchers and
practitioners can derive actionable insights about how to optimally configure
their RAG systems for their specific question-answering tasks.",Jennifer Hsia
2024-03-22T17:13:46Z,http://arxiv.org/abs/2404.07220v2,"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy
  with Semantic Search and Hybrid Query-Based Retrievers","Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a
private knowledge base of documents with Large Language Models (LLM) to build
Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes
increasingly challenging as the corpus of documents scales up, with Retrievers
playing an outsized role in the overall RAG accuracy by extracting the most
relevant document from the corpus to provide context to the LLM. In this paper,
we propose the 'Blended RAG' method of leveraging semantic search techniques,
such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid
query strategies. Our study achieves better retrieval results and sets new
benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID
datasets. We further extend such a 'Blended Retriever' to the RAG system to
demonstrate far superior results on Generative Q\&A datasets like SQUAD, even
surpassing fine-tuning performance.",Kunal Sawarkar
2024-04-24T15:58:59Z,http://arxiv.org/abs/2404.15939v3,"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language
  Models for Telecommunications","The application of Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems in the telecommunication domain presents unique
challenges, primarily due to the complex nature of telecom standard documents
and the rapid evolution of the field. The paper introduces Telco-RAG, an
open-source RAG framework designed to handle the specific needs of
telecommunications standards, particularly 3rd Generation Partnership Project
(3GPP) documents. Telco-RAG addresses the critical challenges of implementing a
RAG pipeline on highly technical content, paving the way for applying LLMs in
telecommunications and offering guidelines for RAG implementation in other
technical domains.",Andrei-Laurentiu Bornea
2024-07-18T06:06:53Z,http://arxiv.org/abs/2407.13193v2,Retrieval-Augmented Generation for Natural Language Processing: A Survey,"Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG training, including RAG with/without datastore update. Then, we
introduce the application of RAG in representative natural language processing
tasks and industrial scenarios. Finally, this paper discusses the future
directions and challenges of RAG for promoting its development.",Shangyu Wu
2024-07-23T20:51:52Z,http://arxiv.org/abs/2407.16833v2,"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach","Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.",Zhuowan Li
2024-09-24T14:52:14Z,http://arxiv.org/abs/2409.16146v2,"Controlling Risk of Retrieval-augmented Generation: A Counterfactual
  Prompting Framework","Retrieval-augmented generation (RAG) has emerged as a popular solution to
mitigate the hallucination issues of large language models. However, existing
studies on RAG seldom address the issue of predictive uncertainty, i.e., how
likely it is that a RAG model's prediction is incorrect, resulting in
uncontrollable risks in real-world applications. In this work, we emphasize the
importance of risk control, ensuring that RAG models proactively refuse to
answer questions with low confidence. Our research identifies two critical
latent factors affecting RAG's confidence in its predictions: the quality of
the retrieved results and the manner in which these results are utilized. To
guide RAG models in assessing their own confidence based on these two latent
factors, we develop a counterfactual prompting framework that induces the
models to alter these factors and analyzes the effect on their answers. We also
introduce a benchmarking procedure to collect answers with the option to
abstain, facilitating a series of experiments. For evaluation, we introduce
several risk-related metrics and the experimental results demonstrate the
effectiveness of our approach. Our code and benchmark dataset are available at
https://github.com/ict-bigdatalab/RC-RAG.",Lu Chen
2024-10-03T09:48:09Z,http://arxiv.org/abs/2410.02338v2,How Much Can RAG Help the Reasoning of LLM?,"Retrieval-Augmented Generation (RAG) has gained significant popularity in
modern Large Language Models (LLMs) due to its effectiveness in introducing new
knowledge and reducing hallucinations. However, the deep understanding of RAG
remains limited, how does RAG help the reasoning process and can RAG help
improve the reasoning capability remains question. While external documents are
typically considered as a method to incorporate domain-specific information,
they also contain intermediate reasoning results related to the query, this
suggests that documents could enhance the reasoning capability of LLMs, which
has not been previously explored. In this paper, we investigate this issue in
depth and find that while RAG can assist with reasoning, the help is limited.
If we conceptualize the reasoning process as a tree with fixed depth, then RAG
struggles to assist LLMs in performing deeper reasoning. Additionally, the
information in the documents requires preprocessing to filter out noise. We
demonstrate that this preprocessing is difficult to achieve simply fine-tuning
of the LLM, it often necessitates numerous additional transformer layers to
solve the problem. To simplify the problem, we propose DPrompt tuning, which
effectively resolves the issue within just limited transformer layers, leading
to improved performance.",Jingyu Liu
2024-10-10T03:51:58Z,http://arxiv.org/abs/2410.07589v1,"No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in
  LLMs, Even for Vigilant Users","Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness
and cost-efficiency in mitigating hallucinations and enhancing the
domain-specific generation capabilities of large language models (LLMs).
However, is this effectiveness and cost-efficiency truly a free lunch? In this
study, we comprehensively investigate the fairness costs associated with RAG by
proposing a practical three-level threat model from the perspective of user
awareness of fairness. Specifically, varying levels of user fairness awareness
result in different degrees of fairness censorship on the external dataset. We
examine the fairness implications of RAG using uncensored, partially censored,
and fully censored datasets. Our experiments demonstrate that fairness
alignment can be easily undermined through RAG without the need for fine-tuning
or retraining. Even with fully censored and supposedly unbiased external
datasets, RAG can lead to biased outputs. Our findings underscore the
limitations of current alignment methods in the context of RAG-based LLMs and
highlight the urgent need for new strategies to ensure fairness. We propose
potential mitigations and call for further research to develop robust fairness
safeguards in RAG-based LLMs.",Mengxuan Hu
2024-10-15T06:39:35Z,http://arxiv.org/abs/2410.11321v1,Self-adaptive Multimodal Retrieval-Augmented Generation,"Traditional Retrieval-Augmented Generation (RAG) methods are limited by their
reliance on a fixed number of retrieved documents, often resulting in
incomplete or noisy information that undermines task performance. Although
recent adaptive approaches alleviated these problems, their application in
intricate and real-world multimodal tasks remains limited. To address these, we
propose a new approach called Self-adaptive Multimodal Retrieval-Augmented
Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG
not only dynamically filters relevant documents based on the input query,
including image captions when needed, but also verifies the quality of both the
retrieved documents and the output. Extensive experimental results show that
SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy
and response generation. By further ablation experiments and effectiveness
analysis, SAM-RAG maintains high recall quality while improving overall task
performance in multimodal RAG task. Our codes are available at
https://github.com/SAM-RAG/SAM_RAG.",Wenjia Zhai
2024-10-20T16:08:54Z,http://arxiv.org/abs/2410.15438v1,"Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based
  LLMs","Retrieval-Augmented Generation (RAG) significantly improved the ability of
Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing
research seeks to enhance RAG performance by retrieving higher-quality
documents or designing RAG-specific LLMs, the internal mechanisms within LLMs
that contribute to the effectiveness of RAG systems remain underexplored. In
this paper, we aim to investigate these internal mechanisms within the popular
Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by
examining expert activations in these LLMs. Our controlled experiments reveal
that several core groups of experts are primarily responsible for RAG-related
behaviors. The activation of these core experts can signify the model's
inclination towards external/internal knowledge and adjust its behavior. For
instance, we identify core experts that can (1) indicate the sufficiency of the
model's internal knowledge, (2) assess the quality of retrieved documents, and
(3) enhance the model's ability to utilize context. Based on these findings, we
propose several strategies to enhance RAG's efficiency and effectiveness
through expert activation. Experimental results across various datasets and
MoE-based LLMs show the effectiveness of our method.",Xin Zhou
2024-10-29T11:03:31Z,http://arxiv.org/abs/2410.21943v1,"Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial
  Applications","Large Language Models (LLMs) have demonstrated impressive capabilities in
answering questions, but they lack domain-specific knowledge and are prone to
hallucinations. Retrieval Augmented Generation (RAG) is one approach to address
these challenges, while multimodal models are emerging as promising AI
assistants for processing both text and images. In this paper we describe a
series of experiments aimed at determining how to best integrate multimodal
models into RAG systems for the industrial domain. The purpose of the
experiments is to determine whether including images alongside text from
documents within the industrial domain increases RAG performance and to find
the optimal configuration for such a multimodal RAG system. Our experiments
include two approaches for image processing and retrieval, as well as two LLMs
(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies
involve the use of multimodal embeddings and the generation of textual
summaries from images. We evaluate our experiments with an LLM-as-a-Judge
approach. Our results reveal that multimodal RAG can outperform single-modality
RAG settings, although image retrieval poses a greater challenge than text
retrieval. Additionally, leveraging textual summaries from images presents a
more promising approach compared to the use of multimodal embeddings, providing
more opportunities for future advancements.",Monica Riedler
2024-11-05T22:37:43Z,http://arxiv.org/abs/2411.03538v1,Long Context RAG Performance of Large Language Models,"Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.",Quinn Leng
2024-11-11T22:06:51Z,http://arxiv.org/abs/2411.07396v1,Toward Optimal Search and Retrieval for RAG,"Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.",Alexandria Leto
2022-10-06T01:21:25Z,http://arxiv.org/abs/2210.02627v1,"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)
  Models for Open Domain Question Answering","Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain
Question Answering (ODQA). RAG has only been trained and explored with a
Wikipedia-based external knowledge base and is not optimized for use in other
specialized domains such as healthcare and news. In this paper, we evaluate the
impact of joint training of the retriever and generator components of RAG for
the task of domain adaptation in ODQA. We propose \textit{RAG-end2end}, an
extension to RAG, that can adapt to a domain-specific knowledge base by
updating all components of the external knowledge base during training. In
addition, we introduce an auxiliary training signal to inject more
domain-specific knowledge. This auxiliary signal forces \textit{RAG-end2end} to
reconstruct a given sentence by accessing the relevant information from the
external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does
joint training of the retriever and generator for the end QA task and domain
adaptation. We evaluate our approach with datasets from three domains:
COVID-19, News, and Conversations, and achieve significant performance
improvements compared to the original RAG model. Our work has been open-sourced
through the Huggingface Transformers library, attesting to our work's
credibility and technical consistency.",Shamane Siriwardhana
2024-06-26T18:26:53Z,http://arxiv.org/abs/2406.18676v2,"Understand What LLM Needs: Dual Preference Alignment for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) has demonstrated effectiveness in
mitigating the hallucination problem of large language models (LLMs). However,
the difficulty of aligning the retriever with the diverse LLMs' knowledge
preferences inevitably poses an inevitable challenge in developing a reliable
RAG system. To address this issue, we propose DPA-RAG, a universal framework
designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction
pipline and incorporate five novel query augmentation strategies to alleviate
preference data scarcity. Based on preference data, DPA-RAG accomplishes both
external and internal preference alignment: 1) It jointly integrate pair-wise,
point-wise, and contrastive preference alignment abilities into the reranker,
achieving external preference alignment among RAG components. 2) It further
introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),
enabling LLMs to implicitly capture knowledge aligned with their reasoning
preferences, achieving LLMs' internal alignment. Experimental results across
four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all
baselines and seamlessly integrates both black-box and open-sourced LLM
readers. Further qualitative analysis and discussions also provide empirical
guidance for achieving reliable RAG systems. Our code is publicly available at
https://github.com/dongguanting/DPA-RAG.",Guanting Dong
2024-09-19T11:48:29Z,http://arxiv.org/abs/2409.12682v1,Retrieval-Augmented Test Generation: How Far Are We?,"Retrieval Augmented Generation (RAG) has shown notable advancements in
software engineering tasks. Despite its potential, RAG's application in unit
test generation remains under-explored. To bridge this gap, we take the
initiative to investigate the efficacy of RAG-based LLMs in test generation. As
RAGs can leverage various knowledge sources to enhance their performance, we
also explore the impact of different sources of RAGs' knowledge bases on unit
test generation to provide insights into their practical benefits and
limitations. Specifically, we examine RAG built upon three types of domain
knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.
Each source offers essential knowledge for creating tests from different
perspectives, i.e., API documentations provide official API usage guidelines,
GitHub issues offer resolutions of issues related to the APIs from the library
developers, and StackOverflow Q&As present community-driven solutions and best
practices. For our experiment, we focus on five widely used and typical
Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,
Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex
neural networks efficiently. We conducted experiments using the top 10% most
widely used APIs across these projects, involving a total of 188 APIs. We
investigate the effectiveness of four state-of-the-art LLMs (open and
closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1
405B. Additionally, we compare three prompting strategies in generating unit
test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an
API-level RAG on the three external sources. Finally, we compare the cost of
different sources of knowledge used for the RAG.",Jiho Shin
2024-10-26T10:43:39Z,http://arxiv.org/abs/2410.20142v1,"Mask-based Membership Inference Attacks for Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) has been an effective approach to
mitigate hallucinations in large language models (LLMs) by incorporating
up-to-date and domain-specific knowledge. Recently, there has been a trend of
storing up-to-date or copyrighted data in RAG knowledge databases instead of
using it for LLM training. This practice has raised concerns about Membership
Inference Attacks (MIAs), which aim to detect if a specific target document is
stored in the RAG system's knowledge database so as to protect the rights of
data producers. While research has focused on enhancing the trustworthiness of
RAG systems, existing MIAs for RAG systems remain largely insufficient.
Previous work either relies solely on the RAG system's judgment or is easily
influenced by other documents or the LLM's internal knowledge, which is
unreliable and lacks explainability. To address these limitations, we propose a
Mask-Based Membership Inference Attacks (MBA) framework. Our framework first
employs a masking algorithm that effectively masks a certain number of words in
the target document. The masked text is then used to prompt the RAG system, and
the RAG system is required to predict the mask values. If the target document
appears in the knowledge database, the masked text will retrieve the complete
target document as context, allowing for accurate mask prediction. Finally, we
adopt a simple yet effective threshold-based method to infer the membership of
target document by analyzing the accuracy of mask prediction. Our mask-based
approach is more document-specific, making the RAG system's generation less
susceptible to distractions from other documents or the LLM's internal
knowledge. Extensive experiments demonstrate the effectiveness of our approach
compared to existing baseline models.",Mingrui Liu
2024-02-29T18:59:01Z,http://arxiv.org/abs/2402.19473v6,Retrieval-Augmented Generation for AI-Generated Content: A Survey,"Advancements in model algorithms, the growth of foundational models, and
access to high-quality datasets have propelled the evolution of Artificial
Intelligence Generated Content (AIGC). Despite its notable successes, AIGC
still faces hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and inference costs.
Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to
address such challenges. In particular, RAG introduces the information
retrieval process, which enhances the generation process by retrieving relevant
objects from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing efforts that
integrate RAG technique into AIGC scenarios. We first classify RAG foundations
according to how the retriever augments the generator, distilling the
fundamental abstractions of the augmentation methodologies for various
retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",Penghao Zhao
2024-03-21T13:05:18Z,http://arxiv.org/abs/2403.14374v1,FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,"Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.",Yuren Mao
2023-12-12T23:22:57Z,http://arxiv.org/abs/2312.07796v1,"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge
  Gaps","The paper presents a methodology for uncovering knowledge gaps on the
internet using the Retrieval Augmented Generation (RAG) model. By simulating
user search behaviour, the RAG system identifies and addresses gaps in
information retrieval systems. The study demonstrates the effectiveness of the
RAG system in generating relevant suggestions with a consistent accuracy of
93%. The methodology can be applied in various fields such as scientific
discovery, educational enhancement, research development, market analysis,
search engine optimisation, and content development. The results highlight the
value of identifying and understanding knowledge gaps to guide future
endeavours.",Joan Figuerola Hurtado
2024-07-17T05:50:32Z,http://arxiv.org/abs/2407.12325v1,Optimizing Query Generation for Enhanced Document Retrieval in RAG,"Large Language Models (LLMs) excel in various language tasks but they often
generate incorrect information, a phenomenon known as ""hallucinations"".
Retrieval-Augmented Generation (RAG) aims to mitigate this by using document
retrieval for accurate responses. However, RAG still faces hallucinations due
to vague queries. This study aims to improve RAG by optimizing query generation
with a query-document alignment score, refining queries using LLMs for better
precision and efficiency of document retrieval. Experiments have shown that our
approach improves document retrieval, resulting in an average accuracy gain of
1.6%.",Hamin Koo
2024-08-19T22:01:45Z,http://arxiv.org/abs/2408.10435v1,Enhanced document retrieval with topic embeddings,"Document retrieval systems have experienced a revitalized interest with the
advent of retrieval-augmented generation (RAG). RAG architecture offers a lower
hallucination rate than LLM-only applications. However, the accuracy of the
retrieval mechanism is known to be a bottleneck in the efficiency of these
applications. A particular case of subpar retrieval performance is observed in
situations where multiple documents from several different but related topics
are in the corpus. We have devised a new vectorization method that takes into
account the topic information of the document. The paper introduces this new
method for text vectorization and evaluates it in the context of RAG.
Furthermore, we discuss the challenge of evaluating RAG systems, which pertains
to the case at hand.",Kavsar Huseynova
2023-12-18T07:47:33Z,http://arxiv.org/abs/2312.10997v5,Retrieval-Augmented Generation for Large Language Models: A Survey,"Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.",Yunfan Gao
2024-02-28T08:24:38Z,http://arxiv.org/abs/2402.18150v2,"Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.",Shicheng Xu
2024-03-12T21:06:31Z,http://arxiv.org/abs/2403.09727v1,"Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems","The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.",Robert Lakatos
2024-04-24T18:38:11Z,http://arxiv.org/abs/2404.16130v1,"From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization","The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as ""What are the main themes in the dataset?"", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities
of text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose a Graph RAG approach to question answering over
private text corpora that scales with both the generality of user questions and
the quantity of source text to be indexed. Our approach uses an LLM to build a
graph-based text index in two stages: first to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely-related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that Graph RAG
leads to substantial improvements over a na\""ive RAG baseline for both the
comprehensiveness and diversity of generated answers. An open-source,
Python-based implementation of both global and local Graph RAG approaches is
forthcoming at https://aka.ms/graphrag.",Darren Edge
2024-05-07T22:31:50Z,http://arxiv.org/abs/2405.04700v1,"Robust Implementation of Retrieval-Augmented Generation on Edge-based
  Computing-in-Memory Architectures","Large Language Models (LLMs) deployed on edge devices learn through
fine-tuning and updating a certain portion of their parameters. Although such
learning methods can be optimized to reduce resource utilization, the overall
required resources remain a heavy burden on edge devices. Instead,
Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method,
can improve the quality of the LLM-generated content without updating model
parameters. However, the RAG-based LLM may involve repetitive searches on the
profile data in every user-LLM interaction. This search can lead to significant
latency along with the accumulation of user data. Conventional efforts to
decrease latency result in restricting the size of saved user data, thus
reducing the scalability of RAG as user data continuously grows. It remains an
open question: how to free RAG from the constraints of latency and scalability
on edge devices? In this paper, we propose a novel framework to accelerate RAG
via Computing-in-Memory (CiM) architectures. It accelerates matrix
multiplications by performing in-situ computation inside the memory while
avoiding the expensive data transfer between the computing unit and memory. Our
framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive
learning-based training method and noise-aware training, can enable RAG to
efficiently search profile data with CiM. To the best of our knowledge, this is
the first work utilizing CiM to accelerate RAG.",Ruiyang Qin
2024-06-09T05:33:51Z,http://arxiv.org/abs/2406.05654v2,"DomainRAG: A Chinese Benchmark for Evaluating Domain-specific
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) offers a promising solution to address
various limitations of Large Language Models (LLMs), such as hallucination and
difficulties in keeping up with real-time updates. This approach is
particularly critical in expert and domain-specific applications where LLMs
struggle to cover expert knowledge. Therefore, evaluating RAG models in such
scenarios is crucial, yet current studies often rely on general knowledge
sources like Wikipedia to assess the models' abilities in solving common-sense
problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific
context, college enrollment. We identified six required abilities for RAG
models, including the ability in conversational RAG, analyzing structural
information, faithfulness to external knowledge, denoising, solving
time-sensitive problems, and understanding multi-document interactions. Each
ability has an associated dataset with shared corpora to evaluate the RAG
models' performance. We evaluated popular LLMs such as Llama, Baichuan,
ChatGLM, and GPT models. Experimental results indicate that existing
closed-book LLMs struggle with domain-specific questions, highlighting the need
for RAG models to solve expert problems. Moreover, there is room for RAG models
to improve their abilities in comprehending conversational history, analyzing
structural information, denoising, processing multi-document interactions, and
faithfulness in expert knowledge. We expect future studies could solve these
problems better.",Shuting Wang
2024-06-17T02:25:45Z,http://arxiv.org/abs/2406.11147v2,"Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level
  RAG","Vulnerability detection is essential for software quality assurance. In
recent years, deep learning models (especially large language models) have
shown promise in vulnerability detection. In this work, we propose a novel
LLM-based vulnerability detection technique Vul-RAG, which leverages
knowledge-level retrieval-augmented generation (RAG) framework to detect
vulnerability for the given code in three phases. First, Vul-RAG constructs a
vulnerability knowledge base by extracting multi-dimension knowledge via LLMs
from existing CVE instances; second, for a given code snippet, Vul-RAG}
retrieves the relevant vulnerability knowledge from the constructed knowledge
base based on functional semantics; third, Vul-RAG leverages LLMs to check the
vulnerability of the given code snippet by reasoning the presence of
vulnerability causes and fixing solutions of the retrieved vulnerability
knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows
that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative
improvement in accuracy/pairwise-accuracy. In addition, our user study shows
that the vulnerability knowledge generated by Vul-RAG can serve as high-quality
explanations which can improve the manual detection accuracy from 0.60 to 0.77.",Xueying Du
2024-06-21T08:52:11Z,http://arxiv.org/abs/2407.00072v5,Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback,"RAG systems face limitations when semantic relevance alone does not guarantee
improved generation quality. This issue becomes particularly evident due to the
sensitivity of large language models (LLMs) to the ordering of few-shot
prompts, which can affect model performance. To address this challenge,
aligning LLM outputs with human preferences using structured feedback, such as
options to copy, regenerate, or dislike, offers a promising method for
improvement. This feedback is applied to the entire list of inputs rather than
giving specific ratings for individual documents, making it a Listwide Labels
Learning-to-Rank task.
  To address this task, we propose Pistis-RAG, a new RAG framework designed
with a content-centric approach to better align LLMs with human preferences.
Pistis-RAG effectively utilizes human feedback, enhancing content ranking and
generation quality. To validate our framework, we use public datasets to
simulate human feedback, allowing us to evaluate and refine our method
effectively. Experimental results indicate that Pistis-RAG improves alignment
with human preferences relative to the baseline RAG system, showing a 6.06%
increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy
metrics. These results highlight Pistis-RAG's effectiveness in overcoming the
limitations associated with traditional RAG approaches.",Yu Bai
2024-07-11T06:50:19Z,http://arxiv.org/abs/2407.08223v1,"Speculative RAG: Enhancing Retrieval Augmented Generation through
  Drafting","Retrieval augmented generation (RAG) combines the generative abilities of
large language models (LLMs) with external knowledge sources to provide more
accurate and up-to-date responses. Recent RAG advancements focus on improving
retrieval outcomes through iterative LLM refinement or self-critique
capabilities acquired through additional instruction tuning of LLMs. In this
work, we introduce Speculative RAG - a framework that leverages a larger
generalist LM to efficiently verify multiple RAG drafts produced in parallel by
a smaller, distilled specialist LM. Each draft is generated from a distinct
subset of retrieved documents, offering diverse perspectives on the evidence
while reducing input token counts per draft. This approach enhances
comprehension of each subset and mitigates potential position bias over long
context. Our method accelerates RAG by delegating drafting to the smaller
specialist LM, with the larger generalist LM performing a single verification
pass over the drafts. Extensive experiments demonstrate that Speculative RAG
achieves state-of-the-art performance with reduced latency on TriviaQA,
MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy
by up to 12.97% while reducing latency by 51% compared to conventional RAG
systems on PubHealth.",Zilong Wang
2024-08-09T05:20:05Z,http://arxiv.org/abs/2408.04870v5,ConfusedPilot: Confused Deputy Risks in RAG-based LLMs,"Retrieval augmented generation (RAG) is a process where a large language
model (LLM) retrieves useful information from a database and then generates the
responses. It is becoming popular in enterprise settings for daily business
operations. For example, Copilot for Microsoft 365 has accumulated millions of
businesses. However, the security implications of adopting such RAG-based
systems are unclear.
  In this paper, we introduce ConfusedPilot, a class of security
vulnerabilities of RAG systems that confuse Copilot and cause integrity and
confidentiality violations in its responses. First, we investigate a
vulnerability that embeds malicious text in the modified prompt in RAG,
corrupting the responses generated by the LLM. Second, we demonstrate a
vulnerability that leaks secret data, which leverages the caching mechanism
during retrieval. Third, we investigate how both vulnerabilities can be
exploited to propagate misinformation within the enterprise and ultimately
impact its operations, such as sales and manufacturing. We also discuss the
root cause of these attacks by investigating the architecture of a RAG-based
system. This study highlights the security vulnerabilities in today's RAG-based
systems and proposes design guidelines to secure future RAG-based systems.",Ayush RoyChowdhury
2024-08-09T12:26:05Z,http://arxiv.org/abs/2408.05025v2,"Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations
  in LLM-based Application Frameworks","Retrieval Augmented Generation (RAG) is a technique commonly used to equip
models with out of distribution knowledge. This process involves collecting,
indexing, retrieving, and providing information to an LLM for generating
responses. Despite its growing popularity due to its flexibility and low cost,
the security implications of RAG have not been extensively studied. The data
for such systems are often collected from public sources, providing an attacker
a gateway for indirect prompt injections to manipulate the responses of the
model. In this paper, we investigate the security of RAG systems against
end-to-end indirect prompt manipulations. First, we review existing RAG
framework pipelines, deriving a prototypical architecture and identifying
critical parameters. We then examine prior works searching for techniques that
attackers can use to perform indirect prompt manipulations. Finally, we
implemented Rag 'n Roll, a framework to determine the effectiveness of attacks
against end-to-end RAG applications. Our results show that existing attacks are
mostly optimized to boost the ranking of malicious documents during the
retrieval phase. However, a higher rank does not immediately translate into a
reliable attack. Most attacks, against various configurations, settle around a
40% success rate, which could rise to 60% when considering ambiguous answers as
successful attacks (those that include the expected benign one as well).
Additionally, when using unoptimized documents, attackers deploying two of them
(or more) for a target query can achieve similar results as those using
optimized ones. Finally, exploration of the configuration space of a RAG showed
limited impact in thwarting the attacks, where the most successful combination
severely undermines functionality.",Gianluca De Stefano
2024-08-12T06:16:37Z,http://arxiv.org/abs/2408.05933v1,"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case
  Study with Locally Deployed Ollama Models","With the growing demand for offline PDF chatbots in automotive industrial
production environments, optimizing the deployment of large language models
(LLMs) in local, low-performance settings has become increasingly important.
This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques
for processing complex automotive industry documents using locally deployed
Ollama models. Based on the Langchain framework, we propose a multi-dimensional
optimization approach for Ollama's local RAG implementation. Our method
addresses key challenges in automotive document processing, including
multi-column layouts and technical specifications. We introduce improvements in
PDF processing, retrieval mechanisms, and context compression, tailored to the
unique characteristics of automotive industry documents. Additionally, we
design custom classes supporting embedding pipelines and an agent supporting
self-RAG based on LangGraph best practices. To evaluate our approach, we
constructed a proprietary dataset comprising typical automotive industry
documents, including technical reports and corporate regulations. We compared
our optimized RAG model and self-RAG agent against a naive RAG baseline across
three datasets: our automotive industry dataset, QReCC, and CoQA. Results
demonstrate significant improvements in context precision, context recall,
answer relevancy, and faithfulness, with particularly notable performance on
the automotive industry dataset. Our optimization scheme provides an effective
solution for deploying local RAG systems in the automotive sector, addressing
the specific needs of PDF chatbots in industrial production environments. This
research has important implications for advancing information processing and
intelligent production in the automotive industry.",Fei Liu
2024-09-24T23:33:07Z,http://arxiv.org/abs/2409.19019v1,RAGProbe: An Automated Approach for Evaluating RAG Applications,"Retrieval Augmented Generation (RAG) is increasingly being used when building
Generative AI applications. Evaluating these applications and RAG pipelines is
mostly done manually, via a trial and error process. Automating evaluation of
RAG pipelines requires overcoming challenges such as context misunderstanding,
wrong format, incorrect specificity, and missing content. Prior works therefore
focused on improving evaluation metrics as well as enhancing components within
the pipeline using available question and answer datasets. However, they have
not focused on 1) providing a schema for capturing different types of
question-answer pairs or 2) creating a set of templates for generating
question-answer pairs that can support automation of RAG pipeline evaluation.
In this paper, we present a technique for generating variations in
question-answer pairs to trigger failures in RAG pipelines. We validate 5
open-source RAG pipelines using 3 datasets. Our approach revealed the highest
failure rates when prompts combine multiple questions: 91% for questions when
spanning multiple documents and 78% for questions from a single document;
indicating a need for developers to prioritise handling these combined
questions. 60% failure rate was observed in academic domain dataset and 53% and
62% failure rates were observed in open-domain datasets. Our automated approach
outperforms the existing state-of-the-art methods, by increasing the failure
rate by 51% on average per dataset. Our work presents an automated approach for
continuously monitoring the health of RAG pipelines, which can be integrated
into existing CI/CD pipelines, allowing for improved quality.",Shangeetha Sivasothy
2024-10-02T17:37:18Z,http://arxiv.org/abs/2410.01782v1,"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large
  Language Models","Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/",Shayekh Bin Islam
2024-10-11T13:36:13Z,http://arxiv.org/abs/2410.08801v1,"A Methodology for Evaluating RAG Systems: A Case Study On Configuration
  Dependency Validation","Retrieval-augmented generation (RAG) is an umbrella of different components,
design decisions, and domain-specific adaptations to enhance the capabilities
of large language models and counter their limitations regarding hallucination
and outdated and missing knowledge. Since it is unclear which design decisions
lead to a satisfactory performance, developing RAG systems is often
experimental and needs to follow a systematic and sound methodology to gain
sound and reliable results. However, there is currently no generally accepted
methodology for RAG evaluation despite a growing interest in this technology.
In this paper, we propose a first blueprint of a methodology for a sound and
reliable evaluation of RAG systems and demonstrate its applicability on a
real-world software engineering research task: the validation of configuration
dependencies across software technologies. In summary, we make two novel
contributions: (i) A novel, reusable methodological design for evaluating RAG
systems, including a demonstration that represents a guideline, and (ii) a RAG
system, which has been developed following this methodology, that achieves the
highest accuracy in the field of dependency validation. For the blueprint's
demonstration, the key insights are the crucial role of choosing appropriate
baselines and metrics, the necessity for systematic RAG refinements derived
from qualitative failure analysis, as well as the reporting practices of key
design decision to foster replication and evaluation.",Sebastian Simon
2024-10-16T05:20:32Z,http://arxiv.org/abs/2410.12248v1,"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for
  Retrieval-Augmented Generation with Enhanced Data Diversity","Retrieval-Augmented Generation (RAG) aims to enhance large language models
(LLMs) to generate more accurate and reliable answers with the help of the
retrieved context from external knowledge sources, thereby reducing the
incidence of hallucinations. Despite the advancements, evaluating these systems
remains a crucial research area due to the following issues: (1) Limited data
diversity: The insufficient diversity of knowledge sources and query types
constrains the applicability of RAG systems; (2) Obscure problems location:
Existing evaluation methods have difficulty in locating the stage of the RAG
pipeline where problems occur; (3) Unstable retrieval evaluation: These methods
often fail to effectively assess retrieval performance, particularly when the
chunking strategy changes. To tackle these challenges, we propose a
Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough
evaluation across the entire RAG pipeline, including chunking, retrieval,
reranking, and generation. To effectively evaluate the first three phases, we
introduce multi-granularity keywords, including coarse-grained and fine-grained
keywords, to assess the retrieved context instead of relying on the annotation
of golden chunks. Moreover, we release a holistic benchmark dataset tailored
for diverse data scenarios covering a wide range of document formats and query
types. We demonstrate the utility of the CoFE-RAG framework by conducting
experiments to evaluate each stage of RAG systems. Our evaluation method
provides unique insights into the effectiveness of RAG systems in handling
diverse data scenarios, offering a more nuanced understanding of their
capabilities and limitations.",Jintao Liu
2024-10-27T21:12:12Z,http://arxiv.org/abs/2410.20598v2,"R^3AG: First Workshop on Refined and Reliable Retrieval Augmented
  Generation","Retrieval-augmented generation (RAG) has gained wide attention as the key
component to improve generative models with external knowledge augmentation
from information retrieval. It has shown great prominence in enhancing the
functionality and performance of large language model (LLM)-based applications.
However, with the comprehensive application of RAG, more and more problems and
limitations have been identified, thus urgently requiring further fundamental
exploration to improve current RAG frameworks. This workshop aims to explore in
depth how to conduct refined and reliable RAG for downstream AI tasks.
  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024
to call for participants to re-examine and formulate the basic principles and
practical implementation of refined and reliable RAG. The workshop serves as a
platform for both academia and industry researchers to conduct discussions,
share insights, and foster research to build the next generation of RAG
systems. Participants will engage in discussions and presentations focusing on
fundamental challenges, cutting-edge research, and potential pathways to
improve RAG. At the end of the workshop, we aim to have a clearer understanding
of how to improve the reliability and applicability of RAG with more robust
information retrieval and language generation.",Zihan Wang
2024-11-05T09:58:36Z,http://arxiv.org/abs/2411.02959v1,"HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge
  in RAG Systems","Retrieval-Augmented Generation (RAG) has been shown to improve knowledge
capabilities and alleviate the hallucination problem of LLMs. The Web is a
major source of external knowledge used in RAG systems, and many commercial
systems such as ChatGPT and Perplexity have used Web search engines as their
major retrieval systems. Typically, such RAG systems retrieve search results,
download HTML sources of the results, and then extract plain texts from the
HTML sources. Plain text documents or chunks are fed into the LLMs to augment
the generation. However, much of the structural and semantic information
inherent in HTML, such as headings and table structures, is lost during this
plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,
which uses HTML instead of plain text as the format of retrieved knowledge in
RAG. We believe HTML is better than plain text in modeling knowledge in
external documents, and most LLMs possess robust capacities to understand HTML.
However, utilizing HTML presents new challenges. HTML contains additional
content such as tags, JavaScript, and CSS specifications, which bring extra
input tokens and noise to the RAG system. To address this issue, we propose
HTML cleaning, compression, and pruning strategies, to shorten the HTML while
minimizing the loss of information. Specifically, we design a two-step
block-tree-based pruning method that prunes useless HTML blocks and keeps only
the relevant part of the HTML. Experiments on six QA datasets confirm the
superiority of using HTML in RAG systems.",Jiejun Tan
2024-11-21T13:18:03Z,http://arxiv.org/abs/2411.14110v1,"RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented
  Generation Applications with Agent-based Attacks","While large language models (LLMs) have achieved notable success in
generative tasks, they still face limitations, such as lacking up-to-date
knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)
enhances LLM performance by integrating external knowledge bases, providing
additional context which significantly improves accuracy and knowledge
coverage. However, building these external knowledge bases often requires
substantial resources and may involve sensitive information. In this paper, we
propose an agent-based automated privacy attack called RAG-Thief, which can
extract a scalable amount of private data from the private database used in RAG
applications. We conduct a systematic study on the privacy risks associated
with RAG applications, revealing that the vulnerability of LLMs makes the
private knowledge bases suffer significant privacy risks. Unlike previous
manual attacks which rely on traditional prompt injection techniques, RAG-Thief
starts with an initial adversarial query and learns from model responses,
progressively generating new queries to extract as many chunks from the
knowledge base as possible. Experimental results show that our RAG-Thief can
extract over 70% information from the private knowledge bases within customized
RAG applications deployed on local machines and real-world platforms, including
OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy
vulnerabilities in current RAG applications and underscore the pressing need
for stronger safeguards.",Changyue Jiang
2024-12-10T04:55:57Z,http://arxiv.org/abs/2412.07189v1,"When Graph Meets Retrieval Augmented Generation for Wireless Networks: A
  Tutorial and Case Study","The rapid development of next-generation networking technologies underscores
their transformative role in revolutionizing modern communication systems,
enabling faster, more reliable, and highly interconnected solutions. However,
such development has also brought challenges to network optimizations. Thanks
to the emergence of Large Language Models (LLMs) in recent years, tools
including Retrieval Augmented Generation (RAG) have been developed and applied
in various fields including networking, and have shown their effectiveness.
Taking one step further, the integration of knowledge graphs into RAG
frameworks further enhanced the performance of RAG in networking applications
such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing
more contextually relevant responses through more accurate retrieval of related
network information. This paper introduces the RAG framework that integrates
knowledge graphs in its database and explores such framework's application in
networking. We begin by exploring RAG's applications in networking and the
limitations of conventional RAG and present the advantages that knowledge
graphs' structured knowledge representation brings to the retrieval and
generation processes. Next, we propose a detailed GraphRAG-based framework for
networking, including a step-by-step tutorial on its construction. Our
evaluation through a case study on channel gain prediction demonstrates
GraphRAG's enhanced capability in generating accurate, contextually rich
responses, surpassing traditional RAG models. Finally, we discuss key future
directions for applying knowledge-graphs-empowered RAG frameworks in
networking, including robust updates, mitigation of hallucination, and enhanced
security measures for networking applications.",Yang Xiong
2023-11-16T00:39:39Z,http://arxiv.org/abs/2311.09476v2,"ARES: An Automated Evaluation Framework for Retrieval-Augmented
  Generation Systems","Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. By creating its own synthetic training data, ARES finetunes
lightweight LM judges to assess the quality of individual RAG components. To
mitigate potential prediction errors, ARES utilizes a small set of
human-annotated datapoints for prediction-powered inference (PPI). Across eight
different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES
accurately evaluates RAG systems while using only a few hundred human
annotations during evaluation. Furthermore, ARES judges remain effective across
domain shifts, proving accurate even after changing the type of queries and/or
documents used in the evaluated RAG systems. We make our code and datasets
publicly available on Github.",Jon Saad-Falcon
2024-01-23T09:54:36Z,http://arxiv.org/abs/2401.12599v1,"Revolutionizing Retrieval-Augmented Generation with Enhanced PDF
  Structure Recognition","With the rapid development of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) has become a predominant method in the
field of professional knowledge-based question answering. Presently, major
foundation model companies have opened up Embedding and Chat API interfaces,
and frameworks like LangChain have already integrated the RAG process. It
appears that the key models and steps in RAG have been resolved, leading to the
question: are professional knowledge QA systems now approaching perfection?
This article discovers that current primary methods depend on the premise of
accessing high-quality text corpora. However, since professional documents are
mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts
the effectiveness of professional knowledge-based QA. We conducted an empirical
RAG experiment across hundreds of questions from the corresponding real-world
professional documents. The results show that, ChatDOC, a RAG system equipped
with a panoptic and pinpoint PDF parser, retrieves more accurate and complete
segments, and thus better answers. Empirical experiments show that ChatDOC is
superior to baseline on nearly 47% of questions, ties for 38% of cases, and
falls short on only 15% of cases. It shows that we may revolutionize RAG with
enhanced PDF structure recognition.",Demiao Lin
2024-02-19T07:06:52Z,http://arxiv.org/abs/2402.11891v1,"FeB4RAG: Evaluating Federated Search in the Context of Retrieval
  Augmented Generation","Federated search systems aggregate results from multiple search engines,
selecting appropriate sources to enhance result quality and align with user
intent. With the increasing uptake of Retrieval-Augmented Generation (RAG)
pipelines, federated search can play a pivotal role in sourcing relevant
information across heterogeneous data sources to generate informed responses.
However, existing datasets, such as those developed in the past TREC FedWeb
tracks, predate the RAG paradigm shift and lack representation of modern
information retrieval challenges. To bridge this gap, we present FeB4RAG, a
novel dataset specifically designed for federated search within RAG frameworks.
This dataset, derived from 16 sub-collections of the widely used \beir
benchmarking collection, includes 790 information requests (akin to
conversational queries) tailored for chatbot applications, along with top
results returned by each resource and associated LLM-derived relevance
judgements. Additionally, to support the need for this collection, we
demonstrate the impact on response generation of a high quality federated
search system for RAG compared to a naive approach to federated search. We do
so by comparing answers generated through the RAG pipeline through a
qualitative side-by-side comparison. Our collection fosters and supports the
development and evaluation of new federated search methods, especially in the
context of RAG pipelines.",Shuai Wang
2024-04-01T10:43:52Z,http://arxiv.org/abs/2404.01037v1,ARAGOG: Advanced RAG Output Grading,"Retrieval-Augmented Generation (RAG) is essential for integrating external
knowledge into Large Language Model (LLM) outputs. While the literature on RAG
is growing, it primarily focuses on systematic reviews and comparisons of new
state-of-the-art (SoTA) techniques against their predecessors, with a gap in
extensive experimental comparisons. This study begins to address this gap by
assessing various RAG methods' impacts on retrieval precision and answer
similarity. We found that Hypothetical Document Embedding (HyDE) and LLM
reranking significantly enhance retrieval precision. However, Maximal Marginal
Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a
baseline Naive RAG system, and Multi-query approaches underperformed. Sentence
Window Retrieval emerged as the most effective for retrieval precision, despite
its variable performance on answer similarity. The study confirms the potential
of the Document Summary Index as a competent retrieval approach. All resources
related to this research are publicly accessible for further investigation
through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We
welcome the community to further this exploratory study in RAG systems.",MatouÅ¡ Eibich
2024-03-23T00:49:40Z,http://arxiv.org/abs/2404.07221v2,"Improving Retrieval for RAG based Question Answering Models on Financial
  Documents","The effectiveness of Large Language Models (LLMs) in generating accurate
responses relies heavily on the quality of input provided, particularly when
employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by
sourcing the most relevant text chunk(s) to base queries upon. Despite the
significant advancements in LLMs' response quality in recent years, users may
still encounter inaccuracies or irrelevant answers; these issues often stem
from suboptimal text chunk retrieval by RAG rather than the inherent
capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine
the RAG process. This paper explores the existing constraints of RAG pipelines
and introduces methodologies for enhancing text retrieval. It delves into
strategies such as sophisticated chunking techniques, query expansion, the
incorporation of metadata annotations, the application of re-ranking
algorithms, and the fine-tuning of embedding algorithms. Implementing these
approaches can substantially improve the retrieval quality, thereby elevating
the overall performance and reliability of LLMs in processing and responding to
queries.",Spurthi Setty
2024-04-17T01:27:42Z,http://arxiv.org/abs/2404.10981v2,"A Survey on Retrieval-Augmented Text Generation for Large Language
  Models","Retrieval-Augmented Generation (RAG) merges retrieval methods with deep
learning advancements to address the static limitations of large language
models (LLMs) by enabling the dynamic integration of up-to-date external
information. This methodology, focusing primarily on the text domain, provides
a cost-effective solution to the generation of plausible but possibly incorrect
responses by LLMs, thereby enhancing the accuracy and reliability of their
outputs through the use of real-world data. As RAG grows in complexity and
incorporates multiple concepts that can influence its performance, this paper
organizes the RAG paradigm into four categories: pre-retrieval, retrieval,
post-retrieval, and generation, offering a detailed perspective from the
retrieval viewpoint. It outlines RAG's evolution and discusses the field's
progression through the analysis of significant studies. Additionally, the
paper introduces evaluation methods for RAG, addressing the challenges faced
and proposing future research directions. By offering an organized framework
and categorization, the study aims to consolidate existing research on RAG,
clarify its technological underpinnings, and highlight its potential to broaden
the adaptability and applications of LLMs.",Yizheng Huang
2024-05-05T05:42:33Z,http://arxiv.org/abs/2405.02816v1,"Stochastic RAG: End-to-End Retrieval-Augmented Generation through
  Expected Utility Maximization","This paper introduces Stochastic RAG--a novel approach for end-to-end
optimization of retrieval-augmented generation (RAG) models that relaxes the
simplifying assumptions of marginalization and document independence, made in
most prior work. Stochastic RAG casts the retrieval process in RAG as a
stochastic sampling without replacement process. Through this formulation, we
employ straight-through Gumbel-top-k that provides a differentiable
approximation for sampling without replacement and enables effective end-to-end
optimization for RAG. We conduct extensive experiments on seven diverse
datasets on a wide range of tasks, from open-domain question answering to fact
verification to slot-filling for relation extraction and to dialogue systems.
By applying this optimization method to a recent and effective RAG model, we
advance state-of-the-art results on six out of seven datasets.",Hamed Zamani
2024-05-13T02:33:25Z,http://arxiv.org/abs/2405.07437v2,Evaluation of Retrieval-Augmented Generation: A Survey,"Retrieval-Augmented Generation (RAG) has recently gained traction in natural
language processing. Numerous studies and real-world applications are
leveraging its ability to enhance generative models through external
information retrieval. Evaluating these RAG systems, however, poses unique
challenges due to their hybrid structure and reliance on dynamic knowledge
sources. To better understand these challenges, we conduct A Unified Evaluation
Process of RAG (Auepora) and aim to provide a comprehensive overview of the
evaluation and benchmarks of RAG systems. Specifically, we examine and compare
several quantifiable metrics of the Retrieval and Generation components, such
as relevance, accuracy, and faithfulness, within the current RAG benchmarks,
encompassing the possible output and ground truth pairs. We then analyze the
various datasets and metrics, discuss the limitations of current benchmarks,
and suggest potential directions to advance the field of RAG benchmarks.",Hao Yu
2024-05-21T07:35:21Z,http://arxiv.org/abs/2405.13084v2,"The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented
  Generation (FutureDial-RAG)","Recently, increasing research interests have focused on retrieval augmented
generation (RAG) to mitigate hallucination for large language models (LLMs).
Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which
aims at promoting the study of RAG for dialog systems. The challenge builds
upon the MobileCS2 dataset, a real-life customer service datasets with nearly
3000 high-quality dialogs containing annotations for knowledge base query and
corresponding results. Over the dataset, we define two tasks, track 1 for
knowledge retrieval and track 2 for response generation, which are core
research questions in dialog systems with RAG. We build baseline systems for
the two tracks and design metrics to measure whether the systems can perform
accurate retrieval and generate informative and coherent response. The baseline
results show that it is very challenging to perform well on the two tasks,
which encourages the participating teams and the community to study how to make
better use of RAG for real-life dialog systems.",Yucheng Cai
2024-05-28T17:56:46Z,http://arxiv.org/abs/2405.18414v1,Don't Forget to Connect! Improving RAG with Graph-based Reranking,"Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.",Jialin Dong
2024-05-31T23:30:52Z,http://arxiv.org/abs/2406.04369v1,RAG Does Not Work for Enterprises,"Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of
large language model outputs by incorporating knowledge retrieval. However,
implementing RAG in enterprises poses challenges around data security,
accuracy, scalability, and integration. This paper explores the unique
requirements for enterprise RAG, surveys current approaches and limitations,
and discusses potential advances in semantic search, hybrid queries, and
optimized retrieval. It proposes an evaluation framework to validate enterprise
RAG solutions, including quantitative testing, qualitative analysis, ablation
studies, and industry case studies. This framework aims to help demonstrate the
ability of purpose-built RAG architectures to deliver accuracy and relevance
improvements with enterprise-grade security, compliance and integration. The
paper concludes with implications for enterprise deployments, limitations, and
future research directions. Close collaboration between researchers and
industry partners may accelerate progress in developing and deploying
retrieval-augmented generation technology.",Tilmann Bruckhaus
2024-06-21T08:31:02Z,http://arxiv.org/abs/2406.14972v1,A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems,"Retrieval Augmented Generation (RAG) represents a significant advancement in
artificial intelligence combining a retrieval phase with a generative phase,
with the latter typically being powered by large language models (LLMs). The
current common practices in RAG involve using ""instructed"" LLMs, which are
fine-tuned with supervised training to enhance their ability to follow
instructions and are aligned with human preferences using state-of-the-art
techniques. Contrary to popular belief, our study demonstrates that base models
outperform their instructed counterparts in RAG tasks by 20% on average under
our experimental settings. This finding challenges the prevailing assumptions
about the superiority of instructed LLMs in RAG applications. Further
investigations reveal a more nuanced situation, questioning fundamental aspects
of RAG and suggesting the need for broader discussions on the topic; or, as
Fromm would have it, ""Seldom is a glance at the statistics enough to understand
the meaning of the figures"".",Florin Cuconasu
2024-07-01T12:06:34Z,http://arxiv.org/abs/2407.01219v1,Searching for Best Practices in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) techniques have proven to be effective
in integrating up-to-date information, mitigating hallucinations, and enhancing
response quality, particularly in specialized domains. While many RAG
approaches have been proposed to enhance large language models through
query-dependent retrievals, these approaches still suffer from their complex
implementation and prolonged response times. Typically, a RAG workflow involves
multiple processing steps, each of which can be executed in various ways. Here,
we investigate existing RAG approaches and their potential combinations to
identify optimal RAG practices. Through extensive experiments, we suggest
several strategies for deploying RAG that balance both performance and
efficiency. Moreover, we demonstrate that multimodal retrieval techniques can
significantly enhance question-answering capabilities about visual inputs and
accelerate the generation of multimodal content using a ""retrieval as
generation"" strategy.",Xiaohua Wang
2024-07-19T03:02:51Z,http://arxiv.org/abs/2407.13998v2,"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval
  Augmented Question Answering","Question answering based on retrieval augmented generation (RAG-QA) is an
important research topic in NLP and has a wide range of real-world
applications. However, most existing datasets for this task are either
constructed using a single source corpus or consist of short extractive
answers, which fall short of evaluating large language model (LLM) based RAG-QA
systems on cross-domain generalization. To address these limitations, we create
Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form
answers that integrate short extractive answers from multiple documents into a
single, coherent narrative, covering 26K queries and large corpora across seven
different domains. We further propose RAG-QA Arena by directly comparing
model-generated answers against LFRQA's answers using LLMs as evaluators. We
show via extensive experiments that RAG-QA Arena and human judgments on answer
quality are highly correlated. Moreover, only 41.3% of the most competitive
LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a
challenging evaluation platform for future research.",Rujun Han
2024-07-25T13:47:01Z,http://arxiv.org/abs/2407.18044v1,"The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented
  Generation","Digital health chatbots powered by Large Language Models (LLMs) have the
potential to significantly improve personal health management for chronic
conditions by providing accessible and on-demand health coaching and
question-answering. However, these chatbots risk providing unverified and
inaccurate information because LLMs generate responses based on patterns
learned from diverse internet data. Retrieval Augmented Generation (RAG) can
help mitigate hallucinations and inaccuracies in LLM responses by grounding it
on reliable content. However, efficiently and accurately retrieving most
relevant set of content for real-time user questions remains a challenge. In
this work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a
novel approach that pre-computes a database of potential queries from a content
base using LLMs. For an incoming patient question, QB-RAG efficiently matches
it against this pre-generated query database using vector search, improving
alignment between user questions and the content. We establish a theoretical
foundation for QB-RAG and provide a comparative analysis of existing retrieval
enhancement techniques for RAG systems. Finally, our empirical evaluation
demonstrates that QB-RAG significantly improves the accuracy of healthcare
question answering, paving the way for robust and trustworthy LLM applications
in digital health.",Eric Yang
2024-07-29T08:38:14Z,http://arxiv.org/abs/2407.19794v2,Introducing a new hyper-parameter for RAG: Context Window Utilization,"This paper introduces a new hyper-parameter for Retrieval-Augmented
Generation (RAG) systems called Context Window Utilization. RAG systems enhance
generative models by incorporating relevant information retrieved from external
knowledge bases, improving the factual accuracy and contextual relevance of
generated responses. The size of the text chunks retrieved and processed is a
critical factor influencing RAG performance. This study aims to identify the
optimal chunk size that maximizes answer generation quality. Through systematic
experimentation, we analyze the effects of varying chunk sizes on the
efficiency and effectiveness of RAG frameworks. Our findings reveal that an
optimal chunk size balances the trade-off between providing sufficient context
and minimizing irrelevant information. These insights are crucial for enhancing
the design and implementation of RAG systems, underscoring the importance of
selecting an appropriate chunk size to achieve superior performance.",Kush Juvekar
2024-08-15T10:20:54Z,http://arxiv.org/abs/2408.08067v2,"RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented
  Generation","Despite Retrieval-Augmented Generation (RAG) showing promising capability in
leveraging external knowledge, a comprehensive evaluation of RAG systems is
still challenging due to the modular nature of RAG, evaluation of long-form
responses and reliability of measurements. In this paper, we propose a
fine-grained evaluation framework, RAGChecker, that incorporates a suite of
diagnostic metrics for both the retrieval and generation modules. Meta
evaluation verifies that RAGChecker has significantly better correlations with
human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8
RAG systems and conduct an in-depth analysis of their performance, revealing
insightful patterns and trade-offs in the design choices of RAG architectures.
The metrics of RAGChecker can guide researchers and practitioners in developing
more effective RAG systems. This work has been open sourced at
https://github.com/amazon-science/RAGChecker.",Dongyu Ru
2024-08-17T13:32:32Z,http://arxiv.org/abs/2408.09199v1,TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems,"In the pursuit of enhancing domain-specific Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) emerges as a promising solution to
mitigate issues such as hallucinations, outdated knowledge, and limited
expertise in highly specialized queries. However, existing approaches to RAG
fall short by neglecting system state variables, which are crucial for ensuring
adaptive control, retrieval halting, and system convergence. In this paper, we
introduce the TC-RAG through rigorous proof, a novel framework that addresses
these challenges by incorporating a Turing Complete System to manage state
variables, thereby enabling more efficient and accurate knowledge retrieval. By
leveraging a memory stack system with adaptive retrieval, reasoning, and
planning capabilities, TC-RAG not only ensures the controlled halting of
retrieval processes but also mitigates the accumulation of erroneous knowledge
via Push and Pop actions. In the case study of the medical domain, our
extensive experiments on real-world healthcare datasets demonstrate the
superiority of TC-RAG over existing methods in accuracy by over 7.20\%. Our
dataset and code have been available at
https://https://github.com/Artessay/SAMA.git.",Xinke Jiang
2024-09-17T14:47:33Z,http://arxiv.org/abs/2409.11242v2,"Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded
  Attributions and Learning to Refuse","LLMs are an integral component of retrieval-augmented generation (RAG)
systems. While many studies focus on evaluating the overall quality of
end-to-end RAG systems, there is a gap in understanding the appropriateness of
LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic
metric that evaluates the trustworthiness of LLMs within the RAG framework. Our
results show that various prompting methods, such as in-context learning, fail
to effectively adapt LLMs to the RAG task as measured by Trust-Score.
Consequently, we propose Trust-Align, a method to align LLMs for improved
Trust-Score performance. The LLaMA-3 family, aligned using our method,
significantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0),
QAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of
Trust-Align across different open-weight models, including the LLaMA series (1b
to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at
\url{https://anonymous.4open.science/r/trust-align}",Maojia Song
2024-09-23T20:05:12Z,http://arxiv.org/abs/2409.15515v1,"Learning When to Retrieve, What to Rewrite, and How to Respond in
  Conversational QA","Augmenting Large Language Models (LLMs) with information retrieval
capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial
for knowledge-intensive tasks. However, understanding users' contextual search
intent when generating responses is an understudied topic for conversational
question answering (QA). This conversational extension leads to additional
concerns when compared to single-turn QA as it is more challenging for systems
to comprehend conversational context and manage retrieved passages over
multiple turns. In this work, we propose a method for enabling LLMs to decide
when to retrieve in RAG settings given a conversational context. When retrieval
is deemed necessary, the LLM then rewrites the conversation for passage
retrieval and judges the relevance of returned passages before response
generation. Operationally, we build on the single-turn SELF-RAG framework (Asai
et al., 2023) and propose SELF-multi-RAG for conversational settings.
SELF-multi-RAG demonstrates improved capabilities over single-turn variants
with respect to retrieving relevant passages (by using summarized
conversational context) and assessing the quality of generated responses.
Experiments on three conversational QA datasets validate the enhanced response
generation capabilities of SELF-multi-RAG, with improvements of ~13% measured
by human annotation.",Nirmal Roy
2024-09-29T22:04:26Z,http://arxiv.org/abs/2409.19804v1,"Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in
  Retrieval-Augmented Generation Systems","RAG (Retrieval-Augmented Generation) have recently gained significant
attention for their enhanced ability to integrate external knowledge sources in
open-domain question answering (QA) tasks. However, it remains unclear how
these models address fairness concerns, particularly with respect to sensitive
attributes such as gender, geographic location, and other demographic factors.
First, as language models evolve to prioritize utility, like improving exact
match accuracy, fairness may have been largely overlooked. Second, RAG methods
are complex pipelines, making it hard to identify and address biases, as each
component is optimized for different goals. In this paper, we aim to
empirically evaluate fairness in several RAG methods. We propose a fairness
evaluation framework tailored to RAG methods, using scenario-based questions
and analyzing disparities across demographic attributes. The experimental
results indicate that, despite recent advances in utility-driven optimization,
fairness issues persist in both the retrieval and generation stages,
highlighting the need for more targeted fairness interventions within RAG
pipelines. We will release our dataset and code upon acceptance of the paper.",Xuyang Wu
2024-10-10T03:52:54Z,http://arxiv.org/abs/2410.07590v1,"TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed
  KV Caches for Chunked Text","Current Retrieval-Augmented Generation (RAG) systems concatenate and process
numerous retrieved document chunks for prefill which requires a large volume of
computation, therefore leading to significant latency in time-to-first-token
(TTFT). To reduce the computation overhead as well as TTFT, we introduce
TurboRAG, a novel RAG system that redesigns the inference paradigm of the
current RAG system by first pre-computing and storing the key-value (KV) caches
of documents offline, and then directly retrieving the saved KV cache for
prefill. Hence, online computation of KV caches is eliminated during inference.
In addition, we provide a number of insights into the mask matrix and
positional embedding mechanisms, plus fine-tune a pretrained language model to
maintain model accuracy of TurboRAG. Our approach is applicable to most
existing large language models and their applications without any requirement
in modification of models and inference systems. Experimental results across a
suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x
compared to the conventional RAG systems (on an average of 8.6x), but reserving
comparable performance to the standard RAG systems.",Songshuo Lu
2024-10-01T03:54:45Z,http://arxiv.org/abs/2410.12812v1,"Optimizing and Evaluating Enterprise Retrieval-Augmented Generation
  (RAG): A Content Design Perspective","Retrieval-augmented generation (RAG) is a popular technique for using large
language models (LLMs) to build customer-support, question-answering solutions.
In this paper, we share our team's practical experience building and
maintaining enterprise-scale RAG solutions that answer users' questions about
our software based on product documentation. Our experience has not always
matched the most common patterns in the RAG literature. This paper focuses on
solution strategies that are modular and model-agnostic. For example, our
experience over the past few years - using different search methods and LLMs,
and many knowledge base collections - has been that simple changes to the way
we create knowledge base content can have a huge impact on our RAG solutions'
success. In this paper, we also discuss how we monitor and evaluate results.
Common RAG benchmark evaluation techniques have not been useful for evaluating
responses to novel user questions, so we have found a flexible, ""human in the
lead"" approach is required.",Sarah Packowski
2024-10-18T04:17:49Z,http://arxiv.org/abs/2410.14167v1,"Optimizing Retrieval-Augmented Generation with Elasticsearch for
  Enhanced Question-Answering Systems","This study aims to improve the accuracy and quality of large-scale language
models (LLMs) in answering questions by integrating Elasticsearch into the
Retrieval Augmented Generation (RAG) framework. The experiment uses the
Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and
compares the performance of different retrieval methods, including traditional
methods based on keyword matching or semantic similarity calculation, BM25-RAG
and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that
ES-RAG not only has obvious advantages in retrieval efficiency but also
performs well in key indicators such as accuracy, which is 0.51 percentage
points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search
capabilities and rich configuration options enable the entire
question-answering system to better handle complex queries and provide more
flexible and efficient responses based on the diverse needs of users. Future
research directions can further explore how to optimize the interaction
mechanism between Elasticsearch and LLM, such as introducing higher-level
semantic understanding and context-awareness capabilities, to achieve a more
intelligent and humanized question-answering experience.",Jiajing Chen
2024-10-30T10:11:53Z,http://arxiv.org/abs/2410.22874v1,"Eliciting Critical Reasoning in Retrieval-Augmented Language Models via
  Contrastive Explanations","Retrieval-augmented generation (RAG) has emerged as a critical mechanism in
contemporary NLP to support Large Language Models(LLMs) in systematically
accessing richer factual context. However, the integration of RAG mechanisms
brings its inherent challenges, as LLMs need to deal with potentially noisy
contexts. Recent studies have shown that LLMs still struggle to critically
analyse RAG-based in-context information, a limitation that may lead to
incorrect inferences and hallucinations. In this paper, we investigate how to
elicit critical reasoning in RAG via contrastive explanations. In particular,
we propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant
documents given a query, (ii) selects and exemplifies relevant passages, and
(iii) generates explanations that explicitly contrast the relevance of the
passages to (iv) support the final answer. We show the impact of C-RAG building
contrastive reasoning demonstrations from LLMs to instruct smaller models for
retrieval-augmented tasks. Extensive experiments demonstrate that C-RAG
improves state-of-the-art RAG models while (a) requiring significantly fewer
prompts and demonstrations and (b) being robust to perturbations in the
retrieved documents.",Leonardo Ranaldi
2024-12-16T15:12:53Z,http://arxiv.org/abs/2412.11854v1,"Towards Understanding Systems Trade-offs in Retrieval-Augmented
  Generation Model Inference","The rapid increase in the number of parameters in large language models
(LLMs) has significantly increased the cost involved in fine-tuning and
retraining LLMs, a necessity for keeping models up to date and improving
accuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to
improving the capabilities and accuracy of LLMs without the necessity of
retraining. Although RAG eliminates the need for continuous retraining to
update model data, it incurs a trade-off in the form of slower model inference
times. Resultingly, the use of RAG in enhancing the accuracy and capabilities
of LLMs often involves diverse performance implications and trade-offs based on
its design. In an effort to begin tackling and mitigating the performance
penalties associated with RAG from a systems perspective, this paper introduces
a detailed taxonomy and characterization of the different elements within the
RAG ecosystem for LLMs that explore trade-offs within latency, throughput, and
memory. Our study reveals underlying inefficiencies in RAG for systems
deployment, that can result in TTFT latencies that are twice as long and
unoptimized datastores that consume terabytes of storage.",Michael Shen
2024-12-16T19:11:55Z,http://arxiv.org/abs/2412.12300v1,Unanswerability Evaluation for Retreival Augmented Generation,"Existing evaluation frameworks for retrieval-augmented generation (RAG)
systems focus on answerable queries, but they overlook the importance of
appropriately rejecting unanswerable requests. In this paper, we introduce
UAEval4RAG, a framework designed to evaluate whether RAG systems can handle
unanswerable queries effectively. We define a taxonomy with six unanswerable
categories, and UAEval4RAG automatically synthesizes diverse and challenging
queries for any given knowledge base with unanswered ratio and acceptable ratio
metrics. We conduct experiments with various RAG components, including
retrieval models, rewriting methods, rerankers, language models, and prompting
strategies, and reveal hidden trade-offs in performance of RAG systems. Our
findings highlight the critical role of component selection and prompt design
in optimizing RAG systems to balance the accuracy of answerable queries with
high rejection rates of unanswerable ones. UAEval4RAG provides valuable
insights and tools for developing more robust and reliable RAG systems.",Xiangyu Peng
2024-12-19T11:30:07Z,http://arxiv.org/abs/2412.14751v1,"Query pipeline optimization for cancer patient question answering
  systems","Retrieval-augmented generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by using query pipelines to retrieve relevant external
information and grounding responses in retrieved knowledge. However, query
pipeline optimization for cancer patient question-answering (CPQA) systems
requires separately optimizing multiple components with domain-specific
considerations. We propose a novel three-aspect optimization approach for the
RAG query pipeline in CPQA systems, utilizing public biomedical databases like
PubMed and PubMed Central. Our optimization includes: (1) document retrieval,
utilizing a comparative analysis of NCBI resources and introducing Hybrid
Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,
identifying optimal pairings of dense retrievers and rerankers; and (3)
semantic representation, introducing Semantic Enhanced Overlap Segmentation
(SEOS) for improved contextual understanding. On a custom-developed dataset
tailored for cancer-related inquiries, our optimized RAG approach improved the
answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and
about 3% over a naive RAG setup. This study highlights the importance of
domain-specific query optimization in realizing the full potential of RAG and
provides a robust framework for building more accurate and reliable CPQA
systems, advancing the development of RAG-based biomedical systems.",Maolin He
2024-01-30T14:25:32Z,http://arxiv.org/abs/2401.17043v3,"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
  Generation of Large Language Models","Retrieval-Augmented Generation (RAG) is a technique that enhances the
capabilities of large language models (LLMs) by incorporating external
knowledge sources. This method addresses common LLM limitations, including
outdated information and the tendency to produce inaccurate ""hallucinated""
content. However, the evaluation of RAG systems is challenging, as existing
benchmarks are limited in scope and diversity. Most of the current benchmarks
predominantly assess question-answering applications, overlooking the broader
spectrum of situations where RAG could prove advantageous. Moreover, they only
evaluate the performance of the LLM component of the RAG pipeline in the
experiments, and neglect the influence of the retrieval component and the
external knowledge database. To address these issues, this paper constructs a
large-scale and more comprehensive benchmark, and evaluates all the components
of RAG systems in various RAG application scenarios. Specifically, we have
categorized the range of RAG applications into four distinct types-Create,
Read, Update, and Delete (CRUD), each representing a unique use case. ""Create""
refers to scenarios requiring the generation of original, varied content.
""Read"" involves responding to intricate questions in knowledge-intensive
situations. ""Update"" focuses on revising and rectifying inaccuracies or
inconsistencies in pre-existing texts. ""Delete"" pertains to the task of
summarizing extensive texts into more concise forms. For each of these CRUD
categories, we have developed comprehensive datasets to evaluate the
performance of RAG systems. We also analyze the effects of various components
of the RAG system, such as the retriever, the context length, the knowledge
base construction, and the LLM. Finally, we provide useful insights for
optimizing the RAG technology for different scenarios.",Yuanjie Lyu
2024-02-05T16:46:16Z,http://arxiv.org/abs/2402.03181v5,"C-RAG: Certified Generation Risks for Retrieval-Augmented Language
  Models","Despite the impressive capabilities of large language models (LLMs) across
diverse applications, they still suffer from trustworthiness issues, such as
hallucinations and misalignments. Retrieval-augmented language models (RAG)
have been proposed to enhance the credibility of generations by grounding
external knowledge, but the theoretical understandings of their generation
risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed
lead to low generation risks, 2) how to provide provable guarantees on the
generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions
enable RAG models to reduce generation risks. We propose C-RAG, the first
framework to certify generation risks for RAG models. Specifically, we provide
conformal risk analysis for RAG models and certify an upper confidence bound of
generation risks, which we refer to as conformal generation risk. We also
provide theoretical guarantees on conformal generation risks for general
bounded risk functions under test distribution shifts. We prove that RAG
achieves a lower conformal generation risk than that of a single LLM when the
quality of the retrieval model and transformer is non-trivial. Our intensive
empirical results demonstrate the soundness and tightness of our conformal
generation risk guarantees across four widely-used NLP datasets on four
state-of-the-art retrieval models.",Mintong Kang
2024-05-07T02:49:59Z,http://arxiv.org/abs/2405.03963v4,ERATTA: Extreme RAG for Table To Answers with Large Language Models,"Large language models (LLMs) with retrieval augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. Although RAG implemented with AI agents (agentic-RAG) has been recently
popularized, its suffers from unstable cost and unreliable performances for
Enterprise-level data-practices. Most existing use-cases that incorporate RAG
with LLMs have been either generic or extremely domain specific, thereby
questioning the scalability and generalizability of RAG-LLM approaches. In this
work, we propose a unique LLM-based system where multiple LLMs can be invoked
to enable data authentication, user-query routing, data-retrieval and custom
prompting for question-answering capabilities from Enterprise-data tables. The
source tables here are highly fluctuating and large in size and the proposed
framework enables structured responses in under 10 seconds per query.
Additionally, we propose a five metric scoring module that detects and reports
hallucinations in the LLM responses. Our proposed system and scoring metrics
achieve >90% confidence scores across hundreds of user queries in the
sustainability, financial health and social media domains. Extensions to the
proposed extreme RAG architectures can enable heterogeneous source querying
using LLMs.",Sohini Roychowdhury
2024-05-30T15:14:24Z,http://arxiv.org/abs/2405.20139v1,GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning,"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form
of triplets (head, relation, tail), which collectively form a graph. Question
Answering over KGs (KGQA) is the task of answering natural questions grounding
the reasoning to the information provided by the KG. Large Language Models
(LLMs) are the state-of-the-art models for QA tasks due to their remarkable
ability to understand natural language. On the other hand, Graph Neural
Networks (GNNs) have been widely used for KGQA as they can handle the complex
graph information stored in the KG. In this work, we introduce GNN-RAG, a novel
method for combining language understanding abilities of LLMs with the
reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for
a given question. Second, the shortest paths in the KG that connect question
entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with
RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to
extract useful graph information, while the LLM leverages its natural language
processing ability for ultimate KGQA. Furthermore, we develop a retrieval
augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in
two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching
GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop
and multi-entity questions outperforming competing approaches by 8.9--15.5%
points at answer F1.",Costas Mavromatis
2024-06-20T23:20:34Z,http://arxiv.org/abs/2406.14783v2,Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework,"Challenges in the automated evaluation of Retrieval-Augmented Generation
(RAG) Question-Answering (QA) systems include hallucination problems in
domain-specific knowledge and the lack of gold standard benchmarks for company
internal tasks. This results in difficulties in evaluating RAG variations, like
RAG-Fusion (RAGF), in the context of a product QA task at Infineon
Technologies. To solve these problems, we propose a comprehensive evaluation
framework, which leverages Large Language Models (LLMs) to generate large
datasets of synthetic queries based on real user queries and in-domain
documents, uses LLM-as-a-judge to rate retrieved documents and answers,
evaluates the quality of answers, and ranks different variants of
Retrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based
competition. LLM-as-a-judge rating of a random sample of synthetic queries
shows a moderate, positive correlation with domain expert scoring in relevance,
accuracy, completeness, and precision. While RAGF outperformed RAG in Elo
score, a significance analysis against expert annotations also shows that RAGF
significantly outperforms RAG in completeness, but underperforms in precision.
In addition, Infineon's RAGF assistant demonstrated slightly higher performance
in document relevance based on MRR@5 scores. We find that RAGElo positively
aligns with the preferences of human annotators, though due caution is still
required. Finally, RAGF's approach leads to more complete answers based on
expert annotations and better answers overall based on RAGElo's evaluation
criteria.",Zackary Rackauckas
2024-06-24T17:37:52Z,http://arxiv.org/abs/2406.16828v1,"RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024
  Retrieval-Augmented Generation Track","Did you try out the new Bing Search? Or maybe you fiddled around with Google
AI~Overviews? These might sound familiar because the modern-day search stack
has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language
models (LLMs) to provide a well-informed, attributed, concise summary in
contrast to the traditional search paradigm that relies on displaying a ranked
list of documents. Therefore, given these recent advancements, it is crucial to
have an arena to build, test, visualize, and systematically evaluate RAG-based
search systems. With this in mind, we propose the TREC 2024 RAG Track to foster
innovation in evaluating RAG systems. In our work, we lay out the steps we've
made towards making this track a reality -- we describe the details of our
reusable framework, Ragnar\""ok, explain the curation of the new MS MARCO V2.1
collection choice, release the development topics for the track, and
standardize the I/O definitions which assist the end user. Next, using
Ragnar\""ok, we identify and provide key industrial baselines such as OpenAI's
GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface
for an interactive arena allowing benchmarking pairwise RAG systems by
crowdsourcing. We open-source our Ragnar\""ok framework and baselines to achieve
a unified standard for future RAG systems.",Ronak Pradeep
2024-06-27T14:58:38Z,http://arxiv.org/abs/2406.19234v2,"Generating Is Believing: Membership Inference Attacks against
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that
mitigates issues such as hallucinations and knowledge staleness in Large
Language Models (LLMs) by retrieving relevant knowledge from an external
database to assist in content generation. Existing research has demonstrated
potential privacy risks associated with the LLMs of RAG. However, the privacy
risks posed by the integration of an external database, which often contains
sensitive data such as medical records or personal identities, have remained
largely unexplored. In this paper, we aim to bridge this gap by focusing on
membership privacy of RAG's external database, with the aim of determining
whether a given sample is part of the RAG's database. Our basic idea is that if
a sample is in the external database, it will exhibit a high degree of semantic
similarity to the text generated by the RAG system. We present S$^2$MIA, a
\underline{M}embership \underline{I}nference \underline{A}ttack that utilizes
the \underline{S}emantic \underline{S}imilarity between a given sample and the
content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate
the potential to breach the membership privacy of the RAG database. Extensive
experiment results demonstrate that S$^2$MIA can achieve a strong inference
performance compared with five existing MIAs, and is able to escape from the
protection of three representative defenses.",Yuying Li
2024-07-15T12:35:00Z,http://arxiv.org/abs/2407.10670v1,"Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for
  Improved Quality and Efficiency in RAG Systems","Retrieval-augmented generation (RAG) techniques leverage the in-context
learning capabilities of large language models (LLMs) to produce more accurate
and relevant responses. Originating from the simple 'retrieve-then-read'
approach, the RAG framework has evolved into a highly flexible and modular
paradigm. A critical component, the Query Rewriter module, enhances knowledge
retrieval by generating a search-friendly query. This method aligns input
questions more closely with the knowledge base. Our research identifies
opportunities to enhance the Query Rewriter module to Query Rewriter+ by
generating multiple queries to overcome the Information Plateaus associated
with a single query and by rewriting questions to eliminate Ambiguity, thereby
clarifying the underlying intent. We also find that current RAG systems exhibit
issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge
Filter. These two modules are both based on the instruction-tuned Gemma-2B
model, which together enhance response quality. The final identified issue is
Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the
Retriever Trigger to solve this. The former supports the dynamic expansion of
the RAG system's knowledge base in a parameter-free manner, while the latter
optimizes the cost for accessing external knowledge, thereby improving resource
utilization and response efficiency. These four RAG modules synergistically
improve the response quality and efficiency of the RAG system. The
effectiveness of these modules has been validated through experiments and
ablation studies across six common QA datasets. The source code can be accessed
at https://github.com/Ancientshi/ERM4.",Yunxiao Shi
2024-07-18T17:55:55Z,http://arxiv.org/abs/2407.13757v1,"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation
  of Large Language Models","Retrieval-Augmented Generation (RAG) is applied to solve hallucination
problems and real-time constraints of large language models, but it also
induces vulnerabilities against retrieval corruption attacks. Existing research
mainly explores the unreliability of RAG in white-box and closed-domain QA
tasks. In this paper, we aim to reveal the vulnerabilities of
Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks
for opinion manipulation. We explore the impact of such attacks on user
cognition and decision-making, providing new insight to enhance the reliability
and security of RAG models. We manipulate the ranking results of the retrieval
model in RAG with instruction and use these results as data to train a
surrogate model. By employing adversarial retrieval attack methods to the
surrogate model, black-box transfer attacks on RAG are further realized.
Experiments conducted on opinion datasets across multiple topics show that the
proposed attack strategy can significantly alter the opinion polarity of the
content generated by RAG. This demonstrates the model's vulnerability and, more
importantly, reveals the potential negative impact on user cognition and
decision-making, making it easier to mislead users into accepting incorrect or
biased information.",Zhuo Chen
2024-09-16T01:08:18Z,http://arxiv.org/abs/2409.09916v1,SFR-RAG: Towards Contextually Faithful LLMs,"Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.",Xuan-Phi Nguyen
2024-09-26T21:44:11Z,http://arxiv.org/abs/2409.18313v4,"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and
  Generation","There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhouse of large-scale
non-parametric knowledge, however existing techniques do not directly transfer
to the embodied domain, which is multimodal, data is highly correlated, and
perception requires abstraction.
  To address these challenges, we introduce Embodied-RAG, a framework that
enhances the foundational model of an embodied agent with a non-parametric
memory system capable of autonomously constructing hierarchical knowledge for
both navigation and language generation. Embodied-RAG handles a full range of
spatial and semantic resolutions across diverse environments and query types,
whether for a specific object or a holistic description of ambiance. At its
core, Embodied-RAG's memory is structured as a semantic forest, storing
language descriptions at varying levels of detail. This hierarchical
organization allows the system to efficiently generate context-sensitive
outputs across different robotic platforms. We demonstrate that Embodied-RAG
effectively bridges RAG to the robotics domain, successfully handling over 200
explanation and navigation queries across 19 environments, highlighting its
promise for general-purpose non-parametric system for embodied agents.",Quanting Xie
2024-10-06T03:42:15Z,http://arxiv.org/abs/2410.04343v1,Inference Scaling for Long-Context Retrieval Augmented Generation,"The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring strategies beyond simply increasing the quantity of
knowledge. We focus on two inference scaling strategies: in-context learning
and iterative prompting. These strategies provide additional flexibility to
scale test-time computation (e.g., by increasing retrieved documents or
generation steps), thereby enhancing LLMs' ability to effectively acquire and
utilize contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",Zhenrui Yue
2024-10-14T15:04:18Z,http://arxiv.org/abs/2410.10594v1,"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
  Documents","Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 25--39\%
end-to-end performance gain over traditional text-based RAG pipeline. Further
analysis reveals that VisRAG is effective in utilizing training data and
demonstrates strong generalization capability, positioning it as a promising
solution for RAG on multi-modality documents. Our code and data are available
at https://github.com/openbmb/visrag .",Shi Yu
2024-11-01T01:40:23Z,http://arxiv.org/abs/2411.00300v1,"Rationale-Guided Retrieval Augmented Generation for Medical Question
  Answering","Large language models (LLM) hold significant potential for applications in
biomedicine, but they struggle with hallucinations and outdated knowledge.
While retrieval-augmented generation (RAG) is generally employed to address
these issues, it also has its own set of challenges: (1) LLMs are vulnerable to
irrelevant or incorrect context, (2) medical queries are often not
well-targeted for helpful information, and (3) retrievers are prone to bias
toward the specific source corpus they were trained on. In this study, we
present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the
reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key
innovations: a small filtering model trained on perplexity-based labels of
rationales, which selectively augments informative snippets of documents while
filtering out distractors; LLM-generated rationales as queries to improve the
utility of retrieved snippets; a structure designed to retrieve snippets evenly
from a comprehensive set of four biomedical corpora, effectively mitigating
retriever bias. Our experiments demonstrate that RAG$^2$ improves the
state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\%, and
it outperforms the previous best medical RAG model by up to 5.6\% across three
medical question-answering benchmarks. Our code is available at
https://github.com/dmis-lab/RAG2.",Jiwoong Sohn
2024-11-29T03:01:05Z,http://arxiv.org/abs/2411.19443v1,"Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language
  Models","Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.",Tian Yu
2024-12-13T21:28:17Z,http://arxiv.org/abs/2412.10571v2,"Evidence Contextualization and Counterfactual Attribution for
  Conversational QA over Heterogeneous Data with RAG Systems","Retrieval Augmented Generation (RAG) works as a backbone for interacting with
an enterprise's own data via Conversational Question Answering (ConvQA). In a
RAG system, a retriever fetches passages from a collection in response to a
question, which are then included in the prompt of a large language model (LLM)
for generating a natural language (NL) answer. However, several RAG systems
today suffer from two shortcomings: (i) retrieved passages usually contain
their raw text and lack appropriate document context, negatively impacting both
retrieval and answering quality; and (ii) attribution strategies that explain
answer generation usually rely only on similarity between the answer and the
retrieved passages, thereby only generating plausible but not causal
explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies
the above concerns by: (i) contextualizing evidence with source metadata and
surrounding text; and (ii) computing counterfactual attribution, a causal
explanation approach where the contribution of an evidence to an answer is
determined by the similarity of the original response to the answer obtained by
removing that evidence. To evaluate our proposals, we release a new benchmark
ConfQuestions, with 300 hand-created conversational questions, each in English
and German, coupled with ground truth URLs, completed questions, and answers
from 215 public Confluence pages, that are typical of enterprise wiki spaces
with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show
the viability of our ideas: contextualization improves RAG performance, and
counterfactual attribution is effective at explaining RAG answers.",Rishiraj Saha Roy
2024-12-17T15:38:42Z,http://arxiv.org/abs/2412.13018v1,"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in
  Financial Domain","As a typical and practical application of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) techniques have gained extensive
attention, particularly in vertical domains where LLMs may lack domain-specific
knowledge. In this paper, we introduce an omnidirectional and automatic RAG
benchmark, OmniEval, in the financial domain. Our benchmark is characterized by
its multi-dimensional evaluation framework, including (1) a matrix-based RAG
scenario evaluation system that categorizes queries into five task classes and
16 financial topics, leading to a structured assessment of diverse query
scenarios; (2) a multi-dimensional evaluation data generation approach, which
combines GPT-4-based automatic generation and human annotation, achieving an
87.47\% acceptance ratio in human evaluations on generated instances; (3) a
multi-stage evaluation system that evaluates both retrieval and generation
performance, result in a comprehensive evaluation on the RAG pipeline; and (4)
robust evaluation metrics derived from rule-based and LLM-based ones, enhancing
the reliability of assessments through manual annotations and supervised
fine-tuning of an LLM evaluator. Our experiments demonstrate the
comprehensiveness of OmniEval, which includes extensive test datasets and
highlights the performance variations of RAG systems across diverse topics and
tasks, revealing significant opportunities for RAG models to improve their
capabilities in vertical domains. We open source the code of our benchmark in
\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",Shuting Wang
2024-12-18T11:28:05Z,http://arxiv.org/abs/2412.13746v1,"RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented
  Generation for Preference Alignment","Despite the significant progress made by existing retrieval augmented
language models (RALMs) in providing trustworthy responses and grounding in
reliable sources, they often overlook effective alignment with human
preferences. In the alignment process, reward models (RMs) act as a crucial
proxy for human values to guide optimization. However, it remains unclear how
to evaluate and select a reliable RM for preference alignment in RALMs. To this
end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG
settings. First, we design four crucial and challenging RAG-specific scenarios
to assess RMs, including multi-hop reasoning, fine-grained citation,
appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG
subsets, six retrievers, and 24 RALMs to increase the diversity of data
sources. Finally, we adopt an LLM-as-a-judge approach to improve preference
annotation efficiency and effectiveness, exhibiting a strong correlation with
human annotations. Based on the RAG-RewardBench, we conduct a comprehensive
evaluation of 45 RMs and uncover their limitations in RAG scenarios.
Additionally, we also reveal that existing trained RALMs show almost no
improvement in preference alignment, highlighting the need for a shift towards
preference-aligned training.We release our benchmark and code publicly at
https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",Zhuoran Jin
2024-09-12T13:50:22Z,http://arxiv.org/abs/2409.08045v1,"Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks
  against RAG-based Inference in Scale and Severity Using Jailbreaking","In this paper, we show that with the ability to jailbreak a GenAI model,
attackers can escalate the outcome of attacks against RAG-based GenAI-powered
applications in severity and scale. In the first part of the paper, we show
that attackers can escalate RAG membership inference attacks and RAG entity
extraction attacks to RAG documents extraction attacks, forcing a more severe
outcome compared to existing attacks. We evaluate the results obtained from
three extraction methods, the influence of the type and the size of five
embeddings algorithms employed, the size of the provided context, and the GenAI
engine. We show that attackers can extract 80%-99.8% of the data stored in the
database used by the RAG of a Q&A chatbot. In the second part of the paper, we
show that attackers can escalate the scale of RAG data poisoning attacks from
compromising a single GenAI-powered application to compromising the entire
GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an
adversarial self-replicating prompt that triggers a chain reaction of a
computer worm within the ecosystem and forces each affected application to
perform a malicious activity and compromise the RAG of additional applications.
We evaluate the performance of the worm in creating a chain of confidential
data extraction about users within a GenAI ecosystem of GenAI-powered email
assistants and analyze how the performance of the worm is affected by the size
of the context, the adversarial self-replicating prompt used, the type and size
of the embeddings algorithm employed, and the number of hops in the
propagation. Finally, we review and analyze guardrails to protect RAG-based
inference and discuss the tradeoffs.",Stav Cohen
2024-09-20T21:06:00Z,http://arxiv.org/abs/2409.13902v1,"Enhancing Large Language Models with Domain-specific Retrieval Augment
  Generation: A Case Study on Long-form Consumer Health Question Answering in
  Ophthalmology","Despite the potential of Large Language Models (LLMs) in medicine, they may
generate responses lacking supporting evidence or based on hallucinated
evidence. While Retrieval Augment Generation (RAG) is popular to address this
issue, few studies implemented and evaluated RAG in downstream domain-specific
applications. We developed a RAG pipeline with 70,000 ophthalmology-specific
documents that retrieve relevant documents to augment LLMs during inference
time. In a case study on long-form consumer health questions, we systematically
evaluated the responses including over 500 references of LLMs with and without
RAG on 100 questions with 10 healthcare professionals. The evaluation focuses
on factuality of evidence, selection and ranking of evidence, attribution of
evidence, and answer accuracy and completeness. LLMs without RAG provided 252
references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor
errors, and 20.6% were correct. In contrast, LLMs with RAG significantly
improved accuracy (54.5% being correct) and reduced error rates (18.8% with
minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents
retrieved by RAG were selected as the top references in the LLM response, with
an average ranking of 4.9. The use of RAG also improved evidence attribution
(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight
decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47
to 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited
hallucinated and erroneous evidence in the responses, raising concerns for
downstream applications in the medical domain. RAG substantially reduced the
proportion of such evidence but encountered challenges.",Aidan Gilson
2024-09-23T21:42:47Z,http://arxiv.org/abs/2409.15566v1,GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation,"The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.",Brendan Hogan Rappazzo
2022-07-17T17:31:07Z,http://arxiv.org/abs/2207.08240v1,"Robust Action Governor for Uncertain Piecewise Affine Systems with
  Non-convex Constraints and Safe Reinforcement Learning","The action governor is an add-on scheme to a nominal control loop that
monitors and adjusts the control actions to enforce safety specifications
expressed as pointwise-in-time state and control constraints. In this paper, we
introduce the Robust Action Governor (RAG) for systems the dynamics of which
can be represented using discrete-time Piecewise Affine (PWA) models with both
parametric and additive uncertainties and subject to non-convex constraints. We
develop the theoretical properties and computational approaches for the RAG.
After that, we introduce the use of the RAG for realizing safe Reinforcement
Learning (RL), i.e., ensuring all-time constraint satisfaction during online RL
exploration-and-exploitation process. This development enables safe real-time
evolution of the control policy and adaptation to changes in the operating
environment and system parameters (due to aging, damage, etc.). We illustrate
the effectiveness of the RAG in constraint enforcement and safe RL using the
RAG by considering their applications to a soft-landing problem of a
mass-spring-damper system.",Yutong Li
2024-01-03T02:32:55Z,http://arxiv.org/abs/2401.01511v1,"Enhancing Multilingual Information Retrieval in Mixed Human Resources
  Environments: A RAG Model Implementation for Multicultural Enterprise","The advent of Large Language Models has revolutionized information retrieval,
ushering in a new era of expansive knowledge accessibility. While these models
excel in providing open-world knowledge, effectively extracting answers in
diverse linguistic environments with varying levels of literacy remains a
formidable challenge. Retrieval Augmented Generation (RAG) emerges as a
promising solution, bridging the gap between information availability and
multilingual comprehension. However, deploying RAG models in real-world
scenarios demands careful consideration of various factors. This paper
addresses the critical challenges associated with implementing RAG models in
multicultural environments. We delve into essential considerations, including
data feeding strategies, timely updates, mitigation of hallucinations,
prevention of erroneous responses, and optimization of delivery speed. Our work
involves the integration of a diverse array of tools, meticulously combined to
facilitate the seamless adoption of RAG models across languages and literacy
levels within a multicultural organizational context. Through strategic tweaks
in our approaches, we achieve not only effectiveness but also efficiency,
ensuring the accelerated and accurate delivery of information in a manner that
is tailored to the unique requirements of multilingual and multicultural
settings.",Syed Rameel Ahmad
2024-01-31T22:06:07Z,http://arxiv.org/abs/2402.03367v2,RAG-Fusion: a New Take on Retrieval-Augmented Generation,"Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.",Zackary Rackauckas
2024-02-05T22:35:42Z,http://arxiv.org/abs/2402.05131v3,Financial Report Chunking for Effective Retrieval Augmented Generation,"Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question & Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.",Antonio Jimeno Yepes
2024-02-26T12:56:17Z,http://arxiv.org/abs/2403.00820v1,"Retrieval Augmented Generation Systems: Automatic Dataset Creation,
  Evaluation and Boolean Agent Setup","Retrieval Augmented Generation (RAG) systems have seen huge popularity in
augmenting Large-Language Model (LLM) outputs with domain specific and time
sensitive data. Very recently a shift is happening from simple RAG setups that
query a vector database for additional information with every user input to
more sophisticated forms of RAG. However, different concrete approaches compete
on mostly anecdotal evidence at the moment. In this paper we present a rigorous
dataset creation and evaluation workflow to quantitatively compare different
RAG strategies. We use a dataset created this way for the development and
evaluation of a boolean agent RAG setup: A system in which a LLM can decide
whether to query a vector database or not, thus saving tokens on questions that
can be answered with internal knowledge. We publish our code and generated
dataset online.",Tristan Kenneweg
2024-03-02T12:19:04Z,http://arxiv.org/abs/2403.01193v3,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,"Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.",Philip Feldman
2024-03-27T04:20:18Z,http://arxiv.org/abs/2403.18243v1,"Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check","Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.",Linhao Ye
2024-04-13T09:33:00Z,http://arxiv.org/abs/2404.08940v1,Introducing Super RAGs in Mistral 8x7B-v1,"The relentless pursuit of enhancing Large Language Models (LLMs) has led to
the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel
approach designed to elevate the performance of LLMs by integrating external
knowledge sources with minimal structural modifications. This paper presents
the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,
and examines the resultant improvements in accuracy, speed, and user
satisfaction. Our methodology uses a fine-tuned instruct model setup and a
cache tuning fork system, ensuring efficient and relevant data retrieval. The
evaluation, conducted over several epochs, demonstrates significant
enhancements across all metrics. The findings suggest that Super RAGs can
effectively augment LLMs, paving the way for more sophisticated and reliable AI
systems. This research contributes to the field by providing empirical evidence
of the benefits of Super RAGs and offering insights into their potential
applications.",Ayush Thakur
2024-04-26T11:51:53Z,http://arxiv.org/abs/2404.17347v1,InspectorRAGet: An Introspection Platform for RAG Evaluation,"Large Language Models (LLM) have become a popular approach for implementing
Retrieval Augmented Generation (RAG) systems, and a significant amount of
effort has been spent on building good models and metrics. In spite of
increased recognition of the need for rigorous evaluation of RAG systems, few
tools exist that go beyond the creation of model output and automatic
calculation. We present InspectorRAGet, an introspection platform for RAG
evaluation. InspectorRAGet allows the user to analyze aggregate and
instance-level performance of RAG systems, using both human and algorithmic
metrics as well as annotator quality. InspectorRAGet is suitable for multiple
use cases and is available publicly to the community. The demo video is
available at https://youtu.be/MJhe8QIXcEc",Kshitij Fadnis
2024-04-30T19:51:37Z,http://arxiv.org/abs/2405.00175v1,"Towards a Search Engine for Machines: Unified Ranking for Multiple
  Retrieval-Augmented Large Language Models","This paper introduces uRAG--a framework with a unified retrieval engine that
serves multiple downstream retrieval-augmented generation (RAG) systems. Each
RAG system consumes the retrieval results for a unique purpose, such as
open-domain question answering, fact verification, entity linking, and relation
extraction. We introduce a generic training guideline that standardizes the
communication between the search engine and the downstream RAG systems that
engage in optimizing the retrieval model. This lays the groundwork for us to
build a large-scale experimentation ecosystem consisting of 18 RAG systems that
engage in training and 18 unknown RAG systems that use the uRAG as the new
users of the search engine. Using this experimentation ecosystem, we answer a
number of fundamental research questions that improve our understanding of
promises and challenges in developing search engines for machines.",Alireza Salemi
2024-06-09T17:55:55Z,http://arxiv.org/abs/2406.05870v2,"Machine Against the RAG: Jamming Retrieval-Augmented Generation with
  Blocker Documents","Retrieval-augmented generation (RAG) systems respond to queries by retrieving
relevant documents from a knowledge database, then generating an answer by
applying an LLM to the retrieved documents. We demonstrate that RAG systems
that operate on databases with untrusted content are vulnerable to a new class
of denial-of-service attacks we call jamming. An adversary can add a single
``blocker'' document to the database that will be retrieved in response to a
specific query and result in the RAG system not answering this query -
ostensibly because it lacks the information or because the answer is unsafe.
  We describe and measure the efficacy of several methods for generating
blocker documents, including a new method based on black-box optimization. This
method (1) does not rely on instruction injection, (2) does not require the
adversary to know the embedding or LLM used by the target RAG system, and (3)
does not use an auxiliary LLM to generate blocker documents.
  We evaluate jamming attacks on several LLMs and embeddings and demonstrate
that the existing safety metrics for LLMs do not capture their vulnerability to
jamming. We then discuss defenses against blocker documents.",Avital Shafran
2024-06-20T22:53:09Z,http://arxiv.org/abs/2406.14773v1,"Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)
  via Pure Synthetic Data","Retrieval-augmented generation (RAG) enhances the outputs of language models
by integrating relevant information retrieved from external knowledge sources.
However, when the retrieval process involves private data, RAG systems may face
severe privacy risks, potentially leading to the leakage of sensitive
information. To address this issue, we propose using synthetic data as a
privacy-preserving alternative for the retrieval data. We propose SAGE, a novel
two-stage synthetic data generation paradigm. In the stage-1, we employ an
attribute-based extraction and generation approach to preserve key contextual
information from the original data. In the stage-2, we further enhance the
privacy properties of the synthetic data through an agent-based iterative
refinement process. Extensive experiments demonstrate that using our synthetic
data as the retrieval context achieves comparable performance to using the
original data while substantially reducing privacy risks. Our work takes the
first step towards investigating the possibility of generating high-utility and
privacy-preserving synthetic data for RAG, opening up new opportunities for the
safe application of RAG systems in various domains.",Shenglai Zeng
2024-07-22T03:44:27Z,http://arxiv.org/abs/2407.15353v2,"Customized Retrieval Augmented Generation and Benchmarking for EDA Tool
  Documentation QA","Retrieval augmented generation (RAG) enhances the accuracy and reliability of
generative AI models by sourcing factual information from external databases,
which is extensively employed in document-grounded question-answering (QA)
tasks. Off-the-shelf RAG flows are well pretrained on general-purpose
documents, yet they encounter significant challenges when being applied to
knowledge-intensive vertical domains, such as electronic design automation
(EDA). This paper addresses such issue by proposing a customized RAG framework
along with three domain-specific techniques for EDA tool documentation QA,
including a contrastive learning scheme for text embedding model fine-tuning, a
reranker distilled from proprietary LLM, and a generative LLM fine-tuned with
high-quality domain corpus. Furthermore, we have developed and released a
documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced
RTL-to-GDSII design platform. Experimental results demonstrate that our
proposed RAG flow and techniques have achieved superior performance on ORD-QA
as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA
benchmark and the training dataset for our customized RAG flow are open-source
at https://github.com/lesliepy99/RAG-EDA.",Yuan Pu
2024-07-31T03:00:59Z,http://arxiv.org/abs/2407.21300v3,Implementing Streaming algorithm and k-means clusters to RAG,"Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.",Haoyu Kang
2024-08-02T13:35:11Z,http://arxiv.org/abs/2408.01262v4,RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,"Retrieval-Augmented Generation (RAG) is a powerful approach that enables
large language models (LLMs) to incorporate external knowledge. However,
evaluating the effectiveness of RAG systems in specialized scenarios remains
challenging due to the high costs of data construction and the lack of suitable
evaluation metrics. This paper introduces RAGEval, a framework designed to
assess RAG systems across diverse scenarios by generating high-quality
documents, questions, answers, and references through a schema-based pipeline.
With a focus on factual accuracy, we propose three novel metrics Completeness,
Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses.
Experimental results show that RAGEval outperforms zero-shot and one-shot
methods in terms of clarity, safety, conformity, and richness of generated
samples. Furthermore, the use of LLMs for scoring the proposed metrics
demonstrates a high level of consistency with human evaluations. RAGEval
establishes a new paradigm for evaluating RAG systems in real-world
applications.",Kunlun Zhu
2024-08-16T05:15:12Z,http://arxiv.org/abs/2408.08535v1,"CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for
  Advanced Retrieval-Augmented Generation in Fact-Checking","Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.",Rong-Ching Chang
2024-08-24T09:23:01Z,http://arxiv.org/abs/2408.13533v1,"Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the
  Role of RAG Noise in Large Language Models","Retrieval-Augmented Generation (RAG) has emerged as a crucial method for
addressing hallucinations in large language models (LLMs). While recent
research has extended RAG models to complex noisy scenarios, these explorations
often confine themselves to limited noise types and presuppose that noise is
inherently detrimental to LLMs, potentially deviating from real-world retrieval
environments and restricting practical applicability. In this paper, we define
seven distinct noise types from a linguistic perspective and establish a Noise
RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing
multiple datasets and reasoning tasks. Through empirical evaluation of eight
representative LLMs with diverse architectures and scales, we reveal that these
noises can be further categorized into two practical groups: noise that is
beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs
(aka harmful noise). While harmful noise generally impairs performance,
beneficial noise may enhance several aspects of model capabilities and overall
performance. Our analysis offers insights for developing more robust, adaptable
RAG solutions and mitigating hallucinations across diverse retrieval scenarios.",Jinyang Wu
2024-08-28T04:44:43Z,http://arxiv.org/abs/2408.15533v2,"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via
  Layer-wise Relevance Propagation","Retrieval-Augmented Generation (RAG) has become a primary technique for
mitigating hallucinations in large language models (LLMs). However, incomplete
knowledge extraction and insufficient understanding can still mislead LLMs to
produce irrelevant or even contradictory responses, which means hallucinations
persist in RAG. In this paper, we propose LRP4RAG, a method based on the
Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations
in RAG. Specifically, we first utilize LRP to compute the relevance between the
input and output of the RAG generator. We then apply further extraction and
resampling to the relevance matrix. The processed relevance data are input into
multiple classifiers to determine whether the output contains hallucinations.
To the best of our knowledge, this is the first time that LRP has been used for
detecting RAG hallucinations, and extensive experiments demonstrate that
LRP4RAG outperforms existing baselines.",Haichuan Hu
2024-08-29T16:11:20Z,http://arxiv.org/abs/2409.09046v1,"HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications","While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.",Rishi Kalra
2024-10-08T08:34:54Z,http://arxiv.org/abs/2410.05801v1,"Retrieving, Rethinking and Revising: The Chain-of-Verification Can
  Improve Retrieval Augmented Generation","Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language
Models (LLMs) by incorporating extensive knowledge retrieved from external
sources. However, such approach encounters some challenges: Firstly, the
original queries may not be suitable for precise retrieval, resulting in
erroneous contextual knowledge; Secondly, the language model can easily
generate inconsistent answer with external references due to their knowledge
boundary limitation. To address these issues, we propose the
chain-of-verification (CoV-RAG) to enhance the external retrieval correctness
and internal generation consistency. Specifically, we integrate the
verification module into the RAG, engaging in scoring, judgment, and rewriting.
To correct external retrieval errors, CoV-RAG retrieves new knowledge using a
revised query. To correct internal generation errors, we unify QA and
verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our
comprehensive experiments across various LLMs demonstrate the effectiveness and
adaptability compared with other strong baselines. Especially, our CoV-RAG can
significantly surpass the state-of-the-art baselines using different LLM
backbones.",Bolei He
2024-10-16T01:57:56Z,http://arxiv.org/abs/2410.12886v1,"AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic
  Filtering and Iterative Reasoning","Recent advancements in QA with LLM, like GPT-4, have shown limitations in
handling complex multi-hop queries. We propose AT-RAG, a novel multistep RAG
incorporating topic modeling for efficient document retrieval and reasoning.
Using BERTopic, our model dynamically assigns topics to queries, improving
retrieval accuracy and efficiency. We evaluated AT-RAG on multihop benchmark
datasets QA and a medical case study QA. Results show significant improvements
in correctness, completeness, and relevance compared to existing methods.
AT-RAG reduces retrieval time while maintaining high precision, making it
suitable for general tasks QA and complex domain-specific challenges such as
medical QA. The integration of topic filtering and iterative reasoning enables
our model to handle intricate queries efficiently, which makes it suitable for
applications that require nuanced information retrieval and decision-making.",Mohammad Reza Rezaei
2024-10-20T22:59:34Z,http://arxiv.org/abs/2410.15531v1,"Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses
  with Sub-Question Coverage","Evaluating retrieval-augmented generation (RAG) systems remains challenging,
particularly for open-ended questions that lack definitive answers and require
coverage of multiple sub-topics. In this paper, we introduce a novel evaluation
framework based on sub-question coverage, which measures how well a RAG system
addresses different facets of a question. We propose decomposing questions into
sub-questions and classifying them into three types -- core, background, and
follow-up -- to reflect their roles and importance. Using this categorization,
we introduce a fine-grained evaluation protocol that provides insights into the
retrieval and generation characteristics of RAG systems, including three
commercial generative answer engines: You.com, Perplexity AI, and Bing Chat.
Interestingly, we find that while all answer engines cover core sub-questions
more often than background or follow-up ones, they still miss around 50% of
core sub-questions, revealing clear opportunities for improvement. Further,
sub-question coverage metrics prove effective for ranking responses, achieving
82% accuracy compared to human preference annotations. Lastly, we also
demonstrate that leveraging core sub-questions enhances both retrieval and
answer generation in a RAG system, resulting in a 74% win rate over the
baseline that lacks sub-questions.",Kaige Xie
2024-10-25T14:07:53Z,http://arxiv.org/abs/2410.19572v4,ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems,"Retrieval-Augmented Generation (RAG) systems using large language models
(LLMs) often generate inaccurate responses due to the retrieval of irrelevant
or loosely related information. Existing methods, which operate at the document
level, fail to effectively filter out such content. We propose LLM-driven chunk
filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and
filtering retrieved information at the chunk level. Our approach employs
semantic chunking to divide documents into coherent sections and utilizes
LLM-based relevance scoring to assess each chunk's alignment with the user's
query. By filtering out less pertinent chunks before the generation phase, we
significantly reduce hallucinations and improve factual accuracy. Experiments
show that our method outperforms existing RAG models, achieving higher accuracy
on tasks requiring precise information retrieval. This advancement enhances the
reliability of RAG systems, making them particularly beneficial for
applications like fact-checking and multi-hop reasoning.",Ishneet Sukhvinder Singh
2024-10-30T09:15:51Z,http://arxiv.org/abs/2410.22832v1,"HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language
  Models","Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge, making them adaptable and
cost-effective for various applications. However, the growing reliance on these
systems also introduces potential security risks. In this work, we reveal a
novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which
enables attackers to manipulate the retrieval mechanisms of RAG systems by
injecting malicious texts into the knowledge database. When the RAG system
encounters target questions, it generates the attacker's pre-determined answers
instead of the correct ones, undermining the integrity and trustworthiness of
the system. We formalize HijackRAG as an optimization problem and propose both
black-box and white-box attack strategies tailored to different levels of the
attacker's knowledge. Extensive experiments on multiple benchmark datasets show
that HijackRAG consistently achieves high attack success rates, outperforming
existing baseline attacks. Furthermore, we demonstrate that the attack is
transferable across different retriever models, underscoring the widespread
risk it poses to RAG systems. Lastly, our exploration of various defense
mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing
the urgent need for more robust security measures to protect RAG systems in
real-world deployments.",Yucheng Zhang
2024-10-30T12:09:29Z,http://arxiv.org/abs/2410.22954v1,Retrieval-Augmented Generation with Estimation of Source Reliability,"Retrieval-augmented generation (RAG) addresses key limitations of large
language models (LLMs), such as hallucinations and outdated knowledge, by
incorporating external databases. These databases typically consult multiple
sources to encompass up-to-date and various information. However, standard RAG
methods often overlook the heterogeneous source reliability in the multi-source
database and retrieve documents solely based on relevance, making them prone to
propagating misinformation. To address this, we propose Reliability-Aware RAG
(RA-RAG) which estimates the reliability of multiple sources and incorporates
this information into both retrieval and aggregation processes. Specifically,
it iteratively estimates source reliability and true answers for a set of
queries with no labelling. Then, it selectively retrieves relevant documents
from a few of reliable sources and aggregates them using weighted majority
voting, where the selective retrieval ensures scalability while not
compromising the performance. We also introduce a benchmark designed to reflect
real-world scenarios with heterogeneous source reliability and demonstrate the
effectiveness of RA-RAG compared to a set of baselines.",Jeongyeon Hwang
2024-10-30T13:29:36Z,http://arxiv.org/abs/2410.23000v2,"Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented
  Generation with Key Point Recall","Retrieval-augmented generation (RAG) is a promising approach to address the
limitations of fixed knowledge in large language models (LLMs). However,
current benchmarks for evaluating RAG systems suffer from two key deficiencies:
(1) they fail to adequately measure LLMs' capability in handling long-context
retrieval due to a lack of datasets that reflect the characteristics of
retrieved documents, and (2) they lack a comprehensive evaluation method for
assessing LLMs' ability to generate long-form responses that effectively
exploits retrieved information. To address these shortcomings, we introduce the
Long$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG
comprises 280 questions spanning 10 domains and across 8 question categories,
each associated with 5 retrieved documents with an average length of 2,444
words. KPR evaluates the extent to which LLMs incorporate key points extracted
from the retrieved documents into their generated responses, providing a more
nuanced assessment of their ability to exploit retrieved information.",Zehan Qi
2024-10-30T15:06:32Z,http://arxiv.org/abs/2410.23090v1,"CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation
  Generation","Retrieval-Augmented Generation (RAG) has become a powerful paradigm for
enhancing large language models (LLMs) through external knowledge retrieval.
Despite its widespread attention, existing academic research predominantly
focuses on single-turn RAG, leaving a significant gap in addressing the
complexities of multi-turn conversations found in real-world applications. To
bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess
RAG systems in realistic multi-turn conversational settings. CORAL includes
diverse information-seeking conversations automatically derived from Wikipedia
and tackles key challenges such as open-domain coverage, knowledge intensity,
free-form responses, and topic shifts. It supports three core tasks of
conversational RAG: passage retrieval, response generation, and citation
labeling. We propose a unified framework to standardize various conversational
RAG methods and conduct a comprehensive evaluation of these methods on CORAL,
demonstrating substantial opportunities for improving existing approaches.",Yiruo Cheng
2024-11-01T01:11:58Z,http://arxiv.org/abs/2411.00294v2,"LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models","Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.",Kazi Ahmed Asif Fuad
2024-11-13T08:43:37Z,http://arxiv.org/abs/2411.08438v1,"Towards Optimizing a Retrieval Augmented Generation using Large Language
  Model on Academic Data","Given the growing trend of many organizations integrating Retrieval Augmented
Generation (RAG) into their operations, we assess RAG on domain-specific data
and test state-of-the-art models across various optimization techniques. We
incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble
Retriever, and In-Context-Learning, to enhance the functionality and
performance in the academic domain. We focus on data retrieval, specifically
targeting various study programs at a large technical university. We
additionally introduce a novel evaluation approach, the RAG Confusion Matrix
designed to assess the effectiveness of various configurations within the RAG
framework. By exploring the integration of both open-source (e.g., Llama2,
Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer
valuable insights into the application and optimization of RAG frameworks in
domain-specific contexts. Our experiments show a significant performance
increase when including multi-query in the retrieval phase.",Anum Afzal
2024-11-20T20:10:43Z,http://arxiv.org/abs/2411.13691v1,"Retrieval-Augmented Generation for Domain-Specific Question Answering: A
  Case Study on Pittsburgh and CMU","We designed a Retrieval-Augmented Generation (RAG) system to provide large
language models with relevant documents for answering domain-specific questions
about Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800
subpages using a greedy scraping strategy and employed a hybrid annotation
process, combining manual and Mistral-generated question-answer pairs,
achieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework
integrates BM25 and FAISS retrievers, enhanced with a reranker for improved
document retrieval accuracy. Experimental results show that the RAG system
significantly outperforms a non-RAG baseline, particularly in time-sensitive
and complex queries, with an F1 score improvement from 5.45% to 42.21% and
recall of 56.18%. This study demonstrates the potential of RAG systems in
enhancing answer precision and relevance, while identifying areas for further
optimization in document retrieval and model training.",Haojia Sun
2024-12-03T16:52:06Z,http://arxiv.org/abs/2412.02563v1,Semantic Tokens in Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) architectures have recently garnered
significant attention for their ability to improve truth grounding and
coherence in natural language processing tasks. However, the reliability of RAG
systems in producing accurate answers diminishes as the volume of data they
access increases. Even with smaller datasets, these systems occasionally fail
to address simple queries. This issue arises from their dependence on
state-of-the-art large language models (LLMs), which can introduce uncertainty
into the system's outputs. In this work, I propose a novel Comparative RAG
system that introduces an evaluator module to bridge the gap between
probabilistic RAG systems and deterministically verifiable responses. The
evaluator compares external recommendations with the retrieved document chunks,
adding a decision-making layer that enhances the system's reliability. This
approach ensures that the chunks retrieved are both semantically relevant and
logically consistent with deterministic insights, thereby improving the
accuracy and overall efficiency of RAG systems. This framework paves the way
for more reliable and scalable question-answering applications in domains
requiring high precision and verifiability.",Joel Suro
2024-12-06T01:20:16Z,http://arxiv.org/abs/2412.04697v1,"Privacy-Preserving Retrieval Augmented Generation with Differential
  Privacy","With the recent remarkable advancement of large language models (LLMs), there
has been a growing interest in utilizing them in the domains with highly
sensitive data that lies outside their training data. For this purpose,
retrieval augmented generation (RAG) is particularly effective -- it assists
LLMs by directly providing relevant information from the external knowledge
sources. However, without extra privacy safeguards, RAG outputs risk leaking
sensitive information from the external data source. In this work, we explore
RAG under differential privacy (DP), a formal guarantee of data privacy. The
main challenge with differentially private RAG is how to generate long accurate
answers within a moderate privacy budget. We address this by proposing an
algorithm that smartly spends privacy budget only for the tokens that require
the sensitive information and uses the non-private LLM for other tokens. Our
extensive empirical evaluations reveal that our algorithm outperforms the
non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$
across different models and datasets.",Tatsuki Koga
2024-12-16T19:40:26Z,http://arxiv.org/abs/2412.12322v1,"RAG Playground: A Framework for Systematic Evaluation of Retrieval
  Strategies and Prompt Engineering in RAG Systems","We present RAG Playground, an open-source framework for systematic evaluation
of Retrieval-Augmented Generation (RAG) systems. The framework implements and
compares three retrieval approaches: naive vector search, reranking, and hybrid
vector-keyword search, combined with ReAct agents using different prompting
strategies. We introduce a comprehensive evaluation framework with novel
metrics and provide empirical results comparing different language models
(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our
experiments demonstrate significant performance improvements through hybrid
search methods and structured self-evaluation prompting, achieving up to 72.7%
pass rate on our multi-metric evaluation framework. The results also highlight
the importance of prompt engineering in RAG systems, with our custom-prompted
agents showing consistent improvements in retrieval accuracy and response
quality.",Ioannis Papadimitriou
2024-12-17T13:05:36Z,http://arxiv.org/abs/2412.12881v1,"RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented
  Verification and Refinement","Existing large language models (LLMs) show exceptional problem-solving
capabilities but might struggle with complex reasoning tasks. Despite the
successes of chain-of-thought and tree-based search methods, they mainly depend
on the internal knowledge of LLMs to search over intermediate reasoning steps,
limited to dealing with simple tasks involving fewer reasoning steps. In this
paper, we propose \textbf{RAG-Star}, a novel RAG approach that integrates the
retrieved information to guide the tree-based deliberative reasoning process
that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree
Search, RAG-Star iteratively plans intermediate sub-queries and answers for
reasoning based on the LLM itself. To consolidate internal and external
knowledge, we propose an retrieval-augmented verification that utilizes query-
and answer-aware reward modeling to provide feedback for the inherent reasoning
of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate
that RAG-Star significantly outperforms previous RAG and reasoning methods.",Jinhao Jiang
2024-12-18T11:00:58Z,http://arxiv.org/abs/2412.13720v1,"Federated Learning and RAG Integration: A Scalable Approach for Medical
  Large Language Models","This study analyzes the performance of domain-specific Large Language Models
(LLMs) for the medical field by integrating Retrieval-Augmented Generation
(RAG) systems within a federated learning framework. Leveraging the inherent
advantages of federated learning, such as preserving data privacy and enabling
distributed computation, this research explores the integration of RAG systems
with models trained under varying client configurations to optimize
performance. Experimental results demonstrate that the federated learning-based
models integrated with RAG systems consistently outperform their non-integrated
counterparts across all evaluation metrics. This study highlights the potential
of combining federated learning and RAG systems for developing domain-specific
LLMs in the medical field, providing a scalable and privacy-preserving solution
for enhancing text generation capabilities.",Jincheol Jung
2024-05-12T09:48:28Z,http://arxiv.org/abs/2405.13002v1,DuetRAG: Collaborative Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) methods augment the input of Large
Language Models (LLMs) with relevant retrieved passages, reducing factual
errors in knowledge-intensive tasks. However, contemporary RAG approaches
suffer from irrelevant knowledge retrieval issues in complex domain questions
(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to
low-quality generations. To address this issue, we propose a novel
Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our
bootstrapping philosophy is to simultaneously integrate the domain fintuning
and RAG models to improve the knowledge retrieval quality, thereby enhancing
generation quality. Finally, we demonstrate DuetRAG' s matches with expert
human researchers on HotPot QA.",Dian Jiao
2024-07-09T09:46:23Z,http://arxiv.org/abs/2407.06718v1,"A Simple Architecture for Enterprise Large Language Model Applications
  based on Role based security and Clearance Levels using Retrieval-Augmented
  Generation or Mixture of Experts","This study proposes a simple architecture for Enterprise application for
Large Language Models (LLMs) for role based security and NATO clearance levels.
Our proposal aims to address the limitations of current LLMs in handling
security and information access. The proposed architecture could be used while
utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of
experts models (MoE). It could be used only with RAG, or only with MoE or with
both of them. Using roles and security clearance level of the user, documents
in RAG and experts in MoE are filtered. This way information leakage is
prevented.",Atilla Ã–zgÃ¼r
2023-09-04T08:28:44Z,http://arxiv.org/abs/2309.01431v2,Benchmarking Large Language Models in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) is a promising approach for mitigating
the hallucination of large language models (LLMs). However, existing research
lacks rigorous evaluation of the impact of retrieval-augmented generation on
different large language models, which make it challenging to identify the
potential bottlenecks in the capabilities of RAG for different LLMs. In this
paper, we systematically investigate the impact of Retrieval-Augmented
Generation on large language models. We analyze the performance of different
large language models in 4 fundamental abilities required for RAG, including
noise robustness, negative rejection, information integration, and
counterfactual robustness. To this end, we establish Retrieval-Augmented
Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and
Chinese. RGB divides the instances within the benchmark into 4 separate
testbeds based on the aforementioned fundamental abilities required to resolve
the case. Then we evaluate 6 representative LLMs on RGB to diagnose the
challenges of current LLMs when applying RAG. Evaluation reveals that while
LLMs exhibit a certain degree of noise robustness, they still struggle
significantly in terms of negative rejection, information integration, and
dealing with false information. The aforementioned assessment outcomes indicate
that there is still a considerable journey ahead to effectively apply RAG to
LLMs.",Jiawei Chen
2024-01-15T18:25:18Z,http://arxiv.org/abs/2401.07883v1,"The Chronicles of RAG: The Retriever, the Chunk and the Generator","Retrieval Augmented Generation (RAG) has become one of the most popular
paradigms for enabling LLMs to access external data, and also as a mechanism
for grounding to mitigate against hallucinations. When implementing RAG you can
face several challenges like effective integration of retrieval models,
efficient representation learning, data diversity, computational efficiency
optimization, evaluation, and quality of text generation. Given all these
challenges, every day a new technique to improve RAG appears, making it
unfeasible to experiment with all combinations for your problem. In this
context, this paper presents good practices to implement, optimize, and
evaluate RAG for the Brazilian Portuguese language, focusing on the
establishment of a simple pipeline for inference and experiments. We explored a
diverse set of methods to answer questions about the first Harry Potter book.
To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,
gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the
retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to
the baseline. When optimizing the input size in the application, we observed
that it is possible to further enhance it by 2.4%. Finally, we present the
complete architecture of the RAG with our recommendations. As result, we moved
from a baseline of 57.88% to a maximum relative score of 98.61%.",Paulo Finardi
2024-01-26T14:14:59Z,http://arxiv.org/abs/2401.14887v4,The Power of Noise: Redefining Retrieval for RAG Systems,"Retrieval-Augmented Generation (RAG) has recently emerged as a method to
extend beyond the pre-trained knowledge of Large Language Models by augmenting
the original prompt with relevant passages or documents retrieved by an
Information Retrieval (IR) system. RAG has become increasingly important for
Generative AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized in the LLM. We
argue here that the retrieval component of RAG systems, be it dense or sparse,
deserves increased attention from the research community, and accordingly, we
conduct the first comprehensive and systematic examination of the retrieval
strategy of RAG systems. We focus, in particular, on the type of passages IR
systems within a RAG solution should retrieve. Our analysis considers multiple
factors, such as the relevance of the passages included in the prompt context,
their position, and their number. One counter-intuitive finding of this work is
that the retriever's highest-scoring documents that are not directly relevant
to the query (e.g., do not contain the answer) negatively impact the
effectiveness of the LLM. Even more surprising, we discovered that adding
random documents in the prompt improves the LLM accuracy by up to 35%. These
results highlight the need to investigate the appropriate strategies when
integrating retrieval with LLMs, thereby laying the groundwork for future
research in this area.",Florin Cuconasu
2024-02-20T17:44:06Z,http://arxiv.org/abs/2402.13178v2,Benchmarking Retrieval-Augmented Generation for Medicine,"While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the ""lost-in-the-middle"" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.",Guangzhi Xiong
2024-05-06T00:18:43Z,http://arxiv.org/abs/2405.03085v1,"Compressing Long Context for Enhancing RAG with AMR-based Concept
  Distillation","Large Language Models (LLMs) have made significant strides in information
acquisition. However, their overreliance on potentially flawed parametric
knowledge leads to hallucinations and inaccuracies, particularly when handling
long-tail, domain-specific queries. Retrieval Augmented Generation (RAG)
addresses this limitation by incorporating external, non-parametric knowledge.
Nevertheless, the retrieved long-context documents often contain noisy,
irrelevant information alongside vital knowledge, negatively diluting LLMs'
attention. Inspired by the supportive role of essential concepts in
individuals' reading comprehension, we propose a novel concept-based RAG
framework with the Abstract Meaning Representation (AMR)-based concept
distillation algorithm. The proposed algorithm compresses the cluttered raw
retrieved documents into a compact set of crucial concepts distilled from the
informative nodes of AMR by referring to reliable linguistic features. The
concepts explicitly constrain LLMs to focus solely on vital information in the
inference process. We conduct extensive experiments on open-domain
question-answering datasets to empirically evaluate the proposed method's
effectiveness. The results indicate that the concept-based RAG framework
outperforms other baseline methods, particularly as the number of supporting
documents increases, while also exhibiting robustness across various backbone
LLMs. This emphasizes the distilled concepts are informative for augmenting the
RAG process by filtering out interference information. To the best of our
knowledge, this is the first work introducing AMR to enhance the RAG,
presenting a potential solution to augment inference performance with
semantic-based context compression.",Kaize Shi
2024-05-25T11:10:04Z,http://arxiv.org/abs/2405.16178v1,"Accelerating Inference of Retrieval-Augmented Generation via Sparse
  Context Selection","Large language models (LLMs) augmented with retrieval exhibit robust
performance and extensive versatility by incorporating external contexts.
However, the input length grows linearly in the number of retrieved documents,
causing a dramatic increase in latency. In this paper, we propose a novel
paradigm named Sparse RAG, which seeks to cut computation costs through
sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel,
which eliminates latency introduced by long-range attention of retrieved
documents. Then, LLMs selectively decode the output by only attending to highly
relevant caches auto-regressively, which are chosen via prompting LLMs with
special control tokens. It is notable that Sparse RAG combines the assessment
of each individual document and the generation of the response into a single
process. The designed sparse mechanism in a RAG system can facilitate the
reduction of the number of documents loaded during decoding for accelerating
the inference of the RAG system. Additionally, filtering out undesirable
contexts enhances the model's focus on relevant context, inherently improving
its generation quality. Evaluation results of two datasets show that Sparse RAG
can strike an optimal balance between generation quality and computational
efficiency, demonstrating its generalizability across both short- and long-form
generation tasks.",Yun Zhu
2024-05-27T08:26:45Z,http://arxiv.org/abs/2405.16933v1,"Empowering Large Language Models to Set up a Knowledge Retrieval Indexer
  via Self-Learning","Retrieval-Augmented Generation (RAG) offers a cost-effective approach to
injecting real-time knowledge into large language models (LLMs). Nevertheless,
constructing and validating high-quality knowledge repositories require
considerable effort. We propose a pre-retrieval framework named Pseudo-Graph
Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students
by providing them with abundant raw reading materials and encouraging them to
engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through
common topics or complementary facts to form a pseudo-graph database. During
the retrieval phase, PG-RAG mimics the human behavior in flipping through
notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates
highly corroborated fact paths to provide a structured and refined sub-graph
assisting LLMs. We validated PG-RAG on three specialized question-answering
datasets. In single-document tasks, PG-RAG significantly outperformed the
current best baseline, KGP-LLaMA, across all key evaluation metrics, with an
average overall performance improvement of 11.6%. Specifically, its BLEU score
increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In
multi-document scenarios, the average metrics of PG-RAG were at least 2.35%
higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed
stable improvements of around 7.55% and 12.75%, respectively. Our code:
https://github.com/IAAR-Shanghai/PGRAG.",Xun Liang
2024-05-30T19:46:36Z,http://arxiv.org/abs/2405.20446v2,"Is My Data in Your Retrieval Database? Membership Inference Attacks
  Against Retrieval Augmented Generation","Retrieval Augmented Generation (RAG) systems have shown great promise in
natural language processing. However, their reliance on data stored in a
retrieval database, which may contain proprietary or sensitive information,
introduces new privacy concerns. Specifically, an attacker may be able to infer
whether a certain text passage appears in the retrieval database by observing
the outputs of the RAG system, an attack known as a Membership Inference Attack
(MIA). Despite the significance of this threat, MIAs against RAG systems have
yet remained under-explored. This study addresses this gap by introducing an
efficient and easy-to-use method for conducting MIA against RAG systems. We
demonstrate the effectiveness of our attack using two benchmark datasets and
multiple generative models, showing that the membership of a document in the
retrieval database can be efficiently determined through the creation of an
appropriate prompt in both black-box and gray-box settings. Moreover, we
introduce an initial defense strategy based on adding instructions to the RAG
template, which shows high effectiveness for some datasets and models. Our
findings highlight the importance of implementing security countermeasures in
deployed RAG systems and developing more advanced defenses to protect the
privacy and security of retrieval databases.",Maya Anderson
2024-06-07T16:59:38Z,http://arxiv.org/abs/2406.05085v2,Multi-Head RAG: Solving Multi-Aspect Problems with LLMs,"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving motivation is that different attention
heads can learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets that we release
online, and real-world use cases to demonstrate MRAG's effectiveness, showing
improvements of up to 20% in relevance over standard RAG baselines. MRAG can be
seamlessly integrated with existing RAG frameworks and benchmarking tools like
RAGAS as well as different classes of data stores.",Maciej Besta
2024-07-28T14:55:22Z,http://arxiv.org/abs/2408.01462v1,"Faculty Perspectives on the Potential of RAG in Computer Science Higher
  Education","The emergence of Large Language Models (LLMs) has significantly impacted the
field of Natural Language Processing and has transformed conversational tasks
across various domains because of their widespread integration in applications
and public access. The discussion surrounding the application of LLMs in
education has raised ethical concerns, particularly concerning plagiarism and
policy compliance. Despite the prowess of LLMs in conversational tasks, the
limitations of reliability and hallucinations exacerbate the need to guardrail
conversations, motivating our investigation of RAG in computer science higher
education. We developed Retrieval Augmented Generation (RAG) applications for
the two tasks of virtual teaching assistants and teaching aids. In our study,
we collected the ratings and opinions of faculty members in undergraduate and
graduate computer science university courses at various levels, using our
personalized RAG systems for each course. This study is the first to gather
faculty feedback on the application of LLM-based RAG in education. The
investigation revealed that while faculty members acknowledge the potential of
RAG systems as virtual teaching assistants and teaching aids, certain barriers
and features are suggested for their full-scale deployment. These findings
contribute to the ongoing discussion on the integration of advanced language
models in educational settings, highlighting the need for careful consideration
of ethical implications and the development of appropriate safeguards to ensure
responsible and effective implementation.",Sagnik Dakshit
2024-08-05T22:34:28Z,http://arxiv.org/abs/2408.02854v3,"Wiping out the limitations of Large Language Models -- A Taxonomy for
  Retrieval Augmented Generation","Current research on RAGs is distributed across various disciplines, and since
the technology is evolving very quickly, its unit of analysis is mostly on
technological innovations, rather than applications in business contexts. Thus,
in this research, we aim to create a taxonomy to conceptualize a comprehensive
overview of the constituting characteristics that define RAG applications,
facilitating the adoption of this technology in the IS community. To the best
of our knowledge, no RAG application taxonomies have been developed so far. We
describe our methodology for developing the taxonomy, which includes the
criteria for selecting papers, an explanation of our rationale for employing a
Large Language Model (LLM)-supported approach to extract and identify initial
characteristics, and a concise overview of our systematic process for
conceptualizing the taxonomy. Our systematic taxonomy development process
includes four iterative phases designed to refine and enhance our understanding
and presentation of RAG's core dimensions. We have developed a total of five
meta-dimensions and sixteen dimensions to comprehensively capture the concept
of Retrieval-Augmented Generation (RAG) applications. When discussing our
findings, we also detail the specific research areas and pose key research
questions to guide future information system researchers as they explore the
emerging topics of RAG systems.",Mahei Manhai Li
2024-08-20T20:47:27Z,http://arxiv.org/abs/2408.11189v1,Reading with Intent,"Retrieval augmented generation (RAG) systems augment how knowledge language
models are by integrating external information sources such as Wikipedia,
internal documents, scientific papers, or the open internet. RAG systems that
rely on the open internet as their knowledge source have to contend with the
complexities of human-generated content. Human communication extends much
deeper than just the words rendered as text. Intent, tonality, and connotation
can all change the meaning of what is being conveyed. Recent real-world
deployments of RAG systems have shown some difficulty in understanding these
nuances of human communication. One significant challenge for these systems
lies in processing sarcasm. Though the Large Language Models (LLMs) that make
up the backbone of these RAG systems are able to detect sarcasm, they currently
do not always use these detections for the subsequent processing of text. To
address these issues, in this paper, we synthetically generate sarcastic
passages from Natural Question's Wikipedia retrieval corpus. We then test the
impact of these passages on the performance of both the retriever and reader
portion of the RAG pipeline. We introduce a prompting system designed to
enhance the model's ability to interpret and generate responses in the presence
of sarcasm, thus improving overall system performance. Finally, we conduct
ablation studies to validate the effectiveness of our approach, demonstrating
improvements in handling sarcastic content within RAG systems.",Benjamin Reichman
2024-09-05T17:14:23Z,http://arxiv.org/abs/2409.03708v2,RAG based Question-Answering for Contextual Response Prediction System,"Large Language Models (LLMs) have shown versatility in various Natural
Language Processing (NLP) tasks, including their potential as effective
question-answering systems. However, to provide precise and relevant
information in response to specific customer queries in industry settings, LLMs
require access to a comprehensive knowledge base to avoid hallucinations.
Retrieval Augmented Generation (RAG) emerges as a promising technique to
address this challenge. Yet, developing an accurate question-answering
framework for real-world applications using RAG entails several challenges: 1)
data availability issues, 2) evaluating the quality of generated content, and
3) the costly nature of human evaluation. In this paper, we introduce an
end-to-end framework that employs LLMs with RAG capabilities for industry use
cases. Given a customer query, the proposed system retrieves relevant knowledge
documents and leverages them, along with previous chat history, to generate
response suggestions for customer service agents in the contact centers of a
major retail company. Through comprehensive automated and human evaluations, we
show that this solution outperforms the current BERT-based algorithms in
accuracy and relevance. Our findings suggest that RAG-based LLMs can be an
excellent support to human customer service representatives by lightening their
workload.",Sriram Veturi
2024-09-16T09:06:44Z,http://arxiv.org/abs/2409.10102v1,Trustworthiness in Retrieval-Augmented Generation Systems: A Survey,"Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.",Yujia Zhou
2024-10-07T07:02:09Z,http://arxiv.org/abs/2410.04790v1,"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA","In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.",Xinyu Wang
2024-10-08T12:30:07Z,http://arxiv.org/abs/2410.05983v1,Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,"Retrieval-augmented generation (RAG) empowers large language models (LLMs) to
utilize external knowledge sources. The increasing capacity of LLMs to process
longer input sequences opens up avenues for providing more retrieved
information, to potentially enhance the quality of generated outputs. It is
plausible to assume that a larger retrieval set would contain more relevant
information (higher recall), that might result in improved performance.
However, our empirical findings demonstrate that for many long-context LLMs,
the quality of generated output initially improves first, but then subsequently
declines as the number of retrieved passages increases. This paper investigates
this phenomenon, identifying the detrimental impact of retrieved ""hard
negatives"" as a key contributor. To mitigate this and enhance the robustness of
long-context LLM-based RAG, we propose both training-free and training-based
approaches. We first showcase the effectiveness of retrieval reordering as a
simple yet powerful training-free optimization. Furthermore, we explore
training-based methods, specifically RAG-specific implicit LLM fine-tuning and
RAG-oriented fine-tuning with intermediate reasoning, demonstrating their
capacity for substantial performance gains. Finally, we conduct a systematic
analysis of design choices for these training-based methods, including data
distribution, retriever selection, and training context length.",Bowen Jin
2024-10-23T11:32:46Z,http://arxiv.org/abs/2410.17783v1,"Leveraging the Domain Adaptation of Retrieval Augmented Generation
  Models for Question Answering and Reducing Hallucination","While ongoing advancements in Large Language Models have demonstrated
remarkable success across various NLP tasks, Retrieval Augmented Generation
Model stands out to be highly effective on downstream applications like
Question Answering. Recently, RAG-end2end model further optimized the
architecture and achieved notable performance improvements on domain
adaptation. However, the effectiveness of these RAG-based architectures remains
relatively unexplored when fine-tuned on specialized domains such as customer
service for building a reliable conversational AI system. Furthermore, a
critical challenge persists in reducing the occurrence of hallucinations while
maintaining high domain-specific accuracy. In this paper, we investigated the
performance of diverse RAG and RAG-like architectures through domain adaptation
and evaluated their ability to generate accurate and relevant response grounded
in the contextual knowledge base. To facilitate the evaluation of the models,
we constructed a novel dataset HotelConvQA, sourced from wide range of
hotel-related conversations and fine-tuned all the models on our domain
specific dataset. We also addressed a critical research gap on determining the
impact of domain adaptation on reducing hallucinations across different RAG
architectures, an aspect that was not properly measured in prior work. Our
evaluation shows positive results in all metrics by employing domain
adaptation, demonstrating strong performance on QA tasks and providing insights
into their efficacy in reducing hallucinations. Our findings clearly indicate
that domain adaptation not only enhances the models' performance on QA tasks
but also significantly reduces hallucination across all evaluated RAG
architectures.",Salman Rakin
2024-11-03T22:27:40Z,http://arxiv.org/abs/2411.01705v1,Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.",Yuefeng Peng
2024-11-28T06:29:46Z,http://arxiv.org/abs/2411.18948v1,"Knowledge Database or Poison Base? Detecting RAG Poisoning Attack
  through LLM Activations","As Large Language Models (LLMs) are progressively deployed across diverse
fields and real-world applications, ensuring the security and robustness of
LLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a
cutting-edge approach designed to address the limitations of large language
models (LLMs). By retrieving information from the relevant knowledge database,
RAG enriches the input to LLMs, enabling them to produce responses that are
more accurate and contextually appropriate. It is worth noting that the
knowledge database, being sourced from publicly available channels such as
Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves
injecting malicious texts into the knowledge database, ultimately leading to
the generation of the attacker's target response (also called poisoned
response). However, there are currently limited methods available for detecting
such poisoning attacks. We aim to bridge the gap in this work. Particularly, we
introduce RevPRAG, a flexible and automated detection pipeline that leverages
the activations of LLMs for poisoned response detection. Our investigation
uncovers distinct patterns in LLMs' activations when generating correct
responses versus poisoned responses. Our results on multiple benchmark datasets
and RAG architectures show our approach could achieve 98% true positive rate,
while maintaining false positive rates close to 1%. We also evaluate recent
backdoor detection methods specifically designed for LLMs and applicable for
identifying poisoned responses in RAG. The results demonstrate that our
approach significantly surpasses them.",Xue Tan
2024-11-29T08:34:07Z,http://arxiv.org/abs/2411.19539v1,Knowledge Management for Automobile Failure Analysis Using Graph RAG,"This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.",Yuta Ojima
2024-11-29T13:57:07Z,http://arxiv.org/abs/2411.19710v1,"Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating
  RAG Systems","Retrieval Augmented Generation (RAG) systems are a widespread application of
Large Language Models (LLMs) in the industry. While many tools exist empowering
developers to build their own systems, measuring their performance locally,
with datasets reflective of the system's use cases, is a technological
challenge. Solutions to this problem range from non-specific and cheap (most
public datasets) to specific and costly (generating data from local documents).
In this paper, we show that using public question and answer (Q&A) datasets to
assess retrieval performance can lead to non-optimal systems design, and that
common tools for RAG dataset generation can lead to unbalanced data. We propose
solutions to these issues based on the characterization of RAG datasets through
labels and through label-targeted data generation. Finally, we show that
fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that
these observations are invaluable to the know-your-data step of RAG systems
development.",Rafael Teixeira de Lima
1998-11-02T09:03:18Z,http://arxiv.org/abs/cond-mat/9811008v1,Fractional Quantization and Fractional Quantum Hall Effect,"A fractional quantization in a two dimensional space is proposed.
  The angular momenta of the two dimensional electrons are quantized in
fractional numbers by the boundary conditions on a multi-layered Riemann
surface. Extended wave functions for the incompressible quantum fluid states
are presented and the cohesive and the excitation energies are given.",Hyeong Rag Lee
2024-01-29T06:49:53Z,http://arxiv.org/abs/2402.01733v1,"Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report","Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.",YuHe Ke
2024-04-09T07:40:37Z,http://arxiv.org/abs/2404.06082v1,A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs,"Although the context length limitation of large language models (LLMs) has
been mitigated, it still hinders their application to software development
tasks. This study proposes a method incorporating execution traces into RAG for
inquiries about source code. Small-scale experiments confirm a tendency for the
method to contribute to improving LLM response quality.",Toshihiro Kamiya
2024-04-18T16:38:02Z,http://arxiv.org/abs/2404.12309v2,iRAG: Advancing RAG for Videos with an Incremental Approach,"Retrieval-augmented generation (RAG) systems combine the strengths of
language generation and information retrieval to power many real-world
applications like chatbots. Use of RAG for understanding of videos is appealing
but there are two critical limitations. One-time, upfront conversion of all
content in large corpus of videos into text descriptions entails high
processing times. Also, not all information in the rich video data is typically
captured in the text descriptions. Since user queries are not known apriori,
developing a system for video to text conversion and interactive querying of
video data is challenging.
  To address these limitations, we propose an incremental RAG system called
iRAG, which augments RAG with a novel incremental workflow to enable
interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG
quickly indexes large repositories of videos, and in the incremental workflow,
it uses the index to opportunistically extract more details from select
portions of the videos to retrieve context relevant to an interactive user
query. Such an incremental workflow avoids long video to text conversion times,
and overcomes information loss issues due to conversion of video to text, by
doing on-demand query-specific extraction of details in video data. This
ensures high quality of responses to interactive user queries that are often
not known apriori. To the best of our knowledge, iRAG is the first system to
augment RAG with an incremental workflow to support efficient interactive
querying of a large corpus of videos. Experimental results on real-world
datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring
that latency and quality of responses to interactive user queries is comparable
to responses from a traditional RAG where all video data is converted to text
upfront before any user querying.",Md Adnan Arefeen
2023-09-26T19:23:54Z,http://arxiv.org/abs/2309.15217v1,RAGAS: Automated Evaluation of Retrieval Augmented Generation,"We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With RAGAs, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.",Shahul Es
2024-02-06T21:14:45Z,http://arxiv.org/abs/2402.04411v2,"DFA-RAG: Conversational Semantic Router for Large Language Model with
  Definite Finite Automaton","This paper introduces the retrieval-augmented large language model with
Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the
capabilities of conversational agents using large language models (LLMs).
Traditional LLMs face challenges in generating regulated and compliant
responses in special scenarios with predetermined response guidelines, like
emotional support and customer service. Our framework addresses these
challenges by embedding a Definite Finite Automaton (DFA), learned from
training dialogues, within the LLM. This structured approach acts as a semantic
router which enables the LLM to adhere to a deterministic response pathway. The
routing is achieved by the retrieval-augmentation generation (RAG) strategy,
which carefully selects dialogue examples aligned with the current
conversational context. The advantages of DFA-RAG include an interpretable
structure through human-readable DFA, context-aware retrieval for responses in
conversations, and plug-and-play compatibility with existing LLMs. Extensive
benchmarks validate DFA-RAG's effectiveness, indicating its potential as a
valuable contribution to the conversational agent.",Yiyou Sun
2024-03-15T16:30:14Z,http://arxiv.org/abs/2403.10446v1,"Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A
  Case Study on Domain-Specific Queries in Private Knowledge-Bases","We proposed an end-to-end system design towards utilizing Retrieval Augmented
Generation (RAG) to improve the factual accuracy of Large Language Models
(LLMs) for domain-specific and time-sensitive queries related to private
knowledge-bases. Our system integrates RAG pipeline with upstream datasets
processing and downstream performance evaluation. Addressing the challenge of
LLM hallucinations, we finetune models with a curated dataset which originates
from CMU's extensive resources and annotated with the teacher model. Our
experiments demonstrate the system's effectiveness in generating more accurate
answers to domain-specific and time-sensitive inquiries. The results also
revealed the limitations of fine-tuning LLMs with small-scale and skewed
datasets. This research highlights the potential of RAG systems in augmenting
LLMs with external datasets for improved performance in knowledge-intensive
tasks. Our code and models are available on Github.",Jiarui Li
2024-03-31T08:58:54Z,http://arxiv.org/abs/2404.00610v1,RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,"Large Language Models (LLMs) exhibit remarkable capabilities but are prone to
generating inaccurate or hallucinatory responses. This limitation stems from
their reliance on vast pretraining datasets, making them susceptible to errors
in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation
(RAG) addresses this by incorporating external, relevant documents into the
response generation process, thus leveraging non-parametric knowledge alongside
LLMs' in-context learning abilities. However, existing RAG implementations
primarily focus on initial input for context retrieval, overlooking the nuances
of ambiguous or complex queries that necessitate further clarification or
decomposition for accurate responses. To this end, we propose learning to
Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper,
endeavoring to enhance the model by equipping it with capabilities for explicit
rewriting, decomposition, and disambiguation. Our experimental results indicate
that our method, when applied to a 7B Llama2 model, surpasses the previous
state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA
datasets, and also demonstrates enhanced performance in handling complex,
multi-hop QA datasets. Our code is available at
https://github.com/chanchimin/RQ-RAG.",Chi-Min Chan
2024-04-04T21:47:43Z,http://arxiv.org/abs/2404.04302v1,"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs
  for Legal Question Answering","Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)
output by providing prior knowledge as context to input. This is beneficial for
knowledge-intensive and expert reliant tasks, including legal
question-answering, which require evidence to validate generated text outputs.
We highlight that Case-Based Reasoning (CBR) presents key opportunities to
structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,
where CBR cycle's initial retrieval stage, its indexing vocabulary, and
similarity knowledge containers are used to enhance LLM queries with
contextually relevant cases. This integration augments the original LLM query,
providing a richer prompt. We present an evaluation of CBR-RAG, and examine
different representations (i.e. general and domain-specific embeddings) and
methods of comparison (i.e. inter, intra and hybrid similarity) on the task of
legal question-answering. Our results indicate that the context provided by
CBR's case reuse enforces similarity between relevant components of the
questions and the evidence base leading to significant improvements in the
quality of generated answers.",Nirmalie Wiratunga
2024-04-18T10:25:42Z,http://arxiv.org/abs/2404.12065v2,"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political
  Fact-Checking using Multimodal Large Language Models","The escalating challenge of misinformation, particularly in political
discourse, requires advanced fact-checking solutions; this is even clearer in
the more complex scenario of multimodal claims. We tackle this issue using a
multimodal large language model in conjunction with retrieval-augmented
generation (RAG), and introduce two novel reasoning techniques: Chain of RAG
(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by
extracting both textual and image content, retrieving external information, and
reasoning subsequent questions to be answered based on prior evidence. We
achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique
by 0.14 points. Human evaluation confirms that the vast majority of our
generated fact-check explanations contain all information from gold standard
data.",M. Abdul Khaliq
2024-04-21T21:22:28Z,http://arxiv.org/abs/2404.13781v1,Evaluating Retrieval Quality in Retrieval-Augmented Generation,"Evaluating retrieval-augmented generation (RAG) presents challenges,
particularly for retrieval models within these systems. Traditional end-to-end
evaluation methods are computationally expensive. Furthermore, evaluation of
the retrieval model's performance based on query-document relevance labels
shows a small correlation with the RAG system's downstream performance. We
propose a novel evaluation approach, eRAG, where each document in the retrieval
list is individually utilized by the large language model within the RAG
system. The output generated for each document is then evaluated based on the
downstream task ground truth labels. In this manner, the downstream performance
for each document serves as its relevance label. We employ various downstream
task metrics to obtain document-level annotations and aggregate them using
set-based or ranking metrics. Extensive experiments on a wide range of datasets
demonstrate that eRAG achieves a higher correlation with downstream RAG
performance compared to baseline methods, with improvements in Kendall's $\tau$
correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant
computational advantages, improving runtime and consuming up to 50 times less
GPU memory than end-to-end evaluation.",Alireza Salemi
2024-04-28T14:58:55Z,http://arxiv.org/abs/2405.01585v1,"Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular
  RAG Applications","In recent times Large Language Models have exhibited tremendous capabilities,
especially in the areas of mathematics, code generation and general-purpose
reasoning. However for specialized domains especially in applications that
require parsing and analyzing large chunks of numeric or tabular data even
state-of-the-art (SOTA) models struggle. In this paper, we introduce a new
approach to solving domain-specific tabular data analysis tasks by presenting a
unique RAG workflow that mitigates the scalability issues of existing tabular
LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel
approach to fine-tune embedding models for tabular Retrieval-Augmentation
Generation (RAG) applications. Embedding models form a crucial component in the
RAG workflow and even current SOTA embedding models struggle as they are
predominantly trained on textual datasets and thus underperform in scenarios
involving complex tabular data. The evaluation results showcase that our
approach not only outperforms current SOTA embedding models in this domain but
also does so with a notably smaller and more efficient model structure.",Sujit Khanna
2024-05-20T20:27:00Z,http://arxiv.org/abs/2405.12363v2,Question-Based Retrieval using Atomic Units for Enterprise RAG,"Enterprise retrieval augmented generation (RAG) offers a highly flexible
framework for combining powerful large language models (LLMs) with internal,
possibly temporally changing, documents. In RAG, documents are first chunked.
Relevant chunks are then retrieved for a user query, which are passed as
context to a synthesizer LLM to generate the query response. However, the
retrieval step can limit performance, as incorrect chunks can lead the
synthesizer LLM to generate a false response. This work applies a zero-shot
adaptation of standard dense retrieval steps for more accurate chunk recall.
Specifically, a chunk is first decomposed into atomic statements. A set of
synthetic questions are then generated on these atoms (with the chunk as the
context). Dense retrieval involves finding the closest set of synthetic
questions, and associated chunks, to the user query. It is found that retrieval
with the atoms leads to higher recall than retrieval with chunks. Further
performance gain is observed with retrieval using the synthetic questions
generated over the atoms. Higher recall at the retrieval step enables higher
performance of the enterprise LLM using the RAG pipeline.",Vatsal Raina
2024-05-21T20:03:40Z,http://arxiv.org/abs/2405.13179v4,"RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented
  Generation and Readability Control for Layman Summarization of Biomedical
  Texts","This paper introduces the RAG-RLRC-LaySum framework, designed to make complex
biomedical research understandable to laymen through advanced Natural Language
Processing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,
enhanced by a reranking method, utilizes multiple knowledge sources to ensure
the precision and pertinence of lay summaries. Additionally, our Reinforcement
Learning for Readability Control (RLRC) strategy improves readability, making
scientific content comprehensible to non-specialists. Evaluations using the
publicly accessible PLOS and eLife datasets show that our methods surpass Plain
Gemini model, demonstrating a 20% increase in readability scores, a 15%
improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual
accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific
knowledge, enhancing public engagement with biomedical discoveries.",Yuelyu Ji
2024-05-22T13:14:11Z,http://arxiv.org/abs/2405.13622v1,"Automated Evaluation of Retrieval-Augmented Language Models with
  Task-Specific Exam Generation","We propose a new method to measure the task-specific accuracy of
Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by
scoring the RAG on an automatically-generated synthetic exam composed of
multiple choice questions based on the corpus of documents associated with the
task. Our method is an automated, cost-efficient, interpretable, and robust
strategy to select the optimal components for a RAG system. We leverage Item
Response Theory (IRT) to estimate the quality of an exam and its
informativeness on task-specific accuracy. IRT also provides a natural way to
iteratively improve the exam by eliminating the exam questions that are not
sufficiently informative about a model's ability. We demonstrate our approach
on four new open-ended Question-Answering tasks based on Arxiv abstracts,
StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In
addition, our experiments reveal more general insights into factors impacting
RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most
notably, our findings show that choosing the right retrieval algorithms often
leads to bigger performance gains than simply using a larger language model.",Gauthier Guinet
2024-05-26T04:03:13Z,http://arxiv.org/abs/2405.16420v1,"M-RAG: Reinforcing Large Language Model Performance through
  Retrieval-Augmented Generation with Multiple Partitions","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant memories from an external database. However, existing RAG
methods typically organize all memories in a whole database, potentially
limiting focus on crucial memories and introducing noise. In this paper, we
introduce a multiple partition paradigm for RAG (called M-RAG), where each
database partition serves as a basic unit for RAG execution. Based on this
paradigm, we propose a novel framework that leverages LLMs with Multi-Agent
Reinforcement Learning to optimize different language generation tasks
explicitly. Through comprehensive experiments conducted on seven datasets,
spanning three language generation tasks and involving three distinct language
model architectures, we confirm that M-RAG consistently outperforms various
baseline methods, achieving improvements of 11%, 8%, and 12% for text
summarization, machine translation, and dialogue generation, respectively.",Zheng Wang
2024-05-29T15:47:57Z,http://arxiv.org/abs/2405.19207v1,A Multi-Source Retrieval Question Answering Framework Based on RAG,"With the rapid development of large-scale language models,
Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing
RAG paradigms are inevitably influenced by erroneous retrieval information,
thereby reducing the reliability and correctness of generated results.
Therefore, to improve the relevance of retrieval information, this study
proposes a method that replaces traditional retrievers with GPT-3.5, leveraging
its vast corpus knowledge to generate retrieval information. We also propose a
web retrieval based method to implement fine-grained knowledge retrieval,
Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic
partitioning of problem.In order to mitigate the illusion of GPT retrieval and
reduce noise in Web retrieval,we proposes a multi-source retrieval framework,
named MSRAG, which combines GPT retrieval with web retrieval. Experiments on
multiple knowledge-intensive QA datasets demonstrate that the proposed
framework in this study performs better than existing RAG framework in
enhancing the overall efficiency and accuracy of QA systems.",Ridong Wu
2024-06-03T19:40:28Z,http://arxiv.org/abs/2406.06575v1,"Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and
  Abbreviation De-hallucination","Electronic design engineers are challenged to find relevant information
efficiently for a myriad of tasks within design construction, verification and
technology development. Large language models (LLM) have the potential to help
improve productivity by serving as conversational agents that effectively
function as subject-matter experts. In this paper we demonstrate Ask-EDA, a
chat agent designed to serve as a 24x7 expert available to provide guidance to
design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation
(RAG) and abbreviation de-hallucination (ADH) techniques to deliver more
relevant and accurate responses. We curated three evaluation datasets, namely
q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct
aspect: general design question answering, design command handling and
abbreviation resolution. We demonstrated that hybrid RAG offers over a 40%
improvement in Recall on the q2a-100 dataset and over a 60% improvement on the
cmds-100 dataset compared to not using RAG, while ADH yields over a 70%
enhancement in Recall on the abbr-100 dataset. The evaluation results show that
Ask-EDA can effectively respond to design-related inquiries.",Luyao Shi
2024-06-10T08:23:52Z,http://arxiv.org/abs/2406.10251v3,"The Impact of Quantization on Retrieval-Augmented Generation: An
  Analysis of Small LLMs","Post-training quantization reduces the computational demand of Large Language
Models (LLMs) but can weaken some of their capabilities. Since LLM abilities
emerge with scale, smaller LLMs are more sensitive to quantization. In this
paper, we explore how quantization affects smaller LLMs' ability to perform
retrieval-augmented generation (RAG), specifically in longer contexts. We chose
personalization for evaluation because it is a challenging domain to perform
using RAG as it requires long-context reasoning over multiple documents. We
compare the original FP16 and the quantized INT4 performance of multiple 7B and
8B LLMs on two tasks while progressively increasing the number of retrieved
documents to test how quantized models fare against longer contexts. To better
understand the effect of retrieval, we evaluate three retrieval models in our
experiments. Our findings reveal that if a 7B LLM performs the task well,
quantization does not impair its performance and long-context reasoning
capabilities. We conclude that it is possible to utilize RAG with quantized
smaller LLMs.",Mert Yazan
2024-06-17T07:52:42Z,http://arxiv.org/abs/2406.11290v1,"Iterative Utility Judgment Framework via LLMs Inspired by Relevance in
  Philosophy","Utility and topical relevance are critical measures in information retrieval
(IR), reflecting system and user perspectives, respectively. While topical
relevance has long been emphasized, utility is a higher standard of relevance
and is more useful for facilitating downstream tasks, e.g., in
Retrieval-Augmented Generation (RAG). When we incorporate utility judgments
into RAG, we realize that the topical relevance, utility, and answering in RAG
are closely related to the three types of relevance that Schutz discussed from
a philosophical perspective. They are topical relevance, interpretational
relevance, and motivational relevance, respectively. Inspired by the dynamic
iterations of the three types of relevance, we propose an Iterative utiliTy
judgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We
conducted extensive experiments on multi-grade passage retrieval and factoid
question-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental
results demonstrate significant improvements in utility judgments, ranking of
topical relevance, and answer generation upon representative baselines,
including multiple single-shot utility judging approaches. Our code and
benchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.",Hengran Zhang
2024-06-17T11:22:25Z,http://arxiv.org/abs/2406.11424v1,"Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG
  Systems: A Comparative Study of Performance and Scalability","This paper presents an analysis of open-source large language models (LLMs)
and their application in Retrieval-Augmented Generation (RAG) tasks, specific
for enterprise-specific data sets scraped from their websites. With the
increasing reliance on LLMs in natural language processing, it is crucial to
evaluate their performance, accessibility, and integration within specific
organizational contexts. This study examines various open-source LLMs, explores
their integration into RAG frameworks using enterprise-specific data, and
assesses the performance of different open-source embeddings in enhancing the
retrieval and generation process. Our findings indicate that open-source LLMs,
combined with effective embedding techniques, can significantly improve the
accuracy and efficiency of RAG systems, offering a viable alternative to
proprietary solutions for enterprises.",Gautam B
2024-06-17T13:01:12Z,http://arxiv.org/abs/2406.11497v3,"CrAM: Credibility-Aware Attention Modification in LLMs for Combating
  Misinformation in RAG","Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of ""credibility-aware RAG"", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen1.5-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.",Boyi Deng
2024-06-24T07:17:59Z,http://arxiv.org/abs/2406.16367v1,"On the Role of Long-tail Knowledge in Retrieval Augmented Large Language
  Models","Retrieval augmented generation (RAG) exhibits outstanding performance in
promoting the knowledge capabilities of large language models (LLMs) with
retrieved documents related to user queries. However, RAG only focuses on
improving the response quality of LLMs via enhancing queries indiscriminately
with retrieved information, paying little attention to what type of knowledge
LLMs really need to answer original queries more accurately. In this paper, we
suggest that long-tail knowledge is crucial for RAG as LLMs have already
remembered common world knowledge during large-scale pre-training. Based on our
observation, we propose a simple but effective long-tail knowledge detection
method for LLMs. Specifically, the novel Generative Expected Calibration Error
(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on
both statistics and semantics. Hence, we retrieve relevant documents and infuse
them into the model for patching knowledge loopholes only when the input query
relates to long-tail knowledge. Experiments show that, compared to existing RAG
pipelines, our method achieves over 4x speedup in average inference time and
consistent performance improvement in downstream tasks.",Dongyang Li
2024-06-26T07:02:49Z,http://arxiv.org/abs/2406.18114v2,"Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode
  and Effects Analysis","Failure mode and effects analysis (FMEA) is a critical tool for mitigating
potential failures, particular during ramp-up phases of new products. However,
its effectiveness is often limited by the missing reasoning capabilities of the
FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for fine-tuning on custom datasets for
reasoning within FMEA contexts. However, LLMs face challenges in tasks that
require factual knowledge, a gap that retrieval-augmented generation (RAG)
approaches aim to fill. RAG retrieves information from a non-parametric data
store and uses a language model to generate responses. Building on this idea,
we propose to advance the non-parametric data store with a knowledge graph
(KG). By enhancing the RAG framework with a KG, our objective is to leverage
analytical and semantic question-answering capabilities on FMEA data. This
paper contributes by presenting a new ontology for FMEA observations, an
algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced
RAG framework. Our approach is validated through a human study and we measure
the performance of the context retrieval recall and precision.",Lukas Bahr
2024-07-01T09:09:27Z,http://arxiv.org/abs/2407.01102v1,BERGEN: A Benchmarking Library for Retrieval-Augmented Generation,"Retrieval-Augmented Generation allows to enhance Large Language Models with
external knowledge. In response to the recent popularity of generative LLMs,
many RAG approaches have been proposed, which involve an intricate number of
different configurations such as evaluation datasets, collections, metrics,
retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in
comparing approaches and understanding the impact of each component in the
pipeline. In this work, we study best practices that lay the groundwork for a
systematic evaluation of RAG and present BERGEN, an end-to-end library for
reproducible research standardizing RAG experiments. In an extensive study
focusing on QA, we benchmark different state-of-the-art retrievers, rerankers,
and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our
open-source library BERGEN is available under
\url{https://github.com/naver/bergen}.",David Rau
2024-07-02T17:59:17Z,http://arxiv.org/abs/2407.02485v1,"RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in
  LLMs","Large language models (LLMs) typically utilize the top-k contexts from a
retriever in retrieval-augmented generation (RAG). In this work, we propose a
novel instruction fine-tuning framework RankRAG, which instruction-tunes a
single LLM for the dual purpose of context ranking and answer generation in
RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding
a small fraction of ranking data into the training blend, and outperform
existing expert ranking models, including the same LLM exclusively fine-tuned
on a large amount of ranking data. For generation, we compare our model with
many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and
ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG
benchmarks. Specifically, our Llama3-RankRAG significantly outperforms
Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In
addition, it also performs comparably to GPT-4 on five RAG benchmarks in the
biomedical domain without instruction fine-tuning on biomedical data,
demonstrating its superb capability for generalization to new domains.",Yue Yu
2024-07-06T17:25:11Z,http://arxiv.org/abs/2407.05138v1,Vortex under Ripplet: An Empirical Study of RAG-enabled Applications,"Large language models (LLMs) enhanced by retrieval-augmented generation (RAG)
provide effective solutions in various application scenarios. However,
developers face challenges in integrating RAG-enhanced LLMs into software
systems, due to lack of interface specification, requirements from software
context, and complicated system management. In this paper, we manually studied
100 open-source applications that incorporate RAG-enhanced LLMs, and their
issue reports. We have found that more than 98% of applications contain
multiple integration defects that harm software functionality, efficiency, and
security. We have also generalized 19 defect patterns and proposed guidelines
to tackle them. We hope this work could aid LLM-enabled software development
and motivate future research.",Yuchen Shao
2024-07-11T05:04:44Z,http://arxiv.org/abs/2407.12057v1,"NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker
  and AWS Trainium and Inferentia2","Retrieval-augmented generation (RAG) techniques are widely used today to
retrieve and present information in a conversational format. This paper
presents a set of enhancements to traditional RAG techniques, focusing on large
language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI
chips via SageMaker. These chips are characterized by their elasticity,
affordability, and efficient performance for AI compute tasks. Besides enabling
deployment on these chips, this work aims to improve tool usage, add citation
capabilities, and mitigate the risks of hallucinations and unsafe responses due
to context bias. We benchmark our RAG system's performance on the Natural
Questions and HotPotQA datasets, achieving an accuracy of 62% and 59%
respectively, exceeding other models such as DBRX and Mixtral Instruct.",Tengfei Xue
2024-07-31T16:04:03Z,http://arxiv.org/abs/2407.21712v1,Adaptive Retrieval-Augmented Generation for Conversational Systems,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.",Xi Wang
2024-07-22T11:44:08Z,http://arxiv.org/abs/2408.03340v1,"An Empirical Comparison of Video Frame Sampling Methods for Multi-Modal
  RAG Retrieval","Numerous video frame sampling methodologies detailed in the literature
present a significant challenge in determining the optimal video frame method
for Video RAG pattern without a comparative side-by-side analysis. In this
work, we investigate the trade-offs in frame sampling methods for Video & Frame
Retrieval using natural language questions. We explore the balance between the
quantity of sampled frames and the retrieval recall score, aiming to identify
efficient video frame sampling strategies that maintain high retrieval efficacy
with reduced storage and processing demands. Our study focuses on the storage
and retrieval of image data (video frames) within a vector database required by
Video RAG pattern, comparing the effectiveness of various frame sampling
techniques. Our investigation indicates that the recall@k metric for both
text-to-video and text-to-frame retrieval tasks using various methods covered
as part of this work is comparable to or exceeds that of storing each frame
from the video. Our findings are intended to inform the selection of frame
sampling methods for practical Video RAG implementations, serving as a
springboard for innovative research in this domain.",Mahesh Kandhare
2024-08-14T10:03:28Z,http://arxiv.org/abs/2408.07425v1,Exploring Retrieval Augmented Generation in Arabic,"Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful
technique in natural language processing, combining the strengths of
retrieval-based and generation-based models to enhance text generation tasks.
However, the application of RAG in Arabic, a language with unique
characteristics and resource constraints, remains underexplored. This paper
presents a comprehensive case study on the implementation and evaluation of RAG
for Arabic text. The work focuses on exploring various semantic embedding
models in the retrieval stage and several LLMs in the generation stage, in
order to investigate what works and what doesn't in the context of Arabic. The
work also touches upon the issue of variations between document dialect and
query dialect in the retrieval stage. Results show that existing semantic
embedding models and LLMs can be effectively employed to build Arabic RAG
pipelines.",Samhaa R. El-Beltagy
2024-08-21T17:43:11Z,http://arxiv.org/abs/2408.11800v2,"WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy
  Domain","In the rapidly evolving landscape of Natural Language Processing (NLP) and
text generation, the emergence of Retrieval Augmented Generation (RAG) presents
a promising avenue for improving the quality and reliability of generated text
by leveraging information retrieved from user specified database. Benchmarking
is essential to evaluate and compare the performance of the different RAG
configurations in terms of retriever and generator, providing insights into
their effectiveness, scalability, and suitability for the specific domain and
applications. In this paper, we present a comprehensive framework to generate a
domain relevant RAG benchmark. Our framework is based on automatic
question-answer generation with Human (domain experts)-AI Large Language Model
(LLM) teaming. As a case study, we demonstrate the framework by introducing
WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises
of multiple scientific documents/reports related to environmental impact of
wind energy projects. Our framework systematically evaluates RAG performance
using diverse metrics and multiple question types with varying complexity
level. We also demonstrate the performance of different models on our
benchmark.",Rounak Meyur
2024-08-21T18:00:21Z,http://arxiv.org/abs/2408.11903v2,"Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for
  Ancient Indian Philosophy","LLMs have revolutionized the landscape of information retrieval and knowledge
dissemination. However, their application in specialized areas is often
hindered by factual inaccuracies and hallucinations, especially in long-tail
knowledge distributions. We explore the potential of retrieval-augmented
generation (RAG) models for long-form question answering (LFQA) in a
specialized knowledge domain. We present VedantaNY-10M, a dataset curated from
extensive public discourses on the ancient Indian philosophy of Advaita
Vedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,
focusing on transcription, retrieval, and generation performance. Human
evaluations by computational linguists and domain experts show that the RAG
model significantly outperforms the standard model in producing factual and
comprehensive responses having fewer hallucinations. In addition, a
keyword-based hybrid retriever that emphasizes unique low-frequency terms
further improves results. Our study provides insights into effectively
integrating modern large language models with ancient knowledge systems.
Project page with dataset and code: https://sites.google.com/view/vedantany-10m",Priyanka Mandikal
2024-08-21T21:34:01Z,http://arxiv.org/abs/2408.12003v1,"RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and
  Personalization","With the development of the modern social economy, tourism has become an
important way to meet people's spiritual needs, bringing development
opportunities to the tourism industry. However, existing large language models
(LLMs) face challenges in personalized recommendation capabilities and the
generation of content that can sometimes produce hallucinations. This study
proposes an optimization scheme for Tibet tourism LLMs based on
retrieval-augmented generation (RAG) technology. By constructing a database of
tourist viewpoints and processing the data using vectorization techniques, we
have significantly improved retrieval accuracy. The application of RAG
technology effectively addresses the hallucination problem in content
generation. The optimized model shows significant improvements in fluency,
accuracy, and relevance of content generation. This research demonstrates the
potential of RAG technology in the standardization of cultural tourism
information and data analysis, providing theoretical and technical support for
the development of intelligent cultural tourism service systems.",Jinhu Qi
2024-09-10T15:39:32Z,http://arxiv.org/abs/2409.06595v1,"GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question
  Answering","Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.",Sacha Muller
2024-09-13T13:34:32Z,http://arxiv.org/abs/2409.08820v1,"A RAG Approach for Generating Competency Questions in Ontology
  Engineering","Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.",Xueli Pan
2024-09-14T19:18:26Z,http://arxiv.org/abs/2409.09510v1,"Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for
  Privacy-Preserving Personalization of Large Language Models","Privacy-preserving methods for personalizing large language models (LLMs) are
relatively under-explored. There are two schools of thought on this topic: (1)
generating personalized outputs by personalizing the input prompt through
retrieval augmentation from the user's personal information (RAG-based
methods), and (2) parameter-efficient fine-tuning of LLMs per user that
considers efficiency and space limitations (PEFT-based methods). This paper
presents the first systematic comparison between two approaches on a wide range
of personalization tasks using seven diverse datasets. Our results indicate
that RAG-based and PEFT-based personalization methods on average yield 14.92%
and 1.07% improvements over the non-personalized LLM, respectively. We find
that combining RAG with PEFT elevates these improvements to 15.98%.
Additionally, we identify a positive correlation between the amount of user
data and PEFT's effectiveness, indicating that RAG is a better choice for
cold-start users (i.e., user's with limited personal data).",Alireza Salemi
2024-09-21T03:03:09Z,http://arxiv.org/abs/2409.13992v1,"SMART-RAG: Selection using Determinantal Matrices for Augmented
  Retrieval","Retrieval-Augmented Generation (RAG) has greatly improved large language
models (LLMs) by enabling them to generate accurate, contextually grounded
responses through the integration of external information. However,
conventional RAG approaches, which prioritize top-ranked documents based solely
on query-context relevance, often introduce redundancy and conflicting
information. This issue is particularly evident in unsupervised retrieval
settings, where there are no mechanisms to effectively mitigate these problems,
leading to suboptimal context selection. To address this, we propose Selection
using Matrices for Augmented Retrieval (SMART) in question answering tasks, a
fully unsupervised and training-free framework designed to optimize context
selection in RAG. SMART leverages Determinantal Point Processes (DPPs) to
simultaneously model relevance, diversity and conflict, ensuring the selection
of potentially high-quality contexts. Experimental results across multiple
datasets demonstrate that SMART significantly enhances QA performance and
surpasses previous unsupervised context selection methods, showing a promising
strategy for RAG.",Jiatao Li
2024-09-09T07:28:14Z,http://arxiv.org/abs/2409.15337v1,Revisiting the Solution of Meta KDD Cup 2024: CRAG,"This paper presents the solution of our team APEX in the Meta KDD CUP 2024:
CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the
limitations of existing QA benchmarks in evaluating the diverse and dynamic
challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a
more comprehensive assessment of RAG performance and contributes to advancing
research in this field. We propose a routing-based domain and dynamic adaptive
RAG pipeline, which performs specific processing for the diverse and dynamic
nature of the question in all three stages: retrieval, augmentation, and
generation. Our method achieved superior performance on CRAG and ranked 2nd for
Task 2&3 on the final competition leaderboard. Our implementation is available
at this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.",Jie Ouyang
2024-09-24T03:25:36Z,http://arxiv.org/abs/2409.15699v1,"Lighter And Better: Towards Flexible Context Adaptation For Retrieval
  Augmented Generation","The existing Retrieval-Augmented Generation (RAG) systems face significant
challenges in terms of cost and effectiveness. On one hand, they need to encode
the lengthy retrieved contexts before responding to the input tasks, which
imposes substantial computational overhead. On the other hand, directly using
generic Large Language Models (LLMs) often leads to sub-optimal answers, while
task-specific fine-tuning may compromise the LLMs' general capabilities. To
address these challenges, we introduce a novel approach called FlexRAG
(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts
are compressed into compact embeddings before being encoded by the LLMs.
Simultaneously, these compressed embeddings are optimized to enhance downstream
RAG performance. A key feature of FlexRAG is its flexibility, which enables
effective support for diverse compression ratios and selective preservation of
important contexts. Thanks to these technical designs, FlexRAG achieves
superior generation quality while significantly reducing running costs.
Comprehensive experiments on various question-answering datasets validate our
approach as a cost-effective and flexible solution for RAG systems.",Zheng Liu
2024-09-24T05:39:53Z,http://arxiv.org/abs/2409.15763v2,"IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through
  Semantic Comprehension in Retrieval-Augmented Generation Scenarios","In Retrieval-Augmented Generation (RAG) tasks using Large Language Models
(LLMs), the quality of retrieved information is critical to the final output.
This paper introduces the IRSC benchmark for evaluating the performance of
embedding models in multilingual RAG tasks. The benchmark encompasses five
retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,
keyword retrieval, and summary retrieval. Our research addresses the current
lack of comprehensive testing and effective comparison methods for embedding
models in RAG scenarios. We introduced new metrics: the Similarity of Semantic
Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),
and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our
contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and
3) insights into the cross-lingual limitations of embedding models. The IRSC
benchmark aims to enhance the understanding and development of accurate
retrieval systems in RAG tasks. All code and datasets are available at:
https://github.com/Jasaxion/IRSC_Benchmark",Hai Lin
2024-09-26T08:55:21Z,http://arxiv.org/abs/2409.17648v3,"Efficient In-Domain Question Answering for Resource-Constrained
  Environments","Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.",Isaac Chung
2024-09-16T20:36:17Z,http://arxiv.org/abs/2409.18986v1,"Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test
  Interpretation in Clinical Medicine","Accurate interpretation of lab results is crucial in clinical medicine, yet
most patient portals use universal normal ranges, ignoring factors like age and
gender. This study introduces Lab-AI, an interactive system that offers
personalized normal ranges using Retrieval-Augmented Generation (RAG) from
credible health sources. Lab-AI has two modules: factor retrieval and normal
range retrieval. We tested these on 68 lab tests-30 with conditional factors
and 38 without. For tests with factors, normal ranges depend on
patient-specific information. Our results show that GPT-4-turbo with RAG
achieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal
range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by
29.1% in factor retrieval and showed 60.9% and 52.9% improvements in
question-level and lab-level performance, respectively, for normal range
retrieval. These findings highlight Lab-AI's potential to enhance patient
understanding of lab results.",Xiaoyu Wang
2024-10-01T16:48:13Z,http://arxiv.org/abs/2410.00857v1,"Quantifying reliance on external information over parametric knowledge
  during Retrieval Augmented Generation (RAG) using mechanistic analysis","Retrieval Augmented Generation (RAG) is a widely used approach for leveraging
external context in several natural language applications such as question
answering and information retrieval. Yet, the exact nature in which a Language
Model (LM) leverages this non-parametric memory or retrieved context isn't
clearly understood. This paper mechanistically examines the RAG pipeline to
highlight that LMs demonstrate a ""shortcut'' effect and have a strong bias
towards utilizing the retrieved context to answer questions, while relying
minimally on model priors. We propose (a) Causal Mediation Analysis; for
proving that parametric memory is minimally utilized when answering a question
and (b) Attention Contributions and Knockouts for showing the last token
residual stream do not get enriched from the subject token in the question, but
gets enriched from tokens of RAG-context. We find this pronounced ""shortcut''
behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi)",Reshmi Ghosh
2024-10-03T19:25:05Z,http://arxiv.org/abs/2410.02932v1,Intrinsic Evaluation of RAG Systems for Deep-Logic Questions,"We introduce the Overall Performance Index (OPI), an intrinsic metric to
evaluate retrieval-augmented generation (RAG) mechanisms for applications
involving deep-logic queries. OPI is computed as the harmonic mean of two key
metrics: the Logical-Relation Correctness Ratio and the average of BERT
embedding similarity scores between ground-truth and generated answers. We
apply OPI to assess the performance of LangChain, a popular RAG tool, using a
logical relations classifier fine-tuned from GPT-4o on the RAG-Dataset-12000
from Hugging Face. Our findings show a strong correlation between BERT
embedding similarity scores and extrinsic evaluation scores. Among the commonly
used retrievers, the cosine similarity retriever using BERT-based embeddings
outperforms others, while the Euclidean distance-based retriever exhibits the
weakest performance. Furthermore, we demonstrate that combining multiple
retrievers, either algorithmically or by merging retrieved sentences, yields
superior performance compared to using any single retriever alone.",Junyi Hu
2024-10-02T05:24:49Z,http://arxiv.org/abs/2410.03754v1,Enhancing Retrieval in QA Systems with Derived Feature Association,"Retrieval augmented generation (RAG) has become the standard in long context
question answering (QA) systems. However, typical implementations of RAG rely
on a rather naive retrieval mechanism, in which texts whose embeddings are most
similar to that of the query are deemed most relevant. This has consequences in
subjective QA tasks, where the most relevant text may not directly contain the
answer. In this work, we propose a novel extension to RAG systems, which we
call Retrieval from AI Derived Documents (RAIDD). RAIDD leverages the full
power of the LLM in the retrieval process by deriving inferred features, such
as summaries and example questions, from the documents at ingest. We
demonstrate that this approach significantly improves the performance of RAG
systems on long-context QA tasks.",Keyush Shah
2024-10-13T17:53:50Z,http://arxiv.org/abs/2410.09942v1,"Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization","This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and retrieval-augmentation strategy.
We introduce an iterative approach where the search engine generates retrieval
results for these RAG agents and gathers feedback on the quality of the
retrieved documents during an offline phase. This feedback is then used to
iteratively optimize the search engine using a novel expectation-maximization
algorithm, with the goal of maximizing each agent's utility function.
Additionally, we adapt this approach to an online setting, allowing the search
engine to refine its behavior based on real-time individual agents feedback to
better serve the results for each of them. Experiments on diverse datasets from
the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our
approach significantly on average outperforms competitive baselines across 18
RAG models. We also demonstrate that our method effectively ``personalizes''
the retrieval process for each RAG agent based on the collected feedback.
Finally, we provide a comprehensive ablation study to explore various aspects
of our method.",Alireza Salemi
2024-10-14T04:06:22Z,http://arxiv.org/abs/2410.10136v1,"Beyond-RAG: Question Identification and Answer Generation in Real-Time
  Conversations","In customer contact centers, human agents often struggle with long average
handling times (AHT) due to the need to manually interpret queries and retrieve
relevant knowledge base (KB) articles. While retrieval augmented generation
(RAG) systems using large language models (LLMs) have been widely adopted in
industry to assist with such tasks, RAG faces challenges in real-time
conversations, such as inaccurate query formulation and redundant retrieval of
frequently asked questions (FAQs). To address these limitations, we propose a
decision support system that can look beyond RAG by first identifying customer
questions in real time. If the query matches an FAQ, the system retrieves the
answer directly from the FAQ database; otherwise, it generates answers via RAG.
Our approach reduces reliance on manual queries, providing responses to agents
within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva
CQ, this system improves efficiency, reduces AHT, and lowers operational costs.
We also introduce an automated LLM-agentic workflow to identify FAQs from
historical transcripts when no predefined FAQs exist.",Garima Agrawal
2024-10-14T04:57:32Z,http://arxiv.org/abs/2410.10913v2,"Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined
  Knowledge Base","Recent advances in audio understanding tasks leverage the reasoning
capabilities of LLMs. However, adapting LLMs to learn audio concepts requires
massive training data and substantial computational resources. To address these
challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs
from a knowledge base (KB) and augments them with query audio to generate
accurate textual responses. In RAG, the relevance of the retrieved information
plays a crucial role in effectively processing the input. In this paper, we
analyze how different retrieval methods and knowledge bases impact the
relevance of audio-text pairs and the performance of audio captioning with RAG.
We propose generative pair-to-pair retrieval, which uses the generated caption
as a text query to accurately find relevant audio-text pairs to the query
audio, thereby improving the relevance and accuracy of retrieved information.
Additionally, we refine the large-scale knowledge base to retain only
audio-text pairs that align with the contextualized intents. Our approach
achieves state-of-the-art results on benchmarks including AudioCaps, Clotho,
and Auto-ACD, with detailed ablation studies validating the effectiveness of
our retrieval and KB construction methods.",Choi Changin
2024-10-15T09:02:09Z,http://arxiv.org/abs/2410.11414v1,"ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via
  Mechanistic Interpretability","Retrieval-Augmented Generation (RAG) models are designed to incorporate
external knowledge, reducing hallucinations caused by insufficient parametric
(internal) knowledge. However, even with accurate and relevant retrieved
content, RAG models can still produce hallucinations by generating outputs that
conflict with the retrieved information. Detecting such hallucinations requires
disentangling how Large Language Models (LLMs) utilize external and parametric
knowledge. Current detection methods often focus on one of these mechanisms or
without decoupling their intertwined effects, making accurate detection
difficult. In this paper, we investigate the internal mechanisms behind
hallucinations in RAG scenarios. We discover hallucinations occur when the
Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual
stream, while Copying Heads fail to effectively retain or integrate external
knowledge from retrieved content. Based on these findings, we propose ReDeEP, a
novel method that detects hallucinations by decoupling LLM's utilization of
external context and parametric knowledge. Our experiments show that ReDeEP
significantly improves RAG hallucination detection accuracy. Additionally, we
introduce AARF, which mitigates hallucinations by modulating the contributions
of Knowledge FFNs and Copying Heads.",Zhongxiang Sun
2024-10-15T10:57:12Z,http://arxiv.org/abs/2410.11494v1,DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG,"In the rapidly evolving landscape of language, resolving new linguistic
expressions in continuously updating knowledge bases remains a formidable
challenge. This challenge becomes critical in retrieval-augmented generation
(RAG) with knowledge bases, as emerging expressions hinder the retrieval of
relevant documents, leading to generator hallucinations. To address this issue,
we introduce a novel task aimed at resolving emerging mentions to dynamic
entities and present DynamicER benchmark. Our benchmark includes dynamic entity
mention resolution and entity-centric knowledge-intensive QA task, evaluating
entity linking and RAG model's adaptability to new expressions, respectively.
We discovered that current entity linking models struggle to link these new
expressions to entities. Therefore, we propose a temporal segmented clustering
method with continual adaptation, effectively managing the temporal dynamics of
evolving entities and emerging mentions. Extensive experiments demonstrate that
our method outperforms existing baselines, enhancing RAG model performance on
QA task with resolved mentions.",Jinyoung Kim
2024-10-17T08:48:54Z,http://arxiv.org/abs/2410.13339v1,"Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval","Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.",Ingeol Baek
2024-10-18T17:47:11Z,http://arxiv.org/abs/2410.14651v1,Real-time Fake News from Adversarial Feedback,"We show that existing evaluations for fake news detection based on
conventional sources, such as claims on fact-checking websites, result in an
increasing accuracy over time for LLM-based detectors -- even after their
knowledge cutoffs. This suggests that recent popular political claims, which
form the majority of fake news on such sources, are easily classified using
surface-level shallow patterns. Instead, we argue that a proper fake news
detection dataset should test a model's ability to reason factually about the
current world by retrieving and reading related evidence. To this end, we
develop a novel pipeline that leverages natural language feedback from a
RAG-based detector to iteratively modify real-time news into deceptive fake
news that challenges LLMs. Our iterative rewrite decreases the binary
classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o
detector. Our experiments reveal the important role of RAG in both detecting
and generating fake news, as retrieval-free LLM detectors are vulnerable to
unseen events and adversarial attacks, while feedback from RAG detection helps
discover more deceitful patterns in fake news.",Sanxing Chen
2024-10-21T12:21:49Z,http://arxiv.org/abs/2410.15944v1,"Developing Retrieval Augmented Generation (RAG) based LLM Systems from
  PDFs: An Experience Report","This paper presents an experience report on the development of Retrieval
Augmented Generation (RAG) systems using PDF documents as the primary data
source. The RAG architecture combines generative capabilities of Large Language
Models (LLMs) with the precision of information retrieval. This approach has
the potential to redefine how we interact with and augment both structured and
unstructured knowledge in generative models to enhance transparency, accuracy,
and contextuality of responses. The paper details the end-to-end pipeline, from
data collection, preprocessing, to retrieval indexing and response generation,
highlighting technical challenges and practical solutions. We aim to offer
insights to researchers and practitioners developing similar systems using two
distinct approaches: OpenAI's Assistant API with GPT Series and Llama's
open-source models. The practical implications of this research lie in
enhancing the reliability of generative AI systems in various sectors where
domain-specific knowledge and real-time information retrieval is important. The
Python code used in this work is also available at:
https://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.",Ayman Asad Khan
2024-10-28T08:32:09Z,http://arxiv.org/abs/2410.20833v1,"LLMs are Biased Evaluators But Not Biased for Retrieval Augmented
  Generation","Recent studies have demonstrated that large language models (LLMs) exhibit
significant biases in evaluation tasks, particularly in preferentially rating
and favoring self-generated content. However, the extent to which this bias
manifests in fact-oriented tasks, especially within retrieval-augmented
generation (RAG) frameworks-where keyword extraction and factual accuracy take
precedence over stylistic elements-remains unclear. Our study addresses this
knowledge gap by simulating two critical phases of the RAG framework. In the
first phase, we access the suitability of human-authored versus model-generated
passages, emulating the pointwise reranking process. The second phase involves
conducting pairwise reading comprehension tests to simulate the generation
process. Contrary to previous findings indicating a self-preference in rating
tasks, our results reveal no significant self-preference effect in RAG
frameworks. Instead, we observe that factual accuracy significantly influences
LLMs' output, even in the absence of prior knowledge. Our research contributes
to the ongoing discourse on LLM biases and their implications for RAG-based
system, offering insights that may inform the development of more robust and
unbiased LLM systems.",Yen-Shan Chen
2024-11-01T23:03:40Z,http://arxiv.org/abs/2411.01073v1,"AttackQA: Development and Adoption of a Dataset for Assisting
  Cybersecurity Operations using Fine-tuned and Open-Source LLMs","Retrieval-augmented generation (RAG) on specialized domain datasets has shown
improved performance when large language models (LLMs) are fine-tuned for
generating responses to user queries. In this study, we develop a cybersecurity
question-answering (Q\&A) dataset, called AttackQA, and employ it to build a
RAG-based Q\&A system designed for analysts in security operations centers. The
dataset comprises 25,335 Q\&A pairs, accompanied by rationales to facilitate
fine-tuning and evaluation. 80\% of the dataset was generated with help of a
lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per
second with full 16-bit precision on SambaNova System's SN40L specialized
hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and
reject low-quality Q\&A pairs. In using the dataset for RAG, we demonstrate
that fine-tuning open-source embeddings and LLMs can yield superior accuracy
compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).
Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,
enabling the creation of a fully open-source, high-speed RAG and evaluation
pipeline with a benchmark for model accuracy.",Varun Badrinath Krishna
2024-11-06T00:23:55Z,http://arxiv.org/abs/2411.03572v1,"Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge
  Reasoning and Text Generation","This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.",Yuxin Dong
2024-11-12T23:55:11Z,http://arxiv.org/abs/2411.08249v1,Retrieval Augmented Time Series Forecasting,"Retrieval-augmented generation (RAG) is a central component of modern LLM
systems, particularly in scenarios where up-to-date information is crucial for
accurately responding to user queries or when queries exceed the scope of the
training data. The advent of time-series foundation models (TSFM), such as
Chronos, and the need for effective zero-shot forecasting performance across
various time-series domains motivates the question: Do benefits of RAG
similarly carry over to time series forecasting? In this paper, we advocate
that the dynamic and event-driven nature of time-series data makes RAG a
crucial component of TSFMs and introduce a principled RAG framework for
time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within
RAF, we develop efficient strategies for retrieving related time-series
examples and incorporating them into forecast. Through experiments and
mechanistic studies, we demonstrate that RAF indeed improves the forecasting
accuracy across diverse time series domains and the improvement is more
significant for larger TSFM sizes.",Kutay Tire
2024-11-14T06:19:18Z,http://arxiv.org/abs/2411.09213v1,"Comprehensive and Practical Evaluation of Retrieval-Augmented Generation
  Systems for Medical Question Answering","Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.",Nghia Trung Ngo
2024-11-14T17:01:24Z,http://arxiv.org/abs/2411.09590v1,Adopting RAG for LLM-Aided Future Vehicle Design,"In this paper, we explore the integration of Large Language Models (LLMs)
with Retrieval-Augmented Generation (RAG) to enhance automated design and
software development in the automotive industry. We present two case studies: a
standardization compliance chatbot and a design copilot, both utilizing RAG to
provide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,
LLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and
execution time. Our results demonstrate that while GPT-4 offers superior
performance, LLAMA3 and Mistral also show promising capabilities for local
deployment, addressing data privacy concerns in automotive applications. This
study highlights the potential of RAG-augmented LLMs in improving design
workflows and compliance in automotive engineering.",Vahid Zolfaghari
2024-11-20T09:43:30Z,http://arxiv.org/abs/2411.13154v1,DMQR-RAG: Diverse Multi-Query Rewriting for RAG,"Large language models often encounter challenges with static knowledge and
hallucinations, which undermine their reliability. Retrieval-augmented
generation (RAG) mitigates these issues by incorporating external information.
However, user queries frequently contain noise and intent deviations,
necessitating query rewriting to improve the relevance of retrieved documents.
In this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework
designed to improve the performance of both document retrieval and final
responses in RAG. Specifically, we investigate how queries with varying
information quantities can retrieve a diverse array of documents, presenting
four rewriting strategies that operate at different levels of information to
enhance the performance of baseline approaches. Additionally, we propose an
adaptive strategy selection method that minimizes the number of rewrites while
optimizing overall performance. Our methods have been rigorously validated
through extensive experiments conducted in both academic and industry settings.",Zhicong Li
2024-11-21T20:39:13Z,http://arxiv.org/abs/2411.14572v1,"Towards Knowledge Checking in Retrieval-augmented Generation: A
  Representation Perspective","Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing
the performance of Large Language Models (LLMs). However, these systems face
challenges in effectively integrating external knowledge with the LLM's
internal knowledge, often leading to issues with misleading or unhelpful
information. This work aims to provide a systematic study on knowledge checking
in RAG systems. We conduct a comprehensive analysis of LLM representation
behaviors and demonstrate the significance of using representations in
knowledge checking. Motivated by the findings, we further develop
representation-based classifiers for knowledge filtering. We show substantial
improvements in RAG performance, even when dealing with noisy knowledge
databases. Our study provides new insights into leveraging LLM representations
for enhancing the reliability and effectiveness of RAG systems.",Shenglai Zeng
2024-12-12T06:38:40Z,http://arxiv.org/abs/2412.08985v1,"Assessing the Robustness of Retrieval-Augmented Generation Systems in
  K-12 Educational Question Answering with Knowledge Discrepancies","Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable
potential as question answering systems in the K-12 Education domain, where
knowledge is typically queried within the restricted scope of authoritative
textbooks. However, the discrepancy between textbooks and the parametric
knowledge in Large Language Models (LLMs) could undermine the effectiveness of
RAG systems. To systematically investigate the robustness of RAG systems under
such knowledge discrepancies, we present EduKDQA, a question answering dataset
that simulates knowledge discrepancies in real applications by applying
hypothetical knowledge updates in answers and source documents. EduKDQA
includes 3,005 questions covering five subjects, under a comprehensive question
typology from the perspective of context utilization and knowledge integration.
We conducted extensive experiments on retrieval and question answering
performance. We find that most RAG systems suffer from a substantial
performance drop in question answering with knowledge discrepancies, while
questions that require integration of contextual knowledge and parametric
knowledge pose a challenge to LLMs.",Tianshi Zheng
2024-01-16T14:44:47Z,http://arxiv.org/abs/2401.08406v3,"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on
  Agriculture","There are two common ways in which developers are incorporating proprietary
and domain-specific data when building applications of Large Language Models
(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the
prompt with the external data, while fine-Tuning incorporates the additional
knowledge into the model itself. However, the pros and cons of both approaches
are not well understood. In this paper, we propose a pipeline for fine-tuning
and RAG, and present the tradeoffs of both for multiple popular LLMs, including
Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,
including extracting information from PDFs, generating questions and answers,
using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We
propose metrics to assess the performance of different stages of the RAG and
fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.
Agriculture as an industry has not seen much penetration of AI, and we study a
potentially disruptive application - what if we could provide location-specific
insights to a farmer? Our results show the effectiveness of our dataset
generation pipeline in capturing geographic-specific knowledge, and the
quantitative and qualitative benefits of RAG and fine-tuning. We see an
accuracy increase of over 6 p.p. when fine-tuning the model and this is
cumulative with RAG, which increases accuracy by 5 p.p. further. In one
particular experiment, we also demonstrate that the fine-tuned model leverages
information from across geographies to answer specific questions, increasing
answer similarity from 47% to 72%. Overall, the results point to how systems
built using LLMs can be adapted to respond and incorporate knowledge across a
dimension that is critical for a specific industry, paving the way for further
applications of LLMs in other industrial domains.",Angels Balaguer
2024-02-12T18:28:36Z,http://arxiv.org/abs/2402.07867v3,"PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented
  Generation of Large Language Models","Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate these limitations. The key idea of RAG is to ground the answer
generation of an LLM on external knowledge retrieved from a knowledge database.
Existing studies mainly focus on improving the accuracy or efficiency of RAG,
leaving its security largely unexplored. We aim to bridge the gap in this work.
We find that the knowledge database in a RAG system introduces a new and
practical attack surface. Based on this attack surface, we propose PoisonedRAG,
the first knowledge corruption attack to RAG, where an attacker could inject a
few malicious texts into the knowledge database of a RAG system to induce an
LLM to generate an attacker-chosen target answer for an attacker-chosen target
question. We formulate knowledge corruption attacks as an optimization problem,
whose solution is a set of malicious texts. Depending on the background
knowledge (e.g., black-box and white-box settings) of an attacker on a RAG
system, we propose two solutions to solve the optimization problem,
respectively. Our results show PoisonedRAG could achieve a 90% attack success
rate when injecting five malicious texts for each target question into a
knowledge database with millions of texts. We also evaluate several defenses
and our results show they are insufficient to defend against PoisonedRAG,
highlighting the need for new defenses.",Wei Zou
2024-06-07T08:43:07Z,http://arxiv.org/abs/2406.04744v2,CRAG -- Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.",Xiao Yang
2024-07-19T17:35:47Z,http://arxiv.org/abs/2407.14482v2,"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities","In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K
context window, designed to bridge the gap between open-source LLMs and leading
proprietary models (e.g., GPT-4-Turbo) in long-context understanding and
retrieval-augmented generation (RAG) capabilities. These two capabilities are
essential for LLMs to process large volumes of information that cannot fit into
a single prompt and are complementary to each other, depending on the
downstream tasks and computational budgets. We present a detailed continued
training recipe to extend the context window of Llama3-70B-base from 8K to 128K
tokens, along with a three-stage instruction tuning process to enhance the
model's instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
outperforms most existing state-of-the-art models, including
GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on
ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only
a 4K context window, showing the strong long context capability across varying
sequence lengths. We further provide extensive comparisons between direct
long-context and RAG solutions using the same state-of-the-art long-context
LLMs. Interestingly, we find that the performance of strong long-context LLMs
using RAG improves when retrieving a larger number of chunks. With a large set
of top-k chunks, RAG consistently outperforms direct long-context solution
using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B
and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To
advance research in this field, we open-sourced the model weights, training
data, and the evaluation setup for the for the community:
https://chatqa2-project.github.io/",Peng Xu
2024-07-19T18:08:39Z,http://arxiv.org/abs/2407.14609v1,"Adversarial Databases Improve Success in Retrieval-based Large Language
  Models","Open-source LLMs have shown great potential as fine-tuned chatbots, and
demonstrate robust abilities in reasoning and surpass many existing benchmarks.
Retrieval-Augmented Generation (RAG) is a technique for improving the
performance of LLMs on tasks that the models weren't explicitly trained on, by
leveraging external knowledge databases. Numerous studies have demonstrated the
effectiveness of RAG to more successfully accomplish downstream tasks when
using vector datasets that consist of relevant background information. It has
been implicitly assumed by those in the field that if adversarial background
information is utilized in this context, that the success of using a RAG-based
approach would be nonexistent or even negatively impact the results. To address
this assumption, we tested several open-source LLMs on the ability of RAG to
improve their success in answering multiple-choice questions (MCQ) in the
medical subspecialty field of Nephrology. Unlike previous studies, we examined
the effect of RAG in utilizing both relevant and adversarial background
databases. We set up several open-source LLMs, including Llama 3, Phi-3,
Mixtral 8x7b, Zephyr$\beta$, and Gemma 7B Instruct, in a zero-shot RAG
pipeline. As adversarial sources of information, text from the Bible and a
Random Words generated database were used for comparison. Our data show that
most of the open-source LLMs improve their multiple-choice test-taking success
as expected when incorporating relevant information vector databases.
Surprisingly however, adversarial Bible text significantly improved the success
of many LLMs and even random word text improved test taking ability of some of
the models. In summary, our results demonstrate for the first time the
countertintuitive ability of adversarial information datasets to improve the
RAG-based LLM success.",Sean Wu
2021-10-19T19:39:04Z,http://arxiv.org/abs/2110.10221v3,"The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal
  Padding","There is often variation in the shape and size of input data used for deep
learning. In many cases, such data can be represented using tensors with
non-uniform shapes, or ragged tensors. Due to limited and non-portable support
for efficient execution on ragged tensors, current deep learning frameworks
generally use techniques such as padding and masking to make the data shapes
uniform and then offload the computations to optimized kernels for dense tensor
algebra. Such techniques can, however, lead to a lot of wasted computation and
therefore, a loss in performance. This paper presents CoRa, a tensor compiler
that allows users to easily generate efficient code for ragged tensor operators
targeting a wide range of CPUs and GPUs. Evaluating CoRa on a variety of
operators on ragged tensors as well as on an encoder layer of the transformer
model, we find that CoRa (i)performs competitively with hand-optimized
implementations of the operators and the transformer encoder and (ii) achieves,
over PyTorch, a 1.6X geomean speedup for the encoder on an Nvidia GPU and a
1.86X geomean speedup for the multi-head attention module used in transformers
on an ARM CPU.",Pratik Fegade
2023-12-31T04:43:45Z,http://arxiv.org/abs/2401.00396v2,"RAGTruth: A Hallucination Corpus for Developing Trustworthy
  Retrieval-Augmented Language Models","Retrieval-augmented generation (RAG) has become a main technique for
alleviating hallucinations in large language models (LLMs). Despite the
integration of RAG, LLMs may still present unsupported or contradictory claims
to the retrieved contents. In order to develop effective hallucination
prevention strategies under RAG, it is important to create benchmark datasets
that can measure the extent of hallucination. This paper presents RAGTruth, a
corpus tailored for analyzing word-level hallucinations in various domains and
tasks within the standard RAG frameworks for LLM applications. RAGTruth
comprises nearly 18,000 naturally generated responses from diverse LLMs using
RAG. These responses have undergone meticulous manual annotations at both the
individual cases and word levels, incorporating evaluations of hallucination
intensity. We not only benchmark hallucination frequencies across different
LLMs, but also critically assess the effectiveness of several existing
hallucination detection methodologies. Furthermore, we show that using a
high-quality dataset such as RAGTruth, it is possible to finetune a relatively
small LLM and achieve a competitive level of performance in hallucination
detection when compared to the existing prompt-based approaches using
state-of-the-art large language models such as GPT-4.",Cheng Niu
2024-02-11T12:25:41Z,http://arxiv.org/abs/2402.07179v3,"Prompt Perturbation in Retrieval-Augmented Generation based Large
  Language Models","The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.",Zhibo Hu
2024-02-12T08:45:08Z,http://arxiv.org/abs/2402.07483v2,T-RAG: Lessons from the LLM Trenches,"Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations, including a Needle in a Haystack test, show that this combination
performs better than a simple RAG or finetuning implementation. Finally, we
share some lessons learned based on our experiences building an LLM application
for real-world use.",Masoomali Fatehkia
2024-02-27T13:22:51Z,http://arxiv.org/abs/2402.17497v2,"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering","Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (eg., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness regarding the reliability of external knowledge for LLMs, so
as to adaptively utilize external knowledge in RAG systems. Specially, we
develop a novel architecture for LLM-based RAG systems, by incorporating a
specially designed assessment module that precisely assesses the relevance of
retrieved documents. Furthermore, we propose an improved training method based
on bi-granularity relevance fusion and noise-resistant training. By combining
the improvements in both architecture and training, our proposed REAR can
better utilize external knowledge by effectively perceiving the relevance of
retrieved documents. Experiments on four open-domain QA tasks show that REAR
significantly outperforms previous a number of competitive RAG approaches. Our
codes can be accessed at https://github.com/RUCAIBox/REAR.",Yuhao Wang
2024-02-27T19:08:05Z,http://arxiv.org/abs/2402.17840v3,"Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems","Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. We also study multiple
effects of RAG setup on the extractability of data, indicating that following
unexpected instructions to regurgitate data can be an outcome of failure in
effectively utilizing contexts for modern LMs, and further show that such
vulnerability can be greatly mitigated by position bias elimination strategies.
Extending our study to production RAG models GPTs, we design an attack that can
cause datastore leakage with a 100% success rate on 25 randomly selected
customized GPTs with at most 2 queries, and we extract text data verbatim at a
rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words
by prompting the GPTs with only 100 queries generated by themselves.",Zhenting Qi
2024-03-03T08:07:55Z,http://arxiv.org/abs/2403.01432v5,"Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge","Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting
strong performance across diverse tasks and domains. However, it has been
observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LMs on
low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning
(FT) over synthetic data. This paper explores and evaluates the impact of RAG
and FT on customizing LMs in handling low-frequency entities on question
answering tasks. We conduct extensive experiments on twelve LMs of varying size
and type and different fine tuning, data augmentation, and retrieval models.
Our findings indicate that while FT boosts the performance across entities of
varying popularity, RAG surpasses FT by a large margin particularly for least
popular factual knowledge. Additionally, the success of both RAG and FT
approaches is amplified by improving retrieval and data augmentation
techniques. Fine tuning, while beneficial for small LMs, requires extensive
resources. To address this issue, we propose the new Stimulus RAG approach that
surpasses the effectiveness of fine tuning based approaches, thereby
eliminating the need for the costly data augmentation and fine tuning step for
enriching LMs with less popular factual knowledge. The code is available at
\url{https://github.com/informagi/RAGvsFT}.",Heydar Soudani
2024-04-04T02:58:21Z,http://arxiv.org/abs/2404.04287v1,CONFLARE: CONFormal LArge language model REtrieval,"Retrieval-augmented generation (RAG) frameworks enable large language models
(LLMs) to retrieve relevant information from a knowledge base and incorporate
it into the context for generating responses. This mitigates hallucinations and
allows for the updating of knowledge without retraining the LLM. However, RAG
does not guarantee valid responses if retrieval fails to identify the necessary
information as the context for response generation. Also, if there is
contradictory content, the RAG response will likely reflect only one of the two
possible responses. Therefore, quantifying uncertainty in the retrieval process
is crucial for ensuring RAG trustworthiness. In this report, we introduce a
four-step framework for applying conformal prediction to quantify retrieval
uncertainty in RAG frameworks. First, a calibration set of questions answerable
from the knowledge base is constructed. Each question's embedding is compared
against document embeddings to identify the most relevant document chunks
containing the answer and record their similarity scores. Given a
user-specified error rate ({\alpha}), these similarity scores are then analyzed
to determine a similarity score cutoff threshold. During inference, all chunks
with similarity exceeding this threshold are retrieved to provide context to
the LLM, ensuring the true answer is captured in the context with a
(1-{\alpha}) confidence level. We provide a Python package that enables users
to implement the entire workflow proposed in our work, only using LLMs and
without human intervention.",Pouria Rouzrokh
2024-04-18T18:32:30Z,http://arxiv.org/abs/2404.12457v2,RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has shown significant improvements in
various natural language processing tasks by integrating the strengths of large
language models (LLMs) and external knowledge databases. However, RAG
introduces long sequence generation and leads to high computation and memory
costs. We propose RAGCache, a novel multilevel dynamic caching system tailored
for RAG. Our analysis benchmarks current RAG systems, pinpointing the
performance bottleneck (i.e., long sequence due to knowledge injection) and
optimization opportunities (i.e., caching knowledge's intermediate states).
Based on these insights, we design RAGCache, which organizes the intermediate
states of retrieved knowledge in a knowledge tree and caches them in the GPU
and host memory hierarchy. RAGCache proposes a replacement policy that is aware
of LLM inference characteristics and RAG retrieval patterns. It also
dynamically overlaps the retrieval and inference steps to minimize the
end-to-end latency. We implement RAGCache and evaluate it on vLLM, a
state-of-the-art LLM inference system and Faiss, a state-of-the-art vector
database. The experimental results show that RAGCache reduces the time to first
token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to
vLLM integrated with Faiss.",Chao Jin
2024-05-05T18:32:06Z,http://arxiv.org/abs/2405.06681v1,"Leveraging Lecture Content for Improved Feedback: Explorations with
  GPT-4 and Retrieval Augmented Generation","This paper presents the use of Retrieval Augmented Generation (RAG) to
improve the feedback generated by Large Language Models for programming tasks.
For this purpose, corresponding lecture recordings were transcribed and made
available to the Large Language Model GPT-4 as external knowledge source
together with timestamps as metainformation by using RAG. The purpose of this
is to prevent hallucinations and to enforce the use of the technical terms and
phrases from the lecture. In an exercise platform developed to solve
programming problems for an introductory programming lecture, students can
request feedback on their solutions generated by GPT-4. For this task GPT-4
receives the students' code solution, the compiler output, the result of unit
tests and the relevant passages from the lecture notes available through the
use of RAG as additional context. The feedback generated by GPT-4 should guide
students to solve problems independently and link to the lecture content, using
the time stamps of the transcript as meta-information. In this way, the
corresponding lecture videos can be viewed immediately at the corresponding
positions. For the evaluation, students worked with the tool in a workshop and
decided for each feedback whether it should be extended by RAG or not. First
results based on a questionnaire and the collected usage data show that the use
of RAG can improve feedback generation and is preferred by students in some
situations. Due to the slower speed of feedback generation, the benefits are
situation dependent.",Sven Jacobs
2024-05-30T21:19:24Z,http://arxiv.org/abs/2405.20485v2,"Phantom: General Trigger Attacks on Retrieval Augmented Language
  Generation","Retrieval Augmented Generation (RAG) expands the capabilities of modern large
language models (LLMs), by anchoring, adapting, and personalizing their
responses to the most relevant knowledge sources. It is particularly useful in
chatbot applications, allowing developers to customize LLM output without
expensive retraining. Despite their significant utility in various
applications, RAG systems present new security risks. In this work, we propose
new attack vectors that allow an adversary to inject a single malicious
document into a RAG system's knowledge base, and mount a backdoor poisoning
attack. We design Phantom, a general two-stage optimization framework against
RAG systems, that crafts a malicious poisoned document leading to an integrity
violation in the model's output. First, the document is constructed to be
retrieved only when a specific trigger sequence of tokens appears in the
victim's queries. Second, the document is further optimized with crafted
adversarial text that induces various adversarial objectives on the LLM output,
including refusal to answer, reputation damage, privacy violations, and harmful
behaviors. We demonstrate our attacks on multiple LLM architectures, including
Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and
GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's
black-box production RAG system, ""Chat with RTX"".",Harsh Chaudhari
2024-05-24T16:36:47Z,http://arxiv.org/abs/2406.00029v1,Clustered Retrieved Augmented Generation (CRAG),"Providing external knowledge to Large Language Models (LLMs) is a key point
for using these models in real-world applications for several reasons, such as
incorporating up-to-date content in a real-time manner, providing access to
domain-specific knowledge, and contributing to hallucination prevention. The
vector database-based Retrieval Augmented Generation (RAG) approach has been
widely adopted to this end. Thus, any part of external knowledge can be
retrieved and provided to some LLM as the input context. Despite RAG approach's
success, it still might be unfeasible for some applications, because the
context retrieved can demand a longer context window than the size supported by
LLM. Even when the context retrieved fits into the context window size, the
number of tokens might be expressive and, consequently, impact costs and
processing time, becoming impractical for most applications. To address these,
we propose CRAG, a novel approach able to effectively reduce the number of
prompting tokens without degrading the quality of the response generated
compared to a solution using RAG. Through our experiments, we show that CRAG
can reduce the number of tokens by at least 46\%, achieving more than 90\% in
some cases, compared to RAG. Moreover, the number of tokens with CRAG does not
increase considerably when the number of reviews analyzed is higher, unlike
RAG, where the number of tokens is almost 9x higher when there are 75 reviews
compared to 4 reviews.",Simon Akesson
2024-05-29T18:19:46Z,http://arxiv.org/abs/2406.00057v2,"Toward Conversational Agents with Context and Time Sensitive Long-term
  Memory","There has recently been growing interest in conversational agents with
long-term memory which has led to the rapid development of language models that
use retrieval-augmented generation (RAG). Until recently, most work on RAG has
focused on information retrieval from large databases of texts, like Wikipedia,
rather than information from long-form conversations. In this paper, we argue
that effective retrieval from long-form conversational data faces two unique
problems compared to static database retrieval: 1) time/event-based queries,
which requires the model to retrieve information about previous conversations
based on time or the order of a conversational event (e.g., the third
conversation on Tuesday), and 2) ambiguous queries that require surrounding
conversational context to understand. To better develop RAG-based agents that
can deal with these challenges, we generate a new dataset of ambiguous and
time-based questions that build upon a recent dataset of long-form, simulated
conversations, and demonstrate that standard RAG based approaches handle such
questions poorly. We then develop a novel retrieval model which combines
chained-of-table search methods, standard vector-database retrieval, and a
prompting method to disambiguate queries, and demonstrate that this approach
substantially improves over current methods at solving these tasks. We believe
that this new dataset and more advanced RAG agent can act as a key benchmark
and stepping stone towards effective memory augmented conversational agents
that can be used in a wide variety of AI applications.",Nick Alonso
2024-06-11T15:15:33Z,http://arxiv.org/abs/2406.07348v3,"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented
  Generation for Question-Answering","Retrieval-Augmented Generation (RAG) has recently demonstrated the
performance of Large Language Models (LLMs) in the knowledge-intensive tasks
such as Question-Answering (QA). RAG expands the query context by incorporating
external knowledge bases to enhance the response accuracy. However, it would be
inefficient to access LLMs multiple times for each query and unreliable to
retrieve all the relevant documents by a single query. We have found that even
though there is low relevance between some critical documents and query, it is
possible to retrieve the remaining documents by combining parts of the
documents with the query. To mine the relevance, a two-stage retrieval
framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is
proposed to improve document retrieval recall and the accuracy of answers while
maintaining efficiency. Additionally, a compact classifier is applied to two
different selection strategies to determine the contribution of the retrieved
documents to answering the query and retrieve the relatively relevant
documents. Meanwhile, DR-RAG call the LLMs only once, which significantly
improves the efficiency of the experiment. The experimental results on
multi-hop QA datasets show that DR-RAG can significantly improve the accuracy
of the answers and achieve new progress in QA systems.",Zijian Hei
2024-07-03T01:28:51Z,http://arxiv.org/abs/2407.02742v1,"A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized
  Retrieval Augmentation","Natural Language to Code Generation has made significant progress in recent
years with the advent of Large Language Models(LLMs). While generation for
general-purpose languages like C, C++, and Python has improved significantly,
LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs
having a high number of custom function names. Additionally, constant updates
to function names add to the challenge as LLMs need to stay up-to-date. In this
paper, we present optimizations for using Retrieval Augmented Generation (or
RAG) with LLMs for DSL generation along with an ablation study comparing these
strategies. We generated a train as well as test dataset with a DSL to
represent automation tasks across roughly 700 APIs in public domain. We used
the training dataset to fine-tune a Codex model for this DSL. Our results
showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The
compilation rate, however, showed that both the models still got the syntax
wrong many times, with RAG-based method being 2 pts better. Conversely,
hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for
API parameter keys. We conclude that an optimized RAG model can match the
quality of fine-tuned models and offer advantages for new, unseen APIs.",Nastaran Bassamzadeh
2024-08-08T03:11:12Z,http://arxiv.org/abs/2408.04187v2,"Medical Graph RAG: Towards Safe Medical Large Language Model via Graph
  Retrieval-Augmented Generation","We introduce a novel graph-based Retrieval-Augmented Generation (RAG)
framework specifically designed for the medical domain, called
\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)
capabilities for generating evidence-based medical responses, thereby improving
safety and reliability when handling private medical data. Graph-based RAG
(GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong
potential for gaining holistic insights from long-form documents. However, its
standard implementation is overly complex for general use and lacks the ability
to generate evidence-based responses, limiting its effectiveness in the medical
field. To extend the capabilities of GraphRAG to the medical domain, we propose
unique Triple Graph Construction and U-Retrieval techniques over it. In our
graph construction, we create a triple-linked structure that connects user
documents to credible medical sources and controlled vocabularies. In the
retrieval process, we propose U-Retrieval which combines Top-down Precise
Retrieval with Bottom-up Response Refinement to balance global context
awareness with precise indexing. These effort enable both source information
retrieval and comprehensive response generation. Our approach is validated on 9
medical Q\&A benchmarks, 2 health fact-checking benchmarks, and one collected
dataset testing long-form generation. The results show that MedGraphRAG
consistently outperforms state-of-the-art models across all benchmarks, while
also ensuring that responses include credible source documentation and
definitions. Our code is released at:
https://github.com/MedicineToken/Medical-Graph-RAG.",Junde Wu
2024-08-16T20:55:21Z,http://arxiv.org/abs/2408.09017v1,Meta Knowledge for Retrieval Augmented Large Language Models,"Retrieval Augmented Generation (RAG) is a technique used to augment Large
Language Models (LLMs) with contextually relevant, time-critical, or
domain-specific information without altering the underlying model parameters.
However, constructing RAG systems that can effectively synthesize information
from large and diverse set of documents remains a significant challenge. We
introduce a novel data-centric RAG workflow for LLMs, transforming the
traditional retrieve-then-read system into a more advanced
prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher
domain expert-level understanding of the knowledge base. Our methodology relies
on generating metadata and synthetic Questions and Answers (QA) for each
document, as well as introducing the new concept of Meta Knowledge Summary (MK
Summary) for metadata-based clusters of documents. The proposed innovations
enable personalized user-query augmentation and in-depth information retrieval
across the knowledge base. Our research makes two significant contributions:
using LLMs as evaluators and employing new comparative performance metrics, we
demonstrate that (1) using augmented queries with synthetic question matching
significantly outperforms traditional RAG pipelines that rely on document
chunking (p < 0.01), and (2) meta knowledge-augmented queries additionally
significantly improve retrieval precision and recall, as well as the final
answers breadth, depth, relevancy, and specificity. Our methodology is
cost-effective, costing less than $20 per 2000 research papers using Claude 3
Haiku, and can be adapted with any fine-tuning of either the language or
embedding models to further enhance the performance of end-to-end RAG
pipelines.",Laurent Mombaerts
2024-08-21T07:20:48Z,http://arxiv.org/abs/2408.11381v2,"RAGLAB: A Modular and Research-Oriented Unified Framework for
  Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate human-level capabilities in
dialogue, reasoning, and knowledge retention. However, even the most advanced
LLMs face challenges such as hallucinations and real-time updating of their
knowledge. Current research addresses this bottleneck by equipping LLMs with
external knowledge, a technique known as Retrieval Augmented Generation (RAG).
However, two key issues constrained the development of RAG. First, there is a
growing lack of comprehensive and fair comparisons between novel RAG
algorithms. Second, open-source tools such as LlamaIndex and LangChain employ
high-level abstractions, which results in a lack of transparency and limits the
ability to develop novel algorithms and evaluation metrics. To close this gap,
we introduce RAGLAB, a modular and research-oriented open-source library.
RAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem
for investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair
comparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers
can efficiently compare the performance of various algorithms and develop novel
algorithms.",Xuanwang Zhang
2024-09-09T13:20:31Z,http://arxiv.org/abs/2409.05591v2,"MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge
  Discovery","Retrieval-Augmented Generation (RAG) leverages retrieval tools to access
external databases, thereby enhancing the generation quality of large language
models (LLMs) through optimized context. However, the existing retrieval
methods are constrained inherently, as they can only perform relevance matching
between explicitly stated queries and well-formed knowledge, but unable to
handle tasks involving ambiguous information needs or unstructured knowledge.
Consequently, existing RAG systems are primarily effective for straightforward
question-answering tasks. In this work, we propose MemoRAG, a novel
retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG
adopts a dual-system architecture. On the one hand, it employs a light but
long-range LLM to form the global memory of database. Once a task is presented,
it generates draft answers, cluing the retrieval tools to locate useful
information within the database. On the other hand, it leverages an expensive
but expressive LLM, which generates the ultimate answer based on the retrieved
information. Building on this general framework, we further optimize MemoRAG's
performance by enhancing its cluing mechanism and memorization capacity. In our
experiment, MemoRAG achieves superior performance across a variety of
evaluation tasks, including both complex ones where conventional RAG fails and
straightforward ones where RAG is commonly applied.",Hongjin Qian
2024-09-29T15:40:54Z,http://arxiv.org/abs/2409.19745v2,"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Generation with Zero Inference Overhead","Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.",Tao Tan
2024-10-13T02:34:47Z,http://arxiv.org/abs/2410.09699v1,"Honest AI: Fine-Tuning ""Small"" Language Models to Say ""I Don't Know"",
  and Reducing Hallucination in RAG","Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune ""small"" language models to say ""I don't know"" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.",Xinxi Chen
2024-10-17T16:18:49Z,http://arxiv.org/abs/2410.13716v1,"MIRAGE-Bench: Automatic Multilingual Benchmark Arena for
  Retrieval-Augmented Generation Systems","Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different
heuristic-based metrics for evaluation, but these require human preferences as
ground truth for reference. In contrast, arena-based benchmarks, where two
models compete each other, require an expensive Large Language Model (LLM) as a
judge for a reliable evaluation. We present an easy and efficient technique to
get the best of both worlds. The idea is to train a learning to rank model as a
""surrogate"" judge using RAG-based evaluation heuristics as input, to produce a
synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a
standardized arena-based multilingual RAG benchmark for 18 diverse languages on
Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and
extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG
extensively coupling both heuristic features and LLM as a judge evaluator. In
our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high
correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned
using heuristic features with pairwise evaluations and between GPT-4o as a
teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We
observe proprietary and large open-source LLMs currently dominate in
multilingual RAG. MIRAGE-Bench is available at:
https://github.com/vectara/mirage-bench.",Nandan Thakur
2024-10-20T03:51:01Z,http://arxiv.org/abs/2410.15267v1,"When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep
  Secret or Forget Knowledge?","The deployment of large language models (LLMs) like ChatGPT and Gemini has
shown their powerful natural language generation capabilities. However, these
models can inadvertently learn and retain sensitive information and harmful
content during training, raising significant ethical and legal concerns. To
address these issues, machine unlearning has been introduced as a potential
solution. While existing unlearning methods take into account the specific
characteristics of LLMs, they often suffer from high computational demands,
limited applicability, or the risk of catastrophic forgetting. To address these
limitations, we propose a lightweight unlearning framework based on
Retrieval-Augmented Generation (RAG) technology. By modifying the external
knowledge base of RAG, we simulate the effects of forgetting without directly
interacting with the unlearned LLM. We approach the construction of unlearned
knowledge as a constrained optimization problem, deriving two key components
that underpin the effectiveness of RAG-based unlearning. This RAG-based
approach is particularly effective for closed-source LLMs, where existing
unlearning methods often fail. We evaluate our framework through extensive
experiments on both open-source and closed-source models, including ChatGPT,
Gemini, Llama-2-7b-chat-hf, and PaLM 2. The results demonstrate that our
approach meets five key unlearning criteria: effectiveness, universality,
harmlessness, simplicity, and robustness. Meanwhile, this approach can extend
to multimodal large language models and LLM-based agents.",Shang Wang
2024-10-23T17:24:58Z,http://arxiv.org/abs/2410.18050v2,"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for
  Long-Context Question Answering","Long-Context Question Answering (LCQA), a challenging task, aims to reason
over long-context documents to yield accurate answers to questions. Existing
long-context Large Language Models (LLMs) for LCQA often struggle with the
""lost in the middle"" issue. Retrieval-Augmented Generation (RAG) mitigates this
issue by providing external factual evidence. However, its chunking strategy
disrupts the global long-context information, and its low-quality retrieval in
long contexts hinders LLMs from identifying effective factual details due to
substantial noise. To this end, we propose LongRAG, a general,
dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance
RAG's understanding of complex long-context knowledge (i.e., global information
and factual details). We design LongRAG as a plug-and-play paradigm,
facilitating adaptation to various domains and LLMs. Extensive experiments on
three multi-hop datasets demonstrate that LongRAG significantly outperforms
long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG
(up by 17.25%). Furthermore, we conduct quantitative ablation studies and
multi-dimensional analyses, highlighting the effectiveness of the system's
components and fine-tuning strategies. Data and code are available at
https://github.com/QingFei1/LongRAG.",Qingfei Zhao
2024-10-30T14:08:50Z,http://arxiv.org/abs/2410.23041v1,Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval,"As LLMs exhibit a high degree of human-like capability, increasing attention
has been paid to role-playing research areas in which responses generated by
LLMs are expected to mimic human replies. This has promoted the exploration of
role-playing agents in various applications, such as chatbots that can engage
in natural conversations with users and virtual assistants that can provide
personalized support and guidance. The crucial factor in the role-playing task
is the effective utilization of character memory, which stores characters'
profiles, experiences, and historical dialogues. Retrieval Augmented Generation
(RAG) technology is used to access the related memory to enhance the response
generation of role-playing agents. Most existing studies retrieve related
information based on the semantic similarity of memory to maintain characters'
personalized traits, and few attempts have been made to incorporate the
emotional factor in the retrieval argument generation (RAG) of LLMs. Inspired
by the Mood-Dependent Memory theory, which indicates that people recall an
event better if they somehow reinstate during recall the original emotion they
experienced during learning, we propose a novel emotion-aware memory retrieval
framework, termed Emotional RAG, which recalls the related memory with
consideration of emotional state in role-playing agents. Specifically, we
design two kinds of retrieval strategies, i.e., combination strategy and
sequential strategy, to incorporate both memory semantic and emotional states
during the retrieval process. Extensive experiments on three representative
role-playing datasets demonstrate that our Emotional RAG framework outperforms
the method without considering the emotional factor in maintaining the
personalities of role-playing agents. This provides evidence to further
reinforce the Mood-Dependent Memory theory in psychology.",Le Huang
2024-11-06T14:42:39Z,http://arxiv.org/abs/2411.03957v1,"Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.",Yuhang Liu
2024-11-10T18:45:41Z,http://arxiv.org/abs/2411.06558v2,"Region-Aware Text-to-Image Generation via Hard Binding and Soft
  Refinement","Regional prompting, or compositional generation, which enables fine-grained
spatial control, has gained increasing attention for its practicality in
real-world applications. However, previous methods either introduce additional
trainable modules, thus only applicable to specific models, or manipulate on
score maps within cross-attention layers using attention masks, resulting in
limited control strength when the number of regions increases. To handle these
limitations, we present RAG, a Regional-Aware text-to-image Generation method
conditioned on regional descriptions for precise layout composition. RAG
decouple the multi-region generation into two sub-tasks, the construction of
individual region (Regional Hard Binding) that ensures the regional prompt is
properly executed, and the overall detail refinement (Regional Soft Refinement)
over regions that dismiss the visual boundaries and enhance adjacent
interactions. Furthermore, RAG novelly makes repainting feasible, where users
can modify specific unsatisfied regions in the last generation while keeping
all other regions unchanged, without relying on additional inpainting models.
Our approach is tuning-free and applicable to other frameworks as an
enhancement to the prompt following property. Quantitative and qualitative
experiments demonstrate that RAG achieves superior performance over attribute
binding and object relationship than previous tuning-free methods.",Zhennan Chen
2024-11-11T14:25:37Z,http://arxiv.org/abs/2411.07021v2,Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation,"Retrieval-augmented generation (RAG) has shown impressive capability in
providing reliable answer predictions and addressing hallucination problems. A
typical RAG implementation uses powerful retrieval models to extract external
information and large language models (LLMs) to generate answers. In contrast,
recent LLM-based retrieval has gained attention for its substantial
improvements in information retrieval (IR) due to the LLMs' semantic
understanding capability. However, directly applying LLM to RAG systems
presents challenges. This may cause feature locality problems as massive
parametric knowledge can hinder effective usage of global information across
the corpus; for example, an LLM-based retriever often inputs document summaries
instead of full documents. Moreover, various pre-trained tasks in LLMs
introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning
architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever
is constructed by integrating LoRA-based representation learning to tackle
feature locality issues. To enhance retrieval performance, we develop two
patterns (invariant and variant patterns) and an invariance loss to reduce LLM
variance. In the generation stage, a refined fine-tuning method is employed to
improve LLM accuracy in generating answers based on retrieved information.
Experimental results show that Invar-RAG significantly outperforms existing
baselines across three open-domain question answering (ODQA) datasets. Code is
available in the Supplementary Material for reproducibility.",Ziwei Liu
2024-11-14T17:25:43Z,http://arxiv.org/abs/2411.09607v1,"Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the
  AutoNuggetizer Framework","This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on ""refactoring"" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.",Ronak Pradeep
2024-11-25T16:10:05Z,http://arxiv.org/abs/2411.16523v1,"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology
  Report Generation","In the current paradigm of image captioning, deep learning models are trained
to generate text from image embeddings of latent features. We challenge the
assumption that these latent features ought to be high-dimensional vectors
which require model fine tuning to handle. Here we propose Label Boosted
Retrieval Augmented Generation (LaB-RAG), a text-based approach to image
captioning that leverages image descriptors in the form of categorical labels
to boost standard retrieval augmented generation (RAG) with pretrained large
language models (LLMs). We study our method in the context of radiology report
generation (RRG), where the task is to generate a clinician's report detailing
their observations from a set of radiological images, such as X-rays. We argue
that simple linear classifiers over extracted image embeddings can effectively
transform X-rays into text-space as radiology-specific labels. In combination
with standard RAG, we show that these derived text labels can be used with
general-domain LLMs to generate radiology reports. Without ever training our
generative language model or image feature encoder models, and without ever
directly ""showing"" the LLM an X-ray, we demonstrate that LaB-RAG achieves
better results across natural language and radiology language metrics compared
with other retrieval-based RRG methods, while attaining competitive results
compared to other fine-tuned vision-language RRG models. We further present
results of our experiments with various components of LaB-RAG to better
understand our method. Finally, we critique the use of a popular RRG metric,
arguing it is possible to artificially inflate its results without true
data-leakage.",Steven Song
2024-12-07T01:32:13Z,http://arxiv.org/abs/2412.06832v1,"SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to
  Question Answering","Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to
generalize to new information by decoupling reasoning capabilities from static
knowledge bases. Traditional RAG enhancements have explored vertical scaling --
assigning subtasks to specialized modules -- and horizontal scaling --
replicating tasks across multiple agents -- to improve performance. However,
real-world applications impose diverse Service Level Agreements (SLAs) and
Quality of Service (QoS) requirements, involving trade-offs among objectives
such as reducing cost, ensuring answer quality, and adhering to specific
operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG
tailored for real-world Question Answering (QA) applications. By integrating
task-specific non-functional requirements -- such as answer quality, cost, and
latency -- into the system, we enable dynamic reconfiguration to meet diverse
SLAs. Our method maps these Service Level Objectives (SLOs) to system-level
parameters, allowing the generation of optimal results within specified
resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic
re-orchestration of a multi-agent RAG system can effectively manage the
trade-off between answer quality and cost. By adjusting the system based on
query intent and operational conditions, we systematically balance performance
and resource utilization. This approach allows the system to meet SLOs for
various query types, showcasing its practicality for real-world applications.",Michael Iannelli
2024-12-18T12:45:55Z,http://arxiv.org/abs/2412.13799v1,"Enhancing Rhetorical Figure Annotation: An Ontology-Based Web
  Application with RAG Integration","Rhetorical figures play an important role in our communication. They are used
to convey subtle, implicit meaning, or to emphasize statements. We notice them
in hate speech, fake news, and propaganda. By improving the systems for
computational detection of rhetorical figures, we can also improve tasks such
as hate speech and fake news detection, sentiment analysis, opinion mining, or
argument mining. Unfortunately, there is a lack of annotated data, as well as
qualified annotators that would help us build large corpora to train machine
learning models for the detection of rhetorical figures. The situation is
particularly difficult in languages other than English, and for rhetorical
figures other than metaphor, sarcasm, and irony. To overcome this issue, we
develop a web application called ""Find your Figure"" that facilitates the
identification and annotation of German rhetorical figures. The application is
based on the German Rhetorical ontology GRhOOT which we have specially adapted
for this purpose. In addition, we improve the user experience with Retrieval
Augmented Generation (RAG). In this paper, we present the restructuring of the
ontology, the development of the web application, and the built-in RAG
pipeline. We also identify the optimal RAG settings for our application. Our
approach is one of the first to practically use rhetorical ontologies in
combination with RAG and shows promising results.",Ramona KÃ¼hn
2024-01-10T02:57:20Z,http://arxiv.org/abs/2401.06800v1,Reinforcement Learning for Optimizing RAG for Domain Chatbots,"With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.",Mandar Kulkarni
2024-01-29T16:03:29Z,http://arxiv.org/abs/2402.01741v2,"Development and Testing of a Novel Large Language Model-Based Clinical
  Decision Support Systems for Medication Safety in 12 Clinical Specialties","Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) framework as a Clinical Decision Support Systems (CDSS) to
support safe medication prescription.
  Objective: To evaluate the efficacy of LLM-based CDSS in correctly
identifying medication errors in different patient case vignettes from diverse
medical and surgical sub-disciplines, against a human expert panel derived
ground truth. We compared performance for under 2 different CDSS practical
healthcare integration modalities: LLM-based CDSS alone (fully autonomous mode)
vs junior pharmacist + LLM-based CDSS (co-pilot, assistive mode).
  Design, Setting, and Participants: Utilizing a RAG model with
state-of-the-art medically-related LLMs (GPT-4, Gemini Pro 1.0 and Med-PaLM 2),
this study used 61 prescribing error scenarios embedded into 23 complex
clinical vignettes across 12 different medical and surgical specialties. A
multidisciplinary expert panel assessed these cases for Drug-Related Problems
(DRPs) using the PCNE classification and graded severity / potential for harm
using revised NCC MERP medication error index. We compared.
  Results RAG-LLM performed better compared to LLM alone. When employed in a
co-pilot mode, accuracy, recall, and F1 scores were optimized, indicating
effectiveness in identifying moderate to severe DRPs. The accuracy of DRP
detection with RAG-LLM improved in several categories but at the expense of
lower precision.
  Conclusions This study established that a RAG-LLM based CDSS significantly
boosts the accuracy of medication error identification when used alongside
junior pharmacists (co-pilot), with notable improvements in detecting severe
DRPs. This study also illuminates the comparative performance of current
state-of-the-art LLMs in RAG-based CDSS systems.",Jasmine Chiat Ling Ong
2024-05-10T02:48:45Z,http://arxiv.org/abs/2405.06211v3,"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language
  Models","As one of the most advanced techniques in AI, Retrieval-Augmented Generation
(RAG) can offer reliable and up-to-date external knowledge, providing huge
convenience for numerous tasks. Particularly in the era of AI-Generated Content
(AIGC), the powerful capacity of retrieval in providing additional knowledge
enables RAG to assist existing generative AI in producing high-quality outputs.
Recently, Large Language Models (LLMs) have demonstrated revolutionary
abilities in language understanding and generation, while still facing inherent
limitations, such as hallucinations and out-of-date internal knowledge. Given
the powerful abilities of RAG in providing the latest and helpful auxiliary
information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged
to harness external and authoritative knowledge bases, rather than solely
relying on the model's internal knowledge, to augment the generation quality of
LLMs. In this survey, we comprehensively review existing research studies in
RA-LLMs, covering three primary technical perspectives: architectures, training
strategies, and applications. As the preliminary knowledge, we briefly
introduce the foundations and recent advances of LLMs. Then, to illustrate the
practical significance of RAG for LLMs, we systematically review mainstream
relevant work by their architectures, training strategies, and application
areas, detailing specifically the challenges of each and the corresponding
capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss
current limitations and several promising directions for future research.
Updated information about this survey can be found at
https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",Wenqi Fan
2024-06-03T02:25:33Z,http://arxiv.org/abs/2406.00083v2,"BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of
  Large Language Models","Large Language Models (LLMs) are constrained by outdated information and a
tendency to generate incorrect data, commonly referred to as ""hallucinations.""
Retrieval-Augmented Generation (RAG) addresses these limitations by combining
the strengths of retrieval-based methods and generative models. This approach
involves retrieving relevant information from a large, up-to-date dataset and
using it to enhance the generation process, leading to more accurate and
contextually appropriate responses. Despite its benefits, RAG introduces a new
attack surface for LLMs, particularly because RAG databases are often sourced
from public data, such as the web. In this paper, we propose \TrojRAG{} to
identify the vulnerabilities and attacks on retrieval parts (RAG database) and
their indirect attacks on generative parts (LLMs). Specifically, we identify
that poisoning several customized content passages could achieve a retrieval
backdoor, where the retrieval works well for clean queries but always returns
customized poisoned adversarial queries. Triggers and poisoned passages can be
highly customized to implement various attacks. For example, a trigger could be
a semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial
passages can be tailored to different contents, not only linked to the triggers
but also used to indirectly attack generative LLMs without modifying them.
These attacks can include denial-of-service attacks on RAG and semantic
steering attacks on LLM generations conditioned by the triggers. Our
experiments demonstrate that by just poisoning 10 adversarial passages can
induce 98.2\% success rate to retrieve the adversarial passages. Then, these
passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\%
or increase the rate of negative responses from 0.22\% to 72\% for targeted
queries.",Jiaqi Xue
2024-10-11T00:34:20Z,http://arxiv.org/abs/2410.08431v1,"oRetrieval Augmented Generation for 10 Large Language Models and its
  Generalizability in Assessing Medical Fitness","Large Language Models (LLMs) show potential for medical applications but
often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)
allows customization with domain-specific information, making it suitable for
healthcare. This study evaluates the accuracy, consistency, and safety of RAG
models in determining fitness for surgery and providing preoperative
instructions. We developed LLM-RAG models using 35 local and 23 international
preoperative guidelines and tested them against human-generated responses. A
total of 3,682 responses were evaluated. Clinical documents were processed
using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were
assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects
of preoperative instructions. Established guidelines and expert judgment were
used to determine correct responses, with human-generated answers serving as
comparisons. The LLM-RAG models generated responses within 20 seconds,
significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model
achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no
hallucinations and producing correct instructions comparable to clinicians.
Results were consistent across both local and international guidelines. This
study demonstrates the potential of LLM-RAG models for preoperative healthcare
tasks, highlighting their efficiency, scalability, and reliability.",Yu He Ke
2023-08-01T12:04:50Z,http://arxiv.org/abs/2308.00479v1,"Retrieval Augmented Generation and Representative Vector Summarization
  for large unstructured textual data in Medical Education","Large Language Models are increasingly being used for various tasks including
content generation and as chatbots. Despite their impressive performances in
general tasks, LLMs need to be aligned when applying for domain specific tasks
to mitigate the problems of hallucination and producing harmful answers.
Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a
non-parametric knowledgebases to LLMs. Applications of RAG in the field of
medical education are discussed in this paper. A combined extractive and
abstractive summarization method for large unstructured textual data using
representative vectors is proposed.",S. S. Manathunga
2024-03-18T02:01:58Z,http://arxiv.org/abs/2403.11413v1,"Dynamic Contexts for Generating Suggestion Questions in RAG Based
  Conversational Systems","When interacting with Retrieval-Augmented Generation (RAG)-based
conversational agents, the users must carefully craft their queries to be
understood correctly. Yet, understanding the system's capabilities can be
challenging for the users, leading to ambiguous questions that necessitate
further clarification. This work aims to bridge the gap by developing a
suggestion question generator. To generate suggestion questions, our approach
involves utilizing dynamic context, which includes both dynamic few-shot
examples and dynamically retrieved contexts. Through experiments, we show that
the dynamic contexts approach can generate better suggestion questions as
compared to other prompting approaches.",Anuja Tayal
2024-04-18T09:58:51Z,http://arxiv.org/abs/2404.12045v2,"RAM: Towards an Ever-Improving Memory System by Learning from
  Communications","We introduce an innovative RAG-based framework with an ever-improving memory.
Inspired by humans'pedagogical process, RAM utilizes recursively
reasoning-based retrieval and experience reflections to continually update the
memory and learn from users' communicative feedback, namely communicative
learning. Extensive experiments with both simulated and real users demonstrate
significant improvements over traditional RAG and self-knowledge methods,
particularly excelling in handling false premise and multi-hop questions.
Furthermore, RAM exhibits promising adaptability to various feedback and
retrieval methods, showcasing its potential for advancing AI capabilities in
dynamic knowledge acquisition and lifelong learning.",Jiaqi Li
2024-05-13T09:17:19Z,http://arxiv.org/abs/2405.13008v1,Control Token with Dense Passage Retrieval,"This study addresses the hallucination problem in large language models
(LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a
technique that involves embedding relevant information in the prompt to obtain
accurate answers. However, RAG also faced inherent issues in retrieving correct
information. To address this, we employed the Dense Passage Retrieval(DPR)
(Karpukhin et al., 2020) model for fetching domain-specific documents related
to user queries. Despite this, the DPR model still lacked accuracy in document
retrieval. We enhanced the DPR model by incorporating control tokens, achieving
significantly superior performance over the standard DPR model, with a 13%
improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",Juhwan Lee
2024-06-18T09:53:37Z,http://arxiv.org/abs/2406.12449v1,"Retrieval-Augmented Generation for Generative Artificial Intelligence in
  Medicine","Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care.",Rui Yang
2024-07-19T12:28:22Z,http://arxiv.org/abs/2407.14246v2,Unipa-GPT: Large Language Models for university-oriented QA in Italian,"This paper illustrates the architecture and training of Unipa-GPT, a chatbot
relying on a Large Language Model, developed for assisting students in choosing
a bachelor/master degree course at the University of Palermo. Unipa-GPT relies
on gpt-3.5-turbo, it was presented in the context of the European Researchers'
Night (SHARPER night). In our experiments we adopted both the Retrieval
Augmented Generation (RAG) approach and fine-tuning to develop the system. The
whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned
systems are compared, and a brief discussion on their performance is reported.
Further comparison with other Large Language Models and the experimental
results during the SHARPER night are illustrated.",Irene Siragusa
2024-08-05T20:21:54Z,http://arxiv.org/abs/2408.02811v1,Development of REGAI: Rubric Enabled Generative Artificial Intelligence,"This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology.",Zach Johnson
2024-10-15T14:42:18Z,http://arxiv.org/abs/2410.11655v1,Retrieval Augmented Spelling Correction for E-Commerce Applications,"The rapid introduction of new brand names into everyday language poses a
unique challenge for e-commerce spelling correction services, which must
distinguish genuine misspellings from novel brand names that use unconventional
spelling. We seek to address this challenge via Retrieval Augmented Generation
(RAG). On this approach, product names are retrieved from a catalog and
incorporated into the context used by a large language model (LLM) that has
been fine-tuned to do contextual spelling correction. Through quantitative
evaluation and qualitative error analyses, we find improvements in spelling
correction utilizing the RAG framework beyond a stand-alone LLM. We also
demonstrate the value of additional finetuning of the LLM to incorporate
retrieved context.",Xuan Guo
2024-10-16T21:53:48Z,http://arxiv.org/abs/2410.13070v1,Is Semantic Chunking Worth the Computational Cost?,"Recent advances in Retrieval-Augmented Generation (RAG) systems have
popularized semantic chunking, which aims to improve retrieval performance by
dividing documents into semantically coherent segments. Despite its growing
adoption, the actual benefits over simpler fixed-size chunking, where documents
are split into consecutive, fixed-size segments, remain unclear. This study
systematically evaluates the effectiveness of semantic chunking using three
common retrieval-related tasks: document retrieval, evidence retrieval, and
retrieval-based answer generation. The results show that the computational
costs associated with semantic chunking are not justified by consistent
performance gains. These findings challenge the previous assumptions about
semantic chunking and highlight the need for more efficient chunking strategies
in RAG systems.",Renyi Qu
2024-10-01T20:29:14Z,http://arxiv.org/abs/2410.13865v1,Classifying Peace in Global Media Using RAG and Intergroup Reciprocity,"This paper presents a novel approach to identifying insights of peace in
global media using a Retrieval Augmented Generation (RAG) model and concepts of
Positive and Negative Intergroup Reciprocity (PIR/NIR). By refining the
definitions of PIR and NIR, we offer a more accurate and meaningful analysis of
intergroup relations as represented in media articles. Our methodology provides
insights into the dynamics that contribute to or detract from peace at a
national level.",K. Lian
2024-11-08T19:44:12Z,http://arxiv.org/abs/2411.05934v1,"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for
  Bengali Mathematical Olympiad Problem Solving","We present an innovative approach for solving mathematical problems in
Bengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our
method uses advanced deep learning models, notably the Qwen 2.5 series, with
improvements made through prompt engineering, model quantization, and Tool
Integrated Reasoning (TIR) to handle complex calculations. Initially, we
explored various model architectures, including fine-tuned Mistral and
quantized Qwen models, refining them with translation techniques,
Retrieval-Augmented Generation (RAG), and custom dataset curation. Manual
hyperparameter tuning optimized parameters like temperature and top-p to
enhance model adaptability and accuracy. Removal of RAG and parameter
adjustments further improved robustness. Our approach highlights the potential
of advanced NLP techniques in solving Bengali mathematical problems.",Saad Tahmid
2024-11-16T03:06:39Z,http://arxiv.org/abs/2411.12759v1,"A Novel Approach to Eliminating Hallucinations in Large Language
  Model-Assisted Causal Discovery","The increasing use of large language models (LLMs) in causal discovery as a
substitute for human domain experts highlights the need for optimal model
selection. This paper presents the first hallucination survey of popular LLMs
for causal discovery. We show that hallucinations exist when using LLMs in
causal discovery so the choice of LLM is important. We propose using Retrieval
Augmented Generation (RAG) to reduce hallucinations when quality data is
available. Additionally, we introduce a novel method employing multiple LLMs
with an arbiter in a debate to audit edges in causal graphs, achieving a
comparable reduction in hallucinations to RAG.",Grace Sng
2013-05-24T14:47:40Z,http://arxiv.org/abs/1305.5756v1,Flooding edge or node weighted graphs,"Reconstruction closings have all properties of a physical flooding of a
topographic surface. They are precious for simplifying gradient images or,
filling unwanted catchment basins, on which a subsequent watershed transform
extracts the targeted objects. Flooding a topographic surface may be modeled as
flooding a node weighted graph (TG), with unweighted edges, the node weights
representing the ground level. The progression of a flooding may also be
modeled on the region adjacency graph (RAG) of a topographic surface. On a RAG
each node represents a catchment basin and edges connect neighboring nodes. The
edges are weighted by the altitude of the pass point between both adjacent
regions. The graph is flooded from sources placed at the marker positions and
each node is assigned to the source by which it has been flooded. The level of
the flood is represented on the nodes on each type of graphs. The same flooding
may thus be modeled on a TG or on a RAG. We characterize all valid floodings on
both types of graphs, as they should verify the laws of hydrostatics. We then
show that each flooding of a node weighted graph also is a flooding of an edge
weighted graph with appropriate edge weights. The highest flooding under a
ceiling function may be interpreted as the shortest distance to the root for
the ultrametric flooding distance in an augmented graph. The ultrametric
distance between two nodes is the minimal altitude of a flooding for which both
nodes are flooded. This remark permits to flood edge or node weighted graphs by
using shortest path algorithms. It appears that the collection of all lakes of
a RAG has the structure of a dendrogram, on which the highest flooding under a
ceiling function may be rapidly found.",Fernand Meyer
2023-07-12T04:44:31Z,http://arxiv.org/abs/2307.05915v2,"Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval
  Augmented Generation Models for Open Book Question-Answering","We propose a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation (RAG) model to the target domain using
supervised fine-tuning and reinforcement learning with synthetic feedback in a
few-shot setting. This, we hypothesize, will yield an aligned, uncertainty
calibrated model that is competitive with GPT-4 based in-context retrieval
augmented generation in generating relevant answers at lower serving costs. The
framework's synthetic generation pipeline will generate synthetic training data
comprising <passage, question, answer> tuples using an open-source LLM and a
novel consistency filtering scheme. The pipeline will be designed to generate
both abstractive and extractive questions that span the entire corpus. The
framework proposes to fine-tune a smaller RAG model comprising a dense
retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In
parallel, the framework will train a Reward model to score domain grounded
answers higher than hallucinated answers using an a priori relevance ordering
of synthetically assembled samples. In the next phase, the framework will align
the RAG model with the target domain using reinforcement learning (Proximal
Policy Optimization). This step may improve the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework will calibrate the model's uncertainty for extractive
question-answers.",C. S. Krishna
2023-09-03T07:03:17Z,http://arxiv.org/abs/2309.01105v2,"A Study on the Implementation of Generative AI Services Using an
  Enterprise Data-Based LLM Application Architecture","This study presents a method for implementing generative AI services by
utilizing the Large Language Models (LLM) application architecture. With recent
advancements in generative AI technology, LLMs have gained prominence across
various domains. In this context, the research addresses the challenge of
information scarcity and proposes specific remedies by harnessing LLM
capabilities. The investigation delves into strategies for mitigating the issue
of inadequate data, offering tailored solutions. The study delves into the
efficacy of employing fine-tuning techniques and direct document integration to
alleviate data insufficiency. A significant contribution of this work is the
development of a Retrieval-Augmented Generation (RAG) model, which tackles the
aforementioned challenges. The RAG model is carefully designed to enhance
information storage and retrieval processes, ensuring improved content
generation. The research elucidates the key phases of the information storage
and retrieval methodology underpinned by the RAG model. A comprehensive
analysis of these steps is undertaken, emphasizing their significance in
addressing the scarcity of data. The study highlights the efficacy of the
proposed method, showcasing its applicability through illustrative instances.
By implementing the RAG model for information storage and retrieval, the
research not only contributes to a deeper comprehension of generative AI
technology but also facilitates its practical usability within enterprises
utilizing LLMs. This work holds substantial value in advancing the field of
generative AI, offering insights into enhancing data-driven content generation
and fostering active utilization of LLM-based services within corporate
settings.",Cheonsu Jeong
2023-10-04T22:09:28Z,http://arxiv.org/abs/2310.03184v2,"Retrieval-augmented Generation to Improve Math Question-Answering:
  Trade-offs Between Groundedness and Human Preference","For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.",Zachary Levonian
2023-10-17T18:18:32Z,http://arxiv.org/abs/2310.11511v1,"Self-RAG: Learning to Retrieve, Generate, and Critique through
  Self-Reflection","Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.",Akari Asai
2023-10-31T03:52:08Z,http://arxiv.org/abs/2310.20158v1,GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval,"Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.",Daman Arora
2023-12-30T16:56:24Z,http://arxiv.org/abs/2401.00280v3,"Advancing TTP Analysis: Harnessing the Power of Large Language Models
  with Retrieval Augmented Generation","Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use
to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK
framework can be challenging for cybersecurity practitioners due to presumed
expertise and complex dependencies. Meanwhile, advancements with Large Language
Models (LLMs) have led to recent surge in studies exploring its uses in
cybersecurity operations. It is, however, unclear how LLMs can be used in an
efficient and proper way to provide accurate responses for critical domains
such as cybersecurity. This leads us to investigate how to better use two types
of LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only
(e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended
purposes (i.e., tactics) of a cyberattack procedure. This work studies and
compares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs.
Retrieval Augmented Generation (RAG) for decoder-only LLMs (without
fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with
relevant contexts for each cyberattack procedure. Our studies show decoder-only
LLMs with RAG achieves better performance than encoder-only models with SFT,
particularly when directly relevant context is extracted by RAG. The
decoder-only results could suffer low `Precision' while achieving high
`Recall'. Our findings further highlight a counter-intuitive observation that
more generic prompts tend to yield better predictions of cyberattack tactics
than those that are more specifically tailored.",Reza Fayyazi
2024-01-18T18:59:11Z,http://arxiv.org/abs/2401.10225v5,ChatQA: Surpassing GPT-4 on Conversational QA and RAG,"In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on
retrieval-augmented generation (RAG) and conversational question answering
(QA). To enhance generation, we propose a two-stage instruction tuning method
that significantly boosts the performance of RAG. For effective retrieval, we
introduce a dense retriever optimized for conversational QA, which yields
results comparable to the alternative state-of-the-art query rewriting models,
while substantially reducing deployment costs. We also present the ChatRAG
Bench, which encompasses ten datasets covering comprehensive evaluations on
RAG, table-related QA, arithmetic calculations, and scenarios involving
unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a
weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:
53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without
relying on any synthetic data from OpenAI GPT models. Notably, the
Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,
achieving a 4.4% improvement. To advance research in this field, we
open-sourced the model weights, instruction tuning data, ChatRAG Bench, and
retriever for the community: https://chatqa-project.github.io/.",Zihan Liu
2024-03-15T09:26:02Z,http://arxiv.org/abs/2403.10131v2,RAFT: Adapting Language Model to Domain Specific RAG,"Pretraining Large Language Models (LLMs) on large corpora of textual data is
now a standard paradigm. When using these LLMs for many downstream
applications, it is common to additionally bake in new knowledge (e.g.,
time-critical news, or private domain knowledge) into the pretrained model
either through RAG-based-prompting, or fine-tuning. However, the optimal
methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented FineTuning (RAFT), a training
recipe that improves the model's ability to answer questions in a ""open-book""
in-domain settings. In RAFT, given a question, and a set of retrieved
documents, we train the model to ignore those documents that don't help in
answering the question, which we call, distractor documents. RAFT accomplishes
this by citing verbatim the right sequence from the relevant document that
would help answer the question. This coupled with RAFT's chain-of-thought-style
response helps improve the model's ability to reason. In domain-specific RAG,
RAFT consistently improves the model's performance across PubMed, HotpotQA, and
Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs
to in-domain RAG. RAFT's code and demo are open-sourced at
github.com/ShishirPatil/gorilla.",Tianjun Zhang
2024-03-23T05:32:46Z,http://arxiv.org/abs/2403.15729v3,Towards a RAG-based Summarization Agent for the Electron-Ion Collider,"The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.",Karthik Suresh
2024-04-17T23:00:03Z,http://arxiv.org/abs/2404.11792v2,"Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning:
  A Comparative Study","This paper investigates the impact of domain-specific model fine-tuning and
of reasoning mechanisms on the performance of question-answering (Q&A) systems
powered by large language models (LLMs) and Retrieval-Augmented Generation
(RAG). Using the FinanceBench SEC financial filings dataset, we observe that,
for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves
better accuracy than generic models, with relatively greater gains attributable
to fine-tuned embedding models. Additionally, employing reasoning iterations on
top of RAG delivers an even bigger jump in performance, enabling the Q&A
systems to get closer to human-expert quality. We discuss the implications of
such findings, propose a structured technical design space capturing major
technical components of Q&A AI, and provide recommendations for making
high-impact technical choices for such components. We plan to follow up on this
work with actionable guides for AI teams and further investigations into the
impact of domain-specific augmentation in RAG and into agentic AI capabilities
such as advanced planning and reasoning.",Zooey Nguyen
2024-05-29T03:17:16Z,http://arxiv.org/abs/2405.18727v2,CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control,"Retrieval-augmented generation (RAG) has emerged as a promising solution for
mitigating hallucinations of large language models (LLMs) with retrieved
external knowledge. Adaptive RAG enhances this approach by enabling dynamic
retrieval during generation, activating retrieval only when the query exceeds
LLM's internal knowledge. Existing methods primarily focus on detecting LLM's
confidence via statistical uncertainty. Instead, we present the first attempts
to solve adaptive RAG from a representation perspective and develop an inherent
control-based framework, termed \name. Specifically, we extract the features
that represent the honesty and confidence directions of LLM and adopt them to
control LLM behavior and guide retrieval timing decisions. We also design a
simple yet effective query formulation strategy to support adaptive retrieval.
Experiments show that \name is superior to existing adaptive RAG methods on a
diverse set of tasks, the honesty steering can effectively make LLMs more
honest and confidence monitoring is a promising indicator of retrieval
trigger.Our code is available at \url{https://github.com/HSLiu-Initial/CtrlA}.",Huanshuo Liu
2024-05-31T14:23:49Z,http://arxiv.org/abs/2405.20834v1,"Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits
  Multimodal Reasoning","Large language models equipped with retrieval-augmented generation (RAG)
represent a burgeoning field aimed at enhancing answering capabilities by
leveraging external knowledge bases. Although the application of RAG with
language-only models has been extensively explored, its adaptation into
multimodal vision-language models remains nascent. Going beyond mere answer
generation, the primary goal of multimodal RAG is to cultivate the models'
ability to reason in response to relevant queries. To this end, we introduce a
novel multimodal RAG framework named RMR (Retrieval Meets Reasoning). The RMR
framework employs a bi-modal retrieval module to identify the most relevant
question-answer pairs, which then serve as scaffolds for the multimodal
reasoning process. This training-free approach not only encourages the model to
engage deeply with the reasoning processes inherent in the retrieved content
but also facilitates the generation of answers that are precise and richly
interpretable. Surprisingly, utilizing solely the ScienceQA dataset, collected
from elementary and high school science curricula, RMR significantly boosts the
performance of various vision-language models across a spectrum of benchmark
datasets, including A-OKVQA, MMBench, and SEED. These outcomes highlight the
substantial potential of our multimodal retrieval and reasoning mechanism to
improve the reasoning capabilities of vision-language models.",Cheng Tan
2024-06-03T07:44:32Z,http://arxiv.org/abs/2406.06566v4,"Natural Language Interaction with a Household Electricity
  Knowledge-based Digital Twin","Domain specific digital twins, representing a digital replica of various
segments of the smart grid, are foreseen as able to model, simulate, and
control the respective segments. At the same time, knowledge-based digital
twins, coupled with AI, may also empower humans to understand aspects of the
system through natural language interaction in view of planning and policy
making. This paper is the first to assess and report on the potential of
Retrieval Augmented Generation (RAG) question answers related to household
electrical energy measurement aspects leveraging a knowledge-based energy
digital twin. Relying on the recently published electricity consumption
knowledge graph that actually represents a knowledge-based digital twin, we
study the capabilities of ChatGPT, Gemini and Llama in answering electricity
related questions. Furthermore, we compare the answers with the ones generated
through a RAG techniques that leverages an existing electricity knowledge-based
digital twin. Our findings illustrate that the RAG approach not only reduces
the incidence of incorrect information typically generated by LLMs but also
significantly improves the quality of the output by grounding responses in
verifiable data. This paper details our methodology, presents a comparative
analysis of responses with and without RAG, and discusses the implications of
our findings for future applications of AI in specialized sectors like energy
data analysis.",Carolina Fortuna
2024-06-12T14:15:15Z,http://arxiv.org/abs/2406.08246v1,Leveraging Large Language Models for Web Scraping,"Large Language Models (LLMs) demonstrate remarkable capabilities in
replicating human tasks and boosting productivity. However, their direct
application for data extraction presents limitations due to a prioritisation of
fluency over factual accuracy and a restricted ability to manipulate specific
information. Therefore to overcome these limitations, this research leverages
the knowledge representation power of pre-trained LLMs and the targeted
information access enabled by RAG models, this research investigates a
general-purpose accurate data scraping recipe for RAG models designed for
language generation. To capture knowledge in a more modular and interpretable
way, we use pre trained language models with a latent knowledge retriever,
which allows the model to retrieve and attend over documents from a large
corpus. We utilised RAG model architecture and did an in-depth analysis of
their capabilities under three tasks: (i) Semantic Classification of HTML
elements, (ii) Chunking HTML text for effective understanding, and (iii)
comparing results from different LLMs and ranking algorithms. While previous
work has developed dedicated architectures and training procedures for HTML
understanding and extraction, we show that LLMs pre-trained on standard natural
language with an addition of effective chunking, searching and ranking
algorithms, can prove to be efficient data scraping tool to extract complex
data from unstructured text. Future research directions include addressing the
challenges of provenance tracking and dynamic knowledge updates within the
proposed RAG-based data extraction framework. By overcoming these limitations,
this approach holds the potential to revolutionise data extraction from vast
repositories of textual information.",Aman Ahluwalia
2024-06-17T09:25:10Z,http://arxiv.org/abs/2406.11357v2,"Refiner: Restructure Retrieval Content Efficiently to Advance
  Question-Answering Capabilities","Large Language Models (LLMs) are limited by their parametric knowledge,
leading to hallucinations in knowledge-extensive tasks. To address this,
Retrieval-Augmented Generation (RAG) incorporates external document chunks to
expand LLM knowledge. Furthermore, compressing information from document chunks
through extraction or summarization can improve LLM performance. Nonetheless,
LLMs still struggle to notice and utilize scattered key information, a problem
known as the ""lost-in-the-middle"" syndrome. Therefore, we typically need to
restructure the content for LLM to recognize the key information. We propose
$\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that
operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a
single decoder-only LLM to adaptively extract query-relevant contents verbatim
along with the necessary context, and section them based on their
interconnectedness, thereby highlights information distinction, and aligns
downstream LLMs with the original context effectively. Experiments show that a
trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to
downstream LLM in improving answer accuracy, and outperforms other
state-of-the-art advanced RAG and concurrent compressing approaches in various
single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5%
tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared
to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that
can be seamlessly integrated with RAG systems, facilitating its application
across diverse open-source frameworks.",Zhonghao Li
2024-06-19T15:25:29Z,http://arxiv.org/abs/2406.13629v2,"InstructRAG: Instructing Retrieval-Augmented Generation via
  Self-Synthesized Rationales","Retrieval-augmented generation (RAG) has shown promising potential to enhance
the accuracy and factuality of language models (LMs). However, imperfect
retrievers or noisy corpora can introduce misleading or even erroneous
information to the retrieved contents, posing a significant challenge to the
generation quality. Existing RAG methods typically address this challenge by
directly predicting final answers despite potentially noisy inputs, resulting
in an implicit denoising process that is difficult to interpret and verify. On
the other hand, the acquisition of explicit denoising supervision is often
costly, involving significant human efforts. In this work, we propose
InstructRAG, where LMs explicitly learn the denoising process through
self-synthesized rationales -- First, we instruct the LM to explain how the
ground-truth answer is derived from retrieved documents. Then, these rationales
can be used either as demonstrations for in-context learning of explicit
denoising or as supervised fine-tuning data to train the model. Compared to
standard RAG approaches, InstructRAG requires no additional supervision, allows
for easier verification of the predicted answers, and effectively improves
generation accuracy. Experiments show InstructRAG consistently outperforms
existing RAG methods in both training-free and trainable scenarios, achieving a
relative improvement of 8.3% over the best baseline method on average across
five knowledge-intensive benchmarks. Extensive analysis indicates that
InstructRAG scales well with increased numbers of retrieved documents and
consistently exhibits robust denoising ability even in out-of-domain datasets,
demonstrating strong generalizability.",Zhepei Wei
2024-06-19T16:10:26Z,http://arxiv.org/abs/2406.13663v4,"Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation","Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.",Jirui Qi
2024-06-20T10:04:09Z,http://arxiv.org/abs/2406.14162v3,"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation","Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.",Jingwei Ni
2024-07-01T05:28:40Z,http://arxiv.org/abs/2407.00978v2,"Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in
  Internet of Medical Things: A Diffusion-based Contract Approach","Secure data management and effective data sharing have become paramount in
the rapidly evolving healthcare landscape, especially with the growing
integration of the Internet of Medical Things (IoMT). The rise of generative
artificial intelligence has further elevated Multi-modal Large Language Models
(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.
MLLMs can support multi-modal inputs and generate diverse types of content by
leveraging large-scale training on vast amounts of multi-modal data. However,
critical challenges persist in developing medical MLLMs, including security and
freshness issues of healthcare data, affecting the output quality of MLLMs. To
this end, in this paper, we propose a hybrid Retrieval-Augmented Generation
(RAG)-empowered medical MLLM framework for healthcare data management. This
framework leverages a hierarchical cross-chain architecture to facilitate
secure data training. Moreover, it enhances the output quality of MLLMs through
hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG
results and incorporates these retrieval results as additional inputs to MLLMs.
Additionally, we employ age of information to indirectly evaluate the data
freshness impact of MLLMs and utilize contract theory to incentivize healthcare
data holders to share their fresh data, mitigating information asymmetry during
data sharing. Finally, we utilize a generative diffusion model-based deep
reinforcement learning algorithm to identify the optimal contract for efficient
data sharing. Numerical results demonstrate the effectiveness of the proposed
schemes, which achieve secure and efficient healthcare data management.",Cheng Su
2024-07-01T15:23:42Z,http://arxiv.org/abs/2407.01370v1,Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems,"LLMs and RAG systems are now capable of handling millions of input tokens or
more. However, evaluating the output quality of such systems on long-context
tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity.
In this work, we argue that summarization can play a central role in such
evaluation. We design a procedure to synthesize Haystacks of documents,
ensuring that specific \textit{insights} repeat across documents. The ""Summary
of a Haystack"" (SummHay) task then requires a system to process the Haystack
and generate, given a query, a summary that identifies the relevant insights
and precisely cites the source documents. Since we have precise knowledge of
what insights should appear in a haystack summary and what documents should be
cited, we implement a highly reproducible automatic evaluation that can score
summaries on two aspects - Coverage and Citation. We generate Haystacks in two
domains (conversation, news), and perform a large-scale evaluation of 10 LLMs
and corresponding 50 RAG systems. Our findings indicate that SummHay is an open
challenge for current systems, as even systems provided with an Oracle signal
of document relevance lag our estimate of human performance (56\%) by 10+
points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and
Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to
study enterprise RAG systems and position bias in long-context models. We hope
future systems can equal and surpass human performance on SummHay.",Philippe Laban
2024-07-04T04:30:04Z,http://arxiv.org/abs/2407.03627v5,"DSLR: Document Refinement with Sentence-Level Re-ranking and
  Reconstruction to Enhance Retrieval-Augmented Generation","Recent advancements in Large Language Models (LLMs) have significantly
improved their performance across various Natural Language Processing (NLP)
tasks. However, LLMs still struggle with generating non-factual responses due
to limitations in their parametric memory. Retrieval-Augmented Generation (RAG)
systems address this issue by incorporating external knowledge with a retrieval
module. Despite their successes, however, current RAG systems face challenges
with retrieval failures and the limited ability of LLMs to filter out
irrelevant information. Therefore, in this work, we propose DSLR (Document
Refinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised
framework that decomposes retrieved documents into sentences, filters out
irrelevant sentences, and reconstructs them again into coherent passages. We
experimentally validate DSLR on multiple open-domain QA datasets and the
results demonstrate that DSLR significantly enhances the RAG performance over
conventional fixed-size passage. Furthermore, our DSLR enhances performance in
specific, yet realistic scenarios without the need for additional training,
providing an effective and efficient solution for refining retrieved documents
in RAG systems.",Taeho Hwang
2024-07-17T17:59:47Z,http://arxiv.org/abs/2407.12784v1,"AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
  Bases","LLM agents have demonstrated remarkable performance across various
applications, primarily due to their advanced capabilities in reasoning,
utilizing external knowledge and tools, calling APIs, and executing actions to
interact with environments. Current agents typically utilize a memory module or
a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and
instances with similar embeddings from knowledge bases to inform task planning
and execution. However, the reliance on unverified knowledge bases raises
significant concerns about their safety and trustworthiness. To uncover such
vulnerabilities, we propose a novel red teaming approach AgentPoison, the first
backdoor attack targeting generic and RAG-based LLM agents by poisoning their
long-term memory or RAG knowledge base. In particular, we form the trigger
generation process as a constrained optimization to optimize backdoor triggers
by mapping the triggered instances to a unique embedding space, so as to ensure
that whenever a user instruction contains the optimized backdoor trigger, the
malicious demonstrations are retrieved from the poisoned memory or knowledge
base with high probability. In the meantime, benign instructions without the
trigger will still maintain normal performance. Unlike conventional backdoor
attacks, AgentPoison requires no additional model training or fine-tuning, and
the optimized backdoor trigger exhibits superior transferability, in-context
coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's
effectiveness in attacking three types of real-world LLM agents: RAG-based
autonomous driving agent, knowledge-intensive QA agent, and healthcare
EHRAgent. On each agent, AgentPoison achieves an average attack success rate
higher than 80% with minimal impact on benign performance (less than 1%) with a
poison rate less than 0.1%.",Zhaorun Chen
2024-07-15T17:40:15Z,http://arxiv.org/abs/2407.12873v1,Evaluation of RAG Metrics for Question Answering in the Telecom Domain,"Retrieval Augmented Generation (RAG) is widely used to enable Large Language
Models (LLMs) perform Question Answering (QA) tasks in various domains.
However, RAG based on open-source LLM for specialized domains has challenges of
evaluating generated responses. A popular framework in the literature is the
RAG Assessment (RAGAS), a publicly available library which uses LLMs for
evaluation. One disadvantage of RAGAS is the lack of details of derivation of
numerical value of the evaluation metrics. One of the outcomes of this work is
a modified version of this package for few metrics (faithfulness, context
relevance, answer relevance, answer correctness, answer similarity and factual
correctness) through which we provide the intermediate outputs of the prompts
by using any LLMs. Next, we analyse the expert evaluations of the output of the
modified RAGAS package and observe the challenges of using it in the telecom
domain. We also study the effect of the metrics under correct vs. wrong
retrieval and observe that few of the metrics have higher values for correct
retrieval. We also study for differences in metrics between base embeddings and
those domain adapted via pre-training and fine-tuning. Finally, we comment on
the suitability and challenges of using these metrics for in-the-wild telecom
QA task.",Sujoy Roychowdhury
2024-07-26T07:05:54Z,http://arxiv.org/abs/2407.18553v2,REAPER: Reasoning based Retrieval Planning for Complex RAG Systems,"Complex dialog systems often use retrieved evidence to facilitate factual
responses. Such RAG (Retrieval Augmented Generation) systems retrieve from
massive heterogeneous data stores that are usually architected as multiple
indexes or APIs instead of a single monolithic source. For a given query,
relevant evidence needs to be retrieved from one or a small subset of possible
retrieval sources. Complex queries can even require multi-step retrieval. For
example, a conversational agent on a retail site answering customer questions
about past orders will need to retrieve the appropriate customer order first
and then the evidence relevant to the customer's question in the context of the
ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by
interleaving reasoning and retrieval steps. However, each reasoning step
directly adds to the latency of the system. For large models this latency cost
is significant -- in the order of multiple seconds. Multi-agent systems may
classify the query to a single Agent associated with a retrieval source, though
this means that a (small) classification model dictates the performance of a
large language model. In this work we present REAPER (REAsoning-based PlannER)
- an LLM based planner to generate retrieval plans in conversational systems.
We show significant gains in latency over Agent-based systems and are able to
scale easily to new and unseen use cases as compared to classification-based
planning. Though our method can be applied to any RAG system, we show our
results in the context of a conversational shopping assistant.",Ashutosh Joshi
2024-08-19T06:05:24Z,http://arxiv.org/abs/2408.09713v2,"Carbon Footprint Accounting Driven by Large Language Models and
  Retrieval-augmented Generation","Carbon footprint accounting is crucial for quantifying greenhouse gas
emissions and achieving carbon neutrality.The dynamic nature of processes,
accounting rules, carbon-related policies, and energy supply structures
necessitates real-time updates of CFA. Traditional life cycle assessment
methods rely heavily on human expertise, making near-real-time updates
challenging. This paper introduces a novel approach integrating large language
models (LLMs) with retrieval-augmented generation technology to enhance the
real-time, professional, and economical aspects of carbon footprint information
retrieval and analysis. By leveraging LLMs' logical and language understanding
abilities and RAG's efficient retrieval capabilities, the proposed method
LLMs-RAG-CFA can retrieve more relevant professional information to assist
LLMs, enhancing the model's generative abilities. This method offers broad
professional coverage, efficient real-time carbon footprint information
acquisition and accounting, and cost-effective automation without frequent
LLMs' parameter updates. Experimental results across five industries(primary
aluminum, lithium battery, photovoltaic, new energy vehicles, and
transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional
methods and other LLMs, achieving higher information retrieval rates and
significantly lower information deviations and carbon footprint accounting
deviations. The economically viable design utilizes RAG technology to balance
real-time updates with cost-effectiveness, providing an efficient, reliable,
and cost-saving solution for real-time carbon emission management, thereby
enhancing environmental sustainability practices.",Haijin Wang
2024-08-26T08:17:42Z,http://arxiv.org/abs/2409.00090v1,Evaluating ChatGPT on Nuclear Domain-Specific Data,"This paper examines the application of ChatGPT, a large language model (LLM),
for question-and-answer (Q&A) tasks in the highly specialized field of nuclear
data. The primary focus is on evaluating ChatGPT's performance on a curated
test dataset, comparing the outcomes of a standalone LLM with those generated
through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their
recent advancements, are prone to generating incorrect or 'hallucinated'
information, which is a significant limitation in applications requiring high
accuracy and reliability. This study explores the potential of utilizing RAG in
LLMs, a method that integrates external knowledge bases and sophisticated
retrieval techniques to enhance the accuracy and relevance of generated
outputs. In this context, the paper evaluates ChatGPT's ability to answer
domain-specific questions, employing two methodologies: A) direct response from
the LLM, and B) response from the LLM within a RAG framework. The effectiveness
of these methods is assessed through a dual mechanism of human and LLM
evaluation, scoring the responses for correctness and other metrics. The
findings underscore the improvement in performance when incorporating a RAG
pipeline in an LLM, particularly in generating more accurate and contextually
appropriate responses for nuclear domain-specific queries. Additionally, the
paper highlights alternative approaches to further refine and improve the
quality of answers in such specialized domains.",Muhammad Anwar
2024-09-03T02:50:04Z,http://arxiv.org/abs/2409.01556v2,"Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka
  Culture","This study introduces a comprehensive benchmark designed to evaluate the
performance of large language models (LLMs) in understanding and processing
cultural knowledge, with a specific focus on Hakka culture as a case study.
Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework
that systematically assesses LLMs across six cognitive domains: Remembering,
Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark
extends beyond traditional single-dimensional evaluations by providing a deeper
analysis of LLMs' abilities to handle culturally specific content, ranging from
basic recall of facts to higher-order cognitive tasks such as creative
synthesis. Additionally, the study integrates Retrieval-Augmented Generation
(RAG) technology to address the challenges of minority cultural knowledge
representation in LLMs, demonstrating how RAG enhances the models' performance
by dynamically incorporating relevant external information. The results
highlight the effectiveness of RAG in improving accuracy across all cognitive
domains, particularly in tasks requiring precise retrieval and application of
cultural knowledge. However, the findings also reveal the limitations of RAG in
creative tasks, underscoring the need for further optimization. This benchmark
provides a robust tool for evaluating and comparing LLMs in culturally diverse
contexts, offering valuable insights for future research and development in
AI-driven cultural knowledge preservation and dissemination.",Chen-Chi Chang
2024-08-16T21:59:59Z,http://arxiv.org/abs/2409.03759v1,VERA: Validation and Evaluation of Retrieval-Augmented Systems,"The increasing use of Retrieval-Augmented Generation (RAG) systems in various
applications necessitates stringent protocols to ensure RAG systems accuracy,
safety, and alignment with user intentions. In this paper, we introduce VERA
(Validation and Evaluation of Retrieval-Augmented Systems), a framework
designed to enhance the transparency and reliability of outputs from large
language models (LLMs) that utilize retrieved information. VERA improves the
way we evaluate RAG systems in two important ways: (1) it introduces a
cross-encoder based mechanism that encompasses a set of multidimensional
metrics into a single comprehensive ranking score, addressing the challenge of
prioritizing individual metrics, and (2) it employs Bootstrap statistics on
LLM-based metrics across the document repository to establish confidence
bounds, ensuring the repositorys topical coverage and improving the overall
reliability of retrieval systems. Through several use cases, we demonstrate how
VERA can strengthen decision-making processes and trust in AI applications. Our
findings not only contribute to the theoretical understanding of LLM-based RAG
evaluation metric but also promote the practical implementation of responsible
AI systems, marking a significant advancement in the development of reliable
and transparent generative AI technologies.",Tianyu Ding
2024-09-14T02:34:26Z,http://arxiv.org/abs/2409.15355v4,Block-Attention for Efficient RAG,"We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.",East Sun
2024-09-12T02:43:40Z,http://arxiv.org/abs/2409.17275v1,"On the Vulnerability of Applying Retrieval-Augmented Generation within
  Knowledge-Intensive Application Domains","Retrieval-Augmented Generation (RAG) has been empirically shown to enhance
the performance of large language models (LLMs) in knowledge-intensive domains
such as healthcare, finance, and legal contexts. Given a query, RAG retrieves
relevant documents from a corpus and integrates them into the LLMs' generation
process. In this study, we investigate the adversarial robustness of RAG,
focusing specifically on examining the retrieval system. First, across 225
different setup combinations of corpus, retriever, query, and targeted
information, we show that retrieval systems are vulnerable to universal
poisoning attacks in medical Q\&A. In such attacks, adversaries generate
poisoned documents containing a broad spectrum of targeted information, such as
personally identifiable information. When these poisoned documents are inserted
into a corpus, they can be accurately retrieved by any users, as long as
attacker-specified queries are used. To understand this vulnerability, we
discovered that the deviation from the query's embedding to that of the
poisoned document tends to follow a pattern in which the high similarity
between the poisoned document and the query is retained, thereby enabling
precise retrieval. Based on these findings, we develop a new detection-based
defense to ensure the safe use of RAG. Through extensive experiments spanning
various Q\&A domains, we observed that our proposed method consistently
achieves excellent detection rates in nearly all cases.",Xun Xian
2024-10-10T19:14:55Z,http://arxiv.org/abs/2410.08320v1,"Do You Know What You Are Talking About? Characterizing Query-Knowledge
  Relevance For Reliable Retrieval Augmented Generation","Language models (LMs) are known to suffer from hallucinations and
misinformation. Retrieval augmented generation (RAG) that retrieves verifiable
information from an external knowledge corpus to complement the parametric
knowledge in LMs provides a tangible solution to these problems. However, the
generation quality of RAG is highly dependent on the relevance between a user's
query and the retrieved documents. Inaccurate responses may be generated when
the query is outside of the scope of knowledge represented in the external
knowledge corpus or if the information in the corpus is out-of-date. In this
work, we establish a statistical framework that assesses how well a query can
be answered by an RAG system by capturing the relevance of knowledge. We
introduce an online testing procedure that employs goodness-of-fit (GoF) tests
to inspect the relevance of each user query to detect out-of-knowledge queries
with low knowledge relevance. Additionally, we develop an offline testing
framework that examines a collection of user queries, aiming to detect
significant shifts in the query distribution which indicates the knowledge
corpus is no longer sufficiently capable of supporting the interests of the
users. We demonstrate the capabilities of these strategies through a systematic
evaluation on eight question-answering (QA) datasets, the results of which
indicate that the new testing framework is an efficient solution to enhance the
reliability of existing RAG systems.",Zhuohang Li
2024-10-11T14:03:29Z,http://arxiv.org/abs/2410.08821v1,"Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors
and hallucinated outputs generated by Large Language Models (LLMs) in
open-domain question-answering tasks (OpenQA) via introducing external
knowledge. For complex QA, however, existing RAG methods use LLMs to actively
predict retrieval timing and directly use the retrieved information for
generation, regardless of whether the retrieval timing accurately reflects the
actual information needs, or sufficiently considers prior retrieved knowledge,
which may result in insufficient information gathering and interaction,
yielding low-quality answers. To address these, we propose a generic RAG
approach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA
tasks, which includes the iterative information collector, adaptive memory
reviewer, and task-oriented generator, while following a new
Retriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an
overarching view of knowledge growth, iteratively gathering new information in
the form of notes and updating them into the existing optimal knowledge
structure, enhancing high-quality knowledge interactions. In addition, we
employ an adaptive, note-based stop-exploration strategy to decide ""what to
retrieve and when to stop"" to encourage sufficient knowledge exploration. We
conduct extensive experiments on five complex QA datasets, and the results
demonstrate the superiority and effectiveness of our method and its components.
The code and data are at https://github.com/thunlp/Adaptive-Note.",Ruobing Wang
2024-10-12T22:31:01Z,http://arxiv.org/abs/2410.09662v1,"Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and
  Nays!","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge bases, achieving state-of-the-art results in
various coding tasks. The core of RAG is retrieving demonstration examples,
which is essential to balance effectiveness (generation quality) and efficiency
(retrieval time) for optimal performance. However, the high-dimensional nature
of code representations and large knowledge bases often create efficiency
bottlenecks, which are overlooked in previous research. This paper
systematically evaluates the efficiency-effectiveness trade-off of retrievers
across three coding tasks: Program Synthesis, Commit Message Generation, and
Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)
and four dense retrievers, including one exhaustive dense retriever (SBERT's
Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).
Our findings show that while BM25 excels in effectiveness, it suffers in
efficiency as the knowledge base grows beyond 1000 entries. In large-scale
retrieval, efficiency differences become more pronounced, with approximate
dense retrievers offering the greatest gains. For instance, in Commit
Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in
RougeL compared with BM25. Our results also show that increasing the number of
demonstrations in the prompt doesn't always improve the effectiveness and can
increase latency and lead to incorrect outputs. Our findings provide valuable
insights for practitioners aiming to build efficient and effective RAG systems
for coding tasks.",Pengfei He
2024-10-14T09:17:43Z,http://arxiv.org/abs/2410.10315v2,"EasyRAG: Efficient Retrieval-Augmented Generation Framework for
  Automated Network Operations","This paper presents EasyRAG, a simple, lightweight, and efficient
retrieval-augmented generation framework for automated network operations. Our
framework has three advantages. The first is accurate question answering. We
designed a straightforward RAG scheme based on (1) a specific data processing
workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker
for reranking (4) LLM answer generation and optimization. This approach
achieved first place in the GLM4 track in the preliminary round and second
place in the GLM4 track in the semifinals. The second is simple deployment. Our
method primarily consists of BM25 retrieval and BGE-reranker reranking,
requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy,
and highly scalable; we provide a flexible code library with various search and
generation strategies, facilitating custom process implementation. The last one
is efficient inference. We designed an efficient inference acceleration scheme
for the entire coarse ranking, reranking, and generation process that
significantly reduces the inference latency of RAG while maintaining a good
level of accuracy; each acceleration scheme can be plug-and-play into any
component of the RAG process, consistently enhancing the efficiency of the RAG
system. Our code and data are released at
\url{https://github.com/BUAADreamer/EasyRAG}.",Zhangchi Feng
2024-10-08T12:42:42Z,http://arxiv.org/abs/2410.10869v1,"Application of NotebookLM, a Large Language Model with
  Retrieval-Augmented Generation, for Lung Cancer Staging","Purpose: In radiology, large language models (LLMs), including ChatGPT, have
recently gained attention, and their utility is being rapidly evaluated.
However, concerns have emerged regarding their reliability in clinical
applications due to limitations such as hallucinations and insufficient
referencing. To address these issues, we focus on the latest technology,
retrieval-augmented generation (RAG), which enables LLMs to reference reliable
external knowledge (REK). Specifically, this study examines the utility and
reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for
staging lung cancer.
  Materials and methods: We summarized the current lung cancer staging
guideline in Japan and provided this as REK to NotebookLM. We then tasked
NotebookLM with staging 100 fictional lung cancer cases based on CT findings
and evaluated its accuracy. For comparison, we performed the same task using a
gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.
  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer
staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the
REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in
searching reference locations within the REK.
  Conclusion: NotebookLM successfully performed lung cancer staging by
utilizing the REK, demonstrating superior performance compared to GPT-4o.
Additionally, it provided highly accurate reference locations within the REK,
allowing radiologists to efficiently evaluate the reliability of NotebookLM's
responses and detect possible hallucinations. Overall, this study highlights
the potential of NotebookLM, a RAG-LLM, in image diagnosis.",Ryota Tozuka
2024-10-14T18:34:29Z,http://arxiv.org/abs/2410.11001v1,"Graph of Records: Boosting Retrieval Augmented Generation for
  Long-context Summarization with Graphs","Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose \textit{graph of
records} (\textbf{GoR}), which leverages historical responses generated by LLMs
to enhance RAG for long-context global summarization. Inspired by the
\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR further features a \textit{graph neural network} and an elaborately
designed \textit{BERTScore}-based objective for self-supervised model training,
enabling seamless supervision signal backpropagation between reference
summaries and node embeddings. We comprehensively compare GoR with 12 baselines
across four long-context summarization datasets, and the results indicate that
our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\%
improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP
dataset). Extensive experiments further demonstrate the effectiveness of GoR.
Code is available at https://github.com/ulab-uiuc/GoR",Haozhen Zhang
2024-10-16T08:55:49Z,http://arxiv.org/abs/2410.12380v1,"Evaluation of Attribution Bias in Retrieval-Augmented Large Language
  Models","Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.",Amin Abolghasemi
2024-10-18T22:07:36Z,http://arxiv.org/abs/2410.14881v2,"Class-RAG: Real-Time Content Moderation with Retrieval Augmented
  Generation","Robust content moderation classifiers are essential for the safety of
Generative AI systems. In this task, differences between safe and unsafe inputs
are often extremely subtle, making it difficult for classifiers (and indeed,
even humans) to properly distinguish violating vs. benign samples without
context or explanation. Scaling risk discovery and mitigation through
continuous model fine-tuning is also slow, challenging and costly, preventing
developers from being able to respond quickly and effectively to emergent
harms. We propose a Classification approach employing Retrieval-Augmented
Generation (Class-RAG). Class-RAG extends the capability of its base LLM
through access to a retrieval library which can be dynamically updated to
enable semantic hotfixing for immediate, flexible risk mitigation. Compared to
model fine-tuning, Class-RAG demonstrates flexibility and transparency in
decision-making, outperforms on classification and is more robust against
adversarial attack, as evidenced by empirical studies. Our findings also
suggest that Class-RAG performance scales with retrieval library size,
indicating that increasing the library size is a viable and low-cost approach
to improve content moderation.",Jianfa Chen
2024-10-21T01:36:08Z,http://arxiv.org/abs/2410.15572v1,"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka
  Chatbots: Design Insights and User Perceptions","In an era where cultural preservation is increasingly intertwined with
technological innovation, this study introduces a groundbreaking approach to
promoting and safeguarding the rich heritage of Taiwanese Hakka culture through
the development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.
Traditional large language models (LLMs), while powerful, often fall short in
delivering accurate and contextually rich responses, particularly in culturally
specific domains. By integrating external databases with generative AI models,
RAG technology bridges this gap, empowering chatbots to not only provide
precise answers but also resonate deeply with the cultural nuances that are
crucial for authentic interactions. This study delves into the intricate
process of augmenting the chatbot's knowledge base with targeted cultural data,
specifically curated to reflect the unique aspects of Hakka traditions,
language, and practices. Through dynamic information retrieval, the
RAG-enhanced chatbot becomes a versatile tool capable of handling complex
inquiries that demand an in-depth understanding of Hakka cultural context. This
is particularly significant in an age where digital platforms often dilute
cultural identities, making the role of culturally aware AI systems more
critical than ever. System usability studies conducted as part of our research
reveal a marked improvement in both user satisfaction and engagement,
highlighting the chatbot's effectiveness in fostering a deeper connection with
Hakka culture. The feedback underscores the potential of RAG technology to not
only enhance user experience but also to serve as a vital instrument in the
broader mission of ethnic mainstreaming and cultural celebration.",Chen-Chi Chang
2024-10-27T16:23:26Z,http://arxiv.org/abs/2410.21330v1,LLM Robustness Against Misinformation in Biomedical Question Answering,"The retrieval-augmented generation (RAG) approach is used to reduce the
confabulation of large language models (LLMs) for question answering by
retrieving and providing additional context coming from external knowledge
sources (e.g., by adding the context to the prompt). However, injecting
incorrect information can mislead the LLM to generate an incorrect answer.
  In this paper, we evaluate the effectiveness and robustness of four LLMs
against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in
answering biomedical questions. We assess the answer accuracy on yes-no and
free-form questions in three scenarios: vanilla LLM answers (no context is
provided), ""perfect"" augmented generation (correct context is provided), and
prompt-injection attacks (incorrect context is provided). Our results show that
Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla
(0.651) and ""perfect"" RAG (0.802) scenarios. However, the accuracy gap between
the models almost disappears with ""perfect"" RAG, suggesting its potential to
mitigate the LLM's size-related effectiveness differences.
  We further evaluate the ability of the LLMs to generate malicious context on
one hand and the LLM's robustness against prompt-injection attacks on the other
hand, using metrics such as attack success rate (ASR), accuracy under attack,
and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2,
GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is
injected in the target model's prompt. Interestingly, Llama is shown to be the
most effective adversary, causing accuracy drops of up to 0.48 for vanilla
answers and 0.63 for ""perfect"" RAG across target models. Our analysis reveals
that robustness rankings vary depending on the evaluation measure, highlighting
the complexity of assessing LLM resilience to adversarial attacks.",Alexander Bondarenko
2024-11-28T12:06:14Z,http://arxiv.org/abs/2412.00125v1,Efficient Learning Content Retrieval with Knowledge Injection,"With the rise of online education platforms, there is a growing abundance of
educational content across various domain. It can be difficult to navigate the
numerous available resources to find the most suitable training, especially in
domains that include many interconnected areas, such as ICT. In this study, we
propose a domain-specific chatbot application that requires limited resources,
utilizing versions of the Phi language model to help learners with educational
content. In the proposed method, Phi-2 and Phi-3 models were fine-tuned using
QLoRA. The data required for fine-tuning was obtained from the Huawei Talent
Platform, where courses are available at different levels of expertise in the
field of computer science. RAG system was used to support the model, which was
fine-tuned by 500 Q&A pairs. Additionally, a total of 420 Q&A pairs of content
were extracted from different formats such as JSON, PPT, and DOC to create a
vector database to be used in the RAG system. By using the fine-tuned model and
RAG approach together, chatbots with different competencies were obtained. The
questions and answers asked to the generated chatbots were saved separately and
evaluated using ROUGE, BERTScore, METEOR, and BLEU metrics. The precision value
of the Phi-2 model supported by RAG was 0.84 and the F1 score was 0.82. In
addition to a total of 13 different evaluation metrics in 4 different
categories, the answers of each model were compared with the created content
and the most appropriate method was selected for real-life applications.",Batuhan Sariturk
2024-12-08T07:18:19Z,http://arxiv.org/abs/2412.05838v1,"A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation
  Across Diverse Data","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
incorporating external, domain-specific data into the generative process. While
LLMs are highly capable, they often rely on static, pre-trained datasets,
limiting their ability to integrate dynamic or private data. Traditional RAG
systems typically use a single-agent architecture to handle query generation,
data retrieval, and response synthesis. However, this approach becomes
inefficient when dealing with diverse data sources, such as relational
databases, document stores, and graph databases, often leading to performance
bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system
to address these limitations. Specialized agents, each optimized for a specific
data source, handle query generation for relational, NoSQL, and document-based
systems. These agents collaborate within a modular framework, with query
execution delegated to an environment designed for compatibility across various
database types. This distributed approach enhances query efficiency, reduces
token overhead, and improves response accuracy by ensuring that each agent
focuses on its specialized task. The proposed system is scalable and adaptable,
making it ideal for generative AI workflows that require integration with
diverse, dynamic, or private data sources. By leveraging specialized agents and
a modular execution environment, the system provides an efficient and robust
solution for handling complex, heterogeneous data environments in generative AI
applications.",Aniruddha Salve
2024-12-08T21:55:12Z,http://arxiv.org/abs/2412.06078v1,"Mixture-of-PageRanks: Replacing Long-Context with Real-Time, Sparse
  GraphRAG","Recent advances have extended the context window of frontier LLMs
dramatically, from a few thousand tokens up to millions, enabling entire books
and codebases to fit into context. However, the compute costs of inferencing
long-context LLMs are massive and often prohibitive in practice. RAG offers an
efficient and effective alternative: retrieve and process only the subset of
the context most important for the current task. Although promising, recent
work applying RAG to long-context tasks has two core limitations: 1) there has
been little focus on making the RAG pipeline compute efficient, and 2) such
works only test on simple QA tasks, and their performance on more challenging
tasks is unclear. To address this, we develop an algorithm based on PageRank, a
graph-based retrieval algorithm, which we call mixture-of-PageRanks (MixPR).
MixPR uses a mixture of PageRank-based graph-retrieval algorithms implemented
using sparse matrices for efficent, cheap retrieval that can deal with a
variety of complex tasks. Our MixPR retriever achieves state-of-the-art results
across a wide range of long-context benchmark tasks, outperforming both
existing RAG methods, specialized retrieval architectures, and long-context
LLMs despite being far more compute efficient. Due to using sparse embeddings,
our retriever is extremely compute efficient, capable of embedding and
retrieving millions of tokens within a few seconds and runs entirely on CPU.",Nicholas Alonso
2024-12-09T18:59:46Z,http://arxiv.org/abs/2412.06786v1,"Retrieving Semantics from the Deep: an RAG Solution for Gesture
  Synthesis","Non-verbal communication often comprises of semantically rich gestures that
help convey the meaning of an utterance. Producing such semantic co-speech
gestures has been a major challenge for the existing neural systems that can
generate rhythmic beat gestures, but struggle to produce semantically
meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based
gesture generation approach that leverages Retrieval Augmented Generation (RAG)
to produce natural-looking and semantically rich gestures. Our neuro-explicit
gesture generation approach is designed to produce semantic gestures grounded
in interpretable linguistic knowledge. We achieve this by using explicit domain
knowledge to retrieve exemplar motions from a database of co-speech gestures.
Once retrieved, we then inject these semantic exemplar gestures into our
diffusion-based gesture generation pipeline using DDIM inversion and retrieval
guidance at the inference time without any need of training. Further, we
propose a control paradigm for guidance, that allows the users to modulate the
amount of influence each retrieval insertion has over the generated sequence.
Our comparative evaluations demonstrate the validity of our approach against
recent gesture generation approaches. The reader is urged to explore the
results on our project page.",M. Hamza Mughal
2024-12-17T10:36:52Z,http://arxiv.org/abs/2412.12775v1,RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service,"Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.",Yihang Cheng
2024-12-17T18:42:21Z,http://arxiv.org/abs/2412.13163v2,C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System,"Organizations seeking to utilize Large Language Models (LLMs) for knowledge
querying and analysis often encounter challenges in maintaining an LLM
fine-tuned on targeted, up-to-date information that keeps answers relevant and
grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible
solution for organizations looking to overcome the challenges of maintaining
proprietary models and to help reduce LLM hallucinations in their query
responses. However, RAG comes with its own issues regarding scaling data
pipelines across tiered-access and disparate data sources. In many scenarios,
it is necessary to query beyond a single data silo to provide richer and more
relevant context for an LLM. Analyzing data sources within and across
organizational trust boundaries is often limited by complex data-sharing
policies that prohibit centralized data storage, therefore, inhibit the fast
and effective setup and scaling of RAG solutions. In this paper, we introduce
Confidential Computing (CC) techniques as a solution for secure Federated
Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG
system (C-FedRAG) enables secure connection and scaling of a RAG workflows
across a decentralized network of data providers by ensuring context
confidentiality. We also demonstrate how to implement a C-FedRAG system using
the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and
MIRAGE benchmarking dataset.",Parker Addison
2023-10-02T05:58:01Z,http://arxiv.org/abs/2310.00912v2,"A Resource-efficient FIR Filter Design Based on an RAG Improved
  Algorithm","In modern digital filter chip design, efficient resource utilization is a hot
topic. Due to the linear phase characteristics of FIR filters, a pulsed fully
parallel structure can be applied to address the problem. To further reduce
hardware resource consumption, especially related to multiplication functions,
an improved RAG algorithm has been proposed. Filters with different orders and
for different algorithms have been compared, and the experimental results show
that the improved RAG algorithm excels in terms of logic resource utilization,
resource allocation, running speed, and power consumption under various
application scenarios. The proposed algorithm introduces a better circuit
structure for FIR filters, fully leveraging resource allocation strategies to
reduce logic resource consumption. The proposed circuit is faster and more
stable, making it suitable for a variety of complex application scenarios.",Mengwei Hu
2023-12-09T23:33:16Z,http://arxiv.org/abs/2312.05708v1,Context Tuning for Retrieval Augmented Generation,"Large language models (LLMs) have the remarkable ability to solve new tasks
with just a few examples, but they need access to the right tools. Retrieval
Augmented Generation (RAG) addresses this problem by retrieving a list of
relevant tools for a given task. However, RAG's tool retrieval step requires
all the required information to be explicitly present in the query. This is a
limitation, as semantic search, the widely adopted tool retrieval method, can
fail when the query is incomplete or lacks context. To address this limitation,
we propose Context Tuning for RAG, which employs a smart context retrieval
system to fetch relevant information that improves both tool retrieval and plan
generation. Our lightweight context retrieval model uses numerical,
categorical, and habitual usage signals to retrieve and rank context items. Our
empirical results demonstrate that context tuning significantly enhances
semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for
context retrieval and tool retrieval tasks respectively, and resulting in an
11.6% increase in LLM-based planner accuracy. Additionally, we show that our
proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART
outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at
plan generation, even after tool retrieval, reduces hallucination.",Raviteja Anantha
2023-12-08T18:50:20Z,http://arxiv.org/abs/2312.07559v2,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,"Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA's matches expert human researchers on LitQA.",Jakub LÃ¡la
2024-01-04T16:16:14Z,http://arxiv.org/abs/2401.02333v3,"Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models","The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.",Uday Allu
2024-01-13T02:20:17Z,http://arxiv.org/abs/2401.06954v2,Bridging the Preference Gap between Retrievers and LLMs,"Large Language Models (LLMs) have demonstrated superior results across a wide
range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to
enhance the performance by locating relevant information and placing it into
the context window of the LLM. However, the relationship between retrievers and
LLMs in a RAG is still under-investigated. Most existing work treats the
retriever and the LLM as independent components and leaves a gap between
retrieving human-""friendly"" information and assembling a LLM-""friendly""
context. In this work, we examine a novel bridge mechanism. We validate the
ranking and selection assumptions of retrievers in the context of RAG and
propose a framework that chains together supervised and reinforcement learning
to train a bridge model that optimizes the connection between the retriever and
the LLM. Empirical results demonstrate the effectiveness of our method in both
question-answering and personalized generation tasks.",Zixuan Ke
2024-02-01T02:24:15Z,http://arxiv.org/abs/2402.01767v2,HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA,"Retrieval-augmented generation (RAG) has rapidly advanced the language model
field, particularly in question-answering (QA) systems. By integrating external
documents during the response generation phase, RAG significantly enhances the
accuracy and reliability of language models. This method elevates the quality
of responses and reduces the frequency of hallucinations, where the model
generates incorrect or misleading information. However, these methods exhibit
limited retrieval accuracy when faced with numerous indistinguishable
documents, presenting notable challenges in their practical application. In
response to these emerging challenges, we present HiQA, an advanced
multi-document question-answering (MDQA) framework that integrates cascading
metadata into content and a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.",Xinyue Chen
2024-02-05T14:36:51Z,http://arxiv.org/abs/2402.03053v1,"Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for
  Semantic Representations","In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
  For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
  In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the ""Melayu""
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
  These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99",Husein Zolkepli
2024-02-13T12:40:39Z,http://arxiv.org/abs/2402.08416v1,Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning,"Large Language Models~(LLMs) have gained immense popularity and are being
increasingly applied in various domains. Consequently, ensuring the security of
these models is of paramount importance. Jailbreak attacks, which manipulate
LLMs to generate malicious content, are recognized as a significant
vulnerability. While existing research has predominantly focused on direct
jailbreak attacks on LLMs, there has been limited exploration of indirect
methods. The integration of various plugins into LLMs, notably Retrieval
Augmented Generation~(RAG), which enables LLMs to incorporate external
knowledge bases into their response generation such as GPTs, introduces new
avenues for indirect jailbreak attacks.
  To fill this gap, we investigate indirect jailbreak attacks on LLMs,
particularly GPTs, introducing a novel attack vector named Retrieval Augmented
Generation Poisoning. This method, Pandora, exploits the synergy between LLMs
and RAG through prompt manipulation to generate unexpected responses. Pandora
uses maliciously crafted content to influence the RAG process, effectively
initiating jailbreak attacks. Our preliminary tests show that Pandora
successfully conducts jailbreak attacks in four different scenarios, achieving
higher success rates than direct attacks, with 64.3\% for GPT-3.5 and 34.8\%
for GPT-4.",Gelei Deng
2024-02-21T06:04:53Z,http://arxiv.org/abs/2402.13547v2,"ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through
  Retrieval-Augmented Agents","Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to
leverage external knowledge, enhancing their performance on knowledge-intensive
tasks. However, existing RAG models often treat LLMs as passive recipients of
information, which can lead to interference from noisy retrieved content. In
this paper, we introduce ActiveRAG, a multi-agent framework that mimics human
learning behavior to help LLMs actively engage with and learn from retrieved
evidence. ActiveRAG designs a knowledge assimilation agent to form the
knowledge understanding by associating external knowledge with the parametric
memory of LLMs. Then our model employs the thought accommodation agent to
calibrate the internal thought of LLMs for response refinement. Our experiments
show that ActiveRAG achieves a 10\% improvement over vanilla RAG on various
question-answering benchmarks. Further analysis reveals that ActiveRAG
mitigates the impact of noisy retrievals, alleviates conflicts between external
knowledge and parametric memory and improves the self-consistency of LLMs in
answering the question. All data and codes are available at
https://github.com/OpenMatch/ActiveRAG.",Zhipeng Xu
2024-02-26T08:59:05Z,http://arxiv.org/abs/2402.16406v1,"From RAGs to riches: Using large language models to write documents for
  clinical trials","Clinical trials require numerous documents to be written -- protocols,
consent forms, clinical study reports and others. Large language models (LLMs)
offer the potential to rapidly generate first versions of these documents,
however there are concerns about the quality of their output Here we report an
evaluation of LLMs in generating parts of one such document, clinical trial
protocols. We find that an offthe-shelf LLM delivers reasonable results,
especially when assessing content relevance and the correct use of terminology.
However, deficiencies remain: specifically clinical thinking and logic, and
appropriate use of references. To improve performance, we used
retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date
information. As a result of using RAG, the writing quality of the LLM improves
substantially, which has implications for the practical useability of LLMs in
clinical trial-related writing.",Nigel Markey
2024-02-06T13:19:53Z,http://arxiv.org/abs/2402.16874v1,"Enhancing Retrieval Processes for Language Generation with Augmented
  Queries","In the rapidly changing world of smart technology, searching for documents
has become more challenging due to the rise of advanced language models. These
models sometimes face difficulties, like providing inaccurate information,
commonly known as ""hallucination."" This research focuses on addressing this
issue through Retrieval-Augmented Generation (RAG), a technique that guides
models to give accurate responses based on real facts. To overcome scalability
issues, the study explores connecting user queries with sophisticated language
models such as BERT and Orca2, using an innovative query optimization process.
The study unfolds in three scenarios: first, without RAG, second, without
additional assistance, and finally, with extra help. Choosing the compact yet
efficient Orca2 7B model demonstrates a smart use of computing resources. The
empirical results indicate a significant improvement in the initial language
model's performance under RAG, particularly when assisted with prompts
augmenters. Consistency in document retrieval across different encodings
highlights the effectiveness of using language model-generated queries. The
introduction of UMAP for BERT further simplifies document retrieval while
maintaining strong results.",Julien Pierre Edmond Ghali
2024-04-10T11:03:17Z,http://arxiv.org/abs/2404.06910v2,"Superposition Prompting: Improving and Accelerating Retrieval-Augmented
  Generation","Despite the successes of large language models (LLMs), they exhibit
significant drawbacks, particularly when processing long contexts. Their
inference cost scales quadratically with respect to sequence length, making it
expensive for deployment in some real-world text processing applications, such
as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the
""distraction phenomenon"", where irrelevant context in the prompt degrades
output quality. To address these drawbacks, we propose a novel RAG prompting
methodology, *superposition prompting*, which can be directly applied to
pre-trained transformer-based LLMs *without the need for fine-tuning*. At a
high level, superposition prompting allows the LLM to process input documents
in parallel *prompt paths*, discarding paths once they are deemed irrelevant.
We demonstrate the capability of our method to simultaneously enhance time
efficiency across a variety of question-answering benchmarks using multiple
pre-trained LLMs. Furthermore, our technique significantly improves accuracy
when the retrieved context is large relative the context the model was trained
on. For example, our approach facilitates a 93x reduction in compute time while
*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the
MPT-7B instruction-tuned model over naive RAG.",Thomas Merth
2024-04-19T13:27:38Z,http://arxiv.org/abs/2404.12879v1,"Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented
  Generation","While Retrieval-Augmented Generation (RAG) plays a crucial role in the
application of Large Language Models (LLMs), existing retrieval methods in
knowledge-dense domains like law and medicine still suffer from a lack of
multi-perspective views, which are essential for improving interpretability and
reliability. Previous research on multi-view retrieval often focused solely on
different semantic forms of queries, neglecting the expression of specific
domain knowledge perspectives. This paper introduces a novel multi-view RAG
framework, MVRAG, tailored for knowledge-dense domains that utilizes
intention-aware query rewriting from multiple domain viewpoints to enhance
retrieval precision, thereby improving the effectiveness of the final
inference. Experiments conducted on legal and medical case retrieval
demonstrate significant improvements in recall and precision rates with our
framework. Our multi-perspective retrieval approach unleashes the potential of
multi-view information enhancing RAG tasks, accelerating the further
application of LLMs in knowledge-intensive fields.",Guanhua Chen
2024-04-23T20:00:37Z,http://arxiv.org/abs/2404.15488v1,"IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &
  Correction Task On the Shoulders of Medical Agents","In natural language processing applied to the clinical domain, utilizing
large language models has emerged as a promising avenue for error detection and
correction on clinical notes, a knowledge-intensive task for which annotated
data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a
suite of four LLM-based medical agents. The MedReAct agent initiates the
process by observing, analyzing, and taking action, generating trajectories to
guide the search to target a potential error in the clinical notes.
Subsequently, the MedEval agent employs five evaluators to assess the targeted
error and the proposed correction. In cases where MedReAct's actions prove
insufficient, the MedReFlex agent intervenes, engaging in reflective analysis
and proposing alternative strategies. Finally, the MedFinalParser agent formats
the final output, preserving the original style while ensuring the integrity of
the error correction process. One core component of our method is our RAG
pipeline based on our ClinicalCorp corpora. Among other well-known sources
containing clinical guidelines and information, we preprocess and release the
open-source MedWiki dataset for clinical RAG application. Our results
demonstrate the central role of our RAG approach with ClinicalCorp leveraged
through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the
MEDIQA-CORR 2024 final leaderboard.",Jean-Philippe Corbeil
2024-04-26T07:11:18Z,http://arxiv.org/abs/2404.17196v1,"Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
  Applications","Presently, with the assistance of advanced LLM application development
frameworks, more and more LLM-powered applications can effortlessly augment the
LLMs' knowledge with external content using the retrieval augmented generation
(RAG) technique. However, these frameworks' designs do not have sufficient
consideration of the risk of external content, thereby allowing attackers to
undermine the applications developed with these frameworks. In this paper, we
reveal a new threat to LLM-powered applications, termed retrieval poisoning,
where attackers can guide the application to yield malicious responses during
the RAG process. Specifically, through the analysis of LLM application
frameworks, attackers can craft documents visually indistinguishable from
benign ones. Despite the documents providing correct information, once they are
used as reference sources for RAG, the application is misled into generating
incorrect responses. Our preliminary experiments indicate that attackers can
mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate
in the real-world application, demonstrating the potential impact of retrieval
poisoning.",Quan Zhang
2024-05-23T11:00:19Z,http://arxiv.org/abs/2405.14431v1,RaFe: Ranking Feedback Improves Query Rewriting for RAG,"As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)
techniques have evolved, query rewriting has been widely incorporated into the
RAG system for downstream tasks like open-domain QA. Many works have attempted
to utilize small models with reinforcement learning rather than costly LLMs to
improve query rewriting. However, current methods require annotations (e.g.,
labeled relevant documents or downstream answers) or predesigned rewards for
feedback, which lack generalization, and fail to utilize signals tailored for
query rewriting. In this paper, we propose ours, a framework for training query
rewriting models free of annotations. By leveraging a publicly available
reranker, ours~provides feedback aligned well with the rewriting objectives.
Experimental results demonstrate that ours~can obtain better performance than
baselines.",Shengyu Mao
2024-05-28T12:18:50Z,http://arxiv.org/abs/2405.18111v3,"ATM: Adversarial Tuning Multi-agent System Makes a Robust
  Retrieval-Augmented Generator","Large language models (LLMs) are proven to benefit a lot from
retrieval-augmented generation (RAG) in alleviating hallucinations confronted
with knowledge-intensive questions. RAG adopts information retrieval techniques
to inject external knowledge from semantic-relevant documents as input
contexts. However, since today's Internet is flooded with numerous noisy and
fabricating content, it is inevitable that RAG systems are vulnerable to these
noises and prone to respond incorrectly. To this end, we propose to optimize
the retrieval-augmented Generator with an Adversarial Tuning Multi-agent system
(ATM). The ATM steers the Generator to have a robust perspective of useful
documents for question answering with the help of an auxiliary Attacker agent
through adversarially tuning the agents for several iterations. After rounds of
multi-agent iterative tuning, the Generator can eventually better discriminate
useful documents amongst fabrications. The experimental results verify the
effectiveness of ATM and we also observe that the Generator can achieve better
performance compared to the state-of-the-art baselines.",Junda Zhu
2024-05-30T03:44:54Z,http://arxiv.org/abs/2405.19670v4,"One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for
  Retrieval-Augmented Large Language Models","Retrieval-augmented generation (RAG) is a promising way to improve large
language models (LLMs) for generating more factual, accurate, and up-to-date
content. Existing methods either optimize prompts to guide LLMs in leveraging
retrieved information or directly fine-tune LLMs to adapt to RAG scenarios.
Although fine-tuning can yield better performance, it often compromises the
LLMs' general generation capabilities by modifying their parameters. This
limitation poses challenges in practical applications, especially when LLMs are
already deployed, as parameter adjustments may affect their original
functionality. To address this, we propose a novel method that involves
learning scalable and pluggable virtual tokens for RAG. By maintaining the
LLMs' original parameters and fine-tuning only the embeddings of these
pluggable tokens, our approach not only enhances LLMs' performance but also
preserves their general generation capabilities. Furthermore, we design several
training strategies to improve the scalability, flexibility, and
generalizability of our method. Comprehensive experiments across 12
question-answering tasks demonstrate the superiority of our approach.",Yutao Zhu
2024-06-03T12:39:04Z,http://arxiv.org/abs/2406.01273v2,SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper introduces SoccerRAG, an innovative framework designed to
harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. Our evaluations indicate that SoccerRAG
effectively handles complex queries, offering significant improvements over
traditional retrieval systems in terms of accuracy and user engagement. The
results underscore the potential of using RAG and LLMs in sports analytics,
paving the way for future advancements in the accessibility and real-time
processing of sports data.",Aleksander Theo Strand
2024-06-06T03:17:44Z,http://arxiv.org/abs/2406.03714v1,"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis
  with Context-Aware Contrastive Language-Audio Pretraining","Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker
using only a short speech prompt. They leverage a strong in-context ability to
mimic the speech prompts, including speaker style, prosody, and emotion.
Therefore, the selection of a speech prompt greatly influences the generated
speech, akin to the importance of a prompt in large language models (LLMs).
However, current prompt-based TTS models choose the speech prompt manually or
simply at random. Hence, in this paper, we adapt retrieval augmented generation
(RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we
additionally consider contextual information during the retrieval process and
present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model
to extract context-aware, style-related features. The objective and subjective
evaluations demonstrate that our proposed RAG method outperforms baselines, and
our CA-CLAP achieves better results than text-only retrieval methods.",Jinlong Xue
2024-06-09T14:11:19Z,http://arxiv.org/abs/2406.05794v3,"RE-RAG: Improving Open-Domain QA Performance and Interpretability with
  Relevance Estimator in Retrieval-Augmented Generation","The Retrieval Augmented Generation (RAG) framework utilizes a combination of
parametric knowledge and external knowledge to demonstrate state-of-the-art
performance on open-domain question answering tasks. However, the RAG framework
suffers from performance degradation when the query is accompanied by
irrelevant contexts. In this work, we propose the RE-RAG framework, which
introduces a relevance estimator (RE) that not only provides relative relevance
between contexts as previous rerankers did, but also provides confidence, which
can be used to classify whether given context is useful for answering the given
question. We propose a weakly supervised method for training the RE simply
utilizing question-answer data without any labels for correct contexts. We show
that RE trained with a small generator (sLM) can not only improve the sLM
fine-tuned together with RE but also improve previously unreferenced large
language models (LLMs). Furthermore, we investigate new decoding strategies
that utilize the proposed confidence measured by RE such as choosing to let the
user know that it is ""unanswerable"" to answer the question given the retrieved
contexts or choosing to rely on LLM's parametric knowledge rather than
unrelated contexts.",Kiseung Kim
2024-06-17T12:23:32Z,http://arxiv.org/abs/2406.11460v1,"TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) offers an effective approach for
addressing question answering (QA) tasks. However, the imperfections of the
retrievers in RAG models often result in the retrieval of irrelevant
information, which could introduce noises and degrade the performance,
especially when handling multi-hop questions that require multiple steps of
reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose
TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series
of logically connected knowledge triples, to identify and integrate supporting
evidence from the retrieved documents for answering questions. Specifically,
TRACE employs a KG Generator to create a knowledge graph (KG) from the
retrieved documents, and then uses an Autoregressive Reasoning Chain
Constructor to build reasoning chains. Experimental results on three multi-hop
QA datasets show that TRACE achieves an average performance improvement of up
to 14.03% compared to using all the retrieved documents. Moreover, the results
indicate that using reasoning chains as context, rather than the entire
documents, is often sufficient to correctly answer questions.",Jinyuan Fang
2024-06-18T17:46:08Z,http://arxiv.org/abs/2406.12824v1,"From RAGs to rich parameters: Probing how language models utilize
  external knowledge over parametric information for factual queries","Retrieval Augmented Generation (RAG) enriches the ability of language models
to reason using external context to augment responses for a given user prompt.
This approach has risen in popularity due to practical applications in various
applications of language models in search, question/answering, and chat-bots.
However, the exact nature of how this approach works isn't clearly understood.
In this paper, we mechanistically examine the RAG pipeline to highlight that
language models take shortcut and have a strong bias towards utilizing only the
context information to answer the question, while relying minimally on their
parametric memory. We probe this mechanistic behavior in language models with:
(i) Causal Mediation Analysis to show that the parametric memory is minimally
utilized when answering a question and (ii) Attention Contributions and
Knockouts to show that the last token residual stream do not get enriched from
the subject token in the question, but gets enriched from other informative
tokens in the context. We find this pronounced shortcut behaviour true across
both LLaMa and Phi family of models.",Hitesh Wadhwa
2024-06-26T05:36:23Z,http://arxiv.org/abs/2406.19417v1,"""Glue pizza and eat rocks"" -- Exploiting Vulnerabilities in
  Retrieval-Augmented Generative Models","Retrieval-Augmented Generative (RAG) models enhance Large Language Models
(LLMs) by integrating external knowledge bases, improving their performance in
applications like fact-checking and information searching. In this paper, we
demonstrate a security threat where adversaries can exploit the openness of
these knowledge bases by injecting deceptive content into the retrieval
database, intentionally changing the model's behavior. This threat is critical
as it mirrors real-world usage scenarios where RAG systems interact with
publicly accessible knowledge bases, such as web scrapings and user-contributed
data pools. To be more realistic, we target a realistic setting where the
adversary has no knowledge of users' queries, knowledge base data, and the LLM
parameters. We demonstrate that it is possible to exploit the model
successfully through crafted content uploads with access to the retriever. Our
findings emphasize an urgent need for security measures in the design and
deployment of RAG systems to prevent potential manipulation and ensure the
integrity of machine-generated content.",Zhen Tan
2024-07-11T08:24:16Z,http://arxiv.org/abs/2407.08275v1,"Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval
  Augmented Generation Systems","The choice of embedding model is a crucial step in the design of Retrieval
Augmented Generation (RAG) systems. Given the sheer volume of available
options, identifying clusters of similar models streamlines this model
selection process. Relying solely on benchmark performance scores only allows
for a weak assessment of model similarity. Thus, in this study, we evaluate the
similarity of embedding models within the context of RAG systems. Our
assessment is two-fold: We use Centered Kernel Alignment to compare embeddings
on a pair-wise level. Additionally, as it is especially pertinent to RAG
systems, we evaluate the similarity of retrieval results between these models
using Jaccard and rank similarity. We compare different families of embedding
models, including proprietary ones, across five datasets from the popular
Benchmark Information Retrieval (BEIR). Through our experiments we identify
clusters of models corresponding to model families, but interestingly, also
some inter-family clusters. Furthermore, our analysis of top-k retrieval
similarity reveals high-variance at low k values. We also identify possible
open-source alternatives to proprietary models, with Mistral exhibiting the
highest similarity to OpenAI models.",Laura Caspari
2024-07-16T23:50:07Z,http://arxiv.org/abs/2407.12216v2,"Mindful-RAG: A Study of Points of Failure in Retrieval Augmented
  Generation","Large Language Models (LLMs) are proficient at generating coherent and
contextually relevant text but face challenges when addressing
knowledge-intensive queries in domain-specific and factual question-answering
tasks. Retrieval-augmented generation (RAG) systems mitigate this by
incorporating external knowledge sources, such as structured knowledge graphs
(KGs). However, LLMs often struggle to produce accurate answers despite access
to KG-extracted information containing necessary facts. Our study investigates
this dilemma by analyzing error patterns in existing KG-based RAG methods and
identifying eight critical failure points. We observed that these errors
predominantly occur due to insufficient focus on discerning the question's
intent and adequately gathering relevant context from the knowledge graph
facts. Drawing on this analysis, we propose the Mindful-RAG approach, a
framework designed for intent-based and contextually aligned knowledge
retrieval. This method explicitly targets the identified failures and offers
improvements in the correctness and relevance of responses provided by LLMs,
representing a significant step forward from existing methods.",Garima Agrawal
2024-07-18T02:19:00Z,http://arxiv.org/abs/2407.13101v1,"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach","Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.",Zhouyu Jiang
2024-07-22T15:53:27Z,http://arxiv.org/abs/2407.15748v1,"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval
  Augmented Generation","In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the
first specialised AI chatbot for cybersecurity. MoRSE aims to provide
comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG
(Retrieval Augmented Generation) systems designed to retrieve and organize
information from multidimensional cybersecurity contexts. MoRSE differs from
traditional RAGs by using parallel retrievers that work together to retrieve
semantically related information in different formats and structures. Unlike
traditional Large Language Models (LLMs) that rely on Parametric Knowledge
Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases
in response to user queries. Subsequently, MoRSE uses this information to
generate accurate answers. In addition, MoRSE benefits from real-time updates
to its knowledge bases, enabling continuous knowledge enrichment without
retraining. We have evaluated the effectiveness of MoRSE against other
state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific
questions. The experimental evaluation has shown that the improvement in terms
of relevance and correctness of the answer is more than 10\% compared to known
solutions such as GPT-4 and Mixtral 7x8.",Marco Simoni
2024-08-02T08:37:03Z,http://arxiv.org/abs/2408.01107v2,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.",Chengrui Wang
2024-08-16T22:00:00Z,http://arxiv.org/abs/2408.09031v1,A Primer on Generative AI for Telecom: From Theory to Practice,"The rise of generative artificial intelligence (GenAI) is transforming the
telecom industry. GenAI models, particularly large language models (LLMs), have
emerged as powerful tools capable of driving innovation, improving efficiency,
and delivering superior customer services in telecom. This paper provides an
overview of GenAI for telecom from theory to practice. We review GenAI models
and discuss their practical applications in telecom. Furthermore, we describe
the key technology enablers and best practices for applying GenAI to telecom
effectively. We highlight the importance of retrieval augmented generation
(RAG) in connecting LLMs to telecom domain specific data sources to enhance the
accuracy of the LLMs' responses. We present a real-world use case on RAG-based
chatbot that can answer open radio access network (O-RAN) specific questions.
The demonstration of the chatbot to the O-RAN Alliance has triggered immense
interest in the industry. We have made the O-RAN RAG chatbot publicly
accessible on GitHub.",Xingqin Lin
2024-08-05T00:43:56Z,http://arxiv.org/abs/2408.11058v1,LLM Agents Improve Semantic Code Search,"Code Search is a key task that many programmers often have to perform while
developing solutions to problems. Current methodologies suffer from an
inability to perform accurately on prompts that contain some ambiguity or ones
that require additional context relative to a code-base. We introduce the
approach of using Retrieval Augmented Generation (RAG) powered agents to inject
information into user prompts allowing for better inputs into embedding models.
By utilizing RAG, agents enhance user queries with relevant details from GitHub
repositories, making them more informative and contextually aligned.
Additionally, we introduce a multi-stream ensemble approach which when paired
with agentic workflow can obtain improved retrieval accuracy, which we deploy
on application called repo-rift.com. Experimental results on the CodeSearchNet
dataset demonstrate that RepoRift significantly outperforms existing methods,
achieving an 78.2% success rate at Success@10 and a 34.6% success rate at
Success@1. This research presents a substantial advancement in semantic code
search, highlighting the potential of agentic LLMs and RAG to enhance code
retrieval systems.",Sarthak Jain
2024-08-22T01:42:34Z,http://arxiv.org/abs/2408.12060v2,"Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning
  with LLMs","Given the widespread dissemination of misinformation on social media,
implementing fact-checking mechanisms for online claims is essential. Manually
verifying every claim is very challenging, underscoring the need for an
automated fact-checking system. This paper presents our system designed to
address this issue. We utilize the Averitec dataset (Schlichtkrull et al.,
2023) to assess the performance of our fact-checking system. In addition to
veracity prediction, our system provides supporting evidence, which is
extracted from the dataset. We develop a Retrieve and Generate (RAG) pipeline
to extract relevant evidence sentences from a knowledge base, which are then
inputted along with the claim into a large language model (LLM) for
classification. We also evaluate the few-shot In-Context Learning (ICL)
capabilities of multiple LLMs. Our system achieves an 'Averitec' score of 0.33,
which is a 22% absolute improvement over the baseline. Our Code is publicly
available on
https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms.",Ronit Singhal
2024-08-22T12:21:22Z,http://arxiv.org/abs/2408.12333v2,Graph Retrieval Augmented Trustworthiness Reasoning,"Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game ""Werewolf,"" comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.",Ying Zhu
2024-09-05T01:58:29Z,http://arxiv.org/abs/2409.03171v2,"MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented
  Generation Question Answering","In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.",Mitchell DeHaven
2024-09-11T08:56:27Z,http://arxiv.org/abs/2409.07110v1,"Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and
  Education","This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed
to enhance user interaction for educational and research purposes. Leveraging
cutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as
a sophisticated AI assistant, exploiting the capabilities of traditional models
like ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval
Augmented Generation (RAG) through three primary methods: integration of
preprocessed documents, real-time processing of user-uploaded files, and
information retrieval from any specified website. Additionally, the chatbot
incorporates image generation via a Stable Diffusion Model (SDM), image
understanding and response generation through LLAVA, and search functionality
on the internet powered by secure search engine such as DuckDuckGo. To provide
comprehensive support, Bio-Eng-LMM offers text summarization, website content
summarization, and both text and voice interaction. The chatbot maintains
session memory to ensure contextually relevant and coherent responses. This
integrated platform builds upon the strengths of RAG-GPT and Web-Based RAG
Query (WBRQ) where the system fetches relevant information directly from the
web to enhance the LLMs response generation.",Ali Forootani
2024-09-04T19:00:59Z,http://arxiv.org/abs/2409.07487v2,MoA is All You Need: Building LLM Research Team using Mixture of Agents,"Large Language Models (LLMs) research in the financial domain is particularly
complex due to the sheer number of approaches proposed in literature.
Retrieval-Augmented Generation (RAG) has emerged as one of the leading methods
in the sector due to its inherent groundedness and data source variability. In
this work, we introduce a RAG framework called Mixture of Agents (MoA) and
demonstrate its viability as a practical, customizable, and highly effective
approach for scaling RAG applications. MoA is essentially a layered network of
individually customized small language models (Hoffmann et al., 2022)
collaborating to answer questions and extract information. While there are many
theoretical propositions for such an architecture and even a few libraries for
generally applying the structure in practice, there are limited documented
studies evaluating the potential of this framework considering real business
constraints such as cost and speed. We find that the MoA framework, consisting
of small language models (Hoffmann et al., 2022), produces higher quality and
more grounded responses across various financial domains that are core to
Vanguard's business while simultaneously maintaining low costs.",Sandy Chen
2024-09-13T02:08:47Z,http://arxiv.org/abs/2409.08479v2,"Exploring Information Retrieval Landscapes: An Investigation of a Novel
  Evaluation Techniques and Comparative Document Splitting Methods","The performance of Retrieval-Augmented Generation (RAG) systems in
information retrieval is significantly influenced by the characteristics of the
documents being processed. In this study, the structured nature of textbooks,
the conciseness of articles, and the narrative complexity of novels are shown
to require distinct retrieval strategies. A comparative evaluation of multiple
document-splitting methods reveals that the Recursive Character Splitter
outperforms the Token-based Splitter in preserving contextual integrity. A
novel evaluation technique is introduced, utilizing an open-source model to
generate a comprehensive dataset of question-and-answer pairs, simulating
realistic retrieval scenarios to enhance testing efficiency and metric
reliability. The evaluation employs weighted scoring metrics, including
SequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy
and relevance. This approach establishes a refined standard for evaluating the
precision of RAG systems, with future research focusing on optimizing chunk and
overlap sizes to improve retrieval accuracy and efficiency.",Esmaeil Narimissa
2024-09-13T07:28:47Z,http://arxiv.org/abs/2409.08597v1,"LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented
  Generation","Recent advancements in integrating speech information into large language
models (LLMs) have significantly improved automatic speech recognition (ASR)
accuracy. However, existing methods often constrained by the capabilities of
the speech encoders under varied acoustic conditions, such as accents. To
address this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)
paradigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech
datastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy
via LLM in-context learning (ICL) capabilities. Experiments on Mandarin and
various Chinese dialect datasets demonstrate significant improvements in ASR
accuracy compared to existing methods, validating the effectiveness of our
approach, especially in handling accent variations.",Shaojun Li
2024-09-19T07:39:22Z,http://arxiv.org/abs/2409.12524v1,"Should RAG Chatbots Forget Unimportant Conversations? Exploring
  Importance and Forgetting with Psychological Insights","While Retrieval-Augmented Generation (RAG) has shown promise in enhancing
long-term conversations, the increasing memory load as conversations progress
degrades retrieval accuracy. Drawing on psychological insights, we propose
LUFY, a simple yet effective method that focuses on emotionally arousing
memories and retains less than 10% of the conversation. In the user experiment,
participants interacted with three types of RAG chatbots, each for 2 hours over
4 sessions, marking the most extensive assessment of a chatbot's long-term
capabilities to date -- more than four times longer than any existing
benchmark. The results demonstrate that prioritizing arousing memories while
forgetting the majority of the conversation significantly enhances user
experience. This study pushes the frontier of long-term conversations and
highlights the importance of forgetting unimportant parts of conversations.
Code and Dataset: https://github.com/ryuichi-sumida/LUFY",Ryuichi Sumida
2024-09-20T10:36:49Z,http://arxiv.org/abs/2409.13385v2,"Contextual Compression in Retrieval-Augmented Generation for Large
  Language Models: A Survey","Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.",Sourav Verma
2024-09-21T15:32:10Z,http://arxiv.org/abs/2409.14175v1,"QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and
  Option Shuffling","Large Language models (LLMs) have brought about substantial advancements in
the field of Question Answering (QA) systems. These models do remarkably well
in addressing intricate inquiries in a variety of disciplines. However, because
of domain-specific vocabulary, complex technological concepts, and the
requirement for exact responses applying LLMs to specialized sectors like
telecommunications presents additional obstacles. GPT-3.5 has been used in
recent work, to obtain noteworthy accuracy for telecom-related questions in a
Retrieval Augmented Generation (RAG) framework. Notwithstanding these
developments, the practical use of models such as GPT-3.5 is restricted by
their proprietary nature and high computing demands. This paper introduces
QMOS, an innovative approach which uses a Question-Masked loss and Option
Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice
Questions in the telecommunications domain. Our focus was on using opensource,
smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.
Our multi-faceted approach involves several enhancements to the whole LLM-RAG
pipeline of finetuning, retrieval, prompt engineering and inference. Our
approaches significantly outperform existing results, achieving accuracy
improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%
to 84.65% with Phi-2.",Blessed Guda
2024-09-28T16:22:53Z,http://arxiv.org/abs/2409.19401v1,"Crafting Personalized Agents through Retrieval-Augmented Generation on
  Editable Memory Graphs","In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.",Zheng Wang
2024-09-30T20:32:29Z,http://arxiv.org/abs/2410.02825v2,"Ingest-And-Ground: Dispelling Hallucinations from Continually-Pretrained
  LLMs with RAG","This paper presents new methods that have the potential to improve privacy
process efficiency with LLM and RAG. To reduce hallucination, we continually
pre-train the base LLM model with a privacy-specific knowledge base and then
augment it with a semantic RAG layer. Our evaluations demonstrate that this
approach enhances the model performance (as much as doubled metrics compared to
out-of-box LLM) in handling privacy-related queries, by grounding responses
with factual information which reduces inaccuracies.",Chenhao Fang
2024-10-04T18:22:58Z,http://arxiv.org/abs/2410.03845v2,ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD,"Open-source Electronic Design Automation (EDA) tools are rapidly transforming
chip design by addressing key barriers of commercial EDA tools such as
complexity, costs, and access. Recent advancements in Large Language Models
(LLMs) have further enhanced efficiency in chip design by providing user
assistance across a range of tasks like setup, decision-making, and flow
automation. This paper introduces ORAssistant, a conversational assistant for
OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to
improve the user experience for the OpenROAD flow, from RTL-GDSII by providing
context-specific responses to common user queries, including installation,
command usage, flow setup, and execution, in prose format. Currently,
ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and
KLayout. The data model is built from publicly available documentation and
GitHub resources. The proposed architecture is scalable, supporting extensions
to other open-source tools, operating modes, and LLM models. We use Google
Gemini as the base LLM model to build and test ORAssistant. Early evaluation
results of the RAG-based model show notable improvements in performance and
accuracy compared to non-fine-tuned LLMs.",Aviral Kaintura
2024-10-11T13:52:44Z,http://arxiv.org/abs/2410.08815v2,"StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via
  Inference-time Hybrid Information Structurization","Retrieval-augmented generation (RAG) is a key means to effectively enhance
large language models (LLMs) in many knowledge-based tasks. However, existing
RAG methods struggle with knowledge-intensive reasoning tasks, because useful
information required to these tasks are badly scattered. This characteristic
makes it difficult for existing RAG methods to accurately identify key
information and perform global reasoning with such noisy augmentation. In this
paper, motivated by the cognitive theories that humans convert raw information
into various structured knowledge when tackling knowledge-intensive reasoning,
we proposes a new framework, StructRAG, which can identify the optimal
structure type for the task at hand, reconstruct original documents into this
structured format, and infer answers based on the resulting structure.
Extensive experiments across various knowledge-intensive tasks show that
StructRAG achieves state-of-the-art performance, particularly excelling in
challenging scenarios, demonstrating its potential as an effective solution for
enhancing LLMs in complex real-world applications.",Zhuoqun Li
2024-10-14T08:47:21Z,http://arxiv.org/abs/2410.10293v1,FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG,"Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the retrieval
paradigm that we design and use remains flat, which treats the retrieval
procedures as a one-off deal with constant granularity. Despite effectiveness,
we argue that they suffer from two limitations: (1) flat retrieval exerts a
significant burden on one retriever; (2) constant granularity limits the
ceiling of retrieval performance. In this work, we propose a progressive
retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,
so as to balance effectiveness and efficiency. Specifically, FunnelRAG
establishes a progressive retrieval pipeline by collaborating coarse-to-fine
granularity, large-to-small quantity, and low-to-high capacity, which can
relieve the burden on one retriever and also promote the ceiling of retrieval
performance. Extensive experiments manifest that FunnelRAG achieves comparable
retrieval performance while the time overhead is reduced by nearly 40 percent.",Xinping Zhao
2024-10-15T02:18:01Z,http://arxiv.org/abs/2410.11195v1,"Athena: Retrieval-augmented Legal Judgment Prediction with Large
  Language Models","Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have
prevailed in countless domains, including legal scenarios. With LLMs' rapid
technological progress, the development of prompt engineering (PE) as an
interface between the LLMs and real-world applications has drawn the attention
of all developers. Various PE methods have been proposed to overcome real-world
challenges, such as few-shot prompting, chain-of-thought, and
retrieval-augmented generation (RAG). However, RAG for legal judgment
prediction (LJP) is still underexplored. To address this, we propose ""Athena"",
a novel framework cultivating RAG as a core preprocess component to enhance
LLMs' performance on specialized tasks. Athena constructs a knowledge base for
accusations, attached with a semantic retrieval mechanism through
vectorization. Our experiments show that Athena's overall performance has
improved significantly, achieving state-of-the-art results on the CAIL2018
dataset. Our ablation study on the in-context window size parameter further
reproduces LLMs' ""lost-in-the-middle"" phenomenon with a relative positional
variation. And with moderate hyper-parameter-tuning, we can achieve at most 95%
of accuracy accordingly. We also study the impact of query rewriting and data
distribution, providing possible directions for future research based on former
analyses.",Xiao Peng
2024-10-15T06:26:24Z,http://arxiv.org/abs/2410.11315v1,"SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented
  Generation","Recent studies in Retrieval-Augmented Generation (RAG) have investigated
extracting evidence from retrieved passages to reduce computational costs and
enhance the final RAG performance, yet it remains challenging. Existing methods
heavily rely on heuristic-based augmentation, encountering several issues: (1)
Poor generalization due to hand-crafted context filtering; (2) Semantics
deficiency due to rule-based context chunking; (3) Skewed length due to
sentence-wise filter learning. To address these issues, we propose a
model-based evidence extraction learning framework, SEER, optimizing a vanilla
model as an evidence extractor with desired properties through self-aligned
learning. Extensive experiments show that our method largely improves the final
RAG performance, enhances the faithfulness, helpfulness, and conciseness of the
extracted evidence, and reduces the evidence length by 9.25 times. The code
will be available at https://github.com/HITsz-TMG/SEER.",Xinping Zhao
2024-10-15T08:39:12Z,http://arxiv.org/abs/2410.11395v1,"Synthetic Interlocutors. Experiments with Generative AI to Prolong
  Ethnographic Encounters","This paper introduces ""Synthetic Interlocutors"" for ethnographic research.
Synthetic Interlocutors are chatbots ingested with ethnographic textual
material (interviews and observations) by using Retrieval Augmented Generation
(RAG). We integrated an open-source large language model with ethnographic data
from three projects to explore two questions: Can RAG digest ethnographic
material and act as ethnographic interlocutor? And, if so, can Synthetic
Interlocutors prolong encounters with the field and extend our analysis?
Through reflections on the process of building our Synthetic Interlocutors and
an experimental collaborative workshop, we suggest that RAG can digest
ethnographic materials, and it might lead to prolonged, yet uneasy ethnographic
encounters that allowed us to partially recreate and re-visit fieldwork
interactions while facilitating opportunities for novel analytic insights.
Synthetic Interlocutors can produce collaborative, ambiguous and serendipitous
moments.",Johan Irving SÃ¸ltoft
2024-10-11T19:49:05Z,http://arxiv.org/abs/2410.12859v1,"Enhancing Long Context Performance in LLMs Through Inner Loop Query
  Mechanism","Transformers have a quadratic scaling of computational complexity with input
size, which limits the input context window size of large language models
(LLMs) in both training and inference. Meanwhile, retrieval-augmented
generation (RAG) besed models can better handle longer contexts by using a
retrieval system to filter out unnecessary information. However, most RAG
methods only perform retrieval based on the initial query, which may not work
well with complex questions that require deeper reasoning. We introduce a novel
approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving
inner-loop queries, based not only on the query question itself but also on
intermediate findings. At inference time, our model retrieves information from
the RAG system, integrating data from lengthy documents at various levels of
abstraction. Based on the information retrieved, the LLM generates texts stored
in an area named Short-Term Memory (STM) which is then used to formulate the
next query. This retrieval process is repeated until the text in STM converged.
Our experiments demonstrate that retrieval with STM offers improvements over
traditional retrieval-augmented LLMs, particularly in long context tests such
as Multi-Needle In A Haystack (M-NIAH) and BABILong.",Yimin Tang
2024-10-16T08:43:39Z,http://arxiv.org/abs/2410.12890v1,"REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via
  Model Fusion of Embedding Models","Retrieval augmented generation (RAG) pipelines are commonly used in tasks
such as question-answering (QA), relying on retrieving relevant documents from
a vector store computed using a pretrained embedding model. However, if the
retrieved context is inaccurate, the answers generated using the large language
model (LLM) may contain errors or hallucinations. Although pretrained embedding
models have advanced, adapting them to new domains remains challenging.
Fine-tuning is a potential solution, but industry settings often lack the
necessary fine-tuning data. To address these challenges, we propose REFINE, a
novel technique that generates synthetic data from available documents and then
uses a model fusion approach to fine-tune embeddings for improved retrieval
performance in new domains, while preserving out-of-domain capability. We
conducted experiments on the two public datasets: SQUAD and RAG-12000 and a
proprietary TOURISM dataset. Results demonstrate that even the standard
fine-tuning with the proposed data augmentation technique outperforms the
vanilla pretrained model. Furthermore, when combined with model fusion, the
proposed approach achieves superior performance, with a 5.76% improvement in
recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and
RAG-12000 respectively.",Ambuje Gupta
2024-10-17T06:30:55Z,http://arxiv.org/abs/2410.13258v1,"A Systematic Investigation of Knowledge Retrieval and Selection for
  Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has emerged as a powerful method for
enhancing natural language generation by integrating external knowledge into a
model's output. While prior work has demonstrated the importance of improving
knowledge retrieval for boosting generation quality, the role of knowledge
selection remains less clear. In this paper, we perform a comprehensive
analysis of how knowledge retrieval and selection influence downstream
generation performance in RAG systems. By simulating different retrieval and
selection conditions through a controlled mixture of gold and distractor
knowledge, we assess the impact of these factors on generation outcomes. Our
findings indicate that the downstream generator model's capability, as well as
the complexity of the task and dataset, significantly influence the impact of
knowledge retrieval and selection on the overall RAG system performance. In
typical scenarios, improving the knowledge recall score is key to enhancing
generation outcomes, with the knowledge selector providing a limited additional
benefit when a strong generator model is used on clear, well-defined tasks. For
weaker generator models or more ambiguous tasks and datasets, the knowledge F1
score becomes a critical factor, and the knowledge selector plays a more
prominent role in improving overall performance.",Xiangci Li
2024-10-18T16:11:29Z,http://arxiv.org/abs/2410.14567v1,RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions,"Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve
their responses to confusing questions. This paper presents a novel synthetic
data generation method to efficiently create a diverse set of context-grounded
confusing questions from a given document corpus. We conduct an empirical
comparative evaluation of several large language models as RAG agents to
measure the accuracy of confusion detection and appropriate response
generation. We contribute a benchmark dataset to the public domain.",Zhiyuan Peng
2024-10-23T15:24:16Z,http://arxiv.org/abs/2410.17952v1,"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large
  Language Models to Specialized Domains","Retrieval-augmented generation (RAG) enhances the question-answering (QA)
abilities of large language models (LLMs) by integrating external knowledge.
However, adapting general-purpose RAG systems to specialized fields such as
science and medicine poses unique challenges due to distribution shifts and
limited access to domain-specific data. To tackle this, we propose SimRAG, a
self-training approach that equips the LLM with joint capabilities of question
answering and question generation for domain adaptation. Our method first
fine-tunes the LLM on instruction-following, question-answering, and
search-related data. Then, it prompts the same LLM to generate diverse
domain-relevant questions from unlabeled corpora, with an additional filtering
strategy to retain high-quality synthetic examples. By leveraging these
synthetic examples, the LLM can improve their performance on domain-specific
RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three
domains, demonstrate that SimRAG outperforms baselines by 1.2\%--8.6\%.",Ran Xu
2024-10-22T11:23:11Z,http://arxiv.org/abs/2410.18141v1,SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback,"RAG systems consist of multiple modules to work together. However, these
modules are usually separately trained. We argue that a system like RAG that
incorporates multiple modules should be jointly optimized to achieve optimal
performance. To demonstrate this, we design a specific pipeline called
\textbf{SmartRAG} that includes a policy network and a retriever. The policy
network can serve as 1) a decision maker that decides when to retrieve, 2) a
query rewriter to generate a query most suited to the retriever, and 3) an
answer generator that produces the final response with/without the
observations. We then propose to jointly optimize the whole system using a
reinforcement learning algorithm, with the reward designed to encourage the
system to achieve the best performance with minimal retrieval cost. When
jointly optimized, all the modules can be aware of how other modules are
working and thus find the best way to work together as a complete system.
Empirical results demonstrate that the jointly optimized SmartRAG can achieve
better performance than separately optimized counterparts.",Jingsheng Gao
2024-10-31T00:18:05Z,http://arxiv.org/abs/2410.23526v1,"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve
  Factualness in Large Language Models","Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.",Hieu Tran
2024-11-04T02:30:05Z,http://arxiv.org/abs/2411.01751v1,RAGViz: Diagnose and Visualize Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",Tevin Wang
2024-11-05T06:11:17Z,http://arxiv.org/abs/2411.02832v2,PersianRAG: A Retrieval-Augmented Generation System for Persian Language,"Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.",Hossein Hosseini
2024-11-07T00:39:34Z,http://arxiv.org/abs/2411.04341v1,Enhancing classroom teaching with LLMs and RAG,"Large Language Models have become a valuable source of information for our
daily inquiries. However, after training, its data source quickly becomes
out-of-date, making RAG a useful tool for providing even more recent or
pertinent data. In this work, we investigate how RAG pipelines, with the course
materials serving as a data source, might help students in K-12 education. The
initial research utilizes Reddit as a data source for up-to-date cybersecurity
information. Chunk size is evaluated to determine the optimal amount of context
needed to generate accurate answers. After running the experiment for different
chunk sizes, answer correctness was evaluated using RAGAs with average answer
correctness not exceeding 50 percent for any chunk size. This suggests that
Reddit is not a good source to mine for data for questions about cybersecurity
threats. The methodology was successful in evaluating the data source, which
has implications for its use to evaluate educational resources for
effectiveness.",Elizabeth A Mullins
2024-11-09T17:38:01Z,http://arxiv.org/abs/2411.06237v2,"Leveraging Retrieval-Augmented Generation for Persian University
  Knowledge Retrieval","This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.",Arshia Hemmat
2024-11-12T14:12:45Z,http://arxiv.org/abs/2411.07820v2,"Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models","We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.",Youan Cong
2024-11-12T15:26:17Z,http://arxiv.org/abs/2411.07870v5,"Trustful LLMs: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders","Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.",Xiaofeng Zhu
2024-11-07T22:11:51Z,http://arxiv.org/abs/2411.11895v1,Deploying Large Language Models With Retrieval Augmented Generation,"Knowing that the generative capabilities of large language models (LLM) are
sometimes hampered by tendencies to hallucinate or create non-factual
responses, researchers have increasingly focused on methods to ground generated
outputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a
key approach for integrating knowledge from data sources outside of the LLM's
training set, including proprietary and up-to-date information. While many
research papers explore various RAG strategies, their true efficacy is tested
in real-world applications with actual data. The journey from conceiving an
idea to actualizing it in the real world is a lengthy process. We present
insights from the development and field-testing of a pilot project that
integrates LLMs with RAG for information retrieval. Additionally, we examine
the impacts on the information value chain, encompassing people, processes, and
technology. Our aim is to identify the opportunities and challenges of
implementing this emerging technology, particularly within the context of
behavioral research in the information systems (IS) field. The contributions of
this work include the development of best practices and recommendations for
adopting this promising technology while ensuring compliance with industry
regulations through a proposed AI governance model.",Sonal Prabhune
2024-11-21T01:00:25Z,http://arxiv.org/abs/2411.13773v1,FastRAG: Retrieval Augmented Generation for Semi-structured Data,"Efficiently processing and interpreting network data is critical for the
operation of increasingly complex networks. Recent advances in Large Language
Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved
data processing in network management. However, existing RAG methods like
VectorRAG and GraphRAG struggle with the complexity and implicit nature of
semi-structured technical data, leading to inefficiencies in time, cost, and
retrieval. This paper introduces FastRAG, a novel RAG approach designed for
semi-structured data. FastRAG employs schema learning and script learning to
extract and structure data without needing to submit entire data sources to an
LLM. It integrates text search with knowledge graph (KG) querying to improve
accuracy in retrieving context-rich information. Evaluation results demonstrate
that FastRAG provides accurate question answering, while improving up to 90% in
time and 85% in cost compared to GraphRAG.",Amar Abane
2024-11-21T21:22:58Z,http://arxiv.org/abs/2411.14592v2,G-RAG: Knowledge Expansion in Material Science,"In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.",Radeen Mostafa
2024-11-25T13:20:19Z,http://arxiv.org/abs/2411.16365v1,"Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark,
  Evaluate Metrics and Strong Baselines","This paper investigates an intriguing task of Multi-modal Retrieval Augmented
Multi-modal Generation (M$^2$RAG). This task requires foundation models to
browse multi-modal web pages, with mixed text and images, and generate
multi-modal responses for solving user queries, which exhibits better
information density and readability. Given the early researching stage of
M$^2$RAG task, there is a lack of systematic studies and analysis. To fill this
gap, we construct a benchmark for M$^2$RAG task, equipped with a suite of
text-modal metrics and multi-modal metrics to analyze the capabilities of
existing foundation models. Besides, we also propose several effective methods
for foundation models to accomplish this task, based on the comprehensive
evaluation results on our benchmark. Extensive experimental results reveal
several intriguing phenomena worth further research.",Zi-Ao Ma
2024-12-05T15:11:12Z,http://arxiv.org/abs/2412.04235v1,"Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM
  Chatbots","I combine detection and mitigation techniques to addresses hallucinations in
Large Language Models (LLMs). Mitigation is achieved in a question-answering
Retrieval-Augmented Generation (RAG) framework while detection is obtained by
introducing the Negative Missing Information Scoring System (NMISS), which
accounts for contextual relevance in responses. While RAG mitigates
hallucinations by grounding answers in external data, NMISS refines the
evaluation by identifying cases where traditional metrics incorrectly flag
contextually accurate responses as hallucinations. I use Italian health news
articles as context to evaluate LLM performance. Results show that Gemma2 and
GPT-4 outperform the other models, with GPT-4 producing answers closely aligned
with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral
benefit significantly from NMISS, highlighting their ability to provide richer
contextual information. This combined approach offers new insights into the
reduction and more accurate assessment of hallucinations in LLMs, with
applications in real-world healthcare tasks and other domains.",Maria Paola Priola
2024-12-06T17:04:21Z,http://arxiv.org/abs/2412.05184v1,QueEn: A Large Language Model for Quechua-English Translation,"Recent studies show that large language models (LLMs) are powerful tools for
working with natural language, bringing advances in many areas of computational
linguistics. However, these models face challenges when applied to low-resource
languages due to limited training data and difficulty in understanding cultural
nuances. In this paper, we propose QueEn, a novel approach for Quechua-English
translation that combines Retrieval-Augmented Generation (RAG) with
parameter-efficient fine-tuning techniques. Our method leverages external
linguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for
efficient model adaptation. Experimental results show that our approach
substantially exceeds baseline models, with a BLEU score of 17.6 compared to
1.5 for standard GPT models. The integration of RAG with fine-tuning allows our
system to address the challenges of low-resource language translation while
maintaining computational efficiency. This work contributes to the broader goal
of preserving endangered languages through advanced language technologies.",Junhao Chen
2024-12-10T11:18:29Z,http://arxiv.org/abs/2412.07420v1,RAG-based Question Answering over Heterogeneous Data and Text,"This article presents the QUASAR system for question answering over
unstructured text, structured tables, and knowledge graphs, with unified
treatment of all sources. The system adopts a RAG-based architecture, with a
pipeline of evidence retrieval followed by answer generation, with the latter
powered by a moderate-sized language model. Additionally and uniquely, QUASAR
has components for question understanding, to derive crisper input for evidence
retrieval, and for re-ranking and filtering the retrieved evidence before
feeding the most informative pieces into the answer generation. Experiments
with three different benchmarks demonstrate the high answering quality of our
approach, being on par with or better than large GPT models, while keeping the
computational cost and energy consumption orders of magnitude lower.",Philipp Christmann
2024-12-11T18:11:39Z,http://arxiv.org/abs/2412.08593v1,"Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based
  Automated Requirement Traceability and Compliance Checks","Ensuring that Software Requirements Specifications (SRS) align with
higher-level organizational or national requirements is vital, particularly in
regulated environments such as finance and aerospace. In these domains,
maintaining consistency, adhering to regulatory frameworks, minimizing errors,
and meeting critical expectations are essential for the reliable functioning of
systems. The widespread adoption of large language models (LLMs) highlights
their immense potential, yet there remains considerable scope for improvement
in retrieving relevant information and enhancing reasoning capabilities. This
study demonstrates that integrating a robust Graph-RAG framework with advanced
prompt engineering techniques, such as Chain of Thought and Tree of Thought,
can significantly enhance performance. Compared to baseline RAG methods and
simple prompting strategies, this approach delivers more accurate and
context-aware results. While this method demonstrates significant improvements
in performance, it comes with challenges. It is both costly and more complex to
implement across diverse contexts, requiring careful adaptation to specific
scenarios. Additionally, its effectiveness heavily relies on having complete
and accurate input data, which may not always be readily available, posing
further limitations to its scalability and practicality.",Arsalan Masoudifard
2024-12-12T18:59:41Z,http://arxiv.org/abs/2412.09614v1,"Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge
  Graph-Based RAG","We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.",Kavana Venkatesh
2024-12-14T06:24:55Z,http://arxiv.org/abs/2412.10704v1,"VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal
  Retrieval-Augmented Generation","Understanding information from a collection of multiple documents,
particularly those with visually rich elements, is important for
document-grounded question answering. This paper introduces VisDoMBench, the
first comprehensive benchmark designed to evaluate QA systems in multi-document
settings with rich multimodal content, including tables, charts, and
presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval
Augmented Generation (RAG) approach that simultaneously utilizes visual and
textual RAG, combining robust visual retrieval capabilities with sophisticated
linguistic reasoning. VisDoMRAG employs a multi-step reasoning process
encompassing evidence curation and chain-of-thought reasoning for concurrent
textual and visual RAG pipelines. A key novelty of VisDoMRAG is its
consistency-constrained modality fusion mechanism, which aligns the reasoning
processes across modalities at inference time to produce a coherent final
answer. This leads to enhanced accuracy in scenarios where critical information
is distributed across modalities and improved answer verifiability through
implicit context attribution. Through extensive experiments involving
open-source and proprietary large language models, we benchmark
state-of-the-art document QA methods on VisDoMBench. Extensive results show
that VisDoMRAG outperforms unimodal and long-context LLM baselines for
end-to-end multimodal document QA by 12-20%.",Manan Suri
2024-12-19T07:01:25Z,http://arxiv.org/abs/2412.14581v1,"CORD: Balancing COnsistency and Rank Distillation for Robust
  Retrieval-Augmented Generation","With the adoption of retrieval-augmented generation (RAG), large language
models (LLMs) are expected to ground their generation to the retrieved
contexts. Yet, this is hindered by position bias of LLMs, failing to evenly
attend to all contexts. Previous work has addressed this by synthesizing
contexts with perturbed positions of gold segment, creating a
position-diversified train set. We extend this intuition to propose consistency
regularization with augmentation and distillation. First, we augment each
training instance with its position perturbation to encourage consistent
predictions, regardless of ordering. We also distill behaviors of this pair,
although it can be counterproductive in certain RAG scenarios where the given
order from the retriever is crucial for generation quality. We thus propose
CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples
noise-controlled perturbations from an interpolation space, ensuring both
consistency and respect for the rank prior. Empirical results show this balance
enables CORD to outperform consistently in diverse RAG benchmarks.",Youngwon Lee
2024-12-19T14:37:11Z,http://arxiv.org/abs/2412.14905v1,"Dehallucinating Parallel Context Extension for Retrieval-Augmented
  Generation","Large language models (LLMs) are susceptible to generating hallucinated
information, despite the integration of retrieval-augmented generation (RAG).
Parallel context extension (PCE) is a line of research attempting to
effectively integrating parallel (unordered) contexts, while it still suffers
from hallucinations when adapted to RAG scenarios. In this paper, we propose
DePaC (Dehallucinating Parallel Context Extension), which alleviates the
hallucination problem with context-aware negative training and
information-calibrated aggregation. DePaC is designed to alleviate two types of
in-context hallucination: fact fabrication (i.e., LLMs present claims that are
not supported by the contexts) and fact omission (i.e., LLMs fail to present
claims that can be supported by the contexts). Specifically, (1) for fact
fabrication, we apply the context-aware negative training that fine-tunes the
LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to
answer when contexts are not related to questions; (2) for fact omission, we
propose the information-calibrated aggregation which prioritizes context
windows with higher information increment from their contexts. The experimental
results on nine RAG tasks demonstrate that DePaC significantly alleviates the
two types of hallucination and consistently achieves better performances on
these tasks.",Zexiong Ma
2024-12-19T15:44:01Z,http://arxiv.org/abs/2412.14964v1,Knowledge Injection via Prompt Distillation,"In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.",Kalle KujanpÃ¤Ã¤
2024-12-19T18:57:11Z,http://arxiv.org/abs/2412.15189v1,"Face the Facts! Evaluating RAG-based Fact-checking Pipelines in
  Realistic Settings","Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under
more realistic scenarios, RAG-based methods for the generation of verdicts -
i.e., short texts discussing the veracity of a claim - evaluating them on
stylistically complex claims and heterogeneous, yet reliable, knowledge bases.
Our findings show a complex landscape, where, for example, LLM-based retrievers
outperform other retrieval techniques, though they still struggle with
heterogeneous knowledge bases; larger models excel in verdict faithfulness,
while smaller models provide better context adherence, with human evaluations
favouring zero-shot and one-shot approaches for informativeness, and fine-tuned
models for emotional alignment.",Daniel Russo
2023-11-29T03:07:00Z,http://arxiv.org/abs/2311.17330v2,"Biomedical knowledge graph-optimized prompt generation for large
  language models","Large Language Models (LLMs) are being adopted at an unprecedented rate, yet
still face challenges in knowledge-intensive domains like biomedicine.
Solutions such as pre-training and domain-specific fine-tuning add substantial
computational overhead, requiring further domain expertise. Here, we introduce
a token-optimized and robust Knowledge Graph-based Retrieval Augmented
Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)
with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful
biomedical text rooted in established knowledge. Compared to the existing RAG
technique for Knowledge Graphs, the proposed method utilizes minimal graph
schema for context extraction and uses embedding methods for context pruning.
This optimization in context extraction results in more than 50% reduction in
token consumption without compromising the accuracy, making a cost-effective
and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced
the performance of LLMs across diverse biomedical prompts by generating
responses rooted in established knowledge, accompanied by accurate provenance
and statistical evidence (if available) to substantiate the claims. Further
benchmarking on human curated datasets, such as biomedical true/false and
multiple-choice questions (MCQ), showed a remarkable 71% boost in the
performance of the Llama-2 model on the challenging MCQ dataset, demonstrating
the framework's capacity to empower open-source models with fewer parameters
for domain specific questions. Furthermore, KG-RAG enhanced the performance of
proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM in a token
optimized fashion, thus enhancing the adaptability of general-purpose LLMs to
tackle domain-specific questions in a cost-effective fashion.",Karthik Soman
2024-07-22T13:29:56Z,http://arxiv.org/abs/2407.15621v1,"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in
  Radiology Using Dynamic Retrieval Augmented Generation","Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.",Soroosh Tayebi Arasteh
2024-08-01T17:18:17Z,http://arxiv.org/abs/2408.00727v3,"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions","The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a conventional RAG system
and they will be further used to guide the query generation in the next
iteration. Our experiments show the improved performance of various LLMs
brought by i-MedRAG compared with conventional RAG on complex questions from
clinical vignettes in the United States Medical Licensing Examination (USMLE),
as well as various knowledge tests in the Massive Multitask Language
Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all
existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an
accuracy of 69.68% on the MedQA dataset. In addition, we characterize the
scaling properties of i-MedRAG with different iterations of follow-up queries
and different numbers of queries per iteration. Our case studies show that
i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing
an in-depth analysis of medical questions. To the best of our knowledge, this
is the first-of-its-kind study on incorporating follow-up queries into medical
RAG. The implementation of i-MedRAG is available at
https://github.com/Teddy-XiongGZ/MedRAG.",Guangzhi Xiong
2008-08-20T21:39:23Z,http://arxiv.org/abs/0808.2826v1,"Crystal Growth and Anisotropic Magnetic Properties of RAg$_2$Ge$_2$ (R =
  Pr, Nd and Sm) Single Crystals","We report the single crystal growth and anisotropic magnetic properties of
the tetragonal RAg$_2$Ge$_2$ (R = Pr, Nd and Sm) compounds which crystallize in
the ThCr$_2$Si$_2$ type crystal structure with the space group \textit{I4/mmm}.
The single crystals of RAg$_2$Ge$_2$ (R = Pr, Nd and Sm) were grown by
self-flux method using Ag:Ge binary alloy as flux. From the magnetic studies on
single crystalline samples we have found that PrAg$_2$Ge$_2$ and NdAg$_2$Ge$_2$
order antiferromagnetically at 12 K and 2 K respectively, thus corroborating
the earlier polycrystalline results. SmAg$_2$Ge$_2$ also orders
antiferromagnetically at 9.2 K. The magnetic susceptibility and magnetization
show a large anisotropy and the easy axis of magnetization for PrAg$_2$Ge$_2$
and NdAg$_2$Ge$_2$ is along the [100] direction where as it changes to [001]
direction for SmAg$_2$Ge$_2$. Two metamagnetic transitions were observed in
NdAg$_2$Ge$_2$ at $H_{\rm m1}$ = 1.25 T and $H_{\rm m2}$ =3.56 T for the field
parallel to [100] direction where as the magnetization along [001] direction
was linear indicating the hard axis of magnetization.",Devang A. Joshi
2013-04-15T21:24:06Z,http://arxiv.org/abs/1304.4272v1,Free Convex Algebraic Geometry,"This chapter is a tutorial on techniques and results in free convex algebraic
geometry and free real algebraic geometry (RAG). The term free refers to the
central role played by algebras of noncommutative polynomials R<x> in free
(freely noncommuting) variables x=(x_1,...,x_g). The subject pertains to
problems where the unknowns are matrices or Hilbert space operators as arise in
linear systems engineering and quantum information theory.
  The subject of free RAG flows in two branches. One, free positivity and
inequalities is an analog of classical real algebraic geometry, a theory of
polynomial inequalities embodied in algebraic formulas called
Positivstellens\""atze; often free Positivstellens\""atze have cleaner statements
than their commutative counterparts. Free convexity, the second branch of free
RAG, arose in an effort to unify a torrent of ad hoc optimization techniques
which came on the linear systems engineering scene in the mid 1990's.
Mathematically, much as in the commutative case, free convexity is connected
with free positivity through the second derivative: A free polynomial is convex
if and only if its Hessian is positive. However, free convexity is a very
restrictive condition, for example, free convex polynomials have degree 2 or
less.
  This article describes for a beginner techniques involving free convexity. As
such it also serves as a point of entry into the larger field of free real
algebraic geometry.",J. William Helton
2021-04-17T18:24:51Z,http://arxiv.org/abs/2104.08610v1,Zero-shot Slot Filling with DPR and RAG,"The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.",Michael Glass
2023-07-07T17:35:55Z,http://arxiv.org/abs/2307.03730v1,"First bromine doped cryogenic implosion at the National Ignition
  Facility","We report on the first experiment dedicated to the study of nuclear reactions
on dopants in a cryogenic capsule at the National Ignition Facility (NIF). This
was accomplished using bromine doping in the inner layers of the CH ablator of
a capsule identical to that used in the NIF shot N140520. The capsule was doped
with 3$\times$10$^{16}$ bromine atoms. The doped capsule shot, N170730,
resulted in a DT yield that was 2.6 times lower than the undoped equivalent.
The Radiochemical Analysis of Gaseous Samples (RAGS) system was used to collect
and detect $^{79}$Kr atoms resulting from energetic deuteron and proton ion
reactions on $^{79}$Br. RAGS was also used to detect $^{13}$N produced
dominantly by knock-on deuteron reactions on the $^{12}$C in the ablator.
High-energy reaction-in-flight neutrons were detected via the
$^{209}$Bi(n,4n)$^{206}$Bi reaction, using bismuth activation foils located 50
cm outside of the target capsule. The robustness of the RAGS signals suggest
that the use of nuclear reactions on dopants as diagnostics is quite feasible.",A. C. Hayes
2023-10-08T01:43:39Z,http://arxiv.org/abs/2310.04963v3,LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,"Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. The goal of this work is to automatically generate tests
and use these tests to validate and verify compiler implementations of a
directive-based parallel programming paradigm, OpenACC. To do so, in this
paper, we explore the capabilities of state-of-the-art LLMs, including
open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama,
Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and
GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using
our own testsuite dataset along with using the OpenACC specification. We also
explored these LLMs using various prompt engineering techniques that include
code template, template with retrieval-augmented generation (RAG), one-shot
example, one-shot with RAG, expressive prompt with code template and RAG. This
paper highlights our findings from over 5000 tests generated via all the above
mentioned methods. Our contributions include: (a) exploring the capabilities of
the latest and relevant LLMs for code generation, (b) investigating fine-tuning
and prompt methods, and (c) analyzing the outcome of LLMs generated tests
including manually analysis of representative set of tests. We found the LLM
Deepseek-Coder-33b-Instruct produced the most passing tests followed by
GPT-4-Turbo.",Christian Munley
2023-11-10T07:13:06Z,http://arxiv.org/abs/2311.05903v2,"Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-Prompting for Non-Specialist LLM Users","Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.",Jennifer Dodgson
2024-01-29T04:36:39Z,http://arxiv.org/abs/2401.15884v3,Corrective Retrieval Augmented Generation,"Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.",Shi-Qi Yan
2024-02-02T06:44:22Z,http://arxiv.org/abs/2402.01176v2,"CorpusLM: Towards a Unified Language Model on Corpus for
  Knowledge-Intensive Tasks","Large language models (LLMs) have gained significant attention in various
fields but prone to hallucination, especially in knowledge-intensive (KI)
tasks. To address this, retrieval-augmented generation (RAG) has emerged as a
popular solution to enhance factual accuracy. However, traditional retrieval
modules often rely on large document index and disconnect with generative
tasks. With the advent of generative retrieval (GR), language models can
retrieve by directly generating document identifiers (DocIDs), offering
superior performance in retrieval tasks. However, the potential relationship
between GR and downstream tasks remains unexplored. In this paper, we propose
\textbf{CorpusLM}, a unified language model that leverages external corpus to
tackle various knowledge-intensive tasks by integrating generative retrieval,
closed-book generation, and RAG through a unified greedy decoding process. We
design the following mechanisms to facilitate effective retrieval and
generation, and improve the end-to-end effectiveness of KI tasks: (1) We
develop a ranking-oriented DocID list generation strategy, which refines GR by
directly learning from a DocID ranking list, to improve retrieval quality. (2)
We design a continuous DocIDs-References-Answer generation strategy, which
facilitates effective and efficient RAG. (3) We employ well-designed
unsupervised DocID understanding tasks, to comprehend DocID semantics and their
relevance to downstream tasks. We evaluate our approach on the widely used KILT
benchmark with two variants of backbone models, i.e., T5 and Llama2.
Experimental results demonstrate the superior performance of our models in both
retrieval and downstream tasks.",Xiaoxi Li
2024-02-15T07:22:04Z,http://arxiv.org/abs/2402.09760v1,Grounding Language Model with Chunking-Free In-Context Retrieval,"This paper presents a novel Chunking-Free In-Context (CFIC) retrieval
approach, specifically tailored for Retrieval-Augmented Generation (RAG)
systems. Traditional RAG systems often struggle with grounding responses using
precise evidence text due to the challenges of processing lengthy documents and
filtering out irrelevant content. Commonly employed solutions, such as document
chunking and adapting language models to handle longer contexts, have their
limitations. These methods either disrupt the semantic coherence of the text or
fail to effectively address the issues of noise and inaccuracy in evidence
retrieval.
  CFIC addresses these challenges by circumventing the conventional chunking
process. It utilizes the encoded hidden states of documents for in-context
retrieval, employing auto-aggressive decoding to accurately identify the
specific evidence text required for user queries, eliminating the need for
chunking. CFIC is further enhanced by incorporating two decoding strategies,
namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies
not only improve the efficiency of the retrieval process but also ensure that
the fidelity of the generated grounding text evidence is maintained. Our
evaluations of CFIC on a range of open QA datasets demonstrate its superiority
in retrieving relevant and accurate evidence, offering a significant
improvement over traditional methods. By doing away with the need for document
chunking, CFIC presents a more streamlined, effective, and efficient retrieval
solution, making it a valuable advancement in the field of RAG systems.",Hongjin Qian
2024-02-19T18:31:11Z,http://arxiv.org/abs/2402.12352v1,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,"Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.",Julien Delile
2024-02-27T21:01:41Z,http://arxiv.org/abs/2402.17887v4,"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability","Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.",Junda Wang
2024-02-28T11:51:56Z,http://arxiv.org/abs/2402.18264v2,"WIKIGENBENCH: Exploring Full-length Wikipedia Generation under
  Real-World Scenario","It presents significant challenges to generate comprehensive and accurate
Wikipedia articles for newly emerging events under a real-world scenario.
Existing attempts fall short either by focusing only on short snippets or by
using metrics that are insufficient to evaluate real-world scenarios. In this
paper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,
designed to align with real-world scenarios in both generation and evaluation.
For generation, we explore a real-world scenario where structured, full-length
Wikipedia articles with citations are generated for new events using input
documents from web sources. For evaluation, we integrate systematic metrics and
LLM-based metrics to assess the verifiability, organization, and other aspects
aligned with real-world scenarios. Based on this benchmark, we conduct
extensive experiments using various models within three commonly used
frameworks: direct RAG, hierarchical structure-based RAG, and RAG with a
fine-tuned generation model. Experimental results show that hierarchical-based
methods can generate more comprehensive content, while fine-tuned methods
achieve better verifiability. However, even the best methods still show a
significant gap compared to existing Wikipedia content, indicating that further
research is necessary.",Jiebin Zhang
2024-02-29T18:20:37Z,http://arxiv.org/abs/2402.19421v1,"Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based
  Search Engines","In the domain of digital information dissemination, search engines act as
pivotal conduits linking information seekers with providers. The advent of
chat-based search engines utilizing Large Language Models (LLMs) and Retrieval
Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary
leap in the search ecosystem. They demonstrate metacognitive abilities in
interpreting web information and crafting responses with human-like
understanding and creativity. Nonetheless, the intricate nature of LLMs renders
their ""cognitive"" processes opaque, challenging even their designers'
understanding. This research aims to dissect the mechanisms through which an
LLM-powered chat-based search engine, specifically Bing Chat, selects
information sources for its responses. To this end, an extensive dataset has
been compiled through engagements with New Bing, documenting the websites it
cites alongside those listed by the conventional search engine. Employing
natural language processing (NLP) techniques, the research reveals that Bing
Chat exhibits a preference for content that is not only readable and formally
structured, but also demonstrates lower perplexity levels, indicating a unique
inclination towards text that is predictable by the underlying LLM. Further
enriching our analysis, we procure an additional dataset through interactions
with the GPT-4 based knowledge retrieval API, unveiling a congruent text
preference between the RAG API and Bing Chat. This consensus suggests that
these text preferences intrinsically emerge from the underlying language
models, rather than being explicitly crafted by Bing Chat's developers.
Moreover, our investigation documents a greater similarity among websites cited
by RAG technologies compared to those ranked highest by conventional search
engines.",Lijia Ma
2024-03-15T06:59:43Z,http://arxiv.org/abs/2403.10059v2,Repoformer: Selective Retrieval for Repository-Level Code Completion,"Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). In this paper, we propose a selective RAG
framework to avoid retrieval when unnecessary. To power this framework, we
design a self-supervised learning approach to enable a code LM to accurately
self-evaluate whether retrieval can improve its output quality and robustly
leverage the potentially noisy retrieved contexts. Using this LM as both the
selective RAG policy and the generation model, our framework achieves
state-of-the-art repository-level code completion performance on diverse
benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new
long-form code completion benchmark. Meanwhile, our analyses show that
selectively retrieving brings as much as 70% inference speedup in the online
serving setting without harming the performance. We further demonstrate that
our framework is able to accommodate different generation models, retrievers,
and programming languages. These advancements position our framework as an
important step towards more accurate and efficient repository-level code
completion.",Di Wu
2024-03-27T18:09:55Z,http://arxiv.org/abs/2403.18920v1,CPR: Retrieval Augmented Generation for Copyright Protection,"Retrieval Augmented Generation (RAG) is emerging as a flexible and robust
technique to adapt models to private users data without training, to handle
credit attribution, and to allow efficient machine unlearning at scale.
However, RAG techniques for image generation may lead to parts of the retrieved
samples being copied in the model's output. To reduce risks of leaking private
information contained in the retrieved set, we introduce Copy-Protected
generation with Retrieval (CPR), a new method for RAG with strong copyright
protection guarantees in a mixed-private setting for diffusion models.CPR
allows to condition the output of diffusion models on a set of retrieved
images, while also guaranteeing that unique identifiable information about
those example is not exposed in the generated outputs. In particular, it does
so by sampling from a mixture of public (safe) distribution and private (user)
distribution by merging their diffusion scores at inference. We prove that CPR
satisfies Near Access Freeness (NAF) which bounds the amount of information an
attacker may be able to extract from the generated images. We provide two
algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously
proposed rejection-sampling-based NAF methods, our methods enable efficient
copyright-protected sampling with a single run of backward diffusion. We show
that our method can be applied to any pre-trained conditional diffusion model,
such as Stable Diffusion or unCLIP. In particular, we empirically show that
applying CPR on top of unCLIP improves quality and text-to-image alignment of
the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit
attribution, copy-right protection, and deterministic, constant time,
unlearning.",Aditya Golatkar
2024-04-10T07:56:26Z,http://arxiv.org/abs/2404.06809v3,Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation,"The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.",Ruotong Pan
2024-04-22T09:56:59Z,http://arxiv.org/abs/2404.14043v1,"LLMs Know What They Need: Leveraging a Missing Information Guided
  Framework to Empower Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating
outdated knowledge or hallucination by supplying LLMs with updated and relevant
knowledge. However, there are still several difficulties for RAG in
understanding complex multi-hop query and retrieving relevant documents, which
require LLMs to perform reasoning and retrieve step by step. Inspired by
human's reasoning process in which they gradually search for the required
information, it is natural to ask whether the LLMs could notice the missing
information in each reasoning step. In this work, we first experimentally
verified the ability of LLMs to extract information as well as to know the
missing. Based on the above discovery, we propose a Missing Information Guided
Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the
identification of missing information to generate a targeted query that steers
the subsequent knowledge retrieval. Besides, we design a sentence-level
re-ranking filtering approach to filter the irrelevant content out from
document, along with the information extraction capability of LLMs to extract
useful information from cleaned-up documents, which in turn to bolster the
overall efficacy of RAG. Extensive experiments conducted on multiple public
datasets reveal the superiority of the proposed MIGRES method, and analytical
experiments demonstrate the effectiveness of our proposed modules.",Keheng Wang
2024-04-27T13:11:42Z,http://arxiv.org/abs/2404.17897v1,"Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented
  Large Language Models","Large-scale language models (LLMs) have achieved remarkable success across
various language tasks but suffer from hallucinations and temporal
misalignment. To mitigate these shortcomings, Retrieval-augmented generation
(RAG) has been utilized to provide external knowledge to facilitate the answer
generation. However, applying such models to the medical domain faces several
challenges due to the lack of domain-specific knowledge and the intricacy of
real-world scenarios. In this study, we explore LLMs with RAG framework for
knowledge-intensive tasks in the medical field. To evaluate the capabilities of
LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates
the real-world medication consultation scenario and requires LLMs to answer
with retrieved evidence from the medicine database. MedicineQA contains 300
multi-round question-answering pairs, each embedded within a detailed dialogue
history, highlighting the challenge posed by this knowledge-intensive task to
current LLMs. We further propose a new \textit{Distill-Retrieve-Read} framework
instead of the previous \textit{Retrieve-then-Read}. Specifically, the
distillation and retrieval process utilizes a tool calling mechanism to
formulate search queries that emulate the keyword-based inquiries used by
search engines. With experimental results, we show that our framework brings
notable performance improvements and surpasses the previous counterparts in the
evidence retrieval process in terms of evidence retrieval accuracy. This
advancement sheds light on applying RAG to the medical domain.",Zhongzhen Huang
2024-05-02T14:19:25Z,http://arxiv.org/abs/2405.01310v1,"Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf
  Disease Remediation","This research introduces an innovative AI-driven precision agriculture
system, leveraging YOLOv8 for disease identification and Retrieval Augmented
Generation (RAG) for context-aware diagnosis. Focused on addressing the
challenges of diseases affecting the coffee production sector in Karnataka, The
system integrates sophisticated object detection techniques with language
models to address the inherent constraints associated with Large Language
Models (LLMs). Our methodology not only tackles the issue of hallucinations in
LLMs, but also introduces dynamic disease identification and remediation
strategies. Real-time monitoring, collaborative dataset expansion, and
organizational involvement ensure the system's adaptability in diverse
agricultural settings. The effect of the suggested system extends beyond
automation, aiming to secure food supplies, protect livelihoods, and promote
eco-friendly farming practices. By facilitating precise disease identification,
the system contributes to sustainable and environmentally conscious
agriculture, reducing reliance on pesticides. Looking to the future, the
project envisions continuous development in RAG-integrated object detection
systems, emphasizing scalability, reliability, and usability. This research
strives to be a beacon for positive change in agriculture, aligning with global
efforts toward sustainable and technologically enhanced food production.",Selva Kumar S
2024-05-06T04:42:18Z,http://arxiv.org/abs/2405.06683v1,"ERAGent: Enhancing Retrieval-Augmented Language Models with Improved
  Accuracy, Efficiency, and Personalization","Retrieval-augmented generation (RAG) for language models significantly
improves language understanding systems. The basic retrieval-then-read pipeline
of response generation has evolved into a more extended process due to the
integration of various components, sometimes even forming loop structures.
Despite its advancements in improving response accuracy, challenges like poor
retrieval quality for complex questions that require the search of multifaceted
semantic information, inefficiencies in knowledge re-retrieval during long-term
serving, and lack of personalized responses persist. Motivated by transcending
these limitations, we introduce ERAGent, a cutting-edge framework that embodies
an advancement in the RAG area. Our contribution is the introduction of the
synergistically operated module: Enhanced Question Rewriter and Knowledge
Filter, for better retrieval quality. Retrieval Trigger is incorporated to
curtail extraneous external knowledge retrieval without sacrificing response
quality. ERAGent also personalizes responses by incorporating a learned user
profile. The efficiency and personalization characteristics of ERAGent are
supported by the Experiential Learner module which makes the AI assistant being
capable of expanding its knowledge and modeling user profile incrementally.
Rigorous evaluations across six datasets and three question-answering tasks
prove ERAGent's superior accuracy, efficiency, and personalization, emphasizing
its potential to advance the RAG field and its applicability in practical
systems.",Yunxiao Shi
2024-05-15T12:41:20Z,http://arxiv.org/abs/2405.13021v1,"IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning
  Inner Monologues","Although the Retrieval-Augmented Generation (RAG) paradigms can use external
knowledge to enhance and ground the outputs of Large Language Models (LLMs) to
mitigate generative hallucinations and static knowledge base problems, they
still suffer from limited flexibility in adopting Information Retrieval (IR)
systems with varying capabilities, constrained interpretability during the
multi-round retrieval process, and a lack of end-to-end optimization. To
address these challenges, we propose a novel LLM-centric approach, IM-RAG, that
integrates IR systems with LLMs to support multi-round RAG through learning
Inner Monologues (IM, i.e., the human inner voice that narrates one's
thoughts). During the IM process, the LLM serves as the core reasoning model
(i.e., Reasoner) to either propose queries to collect more information via the
Retriever or to provide a final answer based on the conversational context. We
also introduce a Refiner that improves the outputs from the Retriever,
effectively bridging the gap between the Reasoner and IR modules with varying
capabilities and fostering multi-round communications. The entire IM process is
optimized via Reinforcement Learning (RL) where a Progress Tracker is
incorporated to provide mid-step rewards, and the answer prediction is further
separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive
experiments with the HotPotQA dataset, a popular benchmark for retrieval-based,
multi-step question-answering. The results show that our approach achieves
state-of-the-art (SOTA) performance while providing high flexibility in
integrating IR modules as well as strong interpretability exhibited in the
learned inner monologues.",Diji Yang
2024-05-27T19:02:18Z,http://arxiv.org/abs/2405.17602v1,Augmenting Textual Generation via Topology Aware Retrieval,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.",Yu Wang
2024-05-31T16:24:53Z,http://arxiv.org/abs/2405.20978v1,"Enhancing Noise Robustness of Retrieval-Augmented Language Models with
  Adaptive Adversarial Training","Large Language Models (LLMs) exhibit substantial capabilities yet encounter
challenges, including hallucination, outdated knowledge, and untraceable
reasoning processes. Retrieval-augmented generation (RAG) has emerged as a
promising solution, integrating knowledge from external databases to mitigate
these challenges. However, inappropriate retrieved passages can potentially
hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine
themselves to a limited set of noise types, deviating from real-world retrieval
environments and limiting practical applicability. In this study, we initially
investigate retrieval noises and categorize them into three distinct types,
reflecting real-world environments. We analyze the impact of these various
retrieval noises on the robustness of LLMs. Subsequently, we propose a novel
RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's
training process in response to retrieval noises. Concurrently, it employs
multi-task learning to ensure the model's capacity to internally recognize
noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model
trained using RAAT exhibits significant improvements in F1 and EM scores under
diverse noise conditions. For reproducibility, we release our code and data at:
https://github.com/calubkk/RAAT.",Feiteng Fang
2024-05-24T20:03:32Z,http://arxiv.org/abs/2406.00031v1,"AMGPT: a Large Language Model for Contextual Querying in Additive
  Manufacturing","Generalized large language models (LLMs) such as GPT-4 may not provide
specific answers to queries formulated by materials science researchers. These
models may produce a high-level outline but lack the capacity to return
detailed instructions on manufacturing and material properties of novel alloys.
Enhancing a smaller model with specialized domain knowledge may provide an
advantage over large language models which cannot be retrained quickly enough
to keep up with the rapid pace of research in metal additive manufacturing
(AM). We introduce ""AMGPT,"" a specialized LLM text generator designed for metal
AM queries. The goal of AMGPT is to assist researchers and users in navigating
the extensive corpus of literature in AM. Instead of training from scratch, we
employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented
Generation (RAG) setup, utilizing it to dynamically incorporate information
from $\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert
these PDF documents into TeX format, facilitating their integration into the
RAG pipeline managed by LlamaIndex. Expert evaluations of this project
highlight that specific embeddings from the RAG setup accelerate response times
and maintain coherence in the generated text.",Achuth Chandrasekhar
2024-06-17T04:35:17Z,http://arxiv.org/abs/2406.11201v2,"Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large
  Language Models","Large Language Models (LLMs) have the unique capability to understand and
generate human-like text from input queries. When fine-tuned, these models show
enhanced performance on domain-specific queries. OpenAI highlights the process
of fine-tuning, stating: ""To fine-tune a model, you are required to provide at
least 10 examples. We typically see clear improvements from fine-tuning on 50
to 100 training examples, but the right number varies greatly based on the
exact use case."" This study extends this concept to the integration of LLMs
within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve
accuracy and relevance by leveraging external corpus data for information
retrieval. However, RAG's promise of delivering optimal responses often falls
short in complex query scenarios. This study aims to specifically examine the
effects of fine-tuning LLMs on their ability to extract and integrate
contextual data to enhance the performance of RAG systems across multiple
domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data
extraction and contextual understanding by comparing the accuracy and
completeness of fine-tuned models against baseline performances across datasets
from multiple domains. Our findings indicate that fine-tuning resulted in a
decline in performance compared to the baseline models, contrary to the
improvements observed in standalone LLM applications as suggested by OpenAI.
This study highlights the need for vigorous investigation and validation of
fine-tuned models for domain-specific tasks.",Scott Barnett
2024-06-17T06:48:31Z,http://arxiv.org/abs/2406.11258v2,"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented
  Generation","Large Language Models (LLMs) have shown great potential in the biomedical
domain with the advancement of retrieval-augmented generation (RAG). However,
existing retrieval-augmented approaches face challenges in addressing diverse
queries and documents, particularly for medical knowledge queries, resulting in
sub-optimal performance. To address these limitations, we propose a novel
plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search
(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree
search, SeRTS boosts the zero-shot performance of retrieving high-quality and
informative results for RAG. We further enhance retrieval performance by
fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the
trajectories collected by SeRTS as feedback. Controlled experiments using the
BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method
significantly improves the performance of the BM25 retriever and surpasses the
strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than
self-reflection. Our proposed method effectively adapts LLMs to document
retrieval tasks, enhancing their ability to retrieve highly relevant documents
for RAG in the context of medical knowledge queries. This work presents a
significant step forward in leveraging LLMs for accurate and comprehensive
biomedical question answering.",Minda Hu
2024-06-17T15:59:49Z,http://arxiv.org/abs/2406.11681v1,"R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval
  Augmented Large Language Models","Large language models have achieved remarkable success on general NLP tasks,
but they may fall short for domain-specific problems. Recently, various
Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this
shortcoming. However, existing evaluation tools only provide a few baselines
and evaluate them on various domains without mining the depth of domain
knowledge. In this paper, we address the challenges of evaluating RALLMs by
introducing the R-Eval toolkit, a Python toolkit designed to streamline the
evaluation of different RAG workflows in conjunction with LLMs. Our toolkit,
which supports popular built-in RAG workflows and allows for the incorporation
of customized testing data on the specific domain, is designed to be
user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs
across three task levels and two representative domains, revealing significant
variations in the effectiveness of RALLMs across different tasks and domains.
Our analysis emphasizes the importance of considering both task and domain
requirements when choosing a RAG workflow and LLM combination. We are committed
to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval
to facilitate both the industry and the researchers.",Shangqing Tu
2024-06-18T12:09:02Z,http://arxiv.org/abs/2406.12534v4,Unified Active Retrieval for Retrieval Augmented Generation,"In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and
applying it to every instruction is sub-optimal. Therefore, determining whether
to retrieve is crucial for RAG, which is usually referred to as Active
Retrieval. However, existing active retrieval methods face two challenges: 1.
They usually rely on a single criterion, which struggles with handling various
types of instructions. 2. They depend on specialized and highly differentiated
procedures, and thus combining them makes the RAG system more complicated and
leads to higher response latency. To address these challenges, we propose
Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts
them into plug-and-play classification tasks, which achieves multifaceted
retrieval timing judgements with negligible extra inference cost. We further
introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to
process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR
significantly outperforms existing work on the retrieval timing judgement and
the performance of downstream tasks, which shows the effectiveness of UAR and
its helpfulness to downstream tasks.",Qinyuan Cheng
2024-06-19T20:13:42Z,http://arxiv.org/abs/2406.13805v1,"WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge
  Conflicts from Wikipedia","Retrieval-augmented generation (RAG) has emerged as a promising solution to
mitigate the limitations of large language models (LLMs), such as
hallucinations and outdated information. However, it remains unclear how LLMs
handle knowledge conflicts arising from different augmented retrieved passages,
especially when these passages originate from the same source and have equal
trustworthiness. In this work, we conduct a comprehensive evaluation of
LLM-generated answers to questions that have varying answers based on
contradictory passages from Wikipedia, a dataset widely regarded as a
high-quality pre-training resource for most LLMs. Specifically, we introduce
WikiContradict, a benchmark consisting of 253 high-quality, human-annotated
instances designed to assess LLM performance when augmented with retrieved
passages containing real-world knowledge conflicts. We benchmark a diverse
range of both closed and open-source LLMs under different QA scenarios,
including RAG with a single passage, and RAG with 2 contradictory passages.
Through rigorous human evaluations on a subset of WikiContradict instances
involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and
limitations of these models. For instance, when provided with two passages
containing contradictory facts, all models struggle to generate answers that
accurately reflect the conflicting nature of the context, especially for
implicit conflicts requiring reasoning. Since human evaluation is costly, we
also introduce an automated model that estimates LLM performance using a strong
open-source language model, achieving an F-score of 0.8. Using this automated
metric, we evaluate more than 1,500 answers from seven LLMs across all
WikiContradict instances. To facilitate future work, we release WikiContradict
on: https://ibm.biz/wikicontradict.",Yufang Hou
2024-07-01T08:35:04Z,http://arxiv.org/abs/2407.01080v2,"Face4RAG: Factual Consistency Evaluation for Retrieval Augmented
  Generation in Chinese","The prevailing issue of factual inconsistency errors in conventional
Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency
Evaluation (FCE). Despite the various FCE methods proposed earlier, these
methods are evaluated on datasets generated by specific Large Language Models
(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE
methods perform on other LLMs with different error distributions or even unseen
error types, as these methods may fail to detect the error types generated by
other LLMs. To fill this gap, in this paper, we propose the first comprehensive
FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our
benchmark consists of a synthetic dataset built upon a carefully designed
typology for factuality inconsistency error and a real-world dataset
constructed from six commonly used LLMs, enabling evaluation of FCE methods on
specific error types or real-world error distributions. On the proposed
benchmark, we discover the failure of existing FCE methods to detect the
logical fallacy, which refers to a mismatch of logic structures between the
answer and the retrieved reference. To fix this issue, we further propose a new
method called \emph{L-Face4RAG} with two novel designs of logic-preserving
answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG
substantially outperforms previous methods for factual inconsistency detection
on a wide range of tasks, notably beyond the RAG task from which it is
originally motivated. Both the benchmark and our proposed method are publicly
available.\footnote{\url{https://huggingface.co/datasets/yq27/Face4RAG}\label{link_face4rag}}",Yunqi Xu
2024-07-01T11:07:23Z,http://arxiv.org/abs/2407.01178v1,$\text{Memory}^3$: Language Modeling with Explicit Memory,"The training and inference of large language models (LLMs) are together a
costly process that transports knowledge from raw data to meaningful
computation. Inspired by the memory hierarchy of the human brain, we reduce
this cost by equipping LLMs with explicit memory, a memory format cheaper than
model parameters and text retrieval-augmented generation (RAG). Conceptually,
with most of its knowledge externalized to explicit memories, the LLM can enjoy
a smaller parameter size, training cost, and inference cost, all proportional
to the amount of remaining ""abstract knowledge"". As a preliminary proof of
concept, we train from scratch a 2.4B LLM, which achieves better performance
than much larger LLMs as well as RAG models, and maintains higher decoding
speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is
the third form of memory in LLMs after implicit memory (model parameters) and
working memory (context key-values). We introduce a memory circuitry theory to
support the externalization of knowledge, and present novel techniques
including a memory sparsification mechanism that makes storage tractable and a
two-stage pretraining scheme that facilitates memory formation.",Hongkang Yang
2024-07-06T16:45:07Z,http://arxiv.org/abs/2407.05131v2,"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language
  Models","The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on medical VQA and report generation tasks across three
datasets, achieving an average improvement of 47.4% in factual accuracy. We
publicly release our benchmark and code in
https://github.com/richard-peng-xia/RULE.",Peng Xia
2024-07-16T18:09:21Z,http://arxiv.org/abs/2407.12101v1,Better RAG using Relevant Information Gain,"A common way to extend the memory of large language models (LLMs) is by
retrieval augmented generation (RAG), which inserts text retrieved from a
larger memory into an LLM's context window. However, the context window is
typically limited to several thousand tokens, which limits the number of
retrieved passages that can inform a model's response. For this reason, it's
important to avoid occupying context window space with redundant information by
ensuring a degree of diversity among retrieved passages. At the same time, the
information should also be relevant to the current task. Most prior methods
that encourage diversity among retrieved results, such as Maximal Marginal
Relevance (MMR), do so by incorporating an objective that explicitly trades off
diversity and relevance. We propose a novel simple optimization metric based on
relevant information gain, a probabilistic measure of the total information
relevant to a query for a set of retrieved results. By optimizing this metric,
diversity organically emerges from our system. When used as a drop-in
replacement for the retrieval component of a RAG system, this method yields
state-of-the-art performance on question answering tasks from the Retrieval
Augmented Generation Benchmark (RGB), outperforming existing metrics that
directly optimize for relevance and diversity.",Marc Pickett
2024-07-20T10:46:42Z,http://arxiv.org/abs/2407.14838v1,"Retrieval Augmented Generation Integrated Large Language Models in Smart
  Contract Vulnerability Detection","The rapid growth of Decentralized Finance (DeFi) has been accompanied by
substantial financial losses due to smart contract vulnerabilities,
underscoring the critical need for effective security auditing. With attacks
becoming more frequent, the necessity and demand for auditing services has
escalated. This especially creates a financial burden for independent
developers and small businesses, who often have limited available funding for
these services. Our study builds upon existing frameworks by integrating
Retrieval-Augmented Generation (RAG) with large language models (LLMs),
specifically employing GPT-4-1106 for its 128k token context window. We
construct a vector store of 830 known vulnerable contracts, leveraging Pinecone
for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and
LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a
binary answer for vulnerability detection. We first test 52 smart contracts 40
times each against a provided vulnerability type, verifying the replicability
and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7%
success rate in guided detection of vulnerabilities. Second, we challenge the
model under a ""blind"" audit setup, without the vulnerability type provided in
the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates
the general vulnerability detection capabilities without hinted context
assistance. Under these conditions, a 60.71% success rate was observed. While
the results are promising, we still emphasize the need for human auditing at
this time. We provide this study as a proof of concept for a cost-effective
smart contract auditing process, moving towards democratic access to security.",Jeffy Yu
2024-06-18T14:23:54Z,http://arxiv.org/abs/2407.16896v1,"Free to play: UN Trade and Development's experience with developing its
  own open-source Retrieval Augmented Generation Large Language Model
  application","Generative artificial intelligence (AI), and in particular Large Language
Models (LLMs), have exploded in popularity and attention since the release to
the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in
November of 2022. Due to the power of these general purpose models and their
ability to communicate in natural language, they can be useful in a range of
domains, including the work of official statistics and international
organizations. However, with such a novel and seemingly complex technology, it
can feel as if generative AI is something that happens to an organization,
something that can be talked about but not understood, that can be commented on
but not contributed to. Additionally, the costs of adoption and operation of
proprietary solutions can be both uncertain and high, a barrier for often
cost-constrained international organizations. In the face of these challenges,
United Nations Trade and Development (UNCTAD), through its Global Crisis
Response Group (GCRG), has explored and developed its own open-source Retrieval
Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more
useful for the organization's domain and work. Developing in-house solutions
comes with pros and cons, with pros including cost, flexibility, and fostering
institutional knowledge. Cons include time and skill investments and gaps and
application polish and power. The three libraries developed to produce the app,
nlp_pipeline for document processing and statistical analysis, local_rag_llm
for running a local RAG LLM, and streamlit_rag for the user interface, are
publicly available on PyPI and GitHub with Dockerfiles. A fourth library,
local_llm_finetune, is also available for fine-tuning existing LLMs which can
then be used in the application.",Daniel Hopp
2024-07-29T00:41:48Z,http://arxiv.org/abs/2407.19619v1,"Enhancing Code Translation in Language Models with Few-Shot Learning via
  Retrieval-Augmented Generation","The advent of large language models (LLMs) has significantly advanced the
field of code translation, enabling automated translation between programming
languages. However, these models often struggle with complex translation tasks
due to inadequate contextual understanding. This paper introduces a novel
approach that enhances code translation through Few-Shot Learning, augmented
with retrieval-based techniques. By leveraging a repository of existing code
translations, we dynamically retrieve the most relevant examples to guide the
model in translating new code segments. Our method, based on
Retrieval-Augmented Generation (RAG), substantially improves translation
quality by providing contextual examples from which the model can learn in
real-time. We selected RAG over traditional fine-tuning methods due to its
ability to utilize existing codebases or a locally stored corpus of code, which
allows for dynamic adaptation to diverse translation tasks without extensive
retraining. Extensive experiments on diverse datasets with open LLM models such
as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code
Instruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5
Turbo and GPT-4o, demonstrate our approach's superiority over traditional
zero-shot methods, especially in translating between Fortran and CPP. We also
explored varying numbers of shots i.e. examples provided during inference,
specifically 1, 2, and 3 shots and different embedding models for RAG,
including Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and
effectiveness of our approach.",Manish Bhattarai
2024-07-24T12:27:33Z,http://arxiv.org/abs/2407.21055v1,"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications","Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.",Cui Long
2024-08-02T19:49:19Z,http://arxiv.org/abs/2408.04645v1,"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors
  for a Robotics Course","This study evaluates the performance of Large Language Models (LLMs) as an
Artificial Intelligence-based tutor for a university course. In particular,
different advanced techniques are utilized, such as prompt engineering,
Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the
different models and applied techniques using common similarity metrics like
BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of
helpfulness and trustworthiness. Our findings indicate that RAG combined with
prompt engineering significantly enhances model responses and produces better
factual answers. In the context of education, RAG appears as an ideal technique
as it is based on enriching the input of the model with additional information
and material which usually is already present for a university course.
Fine-tuning, on the other hand, can produce quite small, still strong expert
models, but poses the danger of overfitting. Our study further asks how we
measure performance of LLMs and how well current measurements represent
correctness or relevance? We find high correlation on similarity metrics and a
bias of most of these metrics towards shorter responses. Overall, our research
points to both the potential and challenges of integrating LLMs in educational
settings, suggesting a need for balanced training approaches and advanced
evaluation frameworks.",Sebastian Kahl
2024-08-09T15:53:55Z,http://arxiv.org/abs/2408.05141v3,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",Ye Yuan
2024-08-12T16:33:51Z,http://arxiv.org/abs/2408.06272v1,"A RAG-Based Question-Answering Solution for Cyber-Attack Investigation
  and Attribution","In the constantly evolving field of cybersecurity, it is imperative for
analysts to stay abreast of the latest attack trends and pertinent information
that aids in the investigation and attribution of cyber-attacks. In this work,
we introduce the first question-answering (QA) model and its application that
provides information to the cybersecurity experts about cyber-attacks
investigations and attribution. Our QA model is based on Retrieval Augmented
Generation (RAG) techniques together with a Large Language Model (LLM) and
provides answers to the users' queries based on either our knowledge base (KB)
that contains curated information about cyber-attacks investigations and
attribution or on outside resources provided by the users. We have tested and
evaluated our QA model with various types of questions, including KB-based,
metadata-based, specific documents from the KB, and external sources-based
questions. We compared the answers for KB-based questions with those from
OpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms
OpenAI's GPT models by providing the source of the answers and overcoming the
hallucination limitations of the GPT models, which is critical for cyber-attack
investigation and attribution. Additionally, our analysis showed that when the
RAG QA model is given few-shot examples rather than zero-shot instructions, it
generates better answers compared to cases where no examples are supplied in
addition to the query.",Sampath Rajapaksha
2024-08-14T15:19:16Z,http://arxiv.org/abs/2408.07611v2,"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs","Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.",Weijian Xie
2024-08-12T08:54:32Z,http://arxiv.org/abs/2408.08901v1,Bayesian inference to improve quality of Retrieval Augmented Generation,"Retrieval Augmented Generation or RAG is the most popular pattern for modern
Large Language Model or LLM applications. RAG involves taking a user query and
finding relevant paragraphs of context in a large corpus typically captured in
a vector database. Once the first level of search happens over a vector
database, the top n chunks of relevant text are included directly in the
context and sent as prompt to the LLM. Problem with this approach is that
quality of text chunks depends on effectiveness of search. There is no strong
post processing after search to determine if the chunk does hold enough
information to include in prompt. Also many times there may be chunks that have
conflicting information on the same subject and the model has no prior
experience which chunk to prioritize to make a decision. Often times, this
leads to the model providing a statement that there are conflicting statements,
and it cannot produce an answer. In this research we propose a Bayesian
approach to verify the quality of text chunks from the search results. Bayes
theorem tries to relate conditional probabilities of the hypothesis with
evidence and prior probabilities. We propose that, finding likelihood of text
chunks to give a quality answer and using prior probability of quality of text
chunks can help us improve overall quality of the responses from RAG systems.
We can use the LLM itself to get a likelihood of relevance of a context
paragraph. For prior probability of the text chunk, we use the page number in
the documents parsed. Assumption is that that paragraphs in earlier pages have
a better probability of being findings and more relevant to generalizing an
answer.",Dattaraj Rao
2024-08-15T12:20:24Z,http://arxiv.org/abs/2408.08921v2,Graph Retrieval-Augmented Generation: A Survey,"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.",Boci Peng
2024-08-20T17:49:51Z,http://arxiv.org/abs/2408.11043v1,"Reconciling Methodological Paradigms: Employing Large Language Models as
  Novice Qualitative Research Assistants in Talent Management Research","Qualitative data collection and analysis approaches, such as those employing
interviews and focus groups, provide rich insights into customer attitudes,
sentiment, and behavior. However, manually analyzing qualitative data requires
extensive time and effort to identify relevant topics and thematic insights.
This study proposes a novel approach to address this challenge by leveraging
Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for
analyzing interview transcripts. The novelty of this work lies in strategizing
the research inquiry as one that is augmented by an LLM that serves as a novice
research assistant. This research explores the mental model of LLMs to serve as
novice qualitative research assistants for researchers in the talent management
space. A RAG-based LLM approach is extended to enable topic modeling of
semi-structured interview data, showcasing the versatility of these models
beyond their traditional use in information retrieval and search. Our findings
demonstrate that the LLM-augmented RAG approach can successfully extract topics
of interest, with significant coverage compared to manually generated topics
from the same dataset. This establishes the viability of employing LLMs as
novice qualitative research assistants. Additionally, the study recommends that
researchers leveraging such models lean heavily on quality criteria used in
traditional qualitative research to ensure rigor and trustworthiness of their
approach. Finally, the paper presents key recommendations for industry
practitioners seeking to reconcile the use of LLMs with established qualitative
research paradigms, providing a roadmap for the effective integration of these
powerful, albeit novice, AI tools in the analysis of qualitative datasets
within talent",Sreyoshi Bhaduri
2024-08-22T09:37:40Z,http://arxiv.org/abs/2408.12249v1,LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction,"Large Language Models (LLMs) are increasingly adopted for applications in
healthcare, reaching the performance of domain experts on tasks such as
question answering and document summarisation. Despite their success on these
tasks, it is unclear how well LLMs perform on tasks that are traditionally
pursued in the biomedical domain, such as structured information extration. To
breach this gap, in this paper, we systematically benchmark LLM performance in
Medical Classification and Named Entity Recognition (NER) tasks. We aim to
disentangle the contribution of different factors to the performance,
particularly the impact of LLMs' task knowledge and reasoning capabilities,
their (parametric) domain knowledge, and addition of external knowledge. To
this end we evaluate various open LLMs -- including BioMistral and Llama-2
models -- on a diverse set of biomedical datasets, using standard prompting,
Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as
Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
Counter-intuitively, our results reveal that standard prompting consistently
outperforms more complex techniques across both tasks, laying bare the
limitations in the current application of CoT, self-consistency and RAG in the
biomedical domain. Our findings suggest that advanced prompting methods
developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are
not easily portable to biomedical tasks where precise structured outputs are
required. This highlights the need for more effective integration of external
knowledge and reasoning mechanisms in LLMs to enhance their performance in
real-world biomedical applications.",Aishik Nagar
2024-08-30T07:57:30Z,http://arxiv.org/abs/2408.17072v2,"MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for
  Retrieval-Augmented Large Language Models","In a real-world RAG system, the current query often involves spoken ellipses
and ambiguous references from dialogue contexts, necessitating query rewriting
to better describe user's information needs. However, traditional context-based
rewriting has minimal enhancement on downstream generation tasks due to the
lengthy process from query rewriting to response generation. Some researchers
try to utilize reinforcement learning with generation feedback to assist the
rewriter, but these sparse rewards provide little guidance in most cases,
leading to unstable training and generation results. We find that user's needs
are also reflected in the gold document, retrieved documents and ground truth.
Therefore, by feeding back these multi-aspect dense rewards to query rewriting,
more stable and satisfactory responses can be achieved. In this paper, we
propose a novel query rewriting method MaFeRw, which improves RAG performance
by integrating multi-aspect feedback from both the retrieval process and
generated results. Specifically, we first use manual data to train a T5 model
for the rewriter initialization. Next, we design three metrics as reinforcement
learning feedback: the similarity between the rewritten query and the gold
document, the ranking metrics, and ROUGE between the generation and the ground
truth. Inspired by RLAIF, we train three kinds of reward models for the above
metrics to achieve more efficient training. Finally, we combine the scores of
these reward models as feedback, and use PPO algorithm to explore the optimal
query rewriting strategy. Experimental results on two conversational RAG
datasets demonstrate that MaFeRw achieves superior generation metrics and more
stable training compared to baselines.",Yujing Wang
2024-09-03T03:25:59Z,http://arxiv.org/abs/2409.01579v1,"AdaComp: Extractive Context Compression with Adaptive Predictor for
  Retrieval-Augmented Large Language Models","Retrieved documents containing noise will hinder RAG from detecting answer
clues and make the inference process slow and expensive. Therefore, context
compression is necessary to enhance its accuracy and efficiency. Existing
context compression methods use extractive or generative models to retain the
most query-relevant sentences or apply the information bottleneck theory to
preserve sufficient information. However, these methods may face issues such as
over-compression or high computational costs. We observe that the retriever
often ranks relevant documents at the top, but the exact number of documents
needed to answer the query is uncertain due to the impact of query complexity
and retrieval quality: complex queries like multi-hop questions may require
retaining more documents than simpler queries, and a low-quality retrieval may
need to rely on more documents to generate accurate outputs. Therefore,
determining the minimum number of required documents (compression rate) is
still a challenge for RAG. In this paper, we introduce AdaComp, a low-cost
extractive context compression method that adaptively determines the
compression rate based on both query complexity and retrieval quality.
Specifically, we first annotate the minimum top-k documents necessary for the
RAG system to answer the current query as the compression rate and then
construct triplets of the query, retrieved documents, and its compression rate.
Then, we use this triplet dataset to train a compression-rate predictor.
Experiments on three QA datasets and one conversational Muiti-doc QA dataset
show that AdaComp significantly reduces inference costs while maintaining
performance nearly identical to uncompressed models, achieving a balance
between efficiency and performance.",Qianchi Zhang
2024-09-05T13:45:42Z,http://arxiv.org/abs/2409.04475v2,"Revolutionizing Database Q&A with Large Language Models: Comprehensive
  Benchmark and Evaluation","The development of Large Language Models (LLMs) has revolutionized QA across
various industries, including the database domain. However, there is still a
lack of a comprehensive benchmark to evaluate the capabilities of different
LLMs and their modular components in database QA. To this end, we introduce
DQABench, the first comprehensive database QA benchmark for LLMs. DQABench
features an innovative LLM-based method to automate the generation, cleaning,
and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in
English and Chinese, separately. These QA pairs cover a wide range of
database-related knowledge extracted from manuals, online communities, and
database instances. This inclusion allows for an additional assessment of LLMs'
Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)
capabilities in the database QA task. Furthermore, we propose a comprehensive
LLM-based database QA testbed DQATestbed. This testbed is highly modular and
scalable, with basic and advanced components such as Question Classification
Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,
DQABench provides a comprehensive evaluation pipeline that computes various
metrics throughout a standardized evaluation process to ensure the accuracy and
fairness of the evaluation. We use DQABench to evaluate the database QA
capabilities under the proposed testbed comprehensively. The evaluation reveals
findings like (i) the strengths and limitations of nine LLM-based QA bots and
(ii) the performance impact and potential improvements of various service
components (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the
future development of LLM-based database QA research.",Yihang Zheng
2024-09-19T17:52:07Z,http://arxiv.org/abs/2409.12941v2,"Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented
  Generation","Large Language Models (LLMs) have demonstrated significant performance
improvements across various cognitive tasks. An emerging application is using
LLMs to enhance retrieval-augmented generation (RAG) capabilities. These
systems require LLMs to understand user queries, retrieve relevant information,
and synthesize coherent and accurate responses. Given the increasing real-world
deployment of such systems, comprehensive evaluation becomes crucial. To this
end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),
a high-quality evaluation dataset designed to test LLMs' ability to provide
factual responses, assess retrieval capabilities, and evaluate the reasoning
required to generate final answers. While previous work has provided datasets
and benchmarks to evaluate these abilities in isolation, FRAMES offers a
unified framework that provides a clearer picture of LLM performance in
end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions
that require the integration of information from multiple sources. We present
baseline results demonstrating that even state-of-the-art LLMs struggle with
this task, achieving 0.40 accuracy with no retrieval. The accuracy is
significantly improved with our proposed multi-step retrieval pipeline,
achieving an accuracy of 0.66 (>50% improvement). We hope our work will help
bridge evaluation gaps and assist in developing more robust and capable RAG
systems.",Satyapriya Krishna
2024-09-03T03:31:37Z,http://arxiv.org/abs/2409.13694v2,"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study","Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach for mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of externeal knowledge source. However, in
real-world applications, most situations involve diverse knowledge from various
sources, yet this area has been less explored. The main dilemma is the lack of
a suitable dataset containing multiple knowledge sources and pre-exploration of
the associated issues. To address these challenges, we standardize a benchmark
dataset that combines structured and unstructured knowledge across diverse and
complementary domains. Based on this dataset, we further develop a
plug-and-play RAG framework, PruningRAG, whose main characteristic is to employ
multi-granularity pruning strategies for optimizing the integration of relevant
information and minimizing misleading context. Building upon the standardized
dataset and PruningRAG, we also report a series of experimental results, as
well as insightful findings. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of
advancing future research in the RAG community.",Shuo Yu
2024-09-23T14:51:22Z,http://arxiv.org/abs/2409.15076v1,"Enhancing Scientific Reproducibility Through Automated BioCompute Object
  Creation Using Retrieval-Augmented Generation from Publications","The exponential growth in computational power and accessibility has
transformed the complexity and scale of bioinformatics research, necessitating
standardized documentation for transparency, reproducibility, and regulatory
compliance. The IEEE BioCompute Object (BCO) standard addresses this need but
faces adoption challenges due to the overhead of creating compliant
documentation, especially for legacy research. This paper presents a novel
approach to automate the creation of BCOs from scientific papers using
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We
describe the development of the BCO assistant tool that leverages RAG to
extract relevant information from source papers and associated code
repositories, addressing key challenges such as LLM hallucination and
long-context understanding. The implementation incorporates optimized retrieval
processes, including a two-pass retrieval with re-ranking, and employs
carefully engineered prompts for each BCO domain. We discuss the tool's
architecture, extensibility, and evaluation methods, including automated and
manual assessment approaches. The BCO assistant demonstrates the potential to
significantly reduce the time and effort required for retroactive documentation
of bioinformatics research while maintaining compliance with the standard. This
approach opens avenues for AI-assisted scientific documentation and knowledge
extraction from publications thereby enhancing scientific reproducibility. The
BCO assistant tool and documentation is available at
https://biocompute-objects.github.io/bco-rag/.",Sean Kim
2024-09-23T17:56:08Z,http://arxiv.org/abs/2409.15260v1,"Generative AI Is Not Ready for Clinical Use in Patient Education for
  Lower Back Pain Patients, Even With Retrieval-Augmented Generation","Low back pain (LBP) is a leading cause of disability globally. Following the
onset of LBP and subsequent treatment, adequate patient education is crucial
for improving functionality and long-term outcomes. Despite advancements in
patient education strategies, significant gaps persist in delivering
personalized, evidence-based information to patients with LBP. Recent
advancements in large language models (LLMs) and generative artificial
intelligence (GenAI) have demonstrated the potential to enhance patient
education. However, their application and efficacy in delivering educational
content to patients with LBP remain underexplored and warrant further
investigation. In this study, we introduce a novel approach utilizing LLMs with
Retrieval-Augmented Generation (RAG) and few-shot learning to generate tailored
educational materials for patients with LBP. Physical therapists manually
evaluated our model responses for redundancy, accuracy, and completeness using
a Likert scale. In addition, the readability of the generated education
materials is assessed using the Flesch Reading Ease score. The findings
demonstrate that RAG-based LLMs outperform traditional LLMs, providing more
accurate, complete, and readable patient education materials with less
redundancy. Having said that, our analysis reveals that the generated materials
are not yet ready for use in clinical practice. This study underscores the
potential of AI-driven models utilizing RAG to improve patient education for
LBP; however, significant challenges remain in ensuring the clinical relevance
and granularity of content generated by these models.",Yi-Fei Zhao
2024-09-24T07:29:05Z,http://arxiv.org/abs/2409.15817v1,"SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs
  and Agents","The advancement of artificial intelligence algorithms has expanded their
application to several fields such as the biomedical domain. Artificial
intelligence systems, including Large Language Models (LLMs), can be
particularly advantageous in drug discovery, which is a very long and expensive
process. However, LLMs by themselves lack in-depth knowledge about specific
domains and can generate factually incorrect information. Moreover, they are
not able to perform more complex actions that imply the usage of external
tools. Our work is focused on these two issues. Firstly, we show how the
implementation of an advanced RAG system can help the LLM to generate more
accurate answers to drug-discovery-related questions. The results show that the
answers generated by the LLM with the RAG system surpass in quality the answers
produced by the model without RAG. Secondly, we show how to create an automatic
target dossier using LLMs and incorporating them with external tools that they
can use to execute more intricate tasks to gather data such as accessing
databases and executing code. The result is a production-ready target dossier
containing the acquired information summarized into a PDF and a PowerPoint
presentation.",Gabriele Fossi
2024-10-02T01:59:07Z,http://arxiv.org/abs/2410.01171v1,"BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented
  Generation","Large language models excel at creative generation but continue to struggle
with the issues of hallucination and bias. While retrieval-augmented generation
(RAG) provides a framework for grounding LLMs' responses in accurate and
up-to-date information, it still raises the question of bias: which sources
should be selected for inclusion in the context? And how should their
importance be weighted? In this paper, we study the challenge of cross-lingual
RAG and present a dataset to investigate the robustness of existing systems at
answering queries about geopolitical disputes, which exist at the intersection
of linguistic, cultural, and political boundaries. Our dataset is sourced from
Wikipedia pages containing information relevant to the given queries and we
investigate the impact of including additional context, as well as the
composition of this context in terms of language and source, on an LLM's
response. Our results show that existing RAG systems continue to be challenged
by cross-lingual use cases and suffer from a lack of consistency when they are
provided with competing information in multiple languages. We present case
studies to illustrate these issues and outline steps for future research to
address these challenges. We make our dataset and code publicly available at
https://github.com/manestay/bordIRlines.",Bryan Li
2024-10-04T14:21:27Z,http://arxiv.org/abs/2410.03461v1,"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding
  Verification in Retrieval Augmented Generation","While retrieval augmented generation (RAG) has been shown to enhance
factuality of large language model (LLM) outputs, LLMs still suffer from
hallucination, generating incorrect or irrelevant information. One common
detection strategy involves prompting the LLM again to assess whether its
response is grounded in the retrieved evidence, but this approach is costly.
Alternatively, lightweight natural language inference (NLI) models for
efficient grounding verification can be used at inference time. While existing
pre-trained NLI models offer potential solutions, their performance remains
subpar compared to larger models on realistic RAG inputs. RAG inputs are more
complex than most datasets used for training NLI models and have
characteristics specific to the underlying knowledge base, requiring adaptation
of the NLI models to a specific target domain. Additionally, the lack of
labeled instances in the target domain makes supervised domain adaptation,
e.g., through fine-tuning, infeasible. To address these challenges, we
introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework
enables unsupervised domain adaptation through synthetic data generation.
Unlike previous methods that rely on handcrafted filtering and augmentation
strategies, Auto-GDA employs an iterative process to continuously improve the
quality of generated samples using weak labels from less efficient teacher
models and discrete optimization to select the most promising augmented
samples. Experimental results demonstrate the effectiveness of our approach,
with models fine-tuned on synthetic data using Auto-GDA often surpassing the
performance of the teacher model and reaching the performance level of LLMs at
10 % of their computational cost.",Tobias Leemann
2024-10-18T16:44:22Z,http://arxiv.org/abs/2410.14594v2,"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases","Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).",Elias Lumer
2024-10-21T09:22:29Z,http://arxiv.org/abs/2410.15805v1,"RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for
  IT Operations and Maintenance","With the ever-increasing demands on Question Answering (QA) systems for IT
operations and maintenance, an efficient and supervised fine-tunable framework
is necessary to ensure the data security, private deployment and continuous
upgrading. Although Large Language Models (LLMs) have notably improved the
open-domain QA's performance, how to efficiently handle enterprise-exclusive
corpora and build domain-specific QA systems are still less-studied for
industrial applications. In this paper, we propose a general and comprehensive
framework based on Retrieval Augmented Generation (RAG) and facilitate the
whole business process of establishing QA systems for IT operations and
maintenance. In accordance with the prevailing RAG method, our proposed
framework, named with RAG4ITOps, composes of two major stages: (1) Models
Fine-tuning \& Data Vectorization, and (2) Online QA System Process. At the
Stage 1, we leverage a contrastive learning method with two negative sampling
strategies to fine-tune the embedding model, and design the instruction
templates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method.
At the Stage 2, an efficient process of QA system is built for serving. We
collect enterprise-exclusive corpora from the domain of cloud computing, and
the extensive experiments show that our method achieves superior results than
counterparts on two kinds of QA tasks. Our experiment also provide a case for
applying the RAG4ITOps to real-world enterprise-level applications.",Tianyang Zhang
2024-10-17T22:04:32Z,http://arxiv.org/abs/2410.16322v1,"SouLLMate: An Application Enhancing Diverse Mental Health Support with
  Adaptive LLMs, Prompt Engineering, and RAG Techniques","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide diverse, accessible, stigma-free, personalized, and
real-time mental health support through cutting-edge AI technologies. It makes
the following contributions: (1) Conducting an extensive survey of recent
mental health support methods to identify prevalent functionalities and unmet
needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates
LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt
engineering, and domain knowledge. This system offers advanced features such as
Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for
personalized profile uploads and Conversational Information Extraction. (3)
Developing novel evaluation approaches for preliminary assessments and risk
detection via professionally annotated interview data and real-life suicide
tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive
Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to
enhance model performance and usability through context-sensitive response
adjustments, semantic coherence evaluations, and enhanced accuracy of
long-context reasoning in language models. This study contributes to advancing
mental health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-28T04:39:32Z,http://arxiv.org/abs/2410.20724v2,"Simple is Effective: The Roles of Graphs and Large Language Models in
  Knowledge-Graph-Based Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.",Mufei Li
2024-11-20T04:47:42Z,http://arxiv.org/abs/2411.15203v1,"Multimodal large language model for wheat breeding: a new exploration of
  smart breeding","UAV remote sensing technology has become a key technology in crop breeding,
which can achieve high-throughput and non-destructive collection of crop
phenotyping data. However, the multidisciplinary nature of breeding has brought
technical barriers and efficiency challenges to knowledge mining. Therefore, it
is important to develop a smart breeding goal tool to mine cross-domain
multimodal data. Based on different pre-trained open-source multimodal large
language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used
supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and
reinforcement learning from human feedback (RLHF) technologies to inject
cross-domain knowledge into MLLMs, thereby constructing multiple multimodal
large language models for wheat breeding (WBLMs). The above WBLMs were
evaluated using the newly created evaluation benchmark in this study. The
results showed that the WBLM constructed using SFT, RAG and RLHF technologies
and InternVL2-8B has leading performance. Then, subsequent experiments were
conducted using the WBLM. Ablation experiments indicated that the combination
of SFT, RAG, and RLHF technologies can improve the overall generation
performance, enhance the generated quality, balance the timeliness and
adaptability of the generated answer, and reduce hallucinations and biases. The
WBLM performed best in wheat yield prediction using cross-domain data (remote
sensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of
0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate
professional decision support answers for phenotyping estimation, environmental
stress assessment, target germplasm screening, cultivation technique
recommendation, and seed price query tasks.",Guofeng Yang
2024-11-24T03:56:43Z,http://arxiv.org/abs/2411.15700v1,"RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large
  Language Models on Dietary Supplements","\textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.",Zaifu Zhan
2024-11-26T03:22:01Z,http://arxiv.org/abs/2411.17073v1,"Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology
  Visual Question Answering","Accurate diagnosis and prognosis assisted by pathology images are essential
for cancer treatment selection and planning. Despite the recent trend of
adopting deep-learning approaches for analyzing complex pathology images, they
fall short as they often overlook the domain-expert understanding of tissue
structure and cell composition. In this work, we focus on a challenging
Open-ended Pathology VQA (PathVQA-Open) task and propose a novel framework
named Path-RAG, which leverages HistoCartography to retrieve relevant domain
knowledge from pathology images and significantly improves performance on
PathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG
adopts a human-centered AI approach by retrieving domain knowledge using
HistoCartography to select the relevant patches from pathology images. Our
experiments suggest that domain guidance can significantly boost the accuracy
of LLaVA-Med from 38% to 47%, with a notable gain of 28% for H&E-stained
pathology images in the PathVQA-Open dataset. For longer-form question and
answer pairs, our model consistently achieves significant improvements of 32.5%
in ARCH-Open PubMed and 30.6% in ARCH-Open Books on H\&E images. Our code and
dataset is available here (https://github.com/embedded-robotics/path-rag).",Awais Naeem
2024-11-28T15:53:27Z,http://arxiv.org/abs/2411.19229v2,Habit Coach: Customising RAG-based chatbots to support behavior change,"This paper presents the iterative development of Habit Coach, a GPT-based
chatbot designed to support users in habit change through personalized
interaction. Employing a user-centered design approach, we developed the
chatbot using a Retrieval-Augmented Generation (RAG) system, which enables
behavior personalization without retraining the underlying language model
(GPT-4). The system leverages document retrieval and specialized prompts to
tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and
narrative therapy techniques. A key challenge in the development process was
the difficulty of translating declarative knowledge into effective interaction
behaviors. In the initial phase, the chatbot was provided with declarative
knowledge about CBT via reference textbooks and high-level conversational
goals. However, this approach resulted in imprecise and inefficient behavior,
as the GPT model struggled to convert static information into dynamic and
contextually appropriate interactions. This highlighted the limitations of
relying solely on declarative knowledge to guide chatbot behavior, particularly
in nuanced, therapeutic conversations. Over four iterations, we addressed this
issue by gradually transitioning towards procedural knowledge, refining the
chatbot's interaction strategies, and improving its overall effectiveness. In
the final evaluation, 5 participants engaged with the chatbot over five
consecutive days, receiving individualized CBT interventions. The Self-Report
Habit Index (SRHI) was used to measure habit strength before and after the
intervention, revealing a reduction in habit strength post-intervention. These
results underscore the importance of procedural knowledge in driving effective,
personalized behavior change support in RAG-based systems.",Arian Fooroogh Mand Arabi
2024-11-29T20:13:56Z,http://arxiv.org/abs/2412.00239v1,Generating a Low-code Complete Workflow via Task Decomposition and RAG,"AI technologies are moving rapidly from research to production. With the
popularity of Foundation Models (FMs) that generate text, images, and video,
AI-based systems are increasing their complexity. Compared to traditional
AI-based software, systems employing FMs, or GenAI-based systems, are more
difficult to design due to their scale and versatility. This makes it necessary
to document best practices, known as design patterns in software engineering,
that can be used across GenAI applications. Our first contribution is to
formalize two techniques, Task Decomposition and Retrieval-Augmented Generation
(RAG), as design patterns for GenAI-based systems. We discuss their trade-offs
in terms of software quality attributes and comment on alternative approaches.
We recommend to AI practitioners to consider these techniques not only from a
scientific perspective but also from the standpoint of desired engineering
properties such as flexibility, maintainability, safety, and security. As a
second contribution, we describe our industry experience applying Task
Decomposition and RAG to build a complex real-world GenAI application for
enterprise users: Workflow Generation. The task of generating workflows entails
generating a specific plan using data from the system environment, taking as
input a user requirement. As these two patterns affect the entire AI
development cycle, we explain how they impacted the dataset creation, model
training, model evaluation, and deployment phases.",Orlando Marquez Ayala
2024-12-10T18:17:02Z,http://arxiv.org/abs/2412.07724v2,Granite Guardian,"We introduce the Granite Guardian models, a suite of safeguards designed to
provide risk detection for prompts and responses, enabling safe and responsible
use in combination with any large language model (LLM). These models offer
comprehensive coverage across multiple risk dimensions, including social bias,
profanity, violence, sexual content, unethical behavior, jailbreaking, and
hallucination-related risks such as context relevance, groundedness, and answer
relevance for retrieval-augmented generation (RAG). Trained on a unique dataset
combining human annotations from diverse sources and synthetic data, Granite
Guardian models address risks typically overlooked by traditional risk
detection models, such as jailbreaks and RAG-specific issues. With AUC scores
of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks
respectively, Granite Guardian is the most generalizable and competitive model
available in the space. Released as open-source, Granite Guardian aims to
promote responsible AI development across the community.
  https://github.com/ibm-granite/granite-guardian",Inkit Padhi
2024-12-17T05:38:27Z,http://arxiv.org/abs/2412.12559v2,"EXIT: Context-Aware Extractive Compression for Enhancing
  Retrieval-Augmented Generation","We introduce EXIT, an extractive context compression framework that enhances
both the effectiveness and efficiency of retrieval-augmented generation (RAG)
in question answering (QA). Current RAG systems often struggle when retrieval
models fail to rank the most relevant documents, leading to the inclusion of
more context at the expense of latency and accuracy. While abstractive
compression methods can drastically reduce token counts, their token-by-token
generation process significantly increases end-to-end latency. Conversely,
existing extractive methods reduce latency but rely on independent,
non-adaptive sentence selection, failing to fully utilize contextual
information. EXIT addresses these limitations by classifying sentences from
retrieved documents - while preserving their contextual dependencies - enabling
parallelizable, context-aware extraction that adapts to query complexity and
retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks
show that EXIT consistently surpasses existing compression methods and even
uncompressed baselines in QA accuracy, while also delivering substantial
reductions in inference time and token count. By improving both effectiveness
and efficiency, EXIT provides a promising direction for developing scalable,
high-quality QA solutions in RAG pipelines. Our code is available at
https://github.com/ThisIsHwang/EXIT",Taeho Hwang
2020-05-22T21:34:34Z,http://arxiv.org/abs/2005.11401v4,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when fine-tuned on
downstream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their
performance lags behind task-specific architectures. Additionally, providing
provenance for their decisions and updating their world knowledge remain open
research problems. Pre-trained models with a differentiable access mechanism to
explicit non-parametric memory can overcome this issue, but have so far been
only investigated for extractive downstream tasks. We explore a general-purpose
fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
combine pre-trained parametric and non-parametric memory for language
generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index
of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
formulations, one which conditions on the same retrieved passages across the
whole generated sequence, the other can use different passages per token. We
fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
tasks and set the state-of-the-art on three open domain QA tasks, outperforming
parametric seq2seq models and task-specific retrieve-and-extract architectures.
For language generation tasks, we find that RAG models generate more specific,
diverse and factual language than a state-of-the-art parametric-only seq2seq
baseline.",Patrick Lewis
2024-03-11T16:12:34Z,http://arxiv.org/abs/2403.06857v1,"Development of a Reliable and Accessible Caregiving Language Model
  (CaLM)","Unlike professional caregivers, family caregivers often assume this role
without formal preparation or training. Because of this, there is an urgent
need to enhance the capacity of family caregivers to provide quality care.
Large language models can potentially be used as a foundation technology for
supporting caregivers as educational tools or as adjunct to care. This study
aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a
caregiving knowledge base, develop an accessible CaLM using a small FM that
requires fewer computing resources, and evaluate the performance of the model
compared to a large FM. We developed CaLM using the Retrieval Augmented
Generation (RAG) framework combined with FM fine-tuning for improving the
quality of FM answers by grounding the model on a caregiving knowledge base. We
used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B
parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving
knowledge base by gathering various types of documents from the Internet. In
this study, we focused on caregivers of individuals with Alzheimer's Disease
Related Dementias. We evaluated the models' performance using the benchmark
metrics commonly used in evaluating language models and their reliability to
provide accurate references with the answers. The RAG framework improved the
performance of all FMs used in this study across all measures. As expected, the
large FM performed better than small FMs across all metrics. The most
interesting result is that small fine-tuned FMs with RAG performed
significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2
small FM performed better than GPT 3.5 (even with RAG) in returning references
with the answers. The study shows that reliable and accessible CaLM can be
developed by using small FMs with a knowledge base specific to the caregiving
domain.",Bambang Parmanto
2024-07-10T17:20:59Z,http://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key
applications to enhance employee productivity. Retrieval Augmented Generation
(RAG), Large Language Models (LLMs), and orchestration frameworks like
Langchain and Llamaindex are crucial for building these chatbots. However,
creating effective enterprise chatbots is challenging and requires meticulous
RAG pipeline engineering. This includes fine-tuning embeddings and LLMs,
extracting documents from vector databases, rephrasing queries, reranking
results, designing prompts, honoring document access controls, providing
concise responses, including references, safeguarding personal information, and
building orchestration agents. We present a framework for building RAG-based
chatbots based on our experience with three NVIDIA chatbots: for IT/HR
benefits, financial earnings, and general content. Our contributions are
three-fold: introducing the FACTS framework (Freshness, Architectures, Cost,
Testing, Security), presenting fifteen RAG pipeline control points, and
providing empirical results on accuracy-latency tradeoffs between large and
small LLMs. To the best of our knowledge, this is the first paper of its kind
that provides a holistic view of the factors as well as solutions for building
secure enterprise-grade chatbots.""",Rama Akkiraju
2024-08-31T16:14:42Z,http://arxiv.org/abs/2409.00494v2,"GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:
  Opportunities and Challenges for Integrating Large Language Models (LLMs) and
  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems","Leveraging recent advances in generative AI, multi-agent systems are
increasingly being developed to enhance the functionality and efficiency of
smart city applications. This paper explores the transformative potential of
large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)
technologies in Intelligent Transportation Systems (ITS), paving the way for
innovative solutions to address critical challenges in urban mobility. We begin
by providing a comprehensive overview of the current state-of-the-art in
mobility data, ITS, and Connected Vehicles (CV) applications. Building on this
review, we discuss the rationale behind RAG and examine the opportunities for
integrating these Generative AI (GenAI) technologies into the smart mobility
sector. We propose a conceptual framework aimed at developing multi-agent
systems capable of intelligently and conversationally delivering smart mobility
services to urban commuters, transportation operators, and decision-makers. Our
approach seeks to foster an autonomous and intelligent approach that (a)
promotes science-based advisory to reduce traffic congestion, accidents, and
carbon emissions at multiple scales, (b) facilitates public education and
engagement in participatory mobility management, and (c) automates specialized
transportation management tasks and the development of critical ITS platforms,
such as data analytics and interpretation, knowledge representation, and
traffic simulations. By integrating LLM and RAG, our approach seeks to overcome
the limitations of traditional rule-based multi-agent systems, which rely on
fixed knowledge bases and limited reasoning capabilities. This integration
paves the way for a more scalable, intuitive, and automated multi-agent
paradigm, driving advancements in ITS and urban mobility.",Haowen Xu
2024-10-03T17:40:55Z,http://arxiv.org/abs/2410.02721v1,"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization","Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.",Ryan C. Barron
2024-11-29T16:09:43Z,http://arxiv.org/abs/2411.19804v1,"Advanced System Integration: Analyzing OpenAPI Chunking for
  Retrieval-Augmented Generation","Integrating multiple (sub-)systems is essential to create advanced
Information Systems (ISs). Difficulties mainly arise when integrating dynamic
environments across the IS lifecycle. A traditional approach is a registry that
provides the API documentation of the systems' endpoints. Large Language Models
(LLMs) have shown to be capable of automatically creating system integrations
(e.g., as service composition) based on this documentation but require concise
input due to input token limitations, especially regarding comprehensive API
descriptions. Currently, it is unknown how best to preprocess these API
descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented
Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing,
of OpenAPIs to reduce the input token length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints and retrieves details on
demand. We evaluate RAG for endpoint discovery using the RestBench benchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval recall, precision, and F1 score. Then, we assess the
Discovery Agent using the same test set. With our prototype, we demonstrate how
to successfully employ RAG for endpoint discovery to reduce the token count.
While revealing high values for recall, precision, and F1, further research is
necessary to retrieve all requisite endpoints. Our experiments show that for
preprocessing, LLM-based and format-specific approaches outperform na\""ive
chunking methods. Relying on an agent further enhances these results as the
agent splits the tasks into multiple fine granular subtasks, improving the
overall RAG performance in the token count, precision, and F1 score.",Robin D. Pesl
2011-12-29T08:31:59Z,http://arxiv.org/abs/1112.6251v1,Semidefinite programming in matrix unknowns which are dimension free,"One of the main applications of semidefinite programming lies in linear
systems and control theory. Many problems in this subject, certainly the
textbook classics, have matrices as variables, and the formulas naturally
contain non-commutative polynomials in matrices. These polynomials depend only
on the system layout and do not change with the size of the matrices involved,
hence such problems are called ""dimension-free"". Analyzing dimension-free
problems has led to the development recently of a non-commutative (nc) real
algebraic geometry (RAG) which, when combined with convexity, produces
dimension-free Semidefinite Programming. This article surveys what is known
about convexity in the non-commutative setting and nc SDP and includes a brief
survey of nc RAG. Typically, the qualitative properties of the non-commutative
case are much cleaner than those of their scalar counterparts - variables in
R^g. Indeed we describe how relaxation of scalar variables by matrix variables
in several natural situations results in a beautiful structure.",J. William Helton
2020-02-13T14:52:32Z,http://arxiv.org/abs/2002.05544v2,Superpixel Image Classification with Graph Attention Networks,"This paper presents a methodology for image classification using Graph Neural
Network (GNN) models. We transform the input images into region adjacency
graphs (RAGs), in which regions are superpixels and edges connect neighboring
superpixels. Our experiments suggest that Graph Attention Networks (GATs),
which combine graph convolutions with self-attention mechanisms, outperforms
other GNN models. Although raw image classifiers perform better than GATs due
to information loss during the RAG generation, our methodology opens an
interesting avenue of research on deep learning beyond rectangular-gridded
images, such as 360-degree field of view panoramas. Traditional convolutional
kernels of current state-of-the-art methods cannot handle panoramas, whereas
the adapted superpixel algorithms and the resulting region adjacency graphs can
naturally feed a GNN, without topology issues.",Pedro H. C. Avelar
2022-06-03T01:18:45Z,http://arxiv.org/abs/2206.01356v2,Structure Learning for Hybrid Bayesian Networks,"Bayesian networks have been used as a mechanism to represent the joint
distribution of multiple random variables in a flexible yet interpretable
manner. One major challenge in learning the structure of a Bayesian network is
how to model networks which include a mixture of continuous and discrete random
variables, known as hybrid Bayesian networks. This paper overviews the
literature on approaches to handle hybrid Bayesian networks. Typically one of
two approaches is taken: either the data are considered to have a joint
distribution which is designed for a mixture of discrete and continuous
variables, or continuous random variables are discretized, resulting in
discrete Bayesian networks. In this paper, we propose a strategy to model all
random variables as Gaussian, referred to it as {\it Run it As Gaussian (RAG)}.
We demonstrate that RAG results in more reliable estimates of graph structures
theoretically and by simulation studies, than converting continuous random
variables to discrete. Both strategies are also implemented on a childhood
obesity data set. The two different strategies give rise to significant
differences in the optimal graph structures, with the results of the simulation
study suggesting that our strategy is more reliable.",Wanchuang Zhu
2023-01-26T19:58:46Z,http://arxiv.org/abs/2301.11385v1,Formation of asymmetric arms in barred galaxies,"We establish a dynamical mechanism to explain the origin of the asymmetry
between the arms observed in some barred disk galaxies, where one of the two
arms emanating from the bar ends is very well defined, while the second one
displays a ragged structure, extending between its ridge and the bar. To this
purpose, we study the invariant manifolds associated to the Lyapunov periodic
orbits around the unstable equilibrium points at the ends of the bar. Matter
from the galaxy center is transported along these manifolds to the periphery,
forming this way the spiral arms that emanate from the bar ends. If the mass
distribution in the galaxy center is not homogeneous, because of an asymmetric
bar with one side stronger than the other, or because of a non-centered bulge,
the dynamics about the two unstable Lagrange points at the ends of the bar will
not be symmetric as well. One of their invariant manifolds becomes more
extended than the other, enclosing a smaller section and the escaping orbits on
it are fewer and dispersed in a wider region. The result is a weaker arm, and
more ragged than the one at the other end of the bar.",P. SÃ¡nchez-MartÃ­n
2023-07-07T02:42:06Z,http://arxiv.org/abs/2307.04642v2,"TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal
  Prediction","When applied to open-domain question answering, large language models (LLMs)
frequently generate incorrect responses based on made-up facts, which are
called $\textit{hallucinations}$. Retrieval augmented generation (RAG) is a
promising strategy to avoid hallucinations, but it does not provide guarantees
on its correctness. To address this challenge, we propose the Trustworthy
Retrieval Augmented Question Answering, or $\textit{TRAQ}$, which provides the
first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal
prediction, a statistical technique for constructing prediction sets that are
guaranteed to contain the semantically correct response with high probability.
Additionally, TRAQ leverages Bayesian optimization to minimize the size of the
constructed sets. In an extensive experimental evaluation, we demonstrate that
TRAQ provides the desired correctness guarantee while reducing prediction set
size by 16.2% on average compared to an ablation. The implementation is
available at $\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.",Shuo Li
2023-09-27T21:26:03Z,http://arxiv.org/abs/2309.16035v3,"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering","Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks such as medical question answering
(QA). In addition, LLMs tend to function as ""black-boxes"", making it
challenging to modify their behavior. To address the problem, our work employs
a transparent process of retrieval augmented generation (RAG), aiming to
improve LLM responses without the need for fine-tuning or retraining.
Specifically, we propose a comprehensive retrieval strategy to extract medical
facts from an external knowledge base, and then inject them into the LLM's
query prompt. Focusing on medical QA, we evaluate the impact of different
retrieval models and the number of facts on LLM performance using the
MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited
an accuracy improvement from 44.46% to 48.54%. This work underscores the
potential of RAG to enhance LLM performance, offering a practical approach to
mitigate the challenges posed by black-box LLMs.",Yucheng Shi
2023-11-07T18:03:23Z,http://arxiv.org/abs/2311.04177v1,"Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for
  Retrieval Augmented Generation","Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,
(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of
performing amazing tasks typically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not improve over time; they neither
acquire new knowledge nor learn from their successes or failures. Some
approaches to improving the intelligence of LLMs include fine-tuning models
based on problem-solving performance (Zelikman et al., 2022), and building
bigger and more sophisticated models (Bubeck et al., 2023). However, these
methods have the drawback of requiring substantial data and computational
resources to retrain existing models. In this paper, we explore the use of
Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to
improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale
Memory for Retrieval Augmented Generation), a system that learns from its
successes without incurring high training costs. We demonstrate that the
storage and subsequent retrieval of reasoning chains have a positive influence
on performance in grade-school math problems.",Eric Melz
2023-11-23T09:58:39Z,http://arxiv.org/abs/2311.13878v1,"Minimizing Factual Inconsistency and Hallucination in Large Language
  Models","Large Language Models (LLMs) are widely used in critical fields such as
healthcare, education, and finance due to their remarkable proficiency in
various language-related tasks. However, LLMs are prone to generating factually
incorrect responses or ""hallucinations,"" which can lead to a loss of
credibility and trust among users. To address this issue, we propose a
multi-stage framework that generates the rationale first, verifies and refines
incorrect ones, and uses them as supporting references to generate the answer.
The generated rationale enhances the transparency of the answer and our
framework provides insights into how the model arrived at this answer, by using
this rationale and the references to the context. In this paper, we demonstrate
its effectiveness in improving the quality of responses to drug-related
inquiries in the life sciences industry. Our framework improves traditional
Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be
14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,
fine-tuning samples based on our framework improves the accuracy of smaller
open-access LLMs by 33-42% and competes with RAG on commercial models.",Muneeswaran I
2023-12-10T16:52:00Z,http://arxiv.org/abs/2312.05934v3,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,"Large language models (LLMs) encapsulate a vast amount of factual information
within their pre-trained weights, as evidenced by their ability to answer
diverse questions across different domains. However, this knowledge is
inherently limited, relying heavily on the characteristics of the training
data. Consequently, using external datasets to incorporate new information or
refine the capabilities of LLMs on previously seen information poses a
significant challenge. In this study, we compare two common approaches:
unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate
both approaches on a variety of knowledge-intensive tasks across different
topics. Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing knowledge
encountered during training and entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information through unsupervised
fine-tuning, and that exposing them to numerous variations of the same fact
during training could alleviate this problem.",Oded Ovadia
2024-01-27T00:18:07Z,http://arxiv.org/abs/2402.01722v1,"Enhancing Large Language Model Performance To Answer Questions and
  Extract Information More Accurately","Large Language Models (LLMs) generate responses to questions; however, their
effectiveness is often hindered by sub-optimal quality of answers and
occasional failures to provide accurate responses to questions. To address
these challenges, a fine-tuning process is employed, involving feedback and
examples to refine models. The objective is to enhance AI models through
continuous feedback loops, utilizing metrics such as cosine similarity, LLM
evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like
GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on
financial datasets, including the FinanceBench and RAG Instruct Benchmark
Tester Dataset, illustrating the necessity of fine-tuning. The results showcase
the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,
providing superior question and answering capabilities. Notably, the
combination of fine-tuning the LLM with a process known as Retrieval Augmented
Generation (RAG) proves to generate responses with improved accuracy.",Liang Zhang
2024-02-05T11:58:56Z,http://arxiv.org/abs/2402.05128v2,"Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation","Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.",Hessa Abdulrahman Alawwad
2024-02-19T14:33:24Z,http://arxiv.org/abs/2402.12177v4,Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning,"Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.",Mingtian Zhang
2024-02-22T06:21:41Z,http://arxiv.org/abs/2402.14318v1,Assessing generalization capability of text ranking models in Polish,"Retrieval-augmented generation (RAG) is becoming an increasingly popular
technique for integrating internal knowledge bases with large language models.
In a typical RAG pipeline, three models are used, responsible for the
retrieval, reranking, and generation stages. In this article, we focus on the
reranking problem for the Polish language, examining the performance of
rerankers and comparing their results with available retrieval models. We
conduct a comprehensive evaluation of existing models and those trained by us,
utilizing a benchmark of 41 diverse information retrieval tasks for the Polish
language. The results of our experiments show that most models struggle with
out-of-domain generalization. However, a combination of effective optimization
method and a large training dataset allows for building rerankers that are both
compact in size and capable of generalization. The best of our models
establishes a new state-of-the-art for reranking in the Polish language,
outperforming existing models with up to 30 times more parameters.",SÅ‚awomir Dadas
2024-03-07T06:38:41Z,http://arxiv.org/abs/2403.04256v1,Federated Recommendation via Hybrid Retrieval Augmented Generation,"Federated Recommendation (FR) emerges as a novel paradigm that enables
privacy-preserving recommendations. However, traditional FR systems usually
represent users/items with discrete identities (IDs), suffering from
performance degradation due to the data sparsity and heterogeneity in FR. On
the other hand, Large Language Models (LLMs) as recommenders have proven
effective across various recommendation scenarios. Yet, LLM-based recommenders
encounter challenges such as low inference efficiency and potential
hallucination, compromising their performance in real-world scenarios. To this
end, we propose GPT-FedRec, a federated recommendation framework leveraging
ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.
GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval
process, mining ID-based user patterns and text-based item features. Next, the
retrieved results are converted into text prompts and fed into GPT for
re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims
to extract generalized features from data and exploit pretrained knowledge
within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the
RAG approach also prevents LLM hallucination, improving the recommendation
performance for real-world users. Experimental results on diverse benchmark
datasets demonstrate the superior performance of GPT-FedRec against
state-of-the-art baseline methods.",Huimin Zeng
2024-03-08T21:09:20Z,http://arxiv.org/abs/2403.05676v1,"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System
  Co-design","Retrieval-augmented generation (RAG) can enhance the generation quality of
large language models (LLMs) by incorporating external token databases.
However, retrievals from large databases can constitute a substantial portion
of the overall generation time, particularly when retrievals are periodically
performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design
approach to reduce generation latency and enhance generation quality. PipeRAG
integrates (1) pipeline parallelism to enable concurrent retrieval and
generation processes, (2) flexible retrieval intervals to maximize the
efficiency of pipeline parallelism, and (3) a performance model to
automatically balance retrieval quality and latency based on the generation
states and underlying hardware. Our evaluation shows that, by combining the
three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in
end-to-end generation latency while improving generation quality. These
promising results showcase the effectiveness of co-designing algorithms with
underlying systems, paving the way for the adoption of PipeRAG in future RAG
systems.",Wenqi Jiang
2024-03-14T09:45:05Z,http://arxiv.org/abs/2403.09226v2,"Retrieval augmented text-to-SQL generation for epidemiological question
  answering using electronic health records","Electronic health records (EHR) and claims data are rich sources of
real-world data that reflect patient health status and healthcare utilization.
Querying these databases to answer epidemiological questions is challenging due
to the intricacy of medical terminology and the need for complex SQL queries.
Here, we introduce an end-to-end methodology that combines text-to-SQL
generation with retrieval augmented generation (RAG) to answer epidemiological
questions using EHR and claims data. We show that our approach, which
integrates a medical coding step into the text-to-SQL process, significantly
improves the performance over simple prompting. Our findings indicate that
although current language models are not yet sufficiently accurate for
unsupervised use, RAG offers a promising direction for improving their
capabilities, as shown in a realistic industry setting.",Angelo Ziletti
2024-03-17T23:02:04Z,http://arxiv.org/abs/2403.11366v2,"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning","The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.",Anique Tahir
2024-03-24T21:02:35Z,http://arxiv.org/abs/2403.16295v1,"LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation","With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.",Ashish Chouhan
2024-03-27T08:42:31Z,http://arxiv.org/abs/2403.18350v2,"Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language","The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).",Ali Mahboub
2024-03-29T00:14:46Z,http://arxiv.org/abs/2403.19889v1,Towards a Robust Retrieval-Based Summarization System,"This paper describes an investigation of the robustness of large language
models (LLMs) for retrieval augmented generation (RAG)-based summarization
tasks. While LLMs provide summarization capabilities, their performance in
complex, real-world scenarios remains under-explored. Our first contribution is
LogicSumm, an innovative evaluation framework incorporating realistic scenarios
to assess LLM robustness during RAG-based summarization. Based on limitations
identified by LogiSumm, we then developed SummRAG, a comprehensive system to
create training dialogues and fine-tune a model to enhance robustness within
LogicSumm's scenarios. SummRAG is an example of our goal of defining structured
methods to test the capabilities of an LLM, rather than addressing issues in a
one-off fashion. Experimental results confirm the power of SummRAG, showcasing
improved logical coherence and summarization quality. Data, corresponding model
weights, and Python code are available online.",Shengjie Liu
2024-04-12T01:42:09Z,http://arxiv.org/abs/2404.08189v1,"Reducing hallucination in structured outputs via Retrieval-Augmented
  Generation","A common and fundamental limitation of Generative AI (GenAI) is its
propensity to hallucinate. While large language models (LLM) have taken the
world by storm, without eliminating or at least reducing hallucinations,
real-world GenAI systems may face challenges in user adoption. In the process
of deploying an enterprise application that produces workflows based on natural
language requirements, we devised a system leveraging Retrieval Augmented
Generation (RAG) to greatly improve the quality of the structured output that
represents such workflows. Thanks to our implementation of RAG, our proposed
system significantly reduces hallucinations in the output and improves the
generalization of our LLM in out-of-domain settings. In addition, we show that
using a small, well-trained retriever encoder can reduce the size of the
accompanying LLM, thereby making deployments of LLM-based systems less
resource-intensive.",Patrice BÃ©chard
2024-04-19T00:48:30Z,http://arxiv.org/abs/2404.12560v1,"Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for
  Text-to-SQL","The current state-of-the-art (SOTA) for automated text-to-SQL still falls
well short of expert human performance as measured by execution accuracy (EX)
on the BIRD-SQL benchmark. The most accurate methods are also slow and
expensive. To advance the SOTA for text-to-SQL while reducing cost and
improving speed, we explore the combination of low-cost fine tuning, novel
methods for diverse retrieval-augmented generation (RAG) and new input and
output formats that help large language models (LLMs) achieve higher EX. We
introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record
for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher
performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI,
but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the
next-best model using OpenAI, which instead uses the more expensive GPT-4.
Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by
over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push
EX higher.",Dayton G. Thorpe
2024-04-24T17:59:36Z,http://arxiv.org/abs/2404.16032v2,"Studying Large Language Model Behaviors Under Context-Memory Conflicts
  With Real Documents","Retrieval-augmented generation (RAG) mitigates many problems of fully
parametric language models, such as temporal degradation, hallucinations, and
lack of grounding. In RAG, the model's knowledge can be updated from documents
provided in context. This leads to cases of conflict between the model's
parametric knowledge and the contextual information, where the model may not
always update its knowledge. Previous work studied context-memory knowledge
conflicts by creating synthetic documents that contradict the model's correct
parametric answers. We present a framework for studying such knowledge
conflicts in a realistic setup. We update incorrect parametric knowledge using
real conflicting documents. This reflects how knowledge conflicts arise in
practice. In this realistic scenario, we find that knowledge updates fail less
often than previously reported. In cases where the models still fail to update
their answers, we find a parametric bias: the incorrect parametric answer
appearing in context makes the knowledge update likelier to fail. These results
suggest that the factual parametric knowledge of LLMs can negatively influence
their reading abilities and behaviors. Our code is available at
https://github.com/kortukov/realistic_knowledge_conflicts/ .",Evgenii Kortukov
2024-05-03T12:30:01Z,http://arxiv.org/abs/2405.02048v1,Comparative Analysis of Retrieval Systems in the Real World,"This research paper presents a comprehensive analysis of integrating advanced
language models with search and retrieval systems in the fields of information
retrieval and natural language processing. The objective is to evaluate and
compare various state-of-the-art methods based on their performance in terms of
accuracy and efficiency. The analysis explores different combinations of
technologies, including Azure Cognitive Search Retriever with GPT-4, Pinecone's
Canopy framework, Langchain with Pinecone and different language models
(OpenAI, Cohere), LlamaIndex with Weaviate Vector Store's hybrid search,
Google's RAG implementation on Cloud VertexAI-Search, Amazon SageMaker's RAG,
and a novel approach called KG-FID Retrieval. The motivation for this analysis
arises from the increasing demand for robust and responsive question-answering
systems in various domains. The RobustQA metric is used to evaluate the
performance of these systems under diverse paraphrasing of questions. The
report aims to provide insights into the strengths and weaknesses of each
method, facilitating informed decisions in the deployment and development of
AI-driven search and retrieval systems.",Dmytro Mozolevskyi
2024-05-13T19:05:42Z,http://arxiv.org/abs/2405.08120v1,"From Questions to Insightful Answers: Building an Informed Chatbot for
  University Resources","This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot
system built using Retrieval Augmented Generation (RAG) pipelines to enhance
the user experience and access to information within academic settings.The
objective of BARKPLUG V.2 is to provide information to users about various
campus resources, including academic departments, programs, campus facilities,
and student resources at a university setting in an interactive fashion. Our
system leverages university data as an external data corpus and ingests it into
our RAG pipelines for domain-specific question-answering tasks. We evaluate the
effectiveness of our system in generating accurate and pertinent responses for
Mississippi State University, as a case study, using quantitative measures,
employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS).
Furthermore, we evaluate the usability of this system via subjective
satisfaction surveys using the System Usability Scale (SUS). Our system
demonstrates impressive quantitative performance, with a mean RAGAS score of
0.96, and experience, as validated by usability assessments.",Subash Neupane
2024-05-20T14:03:05Z,http://arxiv.org/abs/2405.12035v1,KG-RAG: Bridging the Gap Between Knowledge and Creativity,"Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.",Diego Sanmartin
2024-05-24T13:44:25Z,http://arxiv.org/abs/2405.15556v1,Certifiably Robust RAG against Retrieval Corruption,"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval
corruption attacks: an attacker can inject malicious passages into retrieval
results to induce inaccurate responses. In this paper, we propose RobustRAG as
the first defense framework against retrieval corruption attacks. The key
insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM
responses from each passage in isolation and then securely aggregate these
isolated responses. To instantiate RobustRAG, we design keyword-based and
decoding-based algorithms for securely aggregating unstructured text responses.
Notably, RobustRAG can achieve certifiable robustness: we can formally prove
and certify that, for certain queries, RobustRAG can always return accurate
responses, even when the attacker has full knowledge of our defense and can
arbitrarily inject a small number of malicious passages. We evaluate RobustRAG
on open-domain QA and long-form text generation datasets and demonstrate its
effectiveness and generalizability across various tasks and datasets.",Chong Xiang
2024-05-26T10:11:40Z,http://arxiv.org/abs/2405.16506v2,GRAG: Graph Retrieval-Augmented Generation,"Naive Retrieval-Augmented Generation (RAG) focuses on individual documents
during retrieval and, as a result, falls short in handling networked documents
which are very popular in many applications such as citation graphs, social
media, and knowledge graphs. To overcome this limitation, we introduce Graph
Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges
in retrieving textual subgraphs and integrating the joint textual and
topological information into Large Language Models (LLMs) to enhance its
generation. To enable efficient textual subgraph retrieval, we propose a novel
divide-and-conquer strategy that retrieves the optimal subgraph structure in
linear time. To achieve graph context-aware generation, incorporate textual
graphs into LLMs through two complementary views-the text view and the graph
view-enabling LLMs to more effectively comprehend and utilize the graph
context. Extensive experiments on graph reasoning benchmarks demonstrate that
in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach
significantly outperforms current state-of-the-art RAG methods.",Yuntong Hu
2024-05-27T18:40:49Z,http://arxiv.org/abs/2405.17587v2,RAGSys: Item-Cold-Start Recommender as RAG System,"Large Language Models (LLM) hold immense promise for real-world applications,
but their generic knowledge often falls short of domain-specific needs.
Fine-tuning, a common approach, can suffer from catastrophic forgetting and
hinder generalizability. In-Context Learning (ICL) offers an alternative, which
can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant
demonstrations for few-shot learning tasks. This paper explores the desired
qualities of a demonstration retrieval system for ICL. We argue that ICL
retrieval in this context resembles item-cold-start recommender systems,
prioritizing discovery and maximizing information gain over strict relevance.
We propose a novel evaluation method that measures the LLM's subsequent
performance on NLP tasks, eliminating the need for subjective diversity scores.
Our findings demonstrate the critical role of diversity and quality bias in
retrieved demonstrations for effective ICL, and highlight the potential of
recommender system techniques in this domain.",Emile Contal
2024-05-27T23:39:17Z,http://arxiv.org/abs/2405.17706v1,"Video Enriched Retrieval Augmented Generation Using Aligned Video
  Captions","In this work, we propose the use of ""aligned visual captions"" as a mechanism
for integrating information contained within videos into retrieval augmented
generation (RAG) based chat assistant systems. These captions are able to
describe the visual and audio content of videos in a large corpus while having
the advantage of being in a textual format that is both easy to reason about &
incorporate into large language model (LLM) prompts, but also typically require
less multimedia content to be inserted into the multimodal LLM context window,
where typical configurations can aggressively fill up the context window by
sampling video frames from the source video. Furthermore, visual captions can
be adapted to specific use cases by prompting the original foundational model /
captioner for particular visual details or fine tuning. In hopes of helping
advancing progress in this area, we curate a dataset and describe automatic
evaluation procedures on common RAG tasks.",Kevin Dela Rosa
2024-05-29T01:12:53Z,http://arxiv.org/abs/2405.18682v2,"Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical
  Machine Reading Comprehension","Large language models (LLMs) have shown remarkable performance on many tasks
in different domains. However, their performance in closed-book biomedical
machine reading comprehension (MRC) has not been evaluated in depth. In this
work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We
experiment with different conventional prompting techniques as well as
introduce our own novel prompting method. To solve some of the retrieval
problems inherent to LLMs, we propose a prompting strategy named Implicit
Retrieval Augmented Generation (RAG) that alleviates the need for using vector
databases to retrieve important chunks in traditional RAG setups. Moreover, we
report qualitative assessments on the natural language generation outputs from
our approach. The results show that our new prompting technique is able to get
the best performance in two out of four datasets and ranks second in rest of
them. Experiments show that modern-day LLMs like GPT even in a zero-shot
setting can outperform supervised models, leading to new state-of-the-art
(SoTA) results on two of the benchmarks.",Shubham Vatsal
2024-05-29T20:56:52Z,http://arxiv.org/abs/2405.19519v1,"Two-layer retrieval augmented generation framework for low-resource
  medical question-answering: proof of concept using Reddit data","Retrieval augmented generation (RAG) provides the capability to constrain
generative model outputs, and mitigate the possibility of hallucination, by
providing relevant in-context text. The number of tokens a generative large
language model (LLM) can incorporate as context is finite, thus limiting the
volume of knowledge from which to generate an answer. We propose a two-layer
RAG framework for query-focused answer generation and evaluate a
proof-of-concept for this framework in the context of query-focused summary
generation from social media forums, focusing on emerging drug-related
information. The evaluations demonstrate the effectiveness of the two-layer
framework in resource constrained settings to enable researchers in obtaining
near real-time data from users.",Sudeshna Das
2024-05-27T17:55:36Z,http://arxiv.org/abs/2406.00041v2,"QUB-Cirdan at ""Discharge Me!"": Zero shot discharge letter generation by
  open-source LLM","The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to
reduce the administrative burden on clinicians by automating the creation of
critical sections of patient discharge letters. This paper presents our
approach using the Llama3 8B quantized model to generate the ""Brief Hospital
Course"" and ""Discharge Instructions"" sections. We employ a zero-shot method
combined with Retrieval-Augmented Generation (RAG) to produce concise,
contextually accurate summaries. Our contributions include the development of a
curated template-based approach to ensure reliability and consistency, as well
as the integration of RAG for word count prediction. We also describe several
unsuccessful experiments to provide insights into our pathway for the
competition. Our results demonstrate the effectiveness and efficiency of our
approach, achieving high scores across multiple evaluation metrics.",Rui Guo
2024-06-01T14:45:03Z,http://arxiv.org/abs/2406.00456v1,"Mix-of-Granularity: Optimize the Chunking Granularity for
  Retrieval-Augmented Generation","Integrating information from different reference data sources is a major
challenge for Retrieval-Augmented Generation (RAG) systems because each
knowledge source adopts a unique data structure and follows different
conventions. Retrieving from multiple knowledge sources with one fixed strategy
usually leads to under-exploitation of information. To mitigate this drawback,
inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that
dynamically determines the optimal granularity of a knowledge database based on
input queries using a router. The router is efficiently trained with a newly
proposed loss function employing soft labels. We further extend MoG to
Mix-of-Granularity-Graph (MoGG), where reference documents are pre-processed
into graphs, enabling the retrieval of relevant information from distantly
situated chunks. Extensive experiments demonstrate that both MoG and MoGG
effectively predict optimal granularity levels, significantly enhancing the
performance of the RAG system in downstream tasks. The code of both MoG and
MoGG will be made public.",Zijie Zhong
2024-06-03T04:14:21Z,http://arxiv.org/abs/2406.00975v2,"Luna: An Evaluation Foundation Model to Catch Language Model
  Hallucinations with High Accuracy and Low Cost","Retriever Augmented Generation (RAG) systems have become pivotal in enhancing
the capabilities of language models by incorporating external knowledge
retrieval mechanisms. However, a significant challenge in deploying these
systems in industry applications is the detection and mitigation of
hallucinations: instances where the model generates information that is not
grounded in the retrieved context. Addressing this issue is crucial for
ensuring the reliability and accuracy of responses generated by large language
models (LLMs) in diverse industry settings. Current hallucination detection
techniques fail to deliver accuracy, low latency, and low cost simultaneously.
We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination
detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and
commercial evaluation frameworks on the hallucination detection task, with 97%
and 91% reduction in cost and latency, respectively. Luna is lightweight and
generalizes across multiple industry verticals and out-of-domain data, making
it an ideal candidate for industry LLM applications.",Masha Belyi
2024-06-03T12:48:38Z,http://arxiv.org/abs/2406.01280v2,Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper demonstrates SoccerRAG, an innovative framework designed
to harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. We present a novel interactive user interface
(UI) based on the Chainlit framework which wraps around the core functionality,
and enable users to interact with the SoccerRAG framework in a chatbot-like
visual manner.",Aleksander Theo Strand
2024-06-03T20:18:56Z,http://arxiv.org/abs/2406.01768v1,"TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP
  Specifications","Understanding telecom standards involves sorting through numerous technical
documents, such as those produced by the 3rd Generation Partnership Project
(3GPP), which is time-consuming and labor-intensive. While large language
models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive
dataset is crucial for their effective pre-training and fine-tuning. In this
paper, we introduce \textit{TSpec-LLM}, an open-source comprehensive dataset
covering all 3GPP documents from Release 8 to Release 19 (1999--2023). To
evaluate its efficacy, we first select a representative sample of 3GPP
documents, create corresponding technical questions, and assess the baseline
performance of various LLMs. We then incorporate a retrieval-augmented
generation (RAG) framework to enhance LLM capabilities by retrieving relevant
context from the \textit{TSpec-LLM} dataset. Our evaluation shows that using a
naive-RAG framework on \textit{TSpec-LLM} improves the accuracy of GPT-3.5,
Gemini 1.0 Pro, and GPT-4 from 44\%, 46\%, and 51\% to 71\%, 75\%, and 72\%,
respectively.",Rasoul Nikbakht
2024-06-11T13:36:19Z,http://arxiv.org/abs/2406.07257v1,"Scholarly Question Answering using Large Language Models in the
  NFDI4DataScience Gateway","This paper introduces a scholarly Question Answering (QA) system on top of
the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based
(RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a
unified and intuitive interface for querying various scientific databases using
federated search. The RAG-based scholarly QA, powered by a Large Language Model
(LLM), facilitates dynamic interaction with search results, enhancing filtering
capabilities and fostering a conversational engagement with the Gateway search.
The effectiveness of both the Gateway and the scholarly QA system is
demonstrated through experimental analysis.",Hamed Babaei Giglou
2024-06-12T22:05:51Z,http://arxiv.org/abs/2406.09459v1,Ad Auctions for LLMs via Retrieval Augmented Generation,"In the field of computational advertising, the integration of ads into the
outputs of large language models (LLMs) presents an opportunity to support
these services without compromising content integrity. This paper introduces
novel auction mechanisms for ad allocation and pricing within the textual
outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a
segment auction where an ad is probabilistically retrieved for each discourse
segment (paragraph, section, or entire output) according to its bid and
relevance, following the RAG framework, and priced according to competing bids.
We show that our auction maximizes logarithmic social welfare, a new notion of
welfare that balances allocation efficiency and fairness, and we characterize
the associated incentive-compatible pricing rule. These results are extended to
multi-ad allocation per segment. An empirical evaluation validates the
feasibility and effectiveness of our approach over several ad auction
scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more
flexibility to allocate ads.",MohammadTaghi Hajiaghayi
2024-06-14T12:41:07Z,http://arxiv.org/abs/2406.09979v2,HIRO: Hierarchical Information Retrieval Optimization,"Retrieval-Augmented Generation (RAG) has revolutionized natural language
processing by dynamically integrating external knowledge into Large Language
Models (LLMs), addressing their limitation of static training datasets. Recent
implementations of RAG leverage hierarchical data structures, which organize
documents at various levels of summarization and information density. This
complexity, however, can cause LLMs to ""choke"" on information overload,
necessitating more sophisticated querying mechanisms. In this context, we
introduce Hierarchical Information Retrieval Optimization (HIRO), a novel
querying approach that employs a Depth-First Search (DFS)-based recursive
similarity score calculation and branch pruning. This method uniquely minimizes
the context delivered to the LLM without informational loss, effectively
managing the challenge of excessive data. HIRO's refined approach is validated
by a 10.85% improvement in performance on the NarrativeQA dataset.",Krish Goel
2024-06-11T02:37:06Z,http://arxiv.org/abs/2406.10263v1,"A Lightweight Framework for Adaptive Retrieval In Code Completion With
  Critique Model","Recent advancements in Retrieval-Augmented Generation have significantly
enhanced code completion at the repository level. Various RAG-based code
completion systems are proposed based on different design choices. For
instance, gaining more effectiveness at the cost of repeating the
retrieval-generation process multiple times. However, the indiscriminate use of
retrieval in current methods reveals issues in both efficiency and
effectiveness, as a considerable portion of retrievals are unnecessary and may
introduce unhelpful or even harmful suggestions to code language models. To
address these challenges, we introduce CARD, a lightweight critique method
designed to provide insights into the necessity of retrievals and select the
optimal answer from multiple predictions. CARD can seamlessly integrate into
any RAG-based code completion system. Our evaluation shows that CARD saves 21%
to 46% times of retrieval for Line completion, 14% to 40% times of retrieval
for API completion, and 6% to 46.5% times of retrieval for function completion
respectively, while improving the accuracy. CARD reduces latency ranging from
16% to 83%. CARD is generalizable to different LMs, retrievers, and programming
languages. It is lightweight with training in few seconds and inference in few
milliseconds.",Wenrui Zhang
2024-06-17T20:14:16Z,http://arxiv.org/abs/2406.12069v2,Satyrn: A Platform for Analytics Augmented Generation,"Large language models (LLMs) are capable of producing documents, and
retrieval augmented generation (RAG) has shown itself to be a powerful method
for improving accuracy without sacrificing fluency. However, not all
information can be retrieved from text. We propose an approach that uses the
analysis of structured data to generate fact sets that are used to guide
generation in much the same way that retrieved documents are used in RAG. This
analytics augmented generation (AAG) approach supports the ability to utilize
standard analytic techniques to generate facts that are then converted to text
and passed to an LLM. We present a neurosymbolic platform, Satyrn, that
leverages AAG to produce accurate, fluent, and coherent reports grounded in
large scale databases. In our experiments, we find that Satyrn generates
reports in which over 86% of claims are accurate while maintaining high levels
of fluency and coherence, even when using smaller language models such as
Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims
are accurate.",Marko Sterbentz
2024-06-18T00:41:41Z,http://arxiv.org/abs/2406.12169v1,"Intermediate Distillation: Data-Efficient Distillation from Black-Box
  LLMs for Information Retrieval","Recent research has explored distilling knowledge from large language models
(LLMs) to optimize retriever models, especially within the retrieval-augmented
generation (RAG) framework. However, most existing training methods rely on
extracting supervision signals from LLMs' weights or their output
probabilities, which is not only resource-intensive but also incompatible with
black-box LLMs. In this paper, we introduce \textit{Intermediate Distillation},
a data-efficient knowledge distillation training scheme that treats LLMs as
black boxes and distills their knowledge via an innovative LLM-ranker-retriever
pipeline, solely using LLMs' ranking generation as the supervision signal.
Extensive experiments demonstrate that our proposed method can significantly
improve the performance of retriever models with only 1,000 training instances.
Moreover, our distilled retriever model significantly boosts performance in
question-answering tasks within the RAG framework, demonstrating the potential
of LLMs to economically and effectively train smaller models.",Zizhong Li
2024-06-18T09:25:35Z,http://arxiv.org/abs/2406.12430v1,"PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large
  Language Models as Decision Makers","In this paper, we conduct a study to utilize LLMs as a solution for decision
making that requires complex data analysis. We define Decision QA as the task
of answering the best decision, $d_{best}$, for a decision-making question $Q$,
business rules $R$ and a database $D$. Since there is no benchmark that can
examine Decision QA, we propose Decision QA benchmark, DQA. It has two
scenarios, Locating and Building, constructed from two video games (Europa
Universalis IV and Victoria 3) that have almost the same goal as Decision QA.
To address Decision QA effectively, we also propose a new RAG technique called
the iterative plan-then-retrieval augmented generation (PlanRAG). Our
PlanRAG-based LM generates the plan for decision making as the first step, and
the retriever generates the queries for data analysis as the second step. The
proposed method outperforms the state-of-the-art iterative RAG method by 15.8%
in the Locating scenario and by 7.4% in the Building scenario, respectively. We
release our code and benchmark at https://github.com/myeon9h/PlanRAG.",Myeonghwa Lee
2024-06-19T06:19:48Z,http://arxiv.org/abs/2406.13249v2,"R^2AG: Incorporating Retrieval Information into Retrieval Augmented
  Generation","Retrieval augmented generation (RAG) has been applied in many scenarios to
augment large language models (LLMs) with external documents provided by
retrievers. However, a semantic gap exists between LLMs and retrievers due to
differences in their training objectives and architectures. This misalignment
forces LLMs to passively accept the documents provided by the retrievers,
leading to incomprehension in the generation process, where the LLMs are
burdened with the task of distinguishing these documents using their inherent
knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill
this gap by incorporating Retrieval information into Retrieval Augmented
Generation. Specifically, R$^2$AG utilizes the nuanced features from the
retrievers and employs a R$^2$-Former to capture retrieval information. Then, a
retrieval-aware prompting strategy is designed to integrate retrieval
information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios
where LLMs and retrievers are frozen. Extensive experiments across five
datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our
analysis reveals that retrieval information serves as an anchor to aid LLMs in
the generation process, thereby filling the semantic gap.",Fuda Ye
2024-06-20T12:59:27Z,http://arxiv.org/abs/2406.14277v2,"QPaug: Question and Passage Augmentation for Open-Domain Question
  Answering of LLMs","Retrieval-augmented generation (RAG) has received much attention for
Open-domain question-answering (ODQA) tasks as a means to compensate for the
parametric knowledge of large language models (LLMs). While previous approaches
focused on processing retrieved passages to remove irrelevant context, they
still rely heavily on the quality of retrieved passages which can degrade if
the question is ambiguous or complex. In this paper, we propose a simple yet
efficient method called question and passage augmentation (QPaug) via LLMs for
open-domain QA. QPaug first decomposes the original questions into
multiple-step sub-questions. By augmenting the original question with detailed
sub-questions and planning, we are able to make the query more specific on what
needs to be retrieved, improving the retrieval performance. In addition, to
compensate for the case where the retrieved passages contain distracting
information or divided opinions, we augment the retrieved passages with
self-generated passages by LLMs to guide the answer extraction. Experimental
results show that QPaug outperforms the previous state-of-the-art and achieves
significant performance gain over existing RAG methods. The source code is
available at \url{https://github.com/kmswin1/QPaug}.",Minsang Kim
2024-06-21T14:29:39Z,http://arxiv.org/abs/2406.15187v2,"UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world
  Document Analysis","The use of Retrieval-Augmented Generation (RAG) has improved Large Language
Models (LLMs) in collaborating with external data, yet significant challenges
exist in real-world scenarios. In areas such as academic literature and finance
question answering, data are often found in raw text and tables in HTML or PDF
formats, which can be lengthy and highly unstructured. In this paper, we
introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that
involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We
revisit popular LLM- and RAG-based solutions for document analysis and evaluate
the design choices and answer qualities across multiple document domains and
diverse query types. Our evaluation yields interesting findings and highlights
the importance of data parsing and retrieval. We hope our benchmark can shed
light and better serve real-world document analysis applications. The benchmark
suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",Yulong Hui
2024-06-26T04:49:41Z,http://arxiv.org/abs/2406.18064v3,"Evaluating Quality of Answers for Retrieval-Augmented Generation: A
  Strong LLM Is All You Need","We present a comprehensive study of answer quality evaluation in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
""thumbs-up"" or ""thumbs-down"" gesture commonly used in chat applications. This
approach suits factual business contexts where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.",Yang Wang
2024-06-27T13:08:35Z,http://arxiv.org/abs/2406.19150v1,RAVEN: Multitask Retrieval Augmented Vision-Language Learning,"The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.",Varun Nagaraj Rao
2024-06-27T14:38:33Z,http://arxiv.org/abs/2406.19215v1,"SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented
  Generation","This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel
adaptive RAG model that extracts self-aware uncertainty of LLMs from their
internal states. SeaKR activates retrieval when the LLMs present high
self-aware uncertainty for generation. To effectively integrate retrieved
knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty
to preserve the snippet that reduces their uncertainty to the utmost. To
facilitate solving complex tasks that require multiple retrievals, SeaKR
utilizes their self-aware uncertainty to choose among different reasoning
strategies. Our experiments on both complex and simple Question Answering
datasets show that SeaKR outperforms existing adaptive RAG methods. We release
our code at https://github.com/THU-KEG/SeaKR.",Zijun Yao
2024-06-27T15:18:21Z,http://arxiv.org/abs/2406.19251v1,"AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for
  Retrieval-Augmented Generation","Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.",Jia Fu
2024-06-29T08:16:58Z,http://arxiv.org/abs/2407.00361v1,From RAG to RICHES: Retrieval Interlaced with Sequence Generation,"We present RICHES, a novel approach that interleaves retrieval with sequence
generation tasks. RICHES offers an alternative to conventional RAG systems by
eliminating the need for separate retriever and generator. It retrieves
documents by directly decoding their contents, constrained on the corpus.
Unifying retrieval with generation allows us to adapt to diverse new tasks via
prompting alone. RICHES can work with any Instruction-tuned model, without
additional training. It provides attributed evidence, supports multi-hop
retrievals and interleaves thoughts to plan on what to retrieve next, all
within a single decoding pass of the LLM. We demonstrate the strong performance
of RICHES across ODQA tasks including attributed and multi-hop QA.",Palak Jain
2024-07-01T16:56:50Z,http://arxiv.org/abs/2407.01463v1,Retrieval-augmented generation in multilingual settings,"Retrieval-augmented generation (RAG) has recently emerged as a promising
solution for incorporating up-to-date or domain-specific knowledge into large
language models (LLMs) and improving LLM factuality, but is predominantly
studied in English-only settings. In this work, we consider RAG in the
multilingual setting (mRAG), i.e. with user queries and the datastore in 13
languages, and investigate which components and with which adjustments are
needed to build a well-performing mRAG pipeline, that can be used as a strong
baseline in future works. Our findings highlight that despite the availability
of high-quality off-the-shelf multilingual retrievers and generators,
task-specific prompt engineering is needed to enable generation in user
languages. Moreover, current evaluation metrics need adjustments for
multilingual setting, to account for variations in spelling named entities. The
main limitations to be addressed in future works include frequent
code-switching in non-Latin alphabet languages, occasional fluency errors,
wrong reading of the provided documents, or irrelevant retrieval. We release
the code for the resulting mRAG baseline pipeline at
https://github.com/naver/bergen.",Nadezhda Chirkova
2024-07-01T20:47:47Z,http://arxiv.org/abs/2407.01796v1,"Ground Every Sentence: Improving Retrieval-Augmented LLMs with
  Interleaved Reference-Claim Generation","Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large
Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text
Generation (ATG) has attracted growing attention, which provides citations to
support the model's responses in RAG, so as to enhance the credibility of
LLM-generated content and facilitate verification. Prior methods mainly adopt
coarse-grained attributions, linking to passage-level references or providing
paragraph-level citations. However, these methods still fall short in
verifiability and require certain time costs for fact checking. This paper
proposes a fine-grained ATG method called ReClaim(Refer & Claim), which
alternates the generation of references and answers step by step. Unlike
traditional coarse-grained attribution, ReClaim allows the model to add
sentence-level fine-grained citations to each answer sentence in long-form
question-answering tasks. Our experiments encompass various training and
inference methods and multiple LLMs, verifying the effectiveness of our
approach.",Sirui Xia
2024-07-04T13:52:23Z,http://arxiv.org/abs/2407.03937v2,"TongGu: Mastering Classical Chinese Understanding with
  Knowledge-Grounded Large Language Models","Classical Chinese is a gateway to the rich heritage and wisdom of ancient
China, yet its complexities pose formidable comprehension barriers for most
modern people without specialized knowledge. While Large Language Models (LLMs)
have shown remarkable capabilities in Natural Language Processing (NLP), they
struggle with Classical Chinese Understanding (CCU), especially in
data-demanding and knowledge-intensive tasks. In response to this dilemma, we
propose \textbf{TongGu} (mean understanding ancient and modern), the first
CCU-specific LLM, underpinned by three core contributions. First, we construct
a two-stage instruction-tuning dataset ACCN-INS derived from rich classical
Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we
propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,
enabling TongGu to acquire new capabilities while preserving its foundational
knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)
technique to reduce hallucinations based on knowledge-grounding. Extensive
experiments across 24 diverse CCU tasks validate TongGu's superior ability,
underscoring the effectiveness of RAT and CCU-RAG. The model and dataset are
available at \url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",Jiahuan Cao
2024-07-05T14:16:47Z,http://arxiv.org/abs/2407.04528v4,"GPT vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning","Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.",Aleksander Ficek
2024-07-07T21:26:36Z,http://arxiv.org/abs/2407.05502v2,"Faux Polyglot: A Study on Information Disparity in Multilingual Large
  Language Models","With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are
playing a pivotal role in information search and are being adopted globally.
Although the multilingual capability of LLMs offers new opportunities to bridge
the language barrier, do these capabilities translate into real-life scenarios
where linguistic divide and knowledge conflicts between multilingual sources
are known occurrences? In this paper, we studied LLM's linguistic preference in
a RAG-based information search setting. We found that LLMs displayed systemic
bias towards information in the same language as the query language in both
information retrieval and answer generation. Furthermore, in scenarios where
there is little information in the language of the query, LLMs prefer documents
in high-resource languages, reinforcing the dominant views. Such bias exists
for both factual and opinion-based queries. Our results highlight the
linguistic divide within multilingual LLMs in information search systems. The
seemingly beneficial multilingual capability of LLMs may backfire on
information parity by reinforcing language-specific information cocoons or
filter bubbles further marginalizing low-resource views.",Nikhil Sharma
2024-07-11T13:29:28Z,http://arxiv.org/abs/2407.08495v2,"Investigating LLMs as Voting Assistants via Contextual Augmentation: A
  Case Study on the European Parliament Elections 2024","In light of the recent 2024 European Parliament elections, we are
investigating if LLMs can be used as Voting Advice Applications (VAAs). We
audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the
stance of political parties based on the latest ""EU and I"" voting assistance
questionnaire. Furthermore, we explore alternatives to improve models'
performance by augmenting the input context via Retrieval-Augmented Generation
(RAG) relying on web search, and Self-Reflection using staged conversations
that aim to re-collect relevant content from the model's internal memory. We
find that MIXTRAL is highly accurate with an 82% accuracy on average with a
significant performance disparity across different political groups (50-95%).
Augmenting the input context with expert-curated information can lead to a
significant boost of approx. 9%, which remains an open challenge for automated
RAG approaches, even considering curated content.",Ilias Chalkidis
2024-07-12T13:30:44Z,http://arxiv.org/abs/2407.09252v3,Context Embeddings for Efficient Answer Generation in RAG,"Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.",David Rau
2024-07-17T10:40:39Z,http://arxiv.org/abs/2407.12468v2,"Search Engines, LLMs or Both? Evaluating Information Seeking Strategies
  for Answering Health Questions","Search engines have traditionally served as primary tools for information
seeking. However, the new Large Language Models (LLMs) have recently
demonstrated remarkable capabilities in multiple tasks and, specifically, their
adoption as question answering systems is becoming increasingly prevalent. It
is expected that LLM-based conversational systems and traditional web engines
will continue to coexist in the future, supporting end users in various ways.
But there is a need for more scientific research on the effectiveness of both
types of systems in facilitating accurate information seeking. In this study,
we focus on their merits in answering health questions. We conducted an
extensive study comparing different web search engines, LLMs and
retrieval-augmented (RAG) approaches. Our research reveals intriguing
conclusions. For example, we observed that the quality of webpages potentially
responding to a health question does not decline as we navigate further down
the ranked lists. However, according to our evaluation, web engines are less
accurate than LLMs in finding correct answers to health questions. On the other
hand, LLMs are quite sensitive to the input prompts, and we also found out that
RAG leads to highly effective information seeking methods.",Marcos FernÃ¡ndez-Pichel
2024-07-18T21:49:32Z,http://arxiv.org/abs/2407.13909v1,PRAGyan -- Connecting the Dots in Tweets,"As social media platforms grow, understanding the underlying reasons behind
events and statements becomes crucial for businesses, policymakers, and
researchers. This research explores the integration of Knowledge Graphs (KGs)
with Large Language Models (LLMs) to perform causal analysis of tweets dataset.
The LLM aided analysis techniques often lack depth in uncovering the causes
driving observed effects. By leveraging KGs and LLMs, which encode rich
semantic relationships and temporal information, this study aims to uncover the
complex interplay of factors influencing causal dynamics and compare the
results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented
Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data
format, to retrieve relevant context for causal reasoning. Our approach
demonstrates that the KG-enhanced LLM RAG can provide improved results when
compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus
increases in size. Our qualitative analysis highlights the advantages of
combining KGs with LLMs for improved interpretability and actionable insights,
facilitating informed decision-making across various domains. Whereas,
quantitative analysis using metrics such as BLEU and cosine similarity show
that our approach outperforms the baseline by 10\%.",Rahul Ravi
2024-07-20T17:37:51Z,http://arxiv.org/abs/2407.14944v1,"Automatic Generation of Fashion Images using Prompting in Generative
  Machine Learning Models","The advent of artificial intelligence has contributed in a groundbreaking
transformation of the fashion industry, redefining creativity and innovation in
unprecedented ways. This work investigates methodologies for generating
tailored fashion descriptions using two distinct Large Language Models and a
Stable Diffusion model for fashion image creation. Emphasizing adaptability in
AI-driven fashion creativity, we depart from traditional approaches and focus
on prompting techniques, such as zero-shot and few-shot learning, as well as
Chain-of-Thought (CoT), which results in a variety of colors and textures,
enhancing the diversity of the outputs. Central to our methodology is
Retrieval-Augmented Generation (RAG), enriching models with insights from
fashion sources to ensure contemporary representations. Evaluation combines
quantitative metrics such as CLIPscore with qualitative human judgment,
highlighting strengths in creativity, coherence, and aesthetic appeal across
diverse styles. Among the participants, RAG and few-shot learning techniques
are preferred for their ability to produce more relevant and appealing fashion
descriptions. Our code is provided at https://github.com/georgiarg/AutoFashion.",Georgia Argyrou
2024-07-22T22:14:56Z,http://arxiv.org/abs/2407.16073v1,KaPQA: Knowledge-Augmented Product Question-Answering,"Question-answering for domain-specific applications has recently attracted
much interest due to the latest advancements in large language models (LLMs).
However, accurately assessing the performance of these applications remains a
challenge, mainly due to the lack of suitable benchmarks that effectively
simulate real-world scenarios. To address this challenge, we introduce two
product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop
products to help evaluate the performance of existing models on domain-specific
product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA
framework to enhance the performance of the models in the product QA task. Our
experiments demonstrated that inducing domain knowledge through query
reformulation allowed for increased retrieval and generative performance when
compared to standard RAG-QA methods. This improvement, however, is slight, and
thus illustrates the challenge posed by the datasets introduced.",Swetha Eppalapally
2024-07-31T01:51:24Z,http://arxiv.org/abs/2407.21276v2,Multi-Level Querying using A Knowledge Pyramid,"This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.",Rubing Chen
2024-07-20T06:10:46Z,http://arxiv.org/abs/2408.00798v1,"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation
  for Industrial Knowledge Base","This paper introduces Golden-Retriever, designed to efficiently navigate vast
industrial knowledge bases, overcoming challenges in traditional LLM
fine-tuning and RAG frameworks with domain-specific jargon and context
interpretation. Golden-Retriever incorporates a reflection-based question
augmentation step before document retrieval, which involves identifying jargon,
clarifying its meaning based on context, and augmenting the question
accordingly. Specifically, our method extracts and lists all jargon and
abbreviations in the input question, determines the context against a
pre-defined list, and queries a jargon dictionary for extended definitions and
descriptions. This comprehensive augmentation ensures the RAG framework
retrieves the most relevant documents by providing clear context and resolving
ambiguities, significantly improving retrieval accuracy. Evaluations using
three open-source LLMs on a domain-specific question-answer dataset demonstrate
Golden-Retriever's superior performance, providing a robust solution for
efficiently integrating and querying industrial knowledge bases.",Zhiyu An
2024-08-08T06:57:49Z,http://arxiv.org/abs/2408.04259v2,EfficientRAG: Efficient Retriever for Multi-Hop Question Answering,"Retrieval-augmented generation (RAG) methods encounter difficulties when
addressing complex questions like multi-hop queries. While iterative retrieval
methods improve performance by gathering additional information, current
approaches often rely on multiple calls of large language models (LLMs). In
this paper, we introduce EfficientRAG, an efficient retriever for multi-hop
question answering. EfficientRAG iteratively generates new queries without the
need for LLM calls at each iteration and filters out irrelevant information.
Experimental results demonstrate that EfficientRAG surpasses existing RAG
methods on three open-domain multi-hop question-answering datasets.",Ziyuan Zhuang
2024-08-15T22:34:44Z,http://arxiv.org/abs/2408.08444v1,"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question
  Answering","In knowledge-intensive tasks such as open-domain question answering (OpenQA),
Large Language Models (LLMs) often struggle to generate factual answers relying
solely on their internal (parametric) knowledge. To address this limitation,
Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving
relevant information from external sources, thereby positioning the retriever
as a pivotal component. Although dense retrieval demonstrates state-of-the-art
performance, its training poses challenges due to the scarcity of ground-truth
evidence, largely attributed to the high costs of human annotation. In this
paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create
weakly labeled data for training dense retrievers. Specifically, we rerank the
top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will
generate the correct answer based on the question and each passage. The
highest-ranking passages are then used as positive training examples for dense
retrieval. Our comprehensive experiments across four publicly available OpenQA
datasets demonstrate that our approach enhances both retrieval and OpenQA
performance compared to baseline models.",Jinming Nian
2024-08-15T16:53:05Z,http://arxiv.org/abs/2408.08925v1,"Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building
  E-commerce Chat Assistants","This work presents Retail-GPT, an open-source RAG-based chatbot designed to
enhance user engagement in retail e-commerce by guiding users through product
recommendations and assisting with cart operations. The system is
cross-platform and adaptable to various e-commerce domains, avoiding reliance
on specific chat applications or commercial activities. Retail-GPT engages in
human-like conversations, interprets user demands, checks product availability,
and manages cart operations, aiming to serve as a virtual sales agent and test
the viability of such assistants across different retail businesses.",Bruno Amaral Teixeira de Freitas
2024-08-18T11:47:55Z,http://arxiv.org/abs/2408.14484v1,Agentic Retrieval-Augmented Generation for Time Series Analysis,"Time series modeling is crucial for many applications, however, it faces
challenges such as complex spatio-temporal dependencies and distribution shifts
in learning from historical context to predict task-specific outcomes. To
address these challenges, we propose a novel approach using an agentic
Retrieval-Augmented Generation (RAG) framework for time series analysis. The
framework leverages a hierarchical, multi-agent architecture where the master
agent orchestrates specialized sub-agents and delegates the end-user request to
the relevant sub-agent. The sub-agents utilize smaller, pre-trained language
models (SLMs) customized for specific time series tasks through fine-tuning
using instruction tuning and direct preference optimization, and retrieve
relevant prompts from a shared repository of prompt pools containing distilled
knowledge about historical patterns and trends to improve predictions on new
data. Our proposed modular, multi-agent RAG approach offers flexibility and
achieves state-of-the-art performance across major time series tasks by
tackling complex challenges more effectively than task-specific customized
methods across benchmark datasets.",Chidaksh Ravuru
2024-08-30T08:26:55Z,http://arxiv.org/abs/2408.17095v2,"RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation
  and Retrieval-Guidance","Diffusion-based models demonstrate impressive generation capabilities.
However, they also have a massive number of parameters, resulting in enormous
model sizes, thus making them unsuitable for deployment on resource-constraint
devices. Block-wise generation can be a promising alternative for designing
compact-sized (parameter-efficient) deep generative models since the model can
generate one block at a time instead of generating the whole image at once.
However, block-wise generation is also considerably challenging because
ensuring coherence across generated blocks can be non-trivial. To this end, we
design a retrieval-augmented generation (RAG) approach and leverage the
corresponding blocks of the images retrieved by the RAG module to condition the
training and generation stages of a block-wise denoising diffusion model. Our
conditioning schemes ensure coherence across the different blocks during
training and, consequently, during generation. While we showcase our approach
using the latent diffusion model (LDM) as the base model, it can be used with
other variants of denoising diffusion models. We validate the solution of the
coherence problem through the proposed approach by reporting substantive
experiments to demonstrate our approach's effectiveness in compact model size
and excellent generation quality.",Avideep Mukherjee
2024-09-04T01:14:04Z,http://arxiv.org/abs/2409.02361v1,"Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented
  Ambiguous Question Answering","The retrieval augmented generation (RAG) framework addresses an ambiguity in
user queries in QA systems by retrieving passages that cover all plausible
interpretations and generating comprehensive responses based on the passages.
However, our preliminary studies reveal that a single retrieval process often
suffers from low quality results, as the retrieved passages frequently fail to
capture all plausible interpretations. Although the iterative RAG approach has
been proposed to address this problem, it comes at the cost of significantly
reduced efficiency. To address these issues, we propose the
diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved
passages to encompass diverse interpretations. Subsequently, DIVA verifies the
quality of the passages and adapts the most suitable approach tailored to their
quality. This approach improves the QA systems accuracy and robustness by
handling low quality retrieval issue in ambiguous questions, while enhancing
efficiency.",Yeonjun In
2024-09-18T20:03:32Z,http://arxiv.org/abs/2409.12294v1,"RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and
  Language Models","Large language models (LLMs) have recently emerged as promising tools for
solving challenging robotic tasks, even in the presence of action and
observation uncertainties. Recent LLM-based decision-making methods (also
referred to as LLM-based agents), when paired with appropriate critics, have
demonstrated potential in solving complex, long-horizon tasks with relatively
few interactions. However, most existing LLM-based agents lack the ability to
retain and learn from past interactions - an essential trait of learning-based
robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based
agents with a memory of past interactions and incorporates critics to evaluate
the agents' decisions. The memory component allows the agent to automatically
retrieve and incorporate relevant past experiences as in-context examples,
providing context-aware feedback for more informed decision-making. Further by
updating its memory, the agent improves its performance over time, thereby
exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld
domains, we demonstrate significant improvements in task success rates and
efficiency, showing that the proposed RAG-Modulo framework outperforms
state-of-the-art baselines.",Abhinav Jain
2024-09-19T16:23:42Z,http://arxiv.org/abs/2409.12880v1,"Enhancing E-commerce Product Title Translation with Retrieval-Augmented
  Generation and Large Language Models","E-commerce stores enable multilingual product discovery which require
accurate product title translation. Multilingual large language models (LLMs)
have shown promising capacity to perform machine translation tasks, and it can
also enhance and translate product titles cross-lingually in one step. However,
product title translation often requires more than just language conversion
because titles are short, lack context, and contain specialized terminology.
This study proposes a retrieval-augmented generation (RAG) approach that
leverages existing bilingual product information in e-commerce by retrieving
similar bilingual examples and incorporating them as few-shot prompts to
enhance LLM-based product title translation. Experiment results show that our
proposed RAG approach improve product title translation quality with chrF score
gains of up to 15.3% for language pairs where the LLM has limited proficiency.",Bryan Zhang
2024-09-19T23:38:59Z,http://arxiv.org/abs/2409.13122v2,"RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal
  Reinforcement and Retrieval-Augmented Generation","In real-world software engineering tasks, solving a problem often requires
understanding and modifying multiple functions, classes, and files across a
large codebase. Therefore, on the repository level, it is crucial to extract
the relevant information to achieve accurate code completion effectively.
Existing code completion tools have achieved some success, but they struggle to
optimize the retrieval and generation process dynamically. In this paper, we
propose RepoGenReflex, a generic, dynamic, effective framework to address this
challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with
Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal
results for repository-level code completion. RepoGenReflex uses Reflector to
give directional feedback to the next loop. RepoGenReflex chooses the optimal
results stored in the Experience cache based on the RAG-VRL loop. To validate
the framework's generalization ability, we propose a new benchmark RepoGenEval,
which consists of the latest, high-quality real-world repositories in line
completion scenarios. Our experiments demonstrate that RepoGenReflex achieves
significant improvements after optimizing the Reflector component, resulting in
enhanced accuracy and relevance of code completions. Additionally,
RepoGenReflex consistently demonstrates superior performance and effectiveness
across standard code completion tasks, highlighting the robustness and
adaptability of our framework.",Jicheng Wang
2024-09-20T14:30:45Z,http://arxiv.org/abs/2409.13537v1,"ShizishanGPT: An Agricultural Large Language Model Integrating Tools and
  Resources","Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",Shuting Yang
2024-09-24T15:20:39Z,http://arxiv.org/abs/2409.16176v1,Cyber Knowledge Completion Using Large Language Models,"The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.",Braden K Webb
2024-09-25T04:12:38Z,http://arxiv.org/abs/2409.16605v1,"Evaluating and Enhancing Large Language Models for Novelty Assessment in
  Scholarly Publications","Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.",Ethan Lin
2024-09-25T09:41:46Z,http://arxiv.org/abs/2409.16779v1,LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ,"Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements.",Marc-Antoine Allard
2024-09-26T16:12:33Z,http://arxiv.org/abs/2409.18003v1,"Enhancing Tourism Recommender Systems for Sustainable City Trips Using
  Retrieval-Augmented Generation","Tourism Recommender Systems (TRS) have traditionally focused on providing
personalized travel suggestions, often prioritizing user preferences without
considering broader sustainability goals. Integrating sustainability into TRS
has become essential with the increasing need to balance environmental impact,
local community interests, and visitor satisfaction. This paper proposes a
novel approach to enhancing TRS for sustainable city trips using Large Language
Models (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We
enhance the traditional RAG system by incorporating a sustainability metric
based on a city's popularity and seasonal demand during the prompt augmentation
phase. This modification, called Sustainability Augmented Reranking (SAR),
ensures the system's recommendations align with sustainability goals.
Evaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and
Mistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently
matches or outperforms the baseline (without SAR) across most metrics,
highlighting the benefits of incorporating sustainability into TRS.",Ashmi Banerjee
2024-09-30T08:26:53Z,http://arxiv.org/abs/2409.20075v1,"BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the
  E-commerce Domain","Retrieval Augmented Generation (RAG) system is important in domains such as
e-commerce, which has many long-tail entities and frequently updated
information. Most existing works adopt separate modules for retrieval and
generation, which may be suboptimal since the retrieval task and the generation
task cannot benefit from each other to improve performance. We propose a novel
Backbone Shared RAG framework (BSharedRAG). It first uses a domain-specific
corpus to continually pre-train a base model as a domain-specific backbone
model and then trains two plug-and-play Low-Rank Adaptation (LoRA) modules
based on the shared backbone to minimize retrieval and generation losses
respectively. Experimental results indicate that our proposed BSharedRAG
outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in
retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation.
Our codes, models, and dataset are available at https://bsharedrag.github.io.",Kaisi Guan
2024-09-30T15:53:38Z,http://arxiv.org/abs/2409.20434v1,"QAEncoder: Towards Aligned Representation Learning in Question Answering
  System","Modern QA systems entail retrieval-augmented generation (RAG) for accurate
and trustworthy responses. However, the inherent gap between user queries and
relevant documents hinders precise matching. Motivated by our conical
distribution hypothesis, which posits that potential queries and documents form
a cone-like structure in the embedding space, we introduce QAEncoder, a
training-free approach to bridge this gap. Specifically, QAEncoder estimates
the expectation of potential queries in the embedding space as a robust
surrogate for the document embedding, and attaches document fingerprints to
effectively distinguish these embeddings. Extensive experiments on fourteen
embedding models across six languages and eight datasets validate QAEncoder's
alignment capability, which offers a plug-and-play solution that seamlessly
integrates with existing RAG architectures and training-based methods.",Zhengren Wang
2024-09-13T06:10:42Z,http://arxiv.org/abs/2410.00005v1,Winning Solution For Meta KDD Cup' 24,"This paper describes the winning solutions of all tasks in Meta KDD Cup 24
from db3 team. The challenge is to build a RAG system from web sources and
knowledge graphs. We are given multiple sources for each query to help us
answer the question. The CRAG challenge involves three tasks: (1) condensing
information from web pages into accurate answers, (2) integrating structured
data from mock knowledge graphs, and (3) selecting and integrating critical
data from extensive web pages and APIs to reflect real-world retrieval
challenges. Our solution for Task #1 is a framework of web or open-data
retrieval and answering. The large language model (LLM) is tuned for better RAG
performance and less hallucination. Task #2 and Task #3 solutions are based on
a regularized API set for domain questions and the API generation method using
tuned LLM. Our knowledge graph API interface extracts directly relevant
information to help LLMs answer correctly. Our solution achieves 1st place on
all three tasks, achieving a score of 28.4%, 42.7%, and 47.8%, respectively.",Yikuan Xia
2024-10-03T17:39:38Z,http://arxiv.org/abs/2410.02719v1,"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling
  for Retrieval-Augmented Generation","We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.",Zixuan Li
2024-10-05T17:11:37Z,http://arxiv.org/abs/2410.04231v1,"Metadata-based Data Exploration with Retrieval-Augmented Generation for
  Large Language Models","Developing the capacity to effectively search for requisite datasets is an
urgent requirement to assist data users in identifying relevant datasets
considering the very limited available metadata. For this challenge, the
utilization of third-party data is emerging as a valuable source for
improvement. Our research introduces a new architecture for data exploration
which employs a form of Retrieval-Augmented Generation (RAG) to enhance
metadata-based data discovery. The system integrates large language models
(LLMs) with external vector databases to identify semantic relationships among
diverse types of datasets. The proposed framework offers a new method for
evaluating semantic similarity among heterogeneous data sources and for
improving data exploration. Our study includes experimental results on four
critical tasks: 1) recommending similar datasets, 2) suggesting combinable
datasets, 3) estimating tags, and 4) predicting variables. Our results
demonstrate that RAG can enhance the selection of relevant datasets,
particularly from different categories, when compared to conventional metadata
approaches. However, performance varied across tasks and models, which confirms
the significance of selecting appropriate techniques based on specific use
cases. The findings suggest that this approach holds promise for addressing
challenges in data exploration and discovery, although further refinement is
necessary for estimation tasks.",Teruaki Hayashi
2024-10-07T16:14:47Z,http://arxiv.org/abs/2410.05162v1,"Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models","Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.",Mehrdad Farahani
2024-10-08T08:00:12Z,http://arxiv.org/abs/2410.05779v2,LightRAG: Simple and Fast Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG.",Zirui Guo
2024-10-14T14:56:01Z,http://arxiv.org/abs/2410.10584v1,"STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with
  FeedBack","Large Language Models (LLMs) often generate incorrect or outdated
information, especially in low-resource settings or when dealing with private
data. To address this, Retrieval-Augmented Generation (RAG) uses external
knowledge bases (KBs), but these can also suffer from inaccuracies. We
introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base
editing with FEEDback approach that iteratively refines the KB based on expert
feedback using a multi-actor, centralized critic reinforcement learning
framework. Each document is assigned to an actor, modeled as a ReACT agent,
which performs structured edits based on document-specific targeted
instructions from a centralized critic. Experimental results show that
STACKFEED significantly improves KB quality and RAG system performance,
enhancing accuracy by up to 8% over baselines.",Naman Gupta
2024-10-15T03:04:26Z,http://arxiv.org/abs/2410.11217v1,On the Capacity of Citation Generation by Large Language Models,"Retrieval-augmented generation (RAG) appears as a promising method to
alleviate the ""hallucination"" problem in large language models (LLMs), since it
can incorporate external traceable resources for response generation. The
essence of RAG in combating the hallucination issue lies in accurately
attributing claims in responses to the corresponding retrieved documents.
However, most of existing works focus on improving the quality of generated
responses from the LLM, while largely overlooked its ability to attribute
sources accurately. In this study, we conduct a systematic analysis about the
capabilities of LLMs in generating citations within response generation, and
further introduce a novel method to enhance their citation generation
abilities. Specifically, we evaluate both the correctness and citation quality
for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce
new citation evaluation metrics to eliminate the over-penalization of
unnecessary and excessive citations in existing metrics. Furthermore, we
propose a Generate-then-Refine method that completes relevant citations and
removes irrelevant ones without altering the response text. The results on
WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves
the quality of citations in responses generated by LLMs.",Haosheng Qian
2024-10-15T09:50:19Z,http://arxiv.org/abs/2410.11446v1,"AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a
  simple RAG task","This paper describes our $3^{rd}$ place submission in the AVeriTeC shared
task in which we attempted to address the challenge of fact-checking with
evidence retrieved in the wild using a simple scheme of Retrieval-Augmented
Generation (RAG) designed for the task, leveraging the predictive power of
Large Language Models. We release our codebase and explain its two modules -
the Retriever and the Evidence & Label generator - in detail, justifying their
features such as MMR-reranking and Likert-scale confidence estimation. We
evaluate our solution on AVeriTeC dev and test set and interpret the results,
picking the GPT-4o as the most appropriate model for our pipeline at the time
of our publication, with Llama 3.1 70B being a promising open-source
alternative. We perform an empirical error analysis to see that faults in our
predictions often coincide with noise in the data or ambiguous fact-checks,
provoking further research and data augmentation.",Herbert Ullrich
2024-10-06T17:11:29Z,http://arxiv.org/abs/2410.11859v1,"SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health
  Support and Assessment, Based on a Systematic Application Survey","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide accessible, stigma-free, personalized, and real-time
mental health support through cutting-edge AI technologies. It makes the
following contributions: (1) Conducting an extensive survey of recent mental
health support methods to identify prevalent functionalities and unmet needs.
(2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM
technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering,
and domain knowledge. This system offers advanced features such as Suicide Risk
Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized
profile uploads and Conversational Information Extraction. (3) Developing novel
evaluation approaches to assess preliminary assessments and suicide risk
detection, utilizing annotated real-life interview data and professionally
labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator
Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance
model performance and usability through context-sensitive response adjustments
and semantic coherence evaluations. This study contributes to advancing mental
health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-15T21:10:01Z,http://arxiv.org/abs/2410.12069v1,De-jargonizing Science for Journalists with GPT-4: A Pilot Study,"This study offers an initial evaluation of a human-in-the-loop system
leveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented
Generation (RAG) to identify and define jargon terms in scientific abstracts,
based on readers' self-reported knowledge. The system achieves fairly high
recall in identifying jargon and preserves relative differences in readers'
jargon identification, suggesting personalization as a feasible use-case for
LLMs to support sense-making of complex information. Surprisingly, using only
abstracts for context to generate definitions yields slightly more accurate and
higher quality definitions than using RAG-based context from the fulltext of an
article. The findings highlight the potential of generative AI for assisting
science reporters, and can inform future work on developing tools to simplify
dense documents.",Sachita Nishal
2024-10-17T06:57:29Z,http://arxiv.org/abs/2410.13272v1,"FRAG: Toward Federated Vector Database Management for Collaborative and
  Secure Retrieval-Augmented Generation","This paper introduces \textit{Federated Retrieval-Augmented Generation
(FRAG)}, a novel database management paradigm tailored for the growing needs of
retrieval-augmented generation (RAG) systems, which are increasingly powered by
large-language models (LLMs). FRAG enables mutually-distrusted parties to
collaboratively perform Approximate $k$-Nearest Neighbor (ANN) searches on
encrypted query vectors and encrypted data stored in distributed vector
databases, all while ensuring that no party can gain any knowledge about the
queries or data of others. Achieving this paradigm presents two key challenges:
(i) ensuring strong security guarantees, such as Indistinguishability under
Chosen-Plaintext Attack (IND-CPA), under practical assumptions (e.g., we avoid
overly optimistic assumptions like non-collusion among parties); and (ii)
maintaining performance overheads comparable to traditional, non-federated RAG
systems. To address these challenges, FRAG employs a single-key homomorphic
encryption protocol that simplifies key management across mutually-distrusted
parties. Additionally, FRAG introduces a \textit{multiplicative caching}
technique to efficiently encrypt floating-point numbers, significantly
improving computational performance in large-scale federated environments. We
provide a rigorous security proof using standard cryptographic reductions and
demonstrate the practical scalability and efficiency of FRAG through extensive
experiments on both benchmark and real-world datasets.",Dongfang Zhao
2024-10-17T07:46:49Z,http://arxiv.org/abs/2410.13293v2,"SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation","Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations. Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM). Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a ""reasoning score"" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more
structured problem-solving process potentially providing educational benefits
for students.",Prakhar Dixit
2024-10-17T13:51:03Z,http://arxiv.org/abs/2410.13553v1,"Integrating Temporal Representations for Dynamic Memory Retrieval and
  Management in Large Language Models","Conventional dialogue agents often struggle with effective memory recall,
leading to redundant retrieval and inadequate management of unique user
associations. To address this, we propose SynapticRAG, a novel approach
integrating synaptic dynamics into Retrieval-Augmented Generation (RAG).
SynapticRAG integrates temporal representations into memory vectors, mimicking
biological synapses by differentiating events based on occurrence times and
dynamically updating memory significance. This model employs temporal scoring
for memory connections and a synaptic-inspired propagation control mechanism.
Experiments across English, Japanese, and Chinese datasets demonstrate
SynapticRAG's superiority over existing methods, including traditional RAG,
with up to 14.66\% improvement in memory retrieval accuracy. Our approach
advances context-aware dialogue AI systems by enhancing long-term context
maintenance and specific information extraction from conversations.",Yuki Hou
2024-10-18T14:02:34Z,http://arxiv.org/abs/2410.14479v1,"Backdoored Retrievers for Prompt Injection Attacks on Retrieval
  Augmented Generation of Large Language Models","Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating coherent text but remain limited by the static nature of their
training data. Retrieval Augmented Generation (RAG) addresses this issue by
combining LLMs with up-to-date information retrieval, but also expand the
attack surface of the system. This paper investigates prompt injection attacks
on RAG, focusing on malicious objectives beyond misinformation, such as
inserting harmful links, promoting unauthorized services, and initiating
denial-of-service behaviors. We build upon existing corpus poisoning techniques
and propose a novel backdoor attack aimed at the fine-tuning process of the
dense retriever component. Our experiments reveal that corpus poisoning can
achieve significant attack success rates through the injection of a small
number of compromised documents into the retriever corpus. In contrast,
backdoor attacks demonstrate even higher success rates but necessitate a more
complex setup, as the victim must fine-tune the retriever using the attacker
poisoned dataset.",Cody Clop
2024-10-19T01:29:12Z,http://arxiv.org/abs/2410.14926v1,"Aligning LLMs with Human Instructions and Stock Market Feedback in
  Financial Sentiment Analysis","Financial sentiment analysis is crucial for trading and investment
decision-making. This study introduces an adaptive retrieval augmented
framework for Large Language Models (LLMs) that aligns with human instructions
through Instruction Tuning and incorporates market feedback to dynamically
adjust weights across various knowledge sources within the Retrieval-Augmented
Generation (RAG) module. Building upon foundational models like LLaMA 2, we
fine-tune a series of LLMs ranging from 7B to 70B in size, enriched with
Instruction Tuning and RAG, and further optimized through direct feedback and
Reinforcement Learning (RL)-based refinement methods applied to the source
weights of RAG.Through extensive evaluation, we demonstrate that the sentiment
outputs from our LLMs more accurately mirror the intrinsic sentiment of textual
data, showcasing a 1% to 6% boost in accuracy and F1 score over existing
state-of-the-art models and leading conversational AI systems. Moreover, the
sentiments extracted are more indicative of the directions in stock price
movements. On top of that, we successfully construct portfolios that yield a
3.61% higher Sharpe ratio compared to the S&P 500 baseline in bullish markets.
These portfolios also demonstrate resilience in bearish markets, with a 5x
reduction in return losses compared to those typically experienced by the S&P
500.",Zijie Zhao
2024-10-20T04:51:24Z,http://arxiv.org/abs/2410.15285v1,"Contextual Augmented Multi-Model Programming (CAMP): A Hybrid
  Local-Cloud Copilot Framework","The advancements in cloud-based Large Languages Models (LLMs) have
revolutionized AI-assisted programming. However, their integration into certain
local development environments like ones within the Apple software ecosystem
(e.g., iOS apps, macOS) remains challenging due to computational demands and
sandboxed constraints. This paper presents CAMP, a multi-model AI-assisted
programming framework that consists of a local model that employs
Retrieval-Augmented Generation (RAG) to retrieve contextual information from
the codebase to facilitate context-aware prompt construction thus optimizing
the performance of the cloud model, empowering LLMs' capabilities in local
Integrated Development Environments (IDEs). The methodology is actualized in
Copilot for Xcode, an AI-assisted programming tool crafted for Xcode that
employs the RAG module to address software constraints and enables diverse
generative programming tasks, including automatic code completion,
documentation, error detection, and intelligent user-agent interaction. The
results from objective experiments on generated code quality and subjective
experiments on user adoption collectively demonstrate the pilot success of the
proposed system and mark its significant contributions to the realm of
AI-assisted programming.",Yuchen Wang
2024-10-20T21:17:05Z,http://arxiv.org/abs/2410.15511v1,"ConTReGen: Context-driven Tree-structured Retrieval for Open-domain
  Long-form Text Generation","Open-domain long-form text generation requires generating coherent,
comprehensive responses that address complex queries with both breadth and
depth. This task is challenging due to the need to accurately capture diverse
facets of input queries. Existing iterative retrieval-augmented generation
(RAG) approaches often struggle to delve deeply into each facet of complex
queries and integrate knowledge from various sources effectively. This paper
introduces ConTReGen, a novel framework that employs a context-driven,
tree-structured retrieval approach to enhance the depth and relevance of
retrieved content. ConTReGen integrates a hierarchical, top-down in-depth
exploration of query facets with a systematic bottom-up synthesis, ensuring
comprehensive coverage and coherent integration of multifaceted information.
Extensive experiments on multiple datasets, including LFQA and ODSUM, alongside
a newly introduced dataset, ODSUM-WikiHow, demonstrate that ConTReGen
outperforms existing state-of-the-art RAG models.",Kashob Kumar Roy
2024-10-21T07:56:45Z,http://arxiv.org/abs/2410.15737v1,Who's Who: Large Language Models Meet Knowledge Conflicts in Practice,"Retrieval-augmented generation (RAG) methods are viable solutions for
addressing the static memory limits of pre-trained language models.
Nevertheless, encountering conflicting sources of information within the
retrieval context is an inevitable practical challenge. In such situations, the
language models are recommended to transparently inform users about the
conflicts rather than autonomously deciding what to present based on their
inherent biases. To analyze how current large language models (LLMs) align with
our recommendation, we introduce WhoQA, a public benchmark dataset to examine
model's behavior in knowledge conflict situations. We induce conflicts by
asking about a common property among entities having the same name, resulting
in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K
questions across 13 Wikidata property types and 150K Wikipedia entities. Our
experiments show that despite the simplicity of WhoQA questions, knowledge
conflicts significantly degrades LLMs' performance in RAG settings.",Quang Hieu Pham
2024-10-05T14:37:35Z,http://arxiv.org/abs/2410.16285v1,"Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for
  Your Job?","The current paper presents the development and validation of SelfScore, a
novel benchmark designed to assess the performance of automated Large Language
Model (LLM) agents on help desk and professional consultation tasks. Given the
increasing integration of AI in industries, particularly within customer
service, SelfScore fills a crucial gap by enabling the comparison of automated
agents and human workers. The benchmark evaluates agents on problem complexity
and response helpfulness, ensuring transparency and simplicity in its scoring
system. The study also develops automated LLM agents to assess SelfScore and
explores the benefits of Retrieval-Augmented Generation (RAG) for
domain-specific tasks, demonstrating that automated LLM agents incorporating
RAG outperform those without. All automated LLM agents were observed to perform
better than the human control group. Given these results, the study raises
concerns about the potential displacement of human workers, especially in areas
where AI technologies excel. Ultimately, SelfScore provides a foundational tool
for understanding the impact of AI in help desk environments while advocating
for ethical considerations in the ongoing transition towards automation.",John Mavi
2024-10-09T16:35:41Z,http://arxiv.org/abs/2410.18251v1,Context-Augmented Code Generation Using Programming Knowledge Graphs,"Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.",Iman Saberi
2024-10-28T02:55:03Z,http://arxiv.org/abs/2410.20695v1,"Combining Domain-Specific Models and LLMs for Automated Disease
  Phenotyping from Survey Data","This exploratory pilot study investigated the potential of combining a
domain-specific model, BERN2, with large language models (LLMs) to enhance
automated disease phenotyping from research survey data. Motivated by the need
for efficient and accurate methods to harmonize the growing volume of survey
data with standardized disease ontologies, we employed BERN2, a biomedical
named entity recognition and normalization model, to extract disease
information from the ORIGINS birth cohort survey data. After rigorously
evaluating BERN2's performance against a manually curated ground truth dataset,
we integrated various LLMs using prompt engineering, Retrieval-Augmented
Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's
outputs. BERN2 demonstrated high performance in extracting and normalizing
disease mentions, and the integration of LLMs, particularly with Few Shot
Inference and RAG orchestration, further improved accuracy. This approach,
especially when incorporating structured examples, logical reasoning prompts,
and detailed context, offers a promising avenue for developing tools to enable
efficient cohort profiling and data harmonization across large, heterogeneous
research datasets.",Gal Beeri
2024-10-28T12:50:27Z,http://arxiv.org/abs/2410.20975v1,"Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base
  for Geospatial Code Generation Tasks Using Large Language Models","The rise of spatiotemporal data and the need for efficient geospatial
modeling have spurred interest in automating these tasks with large language
models (LLMs). However, general LLMs often generate errors in geospatial code
due to a lack of domain-specific knowledge on functions and operators. To
address this, a retrieval-augmented generation (RAG) approach, utilizing an
external knowledge base of geospatial functions and operators, is proposed.
This study introduces a framework to construct such a knowledge base,
leveraging geospatial script semantics. The framework includes: Function
Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination
Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like
Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and
align geospatial functions. An example knowledge base, Geo-FuB, built from
154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics
show a high accuracy, reaching 88.89% overall, with structural and semantic
accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize
geospatial code generation through the RAG and fine-tuning paradigms is
highlighted.",Shuyang Hou
2024-10-28T14:29:11Z,http://arxiv.org/abs/2410.21067v1,"CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and
  Retrieval-Augmented Translation with Large Language Models","Large language models (LLMs) have shown great promise in machine translation,
but they still struggle with contextually dependent terms, such as new or
domain-specific words. This leads to inconsistencies and errors that are
difficult to address. Existing solutions often depend on manual identification
of such terms, which is impractical given the complexity and evolving nature of
language. While Retrieval-Augmented Generation (RAG) could provide some
assistance, its application to translation is limited by issues such as
hallucinations from information overload. In this paper, we propose CRAT, a
novel multi-agent translation framework that leverages RAG and
causality-enhanced self-reflection to address these challenges. This framework
consists of several specialized agents: the Unknown Terms Identification agent
detects unknown terms within the context, the Knowledge Graph (KG) Constructor
agent extracts relevant internal knowledge about these terms and retrieves
bilingual information from external sources, the Causality-enhanced Judge agent
validates the accuracy of the information, and the Translator agent
incorporates the refined information into the final output. This automated
process allows for more precise and consistent handling of key terms during
translation. Our results show that CRAT significantly improves translation
accuracy, particularly in handling context-sensitive terms and emerging
vocabulary.",Meiqi Chen
2024-10-31T13:05:39Z,http://arxiv.org/abs/2410.23902v1,"Responsible Retrieval Augmented Generation for Climate Decision Making
  from Documents","Climate decision making is constrained by the complexity and inaccessibility
of key information within lengthy, technical, and multi-lingual documents.
Generative AI technologies offer a promising route for improving the
accessibility of information contained within these documents, but suffer from
limitations. These include (1) a tendency to hallucinate or mis-represent
information, (2) difficulty in steering or guaranteeing properties of generated
output, and (3) reduced performance in specific technical domains. To address
these challenges, we introduce a novel evaluation framework with
domain-specific dimensions tailored for climate-related documents. We then
apply this framework to evaluate Retrieval-Augmented Generation (RAG)
approaches and assess retrieval- and generation-quality within a prototype tool
that answers questions about individual climate law and policy documents. In
addition, we publish a human-annotated dataset and scalable automated
evaluation tools, with the aim of facilitating broader adoption and robust
assessment of these systems in the climate domain. Our findings highlight the
key components of responsible deployment of RAG to enhance decision-making,
while also providing insights into user experience (UX) considerations for
safely deploying such systems to build trust with users in high-risk domains.",Matyas Juhasz
2024-10-31T18:43:12Z,http://arxiv.org/abs/2411.00142v1,"JudgeRank: Leveraging Large Language Models for Reasoning-Intensive
  Reranking","Accurate document retrieval is crucial for the success of retrieval-augmented
generation (RAG) applications, including open-domain question answering and
code completion. While large language models (LLMs) have been employed as dense
encoders or listwise rerankers in RAG systems, they often struggle with
reasoning-intensive tasks because they lack nuanced analysis when judging
document relevance. To address this limitation, we introduce JudgeRank, a novel
agentic reranker that emulates human cognitive processes when assessing
document relevance. Our approach consists of three key steps: (1) query
analysis to identify the core problem, (2) document analysis to extract a
query-aware summary, and (3) relevance judgment to provide a concise assessment
of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT
benchmark, demonstrating substantial performance improvements over first-stage
retrieval methods and outperforming other popular reranking approaches. In
addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers
on the popular BEIR benchmark, validating its zero-shot generalization
capability. Through comprehensive ablation studies, we demonstrate that
JudgeRank's performance generalizes well across LLMs of various sizes while
ensembling them yields even more accurate reranking than individual models.",Tong Niu
2024-11-01T15:50:58Z,http://arxiv.org/abs/2411.00689v1,"Towards Multi-Source Retrieval-Augmented Generation via Synergizing
  Reasoning and Preference-Driven Retrieval","Retrieval-Augmented Generation (RAG) has emerged as a reliable external
knowledge augmentation technique to mitigate hallucination issues and
parameterized knowledge limitations in Large Language Models (LLMs). Existing
Adaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval
sources due to their inability to select the right source at the right time. To
address this, we propose a multi-source ARAG framework, termed MSPR, which
synergizes reasoning and preference-driven retrieval to adaptive decide ""when
and what to retrieve"" and ""which retrieval source to use"". To better adapt to
retrieval sources of differing characteristics, we also employ retrieval action
adjustment and answer feedback strategy. They enable our framework to fully
explore the high-quality primary source while supplementing it with secondary
sources at the right time. Extensive and multi-dimensional experiments
conducted on three datasets demonstrate the superiority and effectiveness of
MSPR.",Qingfei Zhao
2024-11-01T20:44:59Z,http://arxiv.org/abs/2411.01022v1,"Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM
  Generation Output","We present a light-weight approach for detecting nonfactual outputs from
retrieval-augmented generation (RAG). Given a context and putative output, we
compute a factuality score that can be thresholded to yield a binary decision
to check the results of LLM-based question-answering, summarization, or other
systems. Unlike factuality checkers that themselves rely on LLMs, we use
compact, open-source natural language inference (NLI) models that yield a
freely accessible solution with low latency and low cost at run-time, and no
need for LLM fine-tuning. The approach also enables downstream mitigation and
correction of hallucinations, by tracing them back to specific context chunks.
Our experiments show high area under the ROC curve (AUC) across a wide range of
relevant open source datasets, indicating the effectiveness of our method for
fact-checking RAG output.",Hithesh Sankararaman
2024-11-04T21:12:08Z,http://arxiv.org/abs/2411.02617v1,"TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support for Network","The telecommunications industry's rapid evolution demands intelligent systems
capable of managing complex networks and adapting to emerging technologies.
While large language models (LLMs) show promise in addressing these challenges,
their deployment in telecom environments faces significant constraints due to
edge device limitations and inconsistent documentation. To bridge this gap, we
present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)
system built on the Phi-2 small language model (SLM). To improve context
retrieval, TeleOracle employs a two-stage retriever that incorporates semantic
chunking and hybrid keyword and semantic search. Additionally, we expand the
context window during inference to enhance the model's performance on
open-ended queries. We also employ low-rank adaption for efficient fine-tuning.
A thorough analysis of the model's performance indicates that our RAG framework
is effective in aligning Phi-2 to the telecom domain in a downstream question
and answer (QnA) task, achieving a 30% improvement in accuracy over the base
Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our
model not only performs on par with the much larger LLMs but also achieves a
higher faithfulness score, indicating higher adherence to the retrieved
context.",Nouf Alabbasi
2024-11-05T06:44:15Z,http://arxiv.org/abs/2411.02850v1,"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene","This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.",Simon Kloker
2024-11-08T18:26:17Z,http://arxiv.org/abs/2411.05764v1,"FinDVer: Explainable Claim Verification over Long and Hybrid-Content
  Financial Documents","We introduce FinDVer, a comprehensive benchmark specifically designed to
evaluate the explainable claim verification capabilities of LLMs in the context
of understanding and analyzing long, hybrid-content financial documents.
FinDVer contains 2,400 expert-annotated examples, divided into three subsets:
information extraction, numerical reasoning, and knowledge-intensive reasoning,
each addressing common scenarios encountered in real-world financial contexts.
We assess a broad spectrum of LLMs under long-context and RAG settings. Our
results show that even the current best-performing system, GPT-4o, still lags
behind human experts. We further provide in-depth analysis on long-context and
RAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering
insights to drive future advancements. We believe that FinDVer can serve as a
valuable benchmark for evaluating LLMs in claim verification over complex,
expert-domain documents.",Yilun Zhao
2024-11-09T13:17:39Z,http://arxiv.org/abs/2411.06175v2,"Clustering Algorithms and RAG Enhancing Semi-Supervised Text
  Classification with Large LLMs","This paper introduces a novel semi-supervised learning framework specifically
designed for text classification tasks, effectively addressing the challenge of
vast datasets with limited labeled examples. By integrating multi-level
similarity based data augmentation techniques from Retrieval-Augmented
Generation (RAG) to Large Language Model (LLM) rewriting and traditional word
substitution-we constructed an intelligent augmentation pipeline. This
framework innovatively employs the selection of representative landmarks
through clustering, which serve as intermediaries in the retrieval and
rewriting processes, ensuring that the augmented data maintains a distribution
similar to the original dataset. Empirical results show that even in complex
text document classification scenarios with over 100 categories, our method
achieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and
Web of Science datasets, respectively. These findings highlight the
effectiveness and broad applicability of our semi-supervised learning approach
for text classification tasks.",Shan Zhong
2024-11-11T09:03:52Z,http://arxiv.org/abs/2411.06805v1,"AssistRAG: Boosting the Potential of Large Language Models with an
  Intelligent Information Assistant","The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as ""hallucination"". Initial retrieval-augmented
generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.",Yujia Zhou
2024-11-04T08:15:22Z,http://arxiv.org/abs/2411.08724v1,"QCG-Rerank: Chunks Graph Rerank with Query Expansion in
  Retrieval-Augmented LLMs for Tourism Domain","Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in
Large Language Models (LLMs) by integrating information retrieval techniques.
However, in the tourism domain, since the query is usually brief and the
content in the database is diverse, existing RAG may contain a significant
amount of irrelevant or contradictory information contents after retrieval. To
address this challenge, we propose the QCG-Rerank model. This model first
performs an initial retrieval to obtain candidate chunks and then enhances
semantics by extracting critical information to expand the original query.
Next, we utilize the expanded query and candidate chunks to calculate
similarity scores as the initial transition probability and construct the
chunks graph. Subsequently, We iteratively compute the transition probabilities
based on an initial estimate until convergence. The chunks with the highest
score are selected and input into the LLMs to generate responses. We evaluate
the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.
The experimental results demonstrate the effectiveness and superiority of the
QCG-Rerank method.",Qikai Wei
2024-10-28T06:41:05Z,http://arxiv.org/abs/2411.08891v1,Calibrated Decision-Making through LLM-Assisted Retrieval,"Recently, large language models (LLMs) have been increasingly used to support
various decision-making tasks, assisting humans in making informed decisions.
However, when LLMs confidently provide incorrect information, it can lead
humans to make suboptimal decisions. To prevent LLMs from generating incorrect
information on topics they are unsure of and to improve the accuracy of
generated content, prior works have proposed Retrieval Augmented Generation
(RAG), where external documents are referenced to generate responses. However,
traditional RAG methods focus only on retrieving documents most relevant to the
input query, without specifically aiming to ensure that the human user's
decisions are well-calibrated. To address this limitation, we propose a novel
retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),
which ensures that decisions informed by the retrieved documents are
well-calibrated. Then we empirically validate that CalibRAG improves
calibration performance as well as accuracy, compared to other baselines across
various datasets.",Chaeyun Jang
2024-11-23T08:24:15Z,http://arxiv.org/abs/2411.15491v1,"Traditional Chinese Medicine Case Analysis System for High-Level
  Semantic Abstraction: Optimized with Prompt and RAG","This paper details a technical plan for building a clinical case database for
Traditional Chinese Medicine (TCM) using web scraping. Leveraging multiple
platforms, including 360doc, we gathered over 5,000 TCM clinical cases,
performed data cleaning, and structured the dataset with crucial fields such as
patient details, pathogenesis, syndromes, and annotations. Using the
$Baidu\_ERNIE\_Speed\_128K$ API, we removed redundant information and generated
the final answers through the $DeepSeekv2$ API, outputting results in standard
JSON format. We optimized data recall with RAG and rerank techniques during
retrieval and developed a hybrid matching scheme. By combining two-stage
retrieval method with keyword matching via Jieba, we significantly enhanced the
accuracy of model outputs.",Peng Xu
2024-11-23T14:47:10Z,http://arxiv.org/abs/2411.15577v1,"From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive
  Grammars","Recent advances in language modeling have demonstrated significant
improvements in zero-shot capabilities, including in-context learning,
instruction following, and machine translation for extremely under-resourced
languages (Tanzer et al., 2024). However, many languages with limited written
resources rely primarily on formal descriptions of grammar and vocabulary.
  In this paper, we introduce a set of benchmarks to evaluate how well models
can extract and classify information from the complex descriptions found in
linguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based
approach that leverages these descriptions for downstream tasks such as machine
translation. Our benchmarks encompass linguistic descriptions for 248 languages
across 142 language families, focusing on typological features from WALS and
Grambank.
  This set of benchmarks offers the first comprehensive evaluation of language
models' in-context ability to accurately interpret and extract linguistic
features, providing a critical resource for scaling NLP to low-resource
languages. The code and data are publicly available at
\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.",Albert Kornilov
2024-11-25T06:48:38Z,http://arxiv.org/abs/2411.16133v1,Context Awareness Gate For Retrieval Augmented Generation,"Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach
to mitigate the limitations of large language models (LLMs) in answering
domain-specific questions. Previous research has predominantly focused on
improving the accuracy and quality of retrieved data chunks to enhance the
overall performance of the generation pipeline. However, despite ongoing
advancements, the critical issue of retrieving irrelevant information -- which
can impair the ability of the model to utilize its internal knowledge
effectively -- has received minimal attention. In this work, we investigate the
impact of retrieving irrelevant information in open-domain question answering,
highlighting its significant detrimental effect on the quality of LLM outputs.
To address this challenge, we propose the Context Awareness Gate (CAG)
architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt
based on whether the user query necessitates external context retrieval.
Additionally, we introduce the Vector Candidates method, a core mathematical
component of CAG that is statistical, LLM-independent, and highly scalable. We
further examine the distributions of relationships between contexts and
questions, presenting a statistical analysis of these distributions. This
analysis can be leveraged to enhance the context retrieval process in Retrieval
Augmented Generation (RAG) systems.",Mohammad Hassan Heydari
2024-11-25T13:53:36Z,http://arxiv.org/abs/2411.16391v2,"Human-Calibrated Automated Testing and Validation of Generative Language
  Models","This paper introduces a comprehensive framework for the evaluation and
validation of generative language models (GLMs), with a focus on
Retrieval-Augmented Generation (RAG) systems deployed in high-stakes domains
such as banking. GLM evaluation is challenging due to open-ended outputs and
subjective quality assessments. Leveraging the structured nature of RAG
systems, where generated responses are grounded in a predefined document
collection, we propose the Human-Calibrated Automated Testing (HCAT) framework.
HCAT integrates a) automated test generation using stratified sampling, b)
embedding-based metrics for explainable assessment of functionality, risk and
safety attributes, and c) a two-stage calibration approach that aligns
machine-generated evaluations with human judgments through probability
calibration and conformal prediction.
  In addition, the framework includes robustness testing to evaluate model
performance against adversarial, out-of-distribution, and varied input
conditions, as well as targeted weakness identification using marginal and
bivariate analysis to pinpoint specific areas for improvement. This
human-calibrated, multi-layered evaluation framework offers a scalable,
transparent, and interpretable approach to GLM assessment, providing a
practical and reliable solution for deploying GLMs in applications where
accuracy, transparency, and regulatory compliance are paramount.",Agus Sudjianto
2024-11-27T10:48:37Z,http://arxiv.org/abs/2411.18216v1,"Evaluating and Improving the Robustness of Security Attack Detectors
  Generated by LLMs","Large Language Models (LLMs) are increasingly used in software development to
generate functions, such as attack detectors, that implement security
requirements. However, LLMs struggle to generate accurate code, resulting,
e.g., in attack detectors that miss well-known attacks when used in practice.
This is most likely due to the LLM lacking knowledge about some existing
attacks and to the generated code being not evaluated in real usage scenarios.
We propose a novel approach integrating Retrieval Augmented Generation (RAG)
and Self-Ranking into the LLM pipeline. RAG enhances the robustness of the
output by incorporating external knowledge sources, while the Self-Ranking
technique, inspired to the concept of Self-Consistency, generates multiple
reasoning paths and creates ranks to select the most robust detector. Our
extensive empirical study targets code generated by LLMs to detect two
prevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL
injection (SQLi). Results show a significant improvement in detection
performance compared to baselines, with an increase of up to 71%pt and 37%pt in
the F2-Score for XSS and SQLi detection, respectively.",Samuele Pasini
2024-12-02T14:55:02Z,http://arxiv.org/abs/2412.01572v3,"MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation
  through Question Complexity","Retrieval Augmented Generation (RAG) has proven to be highly effective in
boosting the generative performance of language model in knowledge-intensive
tasks. However, existing RAG framework either indiscriminately perform
retrieval or rely on rigid single-class classifiers to select retrieval
methods, leading to inefficiencies and suboptimal performance across queries of
varying complexity. To address these challenges, we propose a reinforcement
learning-based framework that dynamically selects the most suitable retrieval
strategy based on query complexity. % our solution Our approach leverages a
multi-armed bandit algorithm, which treats each retrieval method as a distinct
``arm'' and adapts the selection process by balancing exploration and
exploitation. Additionally, we introduce a dynamic reward function that
balances accuracy and efficiency, penalizing methods that require more
retrieval steps, even if they lead to a correct result. Our method achieves new
state of the art results on multiple single-hop and multi-hop datasets while
reducing retrieval costs. Our code are available at
https://github.com/FUTUREEEEEE/MBA .",Xiaqiang Tang
2024-12-03T08:34:42Z,http://arxiv.org/abs/2412.02262v1,"Composing Open-domain Vision with RAG for Ocean Monitoring and
  Conservation","Climate change's destruction of marine biodiversity is threatening
communities and economies around the world which rely on healthy oceans for
their livelihoods. The challenge of applying computer vision to niche,
real-world domains such as ocean conservation lies in the dynamic and diverse
environments where traditional top-down learning struggle with long-tailed
distributions, generalization, and domain transfer. Scalable species
identification for ocean monitoring is particularly difficult due to the need
to adapt models to new environments and identify rare or unseen species. To
overcome these limitations, we propose leveraging bottom-up, open-domain
learning frameworks as a resilient, scalable solution for image and video
analysis in marine applications. Our preliminary demonstration uses pretrained
vision-language models (VLMs) combined with retrieval-augmented generation
(RAG) as grounding, leaving the door open for numerous architectural, training
and engineering optimizations. We validate this approach through a preliminary
application in classifying fish from video onboard fishing vessels,
demonstrating impressive emergent retrieval and prediction capabilities without
domain-specific training or knowledge of the task itself.",Sepand Dyanatkar
2024-12-05T07:23:14Z,http://arxiv.org/abs/2412.03933v1,"Exploring AI Text Generation, Retrieval-Augmented Generation, and
  Detection Technologies: a Comprehensive Overview","The rapid development of Artificial Intelligence (AI) has led to the creation
of powerful text generation models, such as large language models (LLMs), which
are widely used for diverse applications. However, concerns surrounding
AI-generated content, including issues of originality, bias, misinformation,
and accountability, have become increasingly prominent. This paper offers a
comprehensive overview of AI text generators (AITGs), focusing on their
evolution, capabilities, and ethical implications. This paper also introduces
Retrieval-Augmented Generation (RAG), a recent approach that improves the
contextual relevance and accuracy of text generation by integrating dynamic
information retrieval. RAG addresses key limitations of traditional models,
including their reliance on static knowledge and potential inaccuracies in
handling real-world data. Additionally, the paper reviews detection tools that
help differentiate AI-generated text from human-written content and discusses
the ethical challenges these technologies pose. The paper explores future
directions for improving detection accuracy, supporting ethical AI development,
and increasing accessibility. The paper contributes to a more responsible and
reliable use of AI in content creation through these discussions.",Fnu Neha
2024-12-05T23:10:56Z,http://arxiv.org/abs/2412.04661v1,"HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and
  Representation Learning","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external document retrieval to provide domain-specific or
up-to-date knowledge. The effectiveness of RAG depends on the relevance of
retrieved documents, which is influenced by the semantic alignment of
embeddings with the domain's specialized content. Although full fine-tuning can
align language models to specific domains, it is computationally intensive and
demands substantial data. This paper introduces Hierarchical Embedding
Alignment Loss (HEAL), a novel method that leverages hierarchical fuzzy
clustering with matrix factorization within contrastive learning to efficiently
align LLM embeddings with domain-specific content. HEAL computes
level/depth-wise contrastive losses and incorporates hierarchical penalties to
align embeddings with the underlying relationships in label hierarchies. This
approach enhances retrieval relevance and document classification, effectively
reducing hallucinations in LLM outputs. In our experiments, we benchmark and
evaluate HEAL across diverse domains, including Healthcare, Material Science,
Cyber-security, and Applied Maths.",Manish Bhattarai
2024-12-06T22:05:39Z,http://arxiv.org/abs/2412.05447v1,"A Graph-Based Approach for Conversational AI-Driven Personal Memory
  Capture and Retrieval in a Real-world Application","TOBU is a novel mobile application that captures and retrieves `personal
memories' (pictures/videos together with stories and context around those
moments) in a user-engaging AI-guided conversational approach. Our initial
prototype showed that existing retrieval techniques such as retrieval-augmented
generation (RAG) systems fall short due to their limitations in understanding
memory relationships, causing low recall, hallucination, and unsatisfactory
user experience. We design TOBUGraph, a novel graph-based retrieval approach.
During capturing, TOBUGraph leverages large language models (LLMs) to
automatically create a dynamic knowledge graph of memories, establishing
context and relationships of those memories. During retrieval, TOBUGraph
combines LLMs with the memory graph to achieve comprehensive recall through
graph traversal. Our evaluation using real user data demonstrates that
TOBUGraph outperforms multiple RAG implementations in both precision and
recall, significantly improving user experience through improved retrieval
accuracy and reduced hallucination.",Savini Kashmira
2024-12-07T05:49:14Z,http://arxiv.org/abs/2412.05547v1,"KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large
  Language Models","Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.",Weijie Chen
2024-12-13T14:11:26Z,http://arxiv.org/abs/2412.10151v1,"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval
  Augmented Generation","We propose the VLR-Bench, a visual question answering (VQA) benchmark for
evaluating vision language models (VLMs) based on retrieval augmented
generation (RAG). Unlike existing evaluation datasets for external
knowledge-based VQA, the proposed VLR-Bench includes five input passages. This
allows testing of the ability to determine which passage is useful for
answering a given query, a capability lacking in previous research. In this
context, we constructed a dataset of 32,000 automatically generated
instruction-following examples, which we denote as VLR-IF. This dataset is
specifically designed to enhance the RAG capabilities of VLMs by enabling them
to learn how to generate appropriate answers based on input passages. We
evaluated the validity of the proposed benchmark and training data and verified
its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3
model. The proposed VLR-Bench and VLR-IF datasets are publicly available
online.",Hyeonseok Lim
2024-12-14T05:06:43Z,http://arxiv.org/abs/2412.10684v1,Inference Scaling for Bridging Retrieval and Augmented Generation,"Retrieval-augmented generation (RAG) has emerged as a popular approach to
steering the output of a large language model (LLM) by incorporating retrieved
contexts as inputs. However, existing work observed the generator bias, such
that improving the retrieval results may negatively affect the outcome. In this
work, we show such bias can be mitigated, from inference scaling, aggregating
inference calls from the permuted order of retrieved contexts. The proposed
Mixture-of-Intervention (MOI) explicitly models the debiased utility of each
passage with multiple forward passes to construct a new ranking. We also show
that MOI can leverage the retriever's prior knowledge to reduce the
computational cost by minimizing the number of permutations considered and
lowering the cost per LLM call. We showcase the effectiveness of MOI on diverse
RAG tasks, improving ROUGE-L on MS MARCO and EM on HotpotQA benchmarks by ~7
points.",Youngwon Lee
2024-12-17T01:23:45Z,http://arxiv.org/abs/2412.12447v1,"PERC: Plan-As-Query Example Retrieval for Underrepresented Code
  Generation","Code generation with large language models has shown significant promise,
especially when employing retrieval-augmented generation (RAG) with few-shot
examples. However, selecting effective examples that enhance generation quality
remains a challenging task, particularly when the target programming language
(PL) is underrepresented. In this study, we present two key findings: (1)
retrieving examples whose presented algorithmic plans can be referenced for
generating the desired behavior significantly improves generation accuracy, and
(2) converting code into pseudocode effectively captures such algorithmic
plans, enhancing retrieval quality even when the source and the target PLs are
different. Based on these findings, we propose Plan-as-query Example Retrieval
for few-shot prompting in Code generation (PERC), a novel framework that
utilizes algorithmic plans to identify and retrieve effective examples. We
validate the effectiveness of PERC through extensive experiments on the
CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms
the state-of-the-art RAG methods in code generation, both when the source and
target programming languages match or differ, highlighting its adaptability and
robustness in diverse coding environments.",Jaeseok Yoo
2024-12-17T07:49:49Z,http://arxiv.org/abs/2412.12632v1,"What External Knowledge is Preferred by LLMs? Characterizing and
  Exploring Chain of Evidence in Imperfect Context","Incorporating external knowledge into large language models (LLMs) has
emerged as a promising approach to mitigate outdated knowledge and
hallucination in LLMs. However, external knowledge is often imperfect. In
addition to useful knowledge, external knowledge is rich in irrelevant or
misinformation in the context that can impair the reliability of LLM responses.
This paper focuses on LLMs' preferred external knowledge in imperfect contexts
when handling multi-hop QA. Inspired by criminal procedural law's Chain of
Evidence (CoE), we characterize that knowledge preferred by LLMs should
maintain both relevance to the question and mutual support among knowledge
pieces. Accordingly, we propose an automated CoE discrimination approach and
explore LLMs' preferences from their effectiveness, faithfulness and
robustness, as well as CoE's usability in a naive Retrieval-Augmented
Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances
LLMs through more accurate generation, stronger answer faithfulness, better
robustness against knowledge conflict, and improved performance in a popular
RAG case.",Zhiyuan Chang
2024-12-18T08:04:57Z,http://arxiv.org/abs/2412.13582v1,EvoWiki: Evaluating LLMs on Evolving Knowledge,"Knowledge utilization is a critical aspect of LLMs, and understanding how
they adapt to evolving knowledge is essential for their effective deployment.
However, existing benchmarks are predominantly static, failing to capture the
evolving nature of LLMs and knowledge, leading to inaccuracies and
vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an
evolving dataset designed to reflect knowledge evolution by categorizing
information into stable, evolved, and uncharted states. EvoWiki is fully
auto-updatable, enabling precise evaluation of continuously changing knowledge
and newly released LLMs. Through experiments with Retrieval-Augmented
Generation (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs
adapt to evolving knowledge. Our results indicate that current models often
struggle with evolved knowledge, frequently providing outdated or incorrect
responses. Moreover, the dataset highlights a synergistic effect between RAG
and CL, demonstrating their potential to better adapt to evolving knowledge.
EvoWiki provides a robust benchmark for advancing future research on the
knowledge evolution capabilities of large language models.",Wei Tang
2024-12-18T16:07:32Z,http://arxiv.org/abs/2412.13988v1,RAG for Effective Supply Chain Security Questionnaire Automation,"In an era where digital security is crucial, efficient processing of
security-related inquiries through supply chain security questionnaires is
imperative. This paper introduces a novel approach using Natural Language
Processing (NLP) and Retrieval-Augmented Generation (RAG) to automate these
responses. We developed QuestSecure, a system that interprets diverse document
formats and generates precise responses by integrating large language models
(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure
significantly improves response accuracy and operational efficiency. By
employing advanced NLP techniques and tailored retrieval mechanisms, the system
consistently produces contextually relevant and semantically rich responses,
reducing cognitive load on security teams and minimizing potential errors. This
research offers promising avenues for automating complex security management
tasks, enhancing organizational security processes.",Zaynab Batool Reza
2024-12-10T21:52:35Z,http://arxiv.org/abs/2412.14191v1,"Ontology-Aware RAG for Improved Question-Answering in Cybersecurity
  Education","Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.",Chengshuai Zhao
2024-12-19T02:17:35Z,http://arxiv.org/abs/2412.14457v1,VISA: Retrieval Augmented Generation with Visual Source Attribution,"Generation with source attribution is important for enhancing the
verifiability of retrieval-augmented generation (RAG) systems. However,
existing approaches in RAG primarily link generated content to document-level
references, making it challenging for users to locate evidence among multiple
content-rich retrieved documents. To address this challenge, we propose
Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel
approach that combines answer generation with visual source attribution.
Leveraging large vision-language models (VLMs), VISA identifies the evidence
and highlights the exact regions that support the generated answers with
bounding boxes in the retrieved document screenshots. To evaluate its
effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia
webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the
medical domain. Experimental results demonstrate the effectiveness of VISA for
visual source attribution on documents' original look, as well as highlighting
the challenges for improvement. Code, data, and model checkpoints will be
released.",Xueguang Ma
2024-12-19T17:48:23Z,http://arxiv.org/abs/2412.15101v1,"Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering
  with Temporal Adaptability","Retrieve-augmented generation (RAG) frameworks have emerged as a promising
solution to multi-hop question answering(QA) tasks since it enables large
language models (LLMs) to incorporate external knowledge and mitigate their
inherent knowledge deficiencies. Despite this progress, existing RAG
frameworks, which usually follows the retrieve-then-read paradigm, often
struggle with multi-hop QA with temporal information since it has difficulty
retrieving and synthesizing accurate time-related information. To address the
challenge, this paper proposes a novel framework called review-then-refine,
which aims to enhance LLM performance in multi-hop QA scenarios with temporal
information. Our approach begins with a review phase, where decomposed
sub-queries are dynamically rewritten with temporal information, allowing for
subsequent adaptive retrieval and reasoning process. In addition, we implement
adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing
the potential for hallucinations. In the subsequent refine phase, the LLM
synthesizes the retrieved information from each sub-query along with its
internal knowledge to formulate a coherent answer. Extensive experimental
results across multiple datasets demonstrate the effectiveness of our proposed
framework, highlighting its potential to significantly improve multi-hop QA
capabilities in LLMs.",Xiangsen Chen
2023-03-25T01:26:50Z,http://arxiv.org/abs/2303.14322v1,"Spatio-Temporal driven Attention Graph Neural Network with Block
  Adjacency matrix (STAG-NN-BA)","Despite the recent advances in deep neural networks, standard convolutional
kernels limit the applications of these networks to the Euclidean domain only.
Considering the geodesic nature of the measurement of the earth's surface,
remote sensing is one such area that can benefit from non-Euclidean and
spherical domains. For this purpose, we propose a novel Graph Neural Network
architecture for spatial and spatio-temporal classification using satellite
imagery. We propose a hybrid attention method to learn the relative importance
of irregular neighbors in remote sensing data. Instead of classifying each
pixel, we propose a method based on Simple Linear Iterative Clustering (SLIC)
image segmentation and Graph Attention GAT. The superpixels obtained from SLIC
become the nodes of our Graph Convolution Network (GCN). We then construct a
region adjacency graph (RAG) where each superpixel is connected to every other
adjacent superpixel in the image, enabling information to propagate globally.
Finally, we propose a Spatially driven Attention Graph Neural Network (SAG-NN)
to classify each RAG. We also propose an extension to our SAG-NN for
spatio-temporal data. Unlike regular grids of pixels in images, superpixels are
irregular in nature and cannot be used to create spatio-temporal graphs. We
introduce temporal bias by combining unconnected RAGs from each image into one
supergraph. This is achieved by introducing block adjacency matrices resulting
in novel Spatio-Temporal driven Attention Graph Neural Network with Block
Adjacency matrix (STAG-NN-BA). We evaluate our proposed methods on two remote
sensing datasets namely Asia14 and C2D2. In comparison with both non-graph and
graph-based approaches our SAG-NN and STAG-NN-BA achieved superior accuracy on
all the datasets while incurring less computation cost. The code and dataset
will be made public via our GitHub repository.",U. Nazir
2024-05-03T16:38:51Z,http://arxiv.org/abs/2405.02228v2,"REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific
  Sentences using Public and Proprietary LLMs","Automatic citation generation for sentences in a document or report is
paramount for intelligence analysts, cybersecurity, news agencies, and
education personnel. In this research, we investigate whether large language
models (LLMs) are capable of generating references based on two forms of
sentence queries: (a) Direct Queries, LLMs are asked to provide author names of
the given research article, and (b) Indirect Queries, LLMs are asked to provide
the title of a mentioned article when given a sentence from a different
article. To demonstrate where LLM stands in this task, we introduce a large
dataset called REASONS comprising abstracts of the 12 most popular domains of
scientific research on arXiv. From around 20K research articles, we make the
following deductions on public and proprietary LLMs: (a) State-of-the-art,
often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass
percentage (PP) to minimize the hallucination rate (HR). When tested with
Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant
metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented
generation (RAG) using Mistral demonstrates consistent and robust citation
support on indirect queries and matched performance to GPT-3.5 and GPT-4. The
HR across all domains and models decreased by an average of 41.93%, and the PP
was reduced to 0% in most cases. In terms of generation quality, the average F1
Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with
adversarial samples showed that LLMs, including the Advance RAG Mistral,
struggle to understand context, but the extent of this issue was small in
Mistral and GPT-4-Preview. Our study contributes valuable insights into the
reliability of RAG for automated citation generation tasks.",Deepa Tilwani
2024-07-15T15:20:40Z,http://arxiv.org/abs/2407.10805v6,"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has improved large language models
(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.
However, current RAG methods often fall short of ensuring the depth and
completeness of retrieved information, which is necessary for complex reasoning
tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG
framework that iteratively retrieves information from both unstructured and
structured knowledge sources in a tight-coupling manner. Specifically, ToG-2
leverages knowledge graphs (KGs) to link documents via entities, facilitating
deep and knowledge-guided context retrieval. Simultaneously, it utilizes
documents as entity contexts to achieve precise and efficient graph retrieval.
ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate answers. We
conduct a series of well-designed experiments to highlight the following
advantages of ToG-2: 1) ToG-2 tightly couples the processes of context
retrieval and graph retrieval, deepening context retrieval via the KG while
enabling reliable graph retrieval based on contexts; 2) it achieves deep and
faithful reasoning in LLMs through an iterative knowledge retrieval process of
collaboration between contexts and the KG; and 3) ToG-2 is training-free and
plug-and-play compatible with various LLMs. Extensive experiments demonstrate
that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7
knowledge-intensive datasets with GPT-3.5, and can elevate the performance of
smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
The source code is available on https://github.com/IDEA-FinAI/ToG-2.",Shengjie Ma
2024-09-15T15:21:45Z,http://arxiv.org/abs/2409.10576v2,"Language Models and Retrieval Augmented Generation for Automated
  Structured Data Extraction from Diagnostic Reports","Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.",Mohamed Sobhi Jabal
2024-09-10T02:00:28Z,http://arxiv.org/abs/2409.13731v3,"KAG: Boosting LLMs in Professional Domains via Knowledge Augmented
  Generation","The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.",Lei Liang
2024-09-23T11:20:20Z,http://arxiv.org/abs/2409.14924v1,"Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey
  on How to Make your LLMs use External Data More Wisely","Large language models (LLMs) augmented with external data have demonstrated
remarkable capabilities in completing real-world tasks. Techniques for
integrating external data into LLMs, such as Retrieval-Augmented Generation
(RAG) and fine-tuning, are gaining increasing attention and widespread
application. Nonetheless, the effective deployment of data-augmented LLMs
across various specialized fields presents substantial challenges. These
challenges encompass a wide range of issues, from retrieving relevant data and
accurately interpreting user intent to fully harnessing the reasoning
capabilities of LLMs for complex tasks. We believe that there is no
one-size-fits-all solution for data-augmented LLM applications. In practice,
underperformance often arises from a failure to correctly identify the core
focus of a task or because the task inherently requires a blend of multiple
capabilities that must be disentangled for better resolution. In this survey,
we propose a RAG task categorization method, classifying user queries into four
levels based on the type of external data required and primary focus of the
task: explicit fact queries, implicit fact queries, interpretable rationale
queries, and hidden rationale queries. We define these levels of queries,
provide relevant datasets, and summarize the key challenges and most effective
techniques for addressing these challenges. Finally, we discuss three main
forms of integrating external data into LLMs: context, small model, and
fine-tuning, highlighting their respective strengths, limitations, and the
types of problems they are suited to solve. This work aims to help readers
thoroughly understand and decompose the data requirements and key bottlenecks
in building LLM applications, offering solutions to the different challenges
and serving as a guide to systematically developing such applications.",Siyun Zhao
1996-08-29T09:19:23Z,http://arxiv.org/abs/cond-mat/9608150v1,Local Percolation Probabilities for a Natural Sandstone,"Local percolation probabilities are used to characterize the connectivity in
porous and heterogeneous media. Together with local porosity distributions they
allow to predict transport properties \cite{hil91d}. While local porosity
distributions are readily obtained, measurements of the local percolation
probabilities are more difficult and have not been attempted previously. First
measurements of three dimensional local porosity distributions and percolation
probabilities from a pore space reconstruction for a natural sandstone show
that theoretical expectations and experimental results are consistent.",R. Hilfer
2012-05-31T09:00:32Z,http://arxiv.org/abs/1205.6923v1,"Variation of Longitudinal Plasma Wavelength under Irradiation and Double
  Resonance in Coupled Josephson Junctions","The effect of electromagnetic wave irradiation on the phase dynamics of
intrinsic Josephson junctions in high temperature superconductors is
investigated. We predict three novel effects by variation of the radiation
amplitude and frequency: changing of the longitudinal plasma wavelength at
parametric resonance; double resonance of the Josephson oscillations with
radiation and longitudinal plasma wave; charging of superconducting layers in
the current interval corresponding to the Shapiro step. The ""bump"" structure in
IVC recently observed experimentally is demonstrated. We also observe ragged
Shapiro steps at double resonance.",Yu. M. Shukrinov
2012-06-27T19:59:59Z,http://arxiv.org/abs/1206.6410v1,On the Partition Function and Random Maximum A-Posteriori Perturbations,"In this paper we relate the partition function to the max-statistics of
random variables. In particular, we provide a novel framework for approximating
and bounding the partition function using MAP inference on randomly perturbed
models. As a result, we can use efficient MAP solvers such as graph-cuts to
evaluate the corresponding partition function. We show that our method excels
in the typical ""high signal - high coupling"" regime that results in ragged
energy landscapes difficult for alternative approaches.",Tamir Hazan
2012-08-02T14:12:43Z,http://arxiv.org/abs/1208.0490v2,A Universal Behavior of Half BPS Probes in the Superstar Ensemble,"In this paper we probe the typical states of the superstar ensemble of
(hep-th/0508023) using half-BPS states of type-IIB string theory on AdS$_5
\times$ S$^5$. We find a very simple universal result that has the structure
$\log\, \lag\lag \y \; \y \rag\rag_\calo \approx \a\, h \, \log N$, where $h$
is the conformal weight of the probe $\y$ and $\a$ is a constant that depends
mainly of the shape of the probe $\y$. A complete understanding of some
properties of this leading term from the dual effective superstar geometry
point of view is still lacking.",Ilies Messamah
2013-09-29T13:48:52Z,http://arxiv.org/abs/1309.7598v1,"On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori
  Perturbations","In this paper we describe how MAP inference can be used to sample efficiently
from Gibbs distributions. Specifically, we provide means for drawing either
approximate or unbiased samples from Gibbs' distributions by introducing low
dimensional perturbations and solving the corresponding MAP assignments. Our
approach also leads to new ways to derive lower bounds on partition functions.
We demonstrate empirically that our method excels in the typical ""high signal -
high coupling"" regime. The setting results in ragged energy landscapes that are
challenging for alternative approaches to sampling and/or lower bounds.",Tamir Hazan
2017-07-07T20:11:54Z,http://arxiv.org/abs/1707.02351v1,"Two-level atom excitation probability for single- and $N$-photon
  wavepackets","We study how the transient excitation probability of a two-level atom by a
quantized field depends on the temporal profile of the incident pulse, in the
presence of external losses, for both coherent and Fock states, and in two
complementary limits: when the pulse contains only one photon (on average), and
when the number of photons $N$ is large. For the latter case we derive
analytical expressions for the scaling of the excitation probability with $N$
that can be easily evaluated for any pulse shape.",Hemlin Swaran Rag
2020-02-14T18:58:55Z,http://arxiv.org/abs/2002.06187v1,"Reusing Static Analysis across Different Domain-Specific Languages using
  Reference Attribute Grammars","Context: Domain-specific languages (DSLs) enable domain experts to specify
tasks and problems themselves, while enabling static analysis to elucidate
issues in the modelled domain early. Although language workbenches have
simplified the design of DSLs and extensions to general purpose languages,
static analyses must still be implemented manually.
  Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency
analysis, and declaration-use analysis, are usually domain-dependent and cannot
be easily reused. Therefore, transferring existing static analyses to another
DSL incurs a huge implementation overhead. However, this overhead is not always
intrinsically necessary: in many cases, while the concepts of the DSL on which
a static analysis is performed are domain-specific, the underlying algorithm
employed in the analysis is actually domain-independent and thus can be reused
in principle, depending on how it is specified. While current approaches either
implement static analyses internally or with an external Visitor, the
implementation is tied to the language's grammar and cannot be reused easily.
Thus far, a commonly used approach that achieves reusable static analysis
relies on the transformation into an intermediate representation upon which the
analysis is performed. This, however, entails a considerable additional
implementation effort.
  Approach: To remedy this, it has been proposed to map the necessary
domain-specific concepts to the algorithm's domain-independent data structures,
yet without a practical implementation and the demonstration of reuse. Thus, to
make static analysis reusable again, we employ relational Reference Attribute
Grammars (RAGs) by creating such a mapping to a domain-independent overlay
structure using higher-order attributes.
  Knowledge: We describe how static analysis can be specified on
analysis-specific data structures, how relational RAGs can help with the
specification, and how a mapping from the domain-specific language can be
performed. Furthermore, we demonstrate how a static analysis for a DSL can be
externalized and reused in another general purpose language.
  Grounding: The approach was evaluated using the RAG system JastAdd. To
illustrate reusability, we implemented two analyses with two addressed
languages each: a cycle detection analysis used in a small state machine DSL
and for detecting circular dependencies in Java types and packages, and an
analysis of variable shadowing, applied to both Java and the Modelica modelling
language. Thereby, we demonstrate the reuse of two analysis algorithms in three
completely different domains. Additionally, we use the cycle detection analysis
to evaluate the efficiency by comparing our external analysis to an internal
reference implementation analysing all Java programs in the Qualitas Corpus and
thereby are able to show that an externalized analysis incurs only minimal
overhead.
  Importance: We make static analysis reusable, again, showing the practicality
and efficiency of externalizing static analysis for both DSLs and general
purpose languages using relational RAGs.",Johannes Mey
2022-01-06T17:28:18Z,http://arxiv.org/abs/2201.02148v1,"Ecce Signum: An R Package for Multivariate Signal Extraction and Time
  Series Analysis","The package provides multivariate time series models for structural analysis,
allowing one to extract latent signals such as trends or seasonality. Models
are fitted using maximum likelihood estimation, allowing for non-stationarity,
fixed regression effects, and ragged-edge missing values. Simple types of
extreme values can be corrected using the device of entropy maximization. Model
adequacy is assessed through residual diagnostics, and model-based signal
extraction filters can be assessed in time domain and frequency domain.
Extracted signals are produced with uncertainty measures that account for
sample edge effects and missing values, and the signals (as well as the
original time series) can be forecasted.",Tucker S. McElroy
2022-08-18T18:57:23Z,http://arxiv.org/abs/2208.09025v1,Juggler's friezes,"This note generalizes $\mathrm{SL}(k)$-friezes to configurations of numbers
in which one of the boundary rows has been replaced by a ragged edge (described
by a juggling function). We provide several equivalent
definitions/characterizations of these juggler's friezes, in terms of
determinants, linear recurrences, and a dual juggler's frieze. We generalize
classic results, such as periodicity, duality, and a parametrization by part of
a Grassmannian. We also provide a method of constructing such friezes from
certain $k \times n$ matrices using the twist of a matrix.",Roi Docampo
2024-03-03T21:24:35Z,http://arxiv.org/abs/2403.01616v2,"Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models","This paper presents our contributions towards advancing the state of
Vietnamese language understanding and generation through the development and
dissemination of open datasets and pre-trained models for Vietnamese
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).",Nguyen Quang Duc
2024-06-12T18:38:40Z,http://arxiv.org/abs/2406.08582v1,"Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An
  Experimental Study and Quality Assessment Methods","There are various methods for adapting LLMs to different domains. The most
common methods are prompting, finetuning, and RAG. In this work, we explore the
possibility of adapting a model using one of the PEFT methods - QLoRA. The
experiment aims to simulate human responses based on their interviews. The
simulation quality is assessed by comparing the quality of the style and the
quality of the generated facts.",Eugene Vyborov
2024-06-18T01:53:49Z,http://arxiv.org/abs/2406.12197v1,"Debate as Optimization: Adaptive Conformal Prediction and Diverse
  Retrieval for Event Extraction","We propose a multi-agent debate as optimization (DAO) system for event
extraction, where the primary objective is to iteratively refine the large
language models (LLMs) outputs through debating without parameter tuning. In
DAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the
Adaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves
supporting information that best fits the debate discussion, while AdaCP
enhances the accuracy and reliability of event extraction by effectively
rejecting less promising answers. Experimental results demonstrate a
significant reduction in the performance gap between supervised approaches and
tuning-free LLM-based methods by 18.1% and 17.8% on ACE05 and 17.9% and 15.2%
on CASIE for event detection and argument extraction respectively.",Sijia Wang
2024-06-18T09:24:09Z,http://arxiv.org/abs/2406.12429v3,"Query Routing for Homogeneous Tools: An Instantiation in the RAG
  Scenario","Current research on tool learning primarily focuses on selecting the most
effective tool from a wide array of options, often overlooking
cost-effectiveness, a crucial factor in human problem-solving. In this paper,
we address the selection of homogeneous tools by predicting both their
performance and the associated cost required to accomplish a given task. We
then assign queries to the optimal tools in a cost-effective manner. Our
experimental results demonstrate that our method achieves higher performance at
a lower cost compared to strong baseline approaches.",Feiteng Mu
2024-06-21T07:52:01Z,http://arxiv.org/abs/2406.14938v1,Towards Retrieval Augmented Generation over Large Video Libraries,"Video content creators need efficient tools to repurpose content, a task that
often requires complex manual or automated searches. Crafting a new video from
large video libraries remains a challenge. In this paper we introduce the task
of Video Library Question Answering (VLQA) through an interoperable
architecture that applies Retrieval Augmented Generation (RAG) to video
libraries. We propose a system that uses large language models (LLMs) to
generate search queries, retrieving relevant video moments indexed by speech
and visual metadata. An answer generation module then integrates user queries
with this metadata to produce responses with specific video timestamps. This
approach shows promise in multimedia content retrieval, and AI-assisted video
content creation.",Yannis Tevissen
2024-07-12T16:18:00Z,http://arxiv.org/abs/2407.09394v1,"PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with
  User-Centric Agents","Large Language Models (LLMs) struggle with generating reliable outputs due to
outdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG)
models address this by enhancing LLMs with external knowledge, but often fail
to personalize the retrieval process. This paper introduces PersonaRAG, a novel
framework incorporating user-centric agents to adapt retrieval and generation
based on real-time user data and interactions. Evaluated across various
question answering datasets, PersonaRAG demonstrates superiority over baseline
models, providing tailored answers to user needs. The results suggest promising
directions for user-adapted information retrieval systems.",Saber Zerhoudi
2024-10-02T23:14:29Z,http://arxiv.org/abs/2410.03771v1,"SeeSay: An Assistive Device for the Visually Impaired Using Retrieval
  Augmented Generation","In this paper, we present SeeSay, an assistive device designed for
individuals with visual impairments. This system leverages large language
models (LLMs) for speech recognition and visual querying. It effectively
identifies, records, and responds to the user's environment by providing audio
guidance using retrieval-augmented generation (RAG). Our experiments
demonstrate the system's capability to recognize its surroundings and respond
to queries with audio feedback in diverse settings. We hope that the SeeSay
system will facilitate users' comprehension and recollection of their
surroundings, thereby enhancing their environmental perception, improving
navigational capabilities, and boosting overall independence.",Melody Yu
2024-10-08T14:09:12Z,http://arxiv.org/abs/2410.06062v3,"LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge Graphs","We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.",Vincent Emonet
2024-11-11T18:58:46Z,http://arxiv.org/abs/2411.07238v1,OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model,"OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.",Sumeth Yuenyong
2024-12-16T21:09:28Z,http://arxiv.org/abs/2412.12358v1,"BioRAGent: A Retrieval-Augmented Generation System for Showcasing
  Generative Query Expansion and Domain-Specific Search for Scientific Q&A","We present BioRAGent, an interactive web-based retrieval-augmented generation
(RAG) system for biomedical question answering. The system uses large language
models (LLMs) for query expansion, snippet extraction, and answer generation
while maintaining transparency through citation links to the source documents
and displaying generated queries for further editing. Building on our
successful participation in the BioASQ 2024 challenge, we demonstrate how
few-shot learning with LLMs can be effectively applied for a professional
search setting. The system supports both direct short paragraph style responses
and responses with inline citations. Our demo is available online, and the
source code is publicly accessible through GitHub.",Samy Ateia
2015-11-06T18:26:30Z,http://arxiv.org/abs/1511.02182v1,Optimizing Damper Connectors for Adjacent Buildings,"Many theoretical and experimental studies have used heuristic methods to
investigate the dynamic behaviour of the passive coupling of adjacent
structures. However, few papers have used optimization techniques with
guaranteed convergence in order to increase the efficiency of the passive
coupling of adjacent structures. In this paper, the combined problem of optimal
arrangement and mechanical properties of dampers placed between two adjacent
buildings is considered. A new bi-level optimization approach is presented. The
outer-loop of the approach optimizes damper configuration and is solved using
the ``inserting dampers'' method, which was recently shown to be a very
effective heuristic method. Under the assumption that the dampers have varying
damper coefficients, the inner-loop finds the optimal damper coefficients by
solving an $n$-dimensional optimization problem, where derivative information
of the objective function is not available. Three different non-gradient
methods are compared for solving the inner loop: a genetic algorithm (GA), the
mesh adaptive direct search (MADS) algorithm, and the robust approximate
gradient sampling (RAGS) algorithm. It is shown that by exploiting this new
bi-level problem formulation, modern derivative free optimization techniques
with guaranteed convergence (such as MADS and RAGS) can be used. The results
indicate a great increase in the efficiency of the retrofitting system, as well
as the existence of a threshold on the number of dampers inserted with respect
to the efficiency of the retrofitting system.",K. Bigdeli
2019-05-15T17:42:09Z,http://arxiv.org/abs/1905.06313v1,"Laminar Flow and Backrind Formation in Molding of Viscoelastic Silicone
  Rubber","When a thermoset polymer is cured at elevated temperature in a closed mold,
thermal expansion can produce flaws in the finished product. Those flaws occur
when rising internal pressure pushes the mold open and cured polymer flows out
through gaps at the parting lines. Known as backrind, such defects are
particularly common in compression molding, where the increasing pressure of a
trapped, incompressible polymer can overwhelm the clamping pressure on the mold
and expel polymer from the mold pocket. If that ejected material has already
cured, it leaves behind structural damage and consequently a flaw in the
finished product.
  Backrind usually appears as a ragged seam line near the gap where cured
polymer exited the mold. Its appearance is typically irregular and fragmented,
suggesting no particular pattern or uniformity to the process that produced it.
In such cases, the cured polymer acts predominantly as a viscoelastic solid as
it is driven toward and through the parting line. The backrind's ragged
character results from tearing and fragmentation of that solid.
  It is possible, however, for the cured polymer to act predominantly as a
viscoelastic liquid as it flows toward and through the parting line. Since the
Reynolds number is low, the flow is laminar and the backrind bears witness to
that laminar flow. More specifically, the backrind's observed shaped
corresponds to isochronous contours in the laminar flow toward the parting
line, contours that can be predicted using computational fluid dynamics.",Louis A. Bloomfield
2020-12-02T03:14:57Z,http://arxiv.org/abs/2012.00945v2,Two-Stage Single Image Reflection Removal with Reflection-Aware Guidance,"Removing undesired reflection from an image captured through a glass surface
is a very challenging problem with many practical application scenarios. For
improving reflection removal, cascaded deep models have been usually adopted to
estimate the transmission in a progressive manner. However, most existing
methods are still limited in exploiting the result in prior stage for guiding
transmission estimation. In this paper, we present a novel two-stage network
with reflection-aware guidance (RAGNet) for single image reflection removal
(SIRR). To be specific, the reflection layer is firstly estimated due to that
it generally is much simpler and is relatively easier to estimate.
Reflectionaware guidance (RAG) module is then elaborated for better exploiting
the estimated reflection in predicting transmission layer. By incorporating
feature maps from the estimated reflection and observation, RAG can be used (i)
to mitigate the effect of reflection from the observation, and (ii) to generate
mask in partial convolution for mitigating the effect of deviating from linear
combination hypothesis. A dedicated mask loss is further presented for
reconciling the contributions of encoder and decoder features. Experiments on
five commonly used datasets demonstrate the quantitative and qualitative
superiority of our RAGNet in comparison to the state-of-the-art SIRR methods.
The source code and pre-trained model are available at
https://github.com/liyucs/RAGNet.",Yu Li
2022-02-19T01:00:25Z,http://arxiv.org/abs/2202.09485v1,Bayesian inference for link travel time correlation of a bus route,"Estimation of link travel time correlation of a bus route is essential to
many bus operation applications, such as timetable scheduling, travel time
forecasting and transit service assessment/improvement. Most previous studies
rely on either independent assumptions or simplified local spatial correlation
structures. In the real world, however, link travel time on a bus route could
exhibit complex correlation structures, such as long-range correlations,
negative correlations, and time-varying correlations. Therefore, before
introducing strong assumptions, it is essential to empirically quantify and
examine the correlation structure of link travel time from real-world bus
operation data. To this end, this paper develops a Bayesian Gaussian model to
estimate the link travel time correlation matrix of a bus route using
smart-card-like data. Our method overcomes the small-sample-size problem in
correlation matrix estimation by borrowing/integrating those incomplete
observations (i.e., with missing/ragged values and overlapped link segments)
from other bus routes. Next, we propose an efficient Gibbs sampling framework
to marginalize over the missing and ragged values and obtain the posterior
distribution of the correlation matrix. Three numerical experiments are
conducted to evaluate model performance. We first conduct a synthetic
experiment and our results show that the proposed method produces an accurate
estimation for travel time correlations with credible intervals. Next, we
perform experiments on a real-world bus route with smart card data; our results
show that both local and long-range correlations exist on this bus route.
Finally, we demonstrate an application of using the estimated covariance matrix
to make probabilistic forecasting of link and trip travel time.",Xiaoxu Chen
2023-05-31T12:27:58Z,http://arxiv.org/abs/2305.19787v2,DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation,"Image segmentation aims to partition an image according to the objects in the
scene and is a fundamental step in analysing very high spatial-resolution (VHR)
remote sensing imagery. Current methods struggle to effectively consider land
objects with diverse shapes and sizes. Additionally, the determination of
segmentation scale parameters frequently adheres to a static and empirical
doctrine, posing limitations on the segmentation of large-scale remote sensing
images and yielding algorithms with limited interpretability. To address the
above challenges, we propose a deep-learning-based region merging method dubbed
DeepMerge to handle the segmentation of complete objects in large VHR images by
integrating deep learning and region adjacency graph (RAG). This is the first
method to use deep learning to learn the similarity and merge similar adjacent
super-pixels in RAG. We propose a modified binary tree sampling method to
generate shift-scale data, serving as inputs for transformer-based deep
learning networks, a shift-scale attention with 3-Dimension relative position
embedding to learn features across scales, and an embedding to fuse learned
features with hand-crafted features. DeepMerge can achieve high segmentation
accuracy in a supervised manner from large-scale remotely sensed images and
provides an interpretable optimal scale parameter, which is validated using a
remote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The
experimental results show that DeepMerge achieves the highest F value (0.9550)
and the lowest total error TE (0.0895), correctly segmenting objects of
different sizes and outperforming all competing segmentation methods.",Xianwei Lv
2023-09-29T17:26:03Z,http://arxiv.org/abs/2309.17415v3,"Intuitive or Dependent? Investigating LLMs' Behavior Style to
  Conflicting Prompts","This study investigates the behaviors of Large Language Models (LLMs) when
faced with conflicting prompts versus their internal memory. This will not only
help to understand LLMs' decision mechanism but also benefit real-world
applications, such as retrieval-augmented generation (RAG). Drawing on
cognitive theory, we target the first scenario of decision-making styles where
there is no superiority in the conflict and categorize LLMs' preference into
dependent, intuitive, and rational/irrational styles. Another scenario of
factual robustness considers the correctness of prompt and memory in
knowledge-intensive tasks, which can also distinguish if LLMs behave rationally
or irrationally in the first scenario. To quantify them, we establish a
complete benchmarking framework including a dataset, a robustness evaluation
pipeline, and corresponding metrics. Extensive experiments with seven LLMs
reveal their varying behaviors. And, with role play intervention, we can change
the styles, but different models present distinct adaptivity and upper-bound.
One of our key takeaways is to optimize models or the prompts according to the
identified style. For instance, RAG models with high role play adaptability may
dynamically adjust the interventions according to the quality of retrieval
results -- being dependent to better leverage informative context; and, being
intuitive when external prompt is noisy.",Jiahao Ying
2023-10-10T00:39:04Z,http://arxiv.org/abs/2310.06225v2,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",Bruno Silva
2023-10-13T13:17:03Z,http://arxiv.org/abs/2310.09089v2,"Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large
  Language Model","Integrating large language models (LLMs) into healthcare holds great
potential but faces challenges. Pre-training LLMs from scratch for domains like
medicine is resource-heavy and often unfeasible. On the other hand, sole
reliance on Supervised Fine-tuning (SFT) can result in overconfident
predictions and may not tap into domain-specific insights. In response, we
present a multi-stage training method combining Domain-specific Continued
Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In
addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing
medical question answering, plain texts, knowledge graphs, and dialogues,
segmented into three training stages. The medical LLM trained with our
pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and
SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,
respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by
7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the
Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in
BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the
model's performance through the Retrieval Augmented Generation (RAG) approach.
Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%
on CMExam. These results highlight the contribution of our novel training
approach in building LLMs for medical applications.",Qichen Ye
2023-10-20T22:47:18Z,http://arxiv.org/abs/2310.13848v2,"FABULA: Intelligence Report Generation Using Retrieval-Augmented
  Narrative Construction","Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.",Priyanka Ranade
2023-11-05T21:43:02Z,http://arxiv.org/abs/2311.02775v3,"AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using
  Open-Source LLMs","Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of AI-TA, an
intelligent QA assistant customizable for courses with an online QA platform",Yann Hicke
2023-11-16T01:21:33Z,http://arxiv.org/abs/2311.10776v5,"Chemist-X: Large Language Model-empowered Agent for Reaction Condition
  Recommendation in Chemical Synthesis","Recent AI research plots a promising future of automatic chemical reactions
within the chemistry society. This study proposes Chemist-X, a transformative
AI agent that automates the reaction condition recommendation (RCR) task in
chemical synthesis with retrieval-augmented generation (RAG) technology. To
emulate expert chemists' strategies when solving RCR tasks, Chemist-X utilizes
advanced RAG schemes to interrogate online molecular databases and distill
critical data from the latest literature database. Further, the agent leverages
state-of-the-art computer-aided design (CAD) tools with a large language model
(LLM) supervised programming interface. With the ability to utilize updated
chemical knowledge and CAD tools, our agent significantly outperforms
conventional synthesis AIs confined to the fixed knowledge within its training
data. Chemist-X considerably reduces chemists' workload and allows them to
focus on more fundamental and creative problems, thereby bringing closer
computational techniques and chemical research and making a remarkable leap
toward harnessing AI's full capabilities in scientific discovery.",Kexin Chen
2023-11-30T09:48:51Z,http://arxiv.org/abs/2311.18397v1,"IAG: Induction-Augmented Generation Framework for Answering Reasoning
  Questions","Retrieval-Augmented Generation (RAG), by incorporating external knowledge
with parametric memory of language models, has become the state-of-the-art
architecture for open-domain QA tasks. However, common knowledge bases are
inherently constrained by limited coverage and noisy information, making
retrieval-based approaches inadequate to answer implicit reasoning questions.
In this paper, we propose an Induction-Augmented Generation (IAG) framework
that utilizes inductive knowledge along with the retrieved documents for
implicit reasoning. We leverage large language models (LLMs) for deriving such
knowledge via a novel prompting method based on inductive reasoning patterns.
On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,
respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for
answer prediction, while IAG-Student gets rid of dependencies on GPT service at
inference time by incorporating a student inductor model. The inductor is
firstly trained via knowledge distillation and further optimized by
back-propagating the generator feedback via differentiable beam scores.
Experimental results show that IAG outperforms RAG baselines as well as ChatGPT
on two Open-Domain QA tasks. Notably, our best models have won the first place
in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA
(since Jan 8, 2023).",Zhebin Zhang
2023-12-18T17:18:04Z,http://arxiv.org/abs/2312.11361v3,"""Knowing When You Don't Know"": A Multilingual Relevance Assessment
  Dataset for Robust Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.",Nandan Thakur
2024-01-15T05:38:37Z,http://arxiv.org/abs/2401.07483v1,"Graph database while computationally efficient filters out quickly the
  ESG integrated equities in investment management","Design/methodology/approach This research evaluated the databases of SQL,
No-SQL and graph databases to compare and contrast efficiency and performance.
To perform this experiment the data were collected from multiple sources
including stock price and financial news. Python is used as an interface to
connect and query databases (to create database structures according to the
feed file structure, to load data into tables, objects, to read data , to
connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM
(Large language model) including RAG (Retrieval Augmented Generation) with
Machine Learning, deep learning, NLP (natural language processing) or Decision
Analytics are computationally expensive. Finding a better option to consume
less resources and time to get the result. Findings The Graph database of ESG
(Environmental, Social and Governance) is comparatively better and can be
considered for extended analytics to integrate ESG in business and investment.
Practical implications A graph ML with a RAG architecture model can be
introduced as a new framework with less computationally expensive LLM
application in the equity filtering process for portfolio management.
Originality/value Filtering out selective stocks out of two thousand or more
listed companies in any stock exchange for active investment, consuming less
resource consumption especially memory and energy to integrate artificial
intelligence and ESG in business and investment.",Partha Sen
2024-02-10T18:27:28Z,http://arxiv.org/abs/2402.07016v1,"REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models","The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.",Yinghao Zhu
2024-02-12T13:13:04Z,http://arxiv.org/abs/2402.07630v3,"G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering","Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}",Xiaoxin He
2024-02-16T16:57:18Z,http://arxiv.org/abs/2402.10828v2,"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented
  In-Context Learning in Multi-Modal Large Language Model","We need to trust robots that use often opaque AI methods. They need to
explain themselves to us, and we need to trust their explanation. In this
regard, explainability plays a critical role in trustworthy autonomous
decision-making to foster transparency and acceptance among end users,
especially in complex autonomous driving. Recent advancements in Multi-Modal
Large Language models (MLLMs) have shown promising potential in enhancing the
explainability as a driving agent by producing control predictions along with
natural language explanations. However, severe data scarcity due to expensive
annotation costs and significant domain gaps between different datasets makes
the development of a robust and generalisable system an extremely challenging
task. Moreover, the prohibitively expensive training requirements of MLLM and
the unsolved problem of catastrophic forgetting further limit their
generalisability post-deployment. To address these challenges, we present
RAG-Driver, a novel retrieval-augmented multi-modal large language model that
leverages in-context learning for high-performance, explainable, and
generalisable autonomous driving. By grounding in retrieved expert
demonstration, we empirically validate that RAG-Driver achieves
state-of-the-art performance in producing driving action explanations,
justifications, and control signal prediction. More importantly, it exhibits
exceptional zero-shot generalisation capabilities to unseen environments
without further training endeavours.",Jianhao Yuan
2024-02-04T20:42:30Z,http://arxiv.org/abs/2402.14594v1,"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.",Zifei FeiFei Han
2024-02-26T23:37:59Z,http://arxiv.org/abs/2402.17081v1,"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI
  Judge","This study presents an innovative enhancement to retrieval-augmented
generation (RAG) systems by seamlessly integrating fine-tuned large language
models (LLMs) with vector databases. This integration capitalizes on the
combined strengths of structured data retrieval and the nuanced comprehension
provided by advanced LLMs. Central to our approach are the LoRA and QLoRA
methodologies, which stand at the forefront of model refinement through
parameter-efficient fine-tuning and memory optimization. A novel feature of our
research is the incorporation of user feedback directly into the training
process, ensuring the model's continuous adaptation to user expectations and
thus, improving its performance and applicability. Additionally, we introduce a
Quantized Influence Measure (QIM) as an innovative ""AI Judge"" mechanism to
enhance the precision of result selection, further refining the system's
accuracy. Accompanied by an executive diagram and a detailed algorithm for
fine-tuning QLoRA, our work provides a comprehensive framework for implementing
these advancements within chatbot technologies. This research contributes
significant insights into LLM optimization for specific uses and heralds new
directions for further development in retrieval-augmented models. Through
extensive experimentation and analysis, our findings lay a robust foundation
for future advancements in chatbot technology and retrieval systems, marking a
significant step forward in the creation of more sophisticated, precise, and
user-centric conversational AI systems.",Keshav Rangan
2024-02-27T18:42:31Z,http://arxiv.org/abs/2402.17753v1,Evaluating Very Long-Term Conversational Memory of LLM Agents,"Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.",Adyasha Maharana
2024-03-15T07:45:37Z,http://arxiv.org/abs/2403.10081v3,"DRAGIN: Dynamic Retrieval Augmented Generation based on the Information
  Needs of Large Language Models","Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",Weihang Su
2024-03-28T08:27:44Z,http://arxiv.org/abs/2403.19216v2,Are Large Language Models Good at Utility Judgments?,"Retrieval-augmented generation (RAG) is considered to be a promising approach
to alleviate the hallucination issue of large language models (LLMs), and it
has received widespread attention from researchers recently. Due to the
limitation in the semantic understanding of retrieval models, the success of
RAG heavily lies on the ability of LLMs to identify passages with utility.
Recent efforts have explored the ability of LLMs to assess the relevance of
passages in retrieval, but there has been limited work on evaluating the
utility of passages in supporting question answering. In this work, we conduct
a comprehensive study about the capabilities of LLMs in utility evaluation for
open-domain QA. Specifically, we introduce a benchmarking procedure and
collection of candidate passages with different characteristics, facilitating a
series of experiments with five representative LLMs. Our experiments reveal
that: (i) well-instructed LLMs can distinguish between relevance and utility,
and that LLMs are highly receptive to newly generated counterfactual passages.
Moreover, (ii) we scrutinize key factors that affect utility judgments in the
instruction design. And finally, (iii) to verify the efficacy of utility
judgments in practical retrieval augmentation applications, we delve into LLMs'
QA capabilities using the evidence judged with utility and direct dense
retrieval results. (iv) We propose a k-sampling, listwise approach to reduce
the dependency of LLMs on the sequence of input passages, thereby facilitating
subsequent answer generation. We believe that the way we formalize and study
the problem along with our findings contributes to a critical assessment of
retrieval-augmented LLMs. Our code and benchmark can be found at
\url{https://github.com/ict-bigdatalab/utility_judgments}.",Hengran Zhang
2024-03-30T13:24:58Z,http://arxiv.org/abs/2404.00360v1,Reusable Architecture Growth for Continual Stereo Matching,"The remarkable performance of recent stereo depth estimation models benefits
from the successful use of convolutional neural networks to regress dense
disparity. Akin to most tasks, this needs gathering training data that covers a
number of heterogeneous scenes at deployment time. However, training samples
are typically acquired continuously in practical applications, making the
capability to learn new scenes continually even more crucial. For this purpose,
we propose to perform continual stereo matching where a model is tasked to 1)
continually learn new scenes, 2) overcome forgetting previously learned scenes,
and 3) continuously predict disparities at inference. We achieve this goal by
introducing a Reusable Architecture Growth (RAG) framework. RAG leverages
task-specific neural unit search and architecture growth to learn new scenes
continually in both supervised and self-supervised manners. It can maintain
high reusability during growth by reusing previous units while obtaining good
performance. Additionally, we present a Scene Router module to adaptively
select the scene-specific architecture path at inference. Comprehensive
experiments on numerous datasets show that our framework performs impressively
in various weather, road, and city circumstances and surpasses the
state-of-the-art methods in more challenging cross-dataset settings. Further
experiments also demonstrate the adaptability of our method to unseen scenes,
which can facilitate end-to-end stereo architecture learning and practical
deployment.",Chenghao Zhang
2024-04-08T15:03:57Z,http://arxiv.org/abs/2404.05590v2,"MedExpQA: Multilingual Benchmarking of Large Language Models for Medical
  Question Answering","Large Language Models (LLMs) have the potential of facilitating the
development of Artificial Intelligence technology to assist medical experts for
interactive decision support, which has been demonstrated by their competitive
performances in Medical QA. However, while impressive, the required quality bar
for medical applications remains far from being achieved. Currently, LLMs
remain challenged by outdated knowledge and by their tendency to generate
hallucinated content. Furthermore, most benchmarks to assess medical knowledge
lack reference gold explanations which means that it is not possible to
evaluate the reasoning of LLMs predictions. Finally, the situation is
particularly grim if we consider benchmarking LLMs for languages other than
English which remains, as far as we know, a totally neglected topic. In order
to address these shortcomings, in this paper we present MedExpQA, the first
multilingual benchmark based on medical exams to evaluate LLMs in Medical
Question Answering. To the best of our knowledge, MedExpQA includes for the
first time reference gold explanations written by medical doctors which can be
leveraged to establish various gold-based upper-bounds for comparison with LLMs
performance. Comprehensive multilingual experimentation using both the gold
reference explanations and Retrieval Augmented Generation (RAG) approaches show
that performance of LLMs still has large room for improvement, especially for
languages other than English. Furthermore, and despite using state-of-the-art
RAG methods, our results also demonstrate the difficulty of obtaining and
integrating readily available medical knowledge that may positively impact
results on downstream evaluations for Medical Question Answering. So far the
benchmark is available in four languages, but we hope that this work may
encourage further development to other languages.",IÃ±igo Alonso
2024-04-11T21:48:54Z,http://arxiv.org/abs/2404.08137v2,Generative Information Retrieval Evaluation,"This paper is a draft of a chapter intended to appear in a forthcoming book
on generative information retrieval, co-edited by Chirag Shah and Ryen White.
In this chapter, we consider generative information retrieval evaluation from
two distinct but interrelated perspectives. First, large language models (LLMs)
themselves are rapidly becoming tools for evaluation, with current research
indicating that LLMs may be superior to crowdsource workers and other paid
assessors on basic relevance judgement tasks. We review past and ongoing
related research, including speculation on the future of shared task
initiatives, such as TREC, and a discussion on the continuing need for human
assessments. Second, we consider the evaluation of emerging LLM-based
generative information retrieval (GenIR) systems, including retrieval augmented
generation (RAG) systems. We consider approaches that focus both on the
end-to-end evaluation of GenIR systems and on the evaluation of a retrieval
component as an element in a RAG system. Going forward, we expect the
evaluation of GenIR systems to be at least partially based on LLM-based
assessment, creating an apparent circularity, with a system seemingly
evaluating its own output. We resolve this apparent circularity in two ways: 1)
by viewing LLM-based assessment as a form of ""slow search"", where a slower IR
system is used for evaluation and training of a faster production IR system;
and 2) by recognizing a continuing need to ground evaluation in human
assessment, even if the characteristics of that human assessment must change.",Marwah Alaofi
2024-04-16T12:10:01Z,http://arxiv.org/abs/2404.10496v4,"Spiral of Silence: How is Large Language Model Killing Information
  Retrieval? -- A Case Study on Open Domain Question Answering","The practice of Retrieval-Augmented Generation (RAG), which integrates Large
Language Models (LLMs) with retrieval systems, has become increasingly
prevalent. However, the repercussions of LLM-derived content infiltrating the
web and influencing the retrieval-generation feedback loop are largely
uncharted territories. In this study, we construct and iteratively run a
simulation pipeline to deeply investigate the short-term and long-term effects
of LLM text on RAG systems. Taking the trending Open Domain Question Answering
(ODQA) task as a point of entry, our findings reveal a potential digital
""Spiral of Silence"" effect, with LLM-generated text consistently outperforming
human-authored content in search rankings, thereby diminishing the presence and
impact of human contributions online. This trend risks creating an imbalanced
information ecosystem, where the unchecked proliferation of erroneous
LLM-generated content may result in the marginalization of accurate
information. We urge the academic community to take heed of this potential
issue, ensuring a diverse and authentic digital information landscape.",Xiaoyang Chen
2024-04-19T10:27:40Z,http://arxiv.org/abs/2404.12772v1,"Generating Test Scenarios from NL Requirements using Retrieval-Augmented
  LLMs: An Industrial Study","Test scenarios are specific instances of test cases that describe actions to
validate a particular software functionality. By outlining the conditions under
which the software operates and the expected outcomes, test scenarios ensure
that the software functionality is tested in an integrated manner. Test
scenarios are crucial for systematically testing an application under various
conditions, including edge cases, to identify potential issues and guarantee
overall performance and reliability. Specifying test scenarios is tedious and
requires a deep understanding of software functionality and the underlying
domain. It further demands substantial effort and investment from already time-
and budget-constrained requirements engineers and testing teams. This paper
presents an automated approach (RAGTAG) for test scenario generation using
Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG
allows the integration of specific domain knowledge with LLMs' generation
capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post
with bilingual requirements in German and English. Our results from an
interview survey conducted with four experts on five dimensions -- relevance,
coverage, correctness, coherence and feasibility, affirm the potential of
RAGTAG in automating test scenario generation. Specifically, our results
indicate that, despite the difficult task of analyzing bilingual requirements,
RAGTAG is able to produce scenarios that are well-aligned with the underlying
requirements and provide coverage of different aspects of the intended
functionality. The generated scenarios are easily understandable to experts and
feasible for testing in the project environment. The overall correctness is
deemed satisfactory; however, gaps in capturing exact action sequences and
domain nuances remain, underscoring the need for domain expertise when applying
LLMs.",Chetan Arora
2024-04-26T23:05:20Z,http://arxiv.org/abs/2404.17723v2,"Retrieval-Augmented Generation with Knowledge Graphs for Customer
  Service Question Answering","In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.",Zhentao Xu
2024-04-30T03:29:30Z,http://arxiv.org/abs/2404.19232v7,"GRAMMAR: Grounded and Modular Methodology for Assessment of
  Closed-Domain Retrieval-Augmented Language Model","Retrieval-Augmented Generation (RAG) systems are widely used across various
industries for querying closed-domain and in-house knowledge bases. However,
evaluating these systems presents significant challenges due to the private
nature of closed-domain data and a scarcity of queries with verifiable ground
truths. Moreover, there is a lack of analytical methods to diagnose problematic
modules and identify types of failure, such as those caused by knowledge
deficits or issues with robustness. To address these challenges, we introduce
GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation
framework comprising a grounded data generation process and an evaluation
protocol that effectively pinpoints defective modules. Our validation
experiments reveal that GRAMMAR provides a reliable approach for identifying
vulnerable modules and supports hypothesis testing for textual form
vulnerabilities. An open-source tool accompanying this framework is available
in our GitHub repository (see https://github.com/xinzhel/grammar), allowing for
easy reproduction of our results and enabling reliable and modular evaluation
in closed-domain settings.",Xinzhe Li
2024-04-30T13:14:51Z,http://arxiv.org/abs/2404.19543v1,"RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural
  Language Processing","Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.",Yucheng Hu
2024-05-07T04:04:53Z,http://arxiv.org/abs/2405.03989v2,"A Method for Parsing and Vectorization of Semi-structured Data used in
  Retrieval Augmented Generation","This paper presents a novel method for parsing and vectorizing
semi-structured data to enhance the functionality of Retrieval-Augmented
Generation (RAG) within Large Language Models (LLMs). We developed a
comprehensive pipeline for converting various data formats into .docx, enabling
efficient parsing and structured data extraction. The core of our methodology
involves the construction of a vector database using Pinecone, which integrates
seamlessly with LLMs to provide accurate, context-specific responses,
particularly in environmental management and wastewater treatment operations.
Through rigorous testing with both English and Chinese texts in diverse
document formats, our results demonstrate a marked improvement in the precision
and reliability of LLMs outputs. The RAG-enhanced models displayed enhanced
ability to generate contextually rich and technically accurate responses,
underscoring the potential of vector knowledge bases in significantly boosting
the performance of LLMs in specialized domains. This research not only
illustrates the effectiveness of our method but also highlights its potential
to revolutionize data processing and analysis in environmental sciences,
setting a precedent for future advancements in AI-driven applications. Our code
is available at https://github.com/linancn/TianGong-AI-Unstructure.git.",Hang Yang
2024-05-07T21:14:38Z,http://arxiv.org/abs/2405.04674v1,"Towards Accurate and Efficient Document Analytics with Large Language
  Models","Unstructured data formats account for over 80% of the data currently stored,
and extracting value from such formats remains a considerable challenge. In
particular, current approaches for managing unstructured documents do not
support ad-hoc analytical queries on document collections. Moreover, Large
Language Models (LLMs) directly applied to the documents themselves, or on
portions of documents through a process of Retrieval-Augmented Generation
(RAG), fail to provide high accuracy query results, and in the LLM-only case,
additionally incur high costs. Since many unstructured documents in a
collection often follow similar templates that impart a common semantic
structure, we introduce ZenDB, a document analytics system that leverages this
semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document
collections. ZenDB efficiently extracts semantic hierarchical structures from
such templatized documents, and introduces a novel query engine that leverages
these structures for accurate and cost-effective query execution. Users can
impose a schema on their documents, and query it, all via SQL. Extensive
experiments on three real-world document collections demonstrate ZenDB's
benefits, achieving up to 30% cost savings compared to LLM-based baselines,
while maintaining or improving accuracy, and surpassing RAG-based baselines by
up to 61% in precision and 80% in recall, at a marginally higher cost.",Yiming Lin
2024-05-08T22:23:58Z,http://arxiv.org/abs/2405.05444v1,"Evaluating Students' Open-ended Written Responses with LLMs: Using the
  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large","Evaluating open-ended written examination responses from students is an
essential yet time-intensive task for educators, requiring a high degree of
effort, consistency, and precision. Recent developments in Large Language
Models (LLMs) present a promising opportunity to balance the need for thorough
evaluation with efficient use of educators' time. In our study, we explore the
effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in
assessing university students' open-ended answers to questions made about
reference material they have studied. Each model was instructed to evaluate 54
answers repeatedly under two conditions: 10 times (10-shot) with a temperature
setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of
1,080 evaluations per model and 4,320 evaluations across all models. The RAG
(Retrieval Augmented Generation) framework was used as the framework to make
the LLMs to process the evaluation of the answers. As of spring 2024, our
analysis revealed notable variations in consistency and the grading outcomes
provided by studied LLMs. There is a need to comprehend strengths and
weaknesses of LLMs in educational settings for evaluating open-ended written
responses. Further comparative research is essential to determine the accuracy
and cost-effectiveness of using LLMs for educational assessments.",Jussi S. Jauhiainen
2024-05-23T17:47:55Z,http://arxiv.org/abs/2405.14831v1,"HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language
  Models","In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",Bernal JimÃ©nez GutiÃ©rrez
2024-05-27T10:53:15Z,http://arxiv.org/abs/2406.00036v1,EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling,"The integration of multimodal Electronic Health Records (EHR) data has
notably advanced clinical predictive capabilities. However, current models that
utilize clinical notes and multivariate time-series EHR data often lack the
necessary medical context for precise clinical tasks. Previous methods using
knowledge graphs (KGs) primarily focus on structured knowledge extraction. To
address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven
framework aimed at enhancing multimodal EHR predictive modeling. Our approach
extracts entities from both time-series data and clinical notes by prompting
Large Language Models (LLMs) and aligns them with professional PrimeKG to
ensure consistency. Beyond triplet relationships, we include entities'
definitions and descriptions to provide richer semantics. The extracted
knowledge is then used to generate task-relevant summaries of patients' health
statuses. These summaries are fused with other modalities utilizing an adaptive
multimodal fusion network with cross-attention. Extensive experiments on the
MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day
readmission tasks demonstrate the superior performance of the EMERGE framework
compared to baseline models. Comprehensive ablation studies and analyses
underscore the efficacy of each designed module and the framework's robustness
to data sparsity. EMERGE significantly enhances the use of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts crucial for
informed clinical predictions.",Yinghao Zhu
2024-06-04T23:36:08Z,http://arxiv.org/abs/2406.02818v1,"Chain of Agents: Large Language Models Collaborating on Long-Context
  Tasks","Addressing the challenge of effectively processing long contexts has become a
critical issue for Large Language Models (LLMs). Two common strategies have
emerged: 1) reducing the input length, such as retrieving relevant chunks by
Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit
of LLMs. However, both strategies have drawbacks: input reduction has no
guarantee of covering the part with needed information, while window extension
struggles with focusing on the pertinent information for solving the task. To
mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework
that harnesses multi-agent collaboration through natural language to enable
information aggregation and context reasoning across various LLMs over
long-context tasks. CoA consists of multiple worker agents who sequentially
communicate to handle different segmented portions of the text, followed by a
manager agent who synthesizes these contributions into a coherent final output.
CoA processes the entire input by interleaving reading and reasoning, and it
mitigates long context focus issues by assigning each agent a short context. We
perform comprehensive evaluation of CoA on a wide range of long-context tasks
in question answering, summarization, and code completion, demonstrating
significant improvements by up to 10% over strong baselines of RAG,
Full-Context, and multi-agent LLMs.",Yusen Zhang
2024-06-07T17:02:35Z,http://arxiv.org/abs/2406.05087v2,Corpus Poisoning via Approximate Greedy Gradient Descent,"Dense retrievers are widely used in information retrieval and have also been
successfully extended to other knowledge intensive areas such as language
models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they
have recently been shown to be vulnerable to corpus poisoning attacks in which
a malicious user injects a small fraction of adversarial passages into the
retrieval corpus to trick the system into returning these passages among the
top-ranked results for a broad set of user queries. Further study is needed to
understand the extent to which these attacks could limit the deployment of
dense retrievers in real-world applications. In this work, we propose
Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval
systems based on the widely used HotFlip method for efficiently generating
adversarial passages. We demonstrate that AGGD can select a higher quality set
of token-level perturbations than HotFlip by replacing its random token
sampling with a more structured search. Experimentally, we show that our method
achieves a high attack success rate on several datasets and using several
retrievers, and can generalize to unseen queries and new domains. Notably, our
method is extremely effective in attacking the ANCE retrieval model, achieving
attack success rates that are 15.24\% and 17.44\% higher on the NQ and MS MARCO
datasets, respectively, compared to HotFlip. Additionally, we demonstrate
AGGD's potential to replace HotFlip in other adversarial attacks, such as
knowledge poisoning of RAG systems.",Jinyan Su
2024-06-10T15:52:49Z,http://arxiv.org/abs/2406.06399v3,"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt
  LLMs for Dialogue","We study the limitations of Large Language Models (LLMs) for the task of
response generation in human-machine dialogue. Several techniques have been
proposed in the literature for different dialogue types (e.g., Open-Domain).
However, the evaluations of these techniques have been limited in terms of base
LLMs, dialogue types and evaluation metrics. In this work, we extensively
analyze different LLM adaptation techniques when applied to different dialogue
types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue
types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.
We evaluate the performance of in-context learning and fine-tuning techniques
across datasets selected for each dialogue type. We assess the impact of
incorporating external knowledge to ground the generation in both scenarios of
Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent
evaluation and explainability criteria for automatic metrics and human
evaluation protocols. Our analysis shows that there is no universal
best-technique for adapting large language models as the efficacy of each
technique depends on both the base LLM and the specific type of dialogue. Last
but not least, the assessment of the best adaptation technique should include
human evaluation to avoid false expectations and outcomes derived from
automatic metrics.",Simone Alghisi
2024-06-14T13:28:31Z,http://arxiv.org/abs/2406.10018v1,"STALL+: Boosting LLM-based Repository-level Code Completion with Static
  Analysis","Repository-level code completion is challenging as it involves complicated
contexts from multiple files in the repository. To date, researchers have
proposed two technical categories to enhance LLM-based repository-level code
completion, i.e., retrieval-augmented generation (RAG) and static analysis
integration. This work performs the first study on the static analysis
integration in LLM-based repository-level code completion by investigating both
the effectiveness and efficiency of static analysis integration strategies
across different phases of code completion. We first implement a framework
STALL+, which supports an extendable and customizable integration of multiple
static analysis strategies into the complete pipeline of LLM-based
repository-level code completion; and based on STALL+, we perform extensive
experiments by including different code LLMs on the latest repository-level
code completion benchmark CrossCodeEval. Our findings show that integrating
file-level dependencies in prompting phase performs the best while the
integration in post-processing phase performs the worse. Additionally, we
observe different improvements from static analysis between dynamic languages
and static languages, i.e., the best combination is prompting-phase with
decoding-phase integration for Java while the best combination is
prompting-phase with post-processing-phase integration for Python given the
limitations of statically analyzing dynamic languages. Additionally, we find
the complementarity between RAG and static analysis integration as well as
their cost-effectiveness after combination.",Junwei Liu
2024-06-18T12:52:51Z,http://arxiv.org/abs/2406.12566v3,"RichRAG: Crafting Rich Responses for Multi-faceted Queries in
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) effectively addresses issues of static
knowledge and hallucination in large language models. Existing studies mostly
focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with
diverse sub-intents, for which they desire rich and long-form answers covering
multiple relevant aspects. To tackle this important yet underexplored problem,
we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect
explorer to identify potential sub-aspects of input questions, a multi-faceted
retriever to build a candidate pool of diverse external documents related to
these sub-aspects, and a generative list-wise ranker, which is a key module to
provide the top-k most valuable documents for the final generator. These ranked
documents sufficiently cover various query aspects and are aware of the
generator's preferences, hence incentivizing it to produce rich and
comprehensive responses for users. The training of our ranker involves a
supervised fine-tuning stage to ensure the basic coverage of documents, and a
reinforcement learning stage to align downstream LLM's preferences to the
ranking of documents. Experimental results on two publicly available datasets
prove that our framework effectively and efficiently provides comprehensive and
satisfying responses to users.",Shuting Wang
2024-05-25T13:38:15Z,http://arxiv.org/abs/2406.12881v1,Towards Unlocking Insights from Logbooks Using AI,"Electronic logbooks contain valuable information about activities and events
concerning their associated particle accelerator facilities. However, the
highly technical nature of logbook entries can hinder their usability and
automation. As natural language processing (NLP) continues advancing, it offers
opportunities to address various challenges that logbooks present. This work
explores jointly testing a tailored Retrieval Augmented Generation (RAG) model
for enhancing the usability of particle accelerator logbooks at institutes like
DESY, BESSY, Fermilab, BNL, SLAC, LBNL, and CERN. The RAG model uses a corpus
built on logbook contributions and aims to unlock insights from these logbooks
by leveraging retrieval over facility datasets, including discussion about
potential multimodal sources. Our goals are to increase the FAIR-ness
(findability, accessibility, interoperability, and reusability) of logbooks by
exploiting their information content to streamline everyday use, enable
macro-analysis for root cause analysis, and facilitate problem-solving
automation.",Antonin Sulc
2024-06-19T00:28:58Z,http://arxiv.org/abs/2406.13121v1,"Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?","Long-context language models (LCLMs) have the potential to revolutionize our
approach to tasks traditionally reliant on external tools like retrieval
systems or databases. Leveraging LCLMs' ability to natively ingest and process
entire corpora of information offers numerous advantages. It enhances
user-friendliness by eliminating the need for specialized knowledge of tools,
provides robust end-to-end modeling that minimizes cascading errors in complex
pipelines, and allows for the application of sophisticated prompting techniques
across the entire system. To assess this paradigm shift, we introduce LOFT, a
benchmark of real-world tasks requiring context up to millions of tokens
designed to evaluate LCLMs' performance on in-context retrieval and reasoning.
Our findings reveal LCLMs' surprising ability to rival state-of-the-art
retrieval and RAG systems, despite never having been explicitly trained for
these tasks. However, LCLMs still face challenges in areas like compositional
reasoning that are required in SQL-like tasks. Notably, prompting strategies
significantly influence performance, emphasizing the need for continued
research as context lengths grow. Overall, LOFT provides a rigorous testing
ground for LCLMs, showcasing their potential to supplant existing paradigms and
tackle novel tasks as model capabilities scale.",Jinhyuk Lee
2024-06-19T19:06:36Z,http://arxiv.org/abs/2406.13779v1,"FoRAG: Factuality-optimized Retrieval Augmented Generation for
  Web-enhanced Long-form Question Answering","Retrieval Augmented Generation (RAG) has become prevalent in
question-answering (QA) tasks due to its ability of utilizing search engine to
enhance the quality of long-form question-answering (LFQA). Despite the
emergence of various open source methods and web-enhanced commercial systems
such as Bing Chat, two critical problems remain unsolved, i.e., the lack of
factuality and clear logic in the generated long-form answers. In this paper,
we remedy these issues via a systematic study on answer generation in
web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced
generator to achieve clear logic in the generation of multifaceted answers and
construct two datasets accordingly. Then we propose a factuality optimization
method based on a carefully designed doubly fine-grained RLHF framework, which
contains automatic evaluation and reward modeling in different levels of
granularity. Our generic framework comprises conventional fine-grained RLHF
methods as special cases. Extensive experiments verify the superiority of our
proposed \textit{Factuality-optimized RAG (FoRAG)} method on both English and
Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat,
the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly
used metrics (i.e., coherence, helpfulness, and factuality), while the number
of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets
and models are made publicly available for better reproducibility:
https://huggingface.co/forag.",Tianchi Cai
2024-06-20T16:59:52Z,http://arxiv.org/abs/2406.14497v1,CodeRAG-Bench: Can Retrieval Augment Code Generation?,"While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.",Zora Zhiruo Wang
2024-06-20T21:27:57Z,http://arxiv.org/abs/2406.14745v2,"Relation Extraction with Fine-Tuned Large Language Models in Retrieval
  Augmented Generation Frameworks","Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.",Sefika Efeoglu
2024-06-21T17:23:21Z,http://arxiv.org/abs/2406.15319v3,LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,"In traditional RAG framework, the basic retrieval units are normally short.
The common retrievers like DPR normally work with 100-word Wikipedia
paragraphs. Such a design forces the retriever to search over a large corpus to
find the `needle' unit. In contrast, the readers only need to generate answers
from the short retrieved units. The imbalanced `heavy' retriever and `light'
reader design can lead to sub-optimal performance. The loss of contextual
information in the short, chunked units may increase the likelihood of
introducing hard negatives during the retrieval stage. Additionally, the reader
might not fully leverage the capabilities of recent advancements in LLMs. In
order to alleviate the imbalance, we propose a new framework LongRAG,
consisting of a `long retriever' and a `long reader'. In the two
Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire
Wikipedia corpus into 4K-token units by grouping related documents. By
increasing the unit size, we significantly reduce the total number of units.
This greatly reduces the burden on the retriever, resulting in strong retrieval
performance with only a few (less than 8) top units. Without requiring any
training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which
are on par with the (fully-trained) SoTA model. Furthermore, we test on two
non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes
each individual document as a single (long) unit rather than chunking them into
smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%
on MultiFieldQA-en. Our study offers insights into the future roadmap for
combining RAG with long-context LLMs.",Ziyan Jiang
2024-06-26T00:00:45Z,http://arxiv.org/abs/2406.17987v4,Multi-step Inference over Unstructured Data,"The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.",Aditya Kalyanpur
2024-06-26T07:21:02Z,http://arxiv.org/abs/2406.18122v1,Poisoned LangChain: Jailbreak LLMs by LangChain,"With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.",Ziqiu Wang
2024-07-02T06:08:55Z,http://arxiv.org/abs/2407.01972v1,"MeMemo: On-device Retrieval Augmentation for Private and Personalized
  Text Generation","Retrieval-augmented text generation (RAG) addresses the common limitations of
large language models (LLMs), such as hallucination, by retrieving information
from an updatable external knowledge base. However, existing approaches often
require dedicated backend servers for data storage and retrieval, thereby
limiting their applicability in use cases that require strict data privacy,
such as personal finance, education, and medicine. To address the pressing need
for client-side dense retrieval, we introduce MeMemo, the first open-source
JavaScript toolkit that adapts the state-of-the-art approximate nearest
neighbor search technique HNSW to browser environments. Developed with modern
and native Web technologies, such as IndexedDB and Web Workers, our toolkit
leverages client-side hardware capabilities to enable researchers and
developers to efficiently search through millions of high-dimensional vectors
in the browser. MeMemo enables exciting new design and research opportunities,
such as private and personalized content creation and interactive prototyping,
as demonstrated in our example application RAG Playground. Reflecting on our
work, we discuss the opportunities and challenges for on-device dense
retrieval. MeMemo is available at https://github.com/poloclub/mememo.",Zijie J. Wang
2024-07-08T13:07:50Z,http://arxiv.org/abs/2407.06245v2,"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open
  Radio Access Networks","Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.",Pranshav Gajjar
2024-07-13T22:45:46Z,http://arxiv.org/abs/2407.10021v1,"Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation","Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.",Kriti Bhattarai
2024-07-16T08:21:02Z,http://arxiv.org/abs/2407.11485v1,Scientific QA System with Verifiable Answers,"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.",Adela LjajiÄ‡
2024-07-16T11:58:54Z,http://arxiv.org/abs/2407.11638v1,"A Comprehensive Evaluation of Large Language Models on Temporal Event
  Forecasting","Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.",He Chang
2024-07-17T07:44:18Z,http://arxiv.org/abs/2407.12888v1,"Explainable Biomedical Hypothesis Generation via Retrieval Augmented
  Generation enabled Large Language Models","The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.",Alexander R. Pelletier
2024-07-18T13:43:01Z,http://arxiv.org/abs/2407.13511v1,"Can Open-Source LLMs Compete with Commercial Models? Exploring the
  Few-Shot Performance of Current GPT Models in Biomedical Tasks","Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT
and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)
benchmarks across different domains. New competing Open-Source alternatives
like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while
often offering higher throughput and being less costly to use. Open-Source LLMs
can also be self-hosted, which makes them interesting for enterprise and
clinical use cases where sensitive data should not be processed by third
parties. We participated in the 12th BioASQ challenge, which is a retrieval
augmented generation (RAG) setting, and explored the performance of current GPT
models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning
(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional
relevant knowledge from Wikipedia added to the context-window of the LLM might
improve their performance. Mixtral 8x7b was competitive in the 10-shot setting,
both with and without fine-tuning, but failed to produce usable results in the
zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to
measurable performance gains. Our results indicate that the performance gap
between commercial and open-source models in RAG setups exists mainly in the
zero-shot setting and can be closed by simply collecting few-shot examples for
domain-specific use cases. The code needed to rerun these experiments is
available through GitHub.",Samy Ateia
2024-07-20T01:02:27Z,http://arxiv.org/abs/2407.14717v2,Differential Privacy of Cross-Attention with Provable Guarantee,"Cross-attention has become a fundamental module nowadays in many important
artificial intelligence applications, e.g., retrieval-augmented generation
(RAG), system prompt, guided stable diffusion, and many more. Ensuring
cross-attention privacy is crucial and urgently needed because its key and
value matrices may contain sensitive information about model providers and
their users. In this work, we design a novel differential privacy (DP) data
structure to address the privacy security of cross-attention with a theoretical
guarantee. In detail, let $n$ be the input token length of system prompt/RAG
data, $d$ be the feature dimension, $0 < \alpha \le 1$ be the relative error
parameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the
maximum value of the value matrix, and $r,s,\epsilon_s$ be parameters of
polynomial kernel methods. Then, our data structure requires
$\widetilde{O}(ndr^2)$ memory consumption with $\widetilde{O}(nr^2)$
initialization time complexity and $\widetilde{O}(\alpha^{-1} r^2)$ query time
complexity for a single token query. In addition, our data structure can
guarantee that the process of answering user query satisfies $(\epsilon,
\delta)$-DP with $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w
r^2)$ additive error and $n^{-1} (\alpha + \epsilon_s)$ relative error between
our output and the true answer. Furthermore, our result is robust to adaptive
queries in which users can intentionally attack the cross-attention system. To
our knowledge, this is the first work to provide DP for cross-attention and is
promising to inspire more privacy algorithm design in large generative models
(LGMs).",Yingyu Liang
2024-07-31T04:01:08Z,http://arxiv.org/abs/2407.21320v2,MetaOpenFOAM: an LLM-based multi-agent framework for CFD,"Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and so on. MetaOpenFOAM harnesses the power of
MetaGPT's assembly line paradigm, which assigns diverse roles to various
agents, efficiently breaking down complex CFD tasks into manageable subtasks.
Langchain further complements MetaOpenFOAM by integrating Retrieval-Augmented
Generation (RAG) technology, which enhances the framework's ability by
integrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a
benchmark for natural language-based CFD solver, consisting of eight CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The eight CFD
simulation tasks encompass a range of multidimensional flow problems, covering
compressible and incompressible flows with different physical processes. This
demonstrates the capability to automate CFD simulations using only natural
language input, iteratively correcting errors to achieve the desired
simulations. An ablation study was conducted to verify the necessity of each
component in the multi-agent system and the RAG technology. A sensitivity study
on the randomness of LLM showed that LLM with low randomness can obtain more
stable and accurate results. Additionally, MetaOpenFOAM owns the ability to
identify and modify key parameters in user requirements, and excels in
correcting bugs when failure match occur,which demonstrates the generalization
of MetaOpenFOAM.",Yuxuan Chen
2024-08-07T05:52:00Z,http://arxiv.org/abs/2408.03562v1,"A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel
  Chatbot Use Case","This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
""Golden Answers"", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.",Sonia Meyer
2024-08-07T23:22:58Z,http://arxiv.org/abs/2408.04125v2,Exploring RAG-based Vulnerability Augmentation with LLMs,"Detecting vulnerabilities is vital for software security, yet deep
learning-based vulnerability detectors (DLVD) face a data shortage, which
limits their effectiveness. Data augmentation can potentially alleviate the
data shortage, but augmenting vulnerable code is challenging and requires a
generative solution that maintains vulnerability. Previous works have only
focused on generating samples that contain single statements or specific types
of vulnerabilities. Recently, large language models (LLMs) have been used to
solve various code generation and comprehension tasks with inspiring results,
especially when fused with retrieval augmented generation (RAG). Therefore, we
propose VulScribeR, a novel LLM-based solution that leverages carefully curated
prompt templates to augment vulnerable datasets. More specifically, we explore
three strategies to augment both single and multi-statement vulnerabilities,
with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation
across three vulnerability datasets and DLVD models, using two LLMs, show that
our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling
(ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable
samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated
vulnerable samples. Our approach demonstrates its feasibility for large-scale
data augmentation by generating 1K samples at as cheap as US$ 1.88.",Seyed Shayan Daneshvar
2024-08-08T22:18:01Z,http://arxiv.org/abs/2408.04775v1,"Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction","Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.",Reza Khanmohammadi
2024-08-09T09:07:48Z,http://arxiv.org/abs/2408.04948v1,"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction","Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain",Bhaskarjit Sarmah
2024-08-15T04:29:33Z,http://arxiv.org/abs/2408.08335v1,Plan with Code: Comparing approaches for robust NL to DSL generation,"Planning in code is considered a more reliable approach for many
orchestration tasks. This is because code is more tractable than steps
generated via Natural Language and make it easy to support more complex
sequences by abstracting deterministic logic into functions. It also allows
spotting issues with incorrect function names with the help of parsing checks
that can be run on code. Progress in Code Generation methodologies, however,
remains limited to general-purpose languages like C, C++, and Python. LLMs
continue to face challenges with custom function names in Domain Specific
Languages or DSLs, leading to higher hallucination rates and syntax errors.
This is more common for custom function names, that are typically part of the
plan. Moreover, keeping LLMs up-to-date with newer function names is an issue.
This poses a challenge for scenarios like task planning over a large number of
APIs, since the plan is represented as a DSL having custom API names. In this
paper, we focus on workflow automation in RPA (Robotic Process Automation)
domain as a special case of task planning. We present optimizations for using
Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with
an ablation study comparing these strategies with a fine-tuned model. Our
results showed that the fine-tuned model scored the best on code similarity
metric. However, with our optimizations, RAG approach is able to match the
quality for in-domain API names in the test set. Additionally, it offers
significant advantage for out-of-domain or unseen API names, outperforming
Fine-Tuned model on similarity metric by 7 pts.",Nastaran Bassamzadeh
2024-09-09T20:52:25Z,http://arxiv.org/abs/2409.06062v1,Retrieval Augmented Correction of Named Entity Speech Recognition Errors,"In recent years, end-to-end automatic speech recognition (ASR) systems have
proven themselves remarkably accurate and performant, but these systems still
have a significant error rate for entity names which appear infrequently in
their training data. In parallel to the rise of end-to-end ASR systems, large
language models (LLMs) have proven to be a versatile tool for various natural
language processing (NLP) tasks. In NLP tasks where a database of relevant
knowledge is available, retrieval augmented generation (RAG) has achieved
impressive results when used with LLMs. In this work, we propose a RAG-like
technique for correcting speech recognition entity name errors. Our approach
uses a vector database to index a set of relevant entities. At runtime,
database queries are generated from possibly errorful textual ASR hypotheses,
and the entities retrieved using these queries are fed, along with the ASR
hypotheses, to an LLM which has been adapted to correct ASR errors. Overall,
our best system achieves 33%-39% relative word error rate reductions on
synthetic test sets focused on voice assistant queries of rare music entities
without regressing on the STOP test set, a publicly available voice assistant
test set covering many domains.",Ernest Pusateri
2024-09-12T01:51:06Z,http://arxiv.org/abs/2409.07691v1,"Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,
  fine-tuning and deploying Rerankers for RAG","Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.",Gabriel de Souza P. Moreira
2024-09-14T07:19:18Z,http://arxiv.org/abs/2409.09343v1,"Generative AI in Data Center Networking: Fundamentals, Perspectives, and
  Case Study","Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as
OpenAI's ChatGPT, is revolutionizing various fields. Central to this
transformation is Data Center Networking (DCN), which not only provides the
computational power necessary for GenAI training and inference but also
delivers GenAI-driven services to users. This article examines an interplay
between GenAI and DCNs, highlighting their symbiotic relationship and mutual
advancements. We begin by reviewing current challenges within DCNs and discuss
how GenAI contributes to enhancing DCN capabilities through innovations, such
as data augmentation, process automation, and domain transfer. We then focus on
analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining
insights that catalyze the evolution of DCNs to more effectively support GenAI
and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs,
we present a case study on full-lifecycle DCN digital twins. In this study, we
employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate
optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning
(DRL) for optimizing the RAG knowledge placement strategy. This approach not
only demonstrates the application of advanced GenAI methods within DCNs but
also positions the digital twin as a pivotal GenAI service operating on DCNs.
We anticipate that this article can promote further research into enhancing the
virtuous interaction between GenAI and DCNs.",Yinqiu Liu
2024-09-17T16:55:25Z,http://arxiv.org/abs/2409.11353v3,"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models","Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.",Mengfei Liang
2024-09-03T15:30:57Z,http://arxiv.org/abs/2409.13695v1,You Only Use Reactive Attention Slice For Long Context Retrieval,"Supporting longer context for Large Language Models (LLM) is a promising
direction to advance LLMs. As training a model for a longer context window is
computationally expensive, many alternative solutions, such as Retrieval
Augmented Generation (RAG), have been used. However, most existing RAG methods
adopt embedding-based retrieval that falls short on long contexts.
  To address such challenges, we propose an attention-based retrieval
technique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a
novel retrieval heuristic called reaction score to rank the relevance of each
sentence in the input context with the query sentence. Intuitively, we measure
how the per-token attention score ""reacts"" to the query and greedily retrieves
the most reactive sentences. Internally, YOURA generates a token-indexed vector
(called reaction vector) for the whole input context. To map each sentence to
the token-indexed vector, we propose an Embedding-Agnostic Sentence Yield
(EASY), a best-effort token wiggling algorithm.
  We evaluate our retrieval technique on three open-source pre-trained LLM
models across six LongBench QA datasets. Our technique achieves up to 30% vLLM
inference throughput improvement for serving long-context queries with a nearly
identical quality score to the simple yet effective truncate-middle approach.",Yun Joon Soh
2024-09-23T16:16:08Z,http://arxiv.org/abs/2409.15163v1,"Lessons Learned on Information Retrieval in Electronic Health Records: A
  Comparison of Embedding Models and Pooling Strategies","Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.",Skatje Myers
2024-09-23T16:51:43Z,http://arxiv.org/abs/2409.15204v2,RAMBO: Enhancing RAG-based Repository-Level Method Body Completion,"Code completion is essential in software development, helping developers by
predicting code snippets based on context. Among completion tasks, Method Body
Completion (MBC) is particularly challenging as it involves generating complete
method bodies based on their signatures and context. This task becomes
significantly harder in large repositories, where method bodies must integrate
repositoryspecific elements such as custom APIs, inter-module dependencies, and
project-specific conventions. In this paper, we introduce RAMBO, a novel
RAG-based approach for repository-level MBC. Instead of retrieving similar
method bodies, RAMBO identifies essential repository-specific elements, such as
classes, methods, and variables/fields, and their relevant usages. By
incorporating these elements and their relevant usages into the code generation
process, RAMBO ensures more accurate and contextually relevant method bodies.
Our experimental results with leading code LLMs across 40 Java projects show
that RAMBO significantly outperformed the state-of-the-art repository-level MBC
approaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in
Compilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed
RepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark
for repository-level MBC.",Tuan-Dung Bui
2024-09-18T16:10:47Z,http://arxiv.org/abs/2409.15364v1,VERA: Validation and Enhancement for Retrieval Augmented systems,"Large language models (LLMs) exhibit remarkable capabilities but often
produce inaccurate responses, as they rely solely on their embedded knowledge.
Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external
information retrieval system, supplying additional context along with the query
to mitigate inaccuracies for a particular context. However, accuracy issues
still remain, as the model may rely on irrelevant documents or extrapolate
incorrectly from its training knowledge. To assess and improve the performance
of both the retrieval system and the LLM in a RAG framework, we propose
\textbf{VERA} (\textbf{V}alidation and \textbf{E}nhancement for
\textbf{R}etrieval \textbf{A}ugmented systems), a system designed to: 1)
Evaluate and enhance the retrieved context before response generation, and 2)
Evaluate and refine the LLM-generated response to ensure precision and minimize
errors. VERA employs an evaluator-cum-enhancer LLM that first checks if
external retrieval is necessary, evaluates the relevance and redundancy of the
retrieved context, and refines it to eliminate non-essential information.
Post-response generation, VERA splits the response into atomic statements,
assesses their relevance to the query, and ensures adherence to the context.
Our experiments demonstrate VERA's remarkable efficacy not only in improving
the performance of smaller open-source models, but also larger state-of-the art
models. These enhancements underscore VERA's potential to produce accurate and
relevant responses, advancing the state-of-the-art in retrieval-augmented
language modeling. VERA's robust methodology, combining multiple evaluation and
refinement steps, effectively mitigates hallucinations and improves retrieval
and response processes, making it a valuable tool for applications demanding
high accuracy and reliability in information generation. .",Nitin Aravind Birur
2024-10-01T04:20:14Z,http://arxiv.org/abs/2410.00387v1,"Boosting the Capabilities of Compact Models in Low-Data Contexts with
  Large Language Models and Retrieval-Augmented Generation","The data and compute requirements of current language modeling technology
pose challenges for the processing and analysis of low-resource languages.
Declarative linguistic knowledge has the potential to partially bridge this
data scarcity gap by providing models with useful inductive bias in the form of
language-specific rules. In this paper, we propose a retrieval augmented
generation (RAG) framework backed by a large language model (LLM) to correct
the output of a smaller model for the linguistic task of morphological
glossing. We leverage linguistic information to make up for the lack of data
and trainable parameters, while allowing for inputs from written descriptive
grammars interpreted and distilled through an LLM.
  The results demonstrate that significant leaps in performance and efficiency
are possible with the right combination of: a) linguistic inputs in the form of
grammars, b) the interpretive power of LLMs, and c) the trainability of smaller
token classification networks. We show that a compact, RAG-supported model is
highly effective in data-scarce settings, achieving a new state-of-the-art for
this task and our target languages. Our work also offers documentary linguists
a more reliable and more usable tool for morphological glossing by providing
well-reasoned explanations and confidence scores for each output.",Bhargav Shandilya
2024-10-03T00:38:12Z,http://arxiv.org/abs/2410.02115v2,"L-CiteEval: Do Long-Context Models Truly Leverage Context for
  Responding?","Long-context models (LCMs) have made remarkable strides in recent years,
offering users great convenience for handling tasks that involve long context,
such as document summarization. As the community increasingly prioritizes the
faithfulness of generated results, merely ensuring the accuracy of LCM outputs
is insufficient, as it is quite challenging for humans to verify the results
from the extremely lengthy context. Yet, although some efforts have been made
to assess whether LCMs respond truly based on the context, these works either
are limited to specific tasks or heavily rely on external evaluation resources
like GPT4.In this work, we introduce L-CiteEval, a comprehensive multi-task
benchmark for long-context understanding with citations, aiming to evaluate
both the understanding capability and faithfulness of LCMs. L-CiteEval covers
11 tasks from diverse domains, spanning context lengths from 8K to 48K, and
provides a fully automated evaluation suite. Through testing with 11
cutting-edge closed-source and open-source LCMs, we find that although these
models show minor differences in their generated results, open-source models
substantially trail behind their closed-source counterparts in terms of
citation accuracy and recall. This suggests that current open-source LCMs are
prone to responding based on their inherent knowledge rather than the given
context, posing a significant risk to the user experience in practical
applications. We also evaluate the RAG approach and observe that RAG can
significantly improve the faithfulness of LCMs, albeit with a slight decrease
in the generation quality. Furthermore, we discover a correlation between the
attention mechanisms of LCMs and the citation generation process.",Zecheng Tang
2024-10-08T05:13:27Z,http://arxiv.org/abs/2410.09090v1,"Automating Bibliometric Analysis with Sentence Transformers and
  Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and
  Contextual Search for Customized Literature Characterization for High-Impact
  Urban Research","Bibliometric analysis is essential for understanding research trends, scope,
and impact in urban science, especially in high-impact journals, such Nature
Portfolios. However, traditional methods, relying on keyword searches and basic
NLP techniques, often fail to uncover valuable insights not explicitly stated
in article titles or keywords. These approaches are unable to perform semantic
searches and contextual understanding, limiting their effectiveness in
classifying topics and characterizing studies. In this paper, we address these
limitations by leveraging Generative AI models, specifically transformers and
Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric
analysis. We developed a technical workflow that integrates a vector database,
Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and
Large Language Models (LLMs) to enable contextual search, topic ranking, and
characterization of research using customized prompt templates. A pilot study
analyzing 223 urban science-related articles published in Nature Communications
over the past decade highlights the effectiveness of our approach in generating
insightful summary statistics on the quality, scope, and characteristics of
papers in high-impact journals. This study introduces a new paradigm for
enhancing bibliometric analysis and knowledge retrieval in urban research,
positioning an AI agent as a powerful tool for advancing research evaluation
and understanding.",Haowen Xu
2024-10-15T19:04:13Z,http://arxiv.org/abs/2410.11996v1,"Holistic Reasoning with Long-Context LMs: A Benchmark for Database
  Operations on Massive Textual Data","The rapid increase in textual information means we need more efficient
methods to sift through, organize, and understand it all. While
retrieval-augmented generation (RAG) models excel in accessing information from
large document collections, they struggle with complex tasks that require
aggregation and reasoning over information spanning across multiple
documents--what we call holistic reasoning. Long-context language models
(LCLMs) have great potential for managing large-scale documents, but their
holistic reasoning capabilities remain unclear. In this work, we introduce
HoloBench, a novel framework that brings database reasoning operations into
text-based contexts, making it easier to systematically evaluate how LCLMs
handle holistic reasoning across large documents. Our approach adjusts key
factors such as context length, information density, distribution of
information, and query complexity to evaluate LCLMs comprehensively. Our
experiments show that the amount of information in the context has a bigger
influence on LCLM performance than the actual context length. Furthermore, the
complexity of queries affects performance more than the amount of information,
particularly for different types of queries. Interestingly, queries that
involve finding maximum or minimum values are easier for LCLMs and are less
affected by context length, even though they pose challenges for RAG systems.
However, tasks requiring the aggregation of multiple pieces of information show
a noticeable drop in accuracy as context length increases. Additionally, we
find that while grouping relevant information generally improves performance,
the optimal positioning varies across models. Our findings surface both the
advancements and the ongoing challenges in achieving a holistic understanding
of long contexts.",Seiji Maekawa
2024-10-16T17:59:32Z,http://arxiv.org/abs/2410.12788v2,"Meta-Chunking: Learning Efficient Text Segmentation via Logical
  Perception","Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances
performance and speed, and precisely identifies the boundaries of text chunks
by analyzing the characteristics of context perplexity distribution.
Additionally, considering the inherent complexity of different texts, we
propose a strategy that combines PPL Chunking with dynamic merging to achieve a
balance between fine-grained and coarse-grained text chunking. Experiments
conducted on eleven datasets demonstrate that Meta-Chunking can more
efficiently improve the performance of single-hop and multi-hop question
answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it
outperforms similarity chunking by 1.32 while only consuming 45.8% of the time.
Furthermore, through the analysis of models of various scales and types, we
observed that PPL Chunking exhibits notable flexibility and adaptability. Our
code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",Jihao Zhao
2024-10-17T13:33:12Z,http://arxiv.org/abs/2410.13542v1,LLM-based Unit Test Generation via Property Retrieval,"Automated unit test generation has been widely studied, with Large Language
Models (LLMs) recently showing significant potential. Moreover, in the context
of unit test generation, these tools prioritize high code coverage, often at
the expense of practical usability, correctness, and maintainability. In
response, we propose Property-Based Retrieval Augmentation, a novel mechanism
that extends LLM-based Retrieval-Augmented Generation (RAG) beyond basic
vector, text similarity, and graph-based methods. Our approach considers
task-specific context and introduces a tailored property retrieval mechanism.
Specifically, in the unit test generation task, we account for the unique
structure of unit tests by dividing the test generation process into Given,
When, and Then phases. When generating tests for a focal method, we not only
retrieve general context for the code under test but also consider
task-specific context such as pre-existing tests of other methods, which can
provide valuable insights for any of the Given, When, and Then phases. This
forms property relationships between focal method and other methods, thereby
expanding the scope of retrieval beyond traditional RAG. We implement this
approach in a tool called APT, which sequentially performs preprocessing,
property retrieval, and unit test generation, using an iterative strategy where
newly generated tests guide the creation of subsequent ones. We evaluated APT
on 12 open-source projects with 1515 methods, and the results demonstrate that
APT consistently outperforms existing tools in terms of correctness,
completeness, and maintainability of the generated tests. Moreover, we
introduce a novel code-context-aware retrieval mechanism for LLMs beyond
general context, offering valuable insights and potential applications for
other code-related tasks.",Zhe Zhang
2024-10-19T16:46:21Z,http://arxiv.org/abs/2410.15154v1,"MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation
  and Rigorous Verification","Large Language Models (LLMs) have shown considerable promise in code
generation. However, the automation sector, especially in motion control,
continues to rely heavily on manual programming due to the complexity of tasks
and critical safety considerations. In this domain, incorrect code execution
can pose risks to both machinery and personnel, necessitating specialized
expertise. To address these challenges, we introduce MCCoder, an LLM-powered
system designed to generate code that addresses complex motion control tasks,
with integrated soft-motion data verification. MCCoder enhances code generation
through multitask decomposition, hybrid retrieval-augmented generation (RAG),
and self-correction with a private motion library. Moreover, it supports data
verification by logging detailed trajectory data and providing simulations and
plots, allowing users to assess the accuracy of the generated code and
bolstering confidence in LLM-based programming. To ensure robust validation, we
propose MCEVAL, an evaluation dataset with metrics tailored to motion control
tasks of varying difficulties. Experiments indicate that MCCoder improves
performance by 11.61% overall and by 66.12% on complex tasks in MCEVAL dataset
compared with base models with naive RAG. This system and dataset aim to
facilitate the application of code generation in automation settings with
strict safety requirements. MCCoder is publicly available at
https://github.com/MCCodeAI/MCCoder.",Yin Li
2024-10-22T00:47:54Z,http://arxiv.org/abs/2410.16597v1,"Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency","Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.",Prafulla Kumar Choubey
2024-10-23T09:14:57Z,http://arxiv.org/abs/2410.17694v1,"An Adaptive Framework for Generating Systematic Explanatory Answer in
  Online Q&A Platforms","Question Answering (QA) systems face challenges in handling complex questions
that require multi-domain knowledge synthesis. The naive RAG models, although
effective in information retrieval, struggle with complex questions that
require comprehensive and in-depth answers. The pioneering task is defined as
explanatory answer generation, which entails handling identified challenges
such as the requirement for comprehensive information and logical coherence
within the generated context. To address these issues, we refer to systematic
thinking theory and propose SynthRAG, an innovative framework designed to
enhance QA performance. SynthRAG improves on conventional models by employing
adaptive outlines for dynamic content structuring, generating systematic
information to ensure detailed coverage, and producing customized answers
tailored to specific user inquiries. This structured approach guarantees
logical coherence and thorough integration of information, yielding responses
that are both insightful and methodically organized. Empirical evaluations
underscore SynthRAG's effectiveness, demonstrating its superiority in handling
complex questions, overcoming the limitations of naive RAG models, and
significantly improving answer quality and depth. Furthermore, an online
deployment on the Zhihu platform revealed that SynthRAG's answers achieved
notable user engagement, with each response averaging 5.73 upvotes and
surpassing the performance of 79.8% of human contributors, highlighting the
practical relevance and impact of the proposed framework. Our code is available
at https://github.com/czy1999/SynthRAG .",Ziyang Chen
2024-10-15T16:37:18Z,http://arxiv.org/abs/2410.19790v1,"Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP
  Technical Specifications","This paper proposes a Question-Answering (QA) system for the telecom domain
using 3rd Generation Partnership Project (3GPP) technical documents. Alongside,
a hybrid dataset, Telco-DPR, which consists of a curated 3GPP corpus in a
hybrid format, combining text and tables, is presented. Additionally, the
dataset includes a set of synthetic question/answer pairs designed to evaluate
the retrieval performance of QA systems on this type of data. The retrieval
models, including the sparse model, Best Matching 25 (BM25), as well as dense
models, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval
(DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank
(MRR). The results show that DHR, a retriever model utilising hierarchical
passage selection through fine-tuning at both the document and passage levels,
outperforms traditional methods in retrieving relevant technical information,
achieving a Top-10 accuracy of 86.2%. Additionally, the Retriever-Augmented
Generation (RAG) technique, used in the proposed QA system, is evaluated to
demonstrate the benefits of using the hybrid dataset and the DHR. The proposed
QA system, using the developed RAG model and the Generative Pretrained
Transformer (GPT)-4, achieves a 14% improvement in answer accuracy, when
compared to a previous benchmark on the same dataset.",Thaina Saraiva
2024-10-15T14:51:45Z,http://arxiv.org/abs/2410.22353v1,"RuleRAG: Rule-guided retrieval-augmented generation with language models
  for question answering","Retrieval-augmented generation (RAG) framework has shown promising potential
in knowledge-intensive question answering (QA) by retrieving external corpus
and generating based on augmented context. However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-Guided
Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic
rules as demonstrations for in-context learning (RuleRAG-ICL) to guide
retrievers to retrieve logically related documents in the directions of rules
and uniformly guide generators to generate answers attributed by the guidance
of the same set of rules. Moreover, the combination of queries and rules can be
further used as supervised fine-tuning data to update retrievers and generators
(RuleRAG-FT) to achieve better rule-based instruction following capability,
leading to retrieve more supportive results and generate more acceptable
answers. To emphasize the attribution of rules, we construct five rule-aware QA
benchmarks, including three temporal and two static scenarios, and equip
RuleRAG with several kinds of retrievers and generators. Experiments
demonstrate that training-free RuleRAG-ICL effectively improves the retrieval
quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in
exact match scores over standard RAG on average across the five benchmarks, and
further fine-tuned RuleRAG-FT consistently yields more significant performance
enhancement. Extensive analyses indicate that RuleRAG scales well with
increasing numbers of retrieved documents and exhibits generalization ability
for untrained rules.",Zhongwu Chen
2024-11-01T17:11:16Z,http://arxiv.org/abs/2411.00744v1,"CORAG: A Cost-Constrained Retrieval Optimization System for
  Retrieval-Augmented Generation","Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.",Ziting Wang
2024-11-07T07:07:34Z,http://arxiv.org/abs/2411.04476v1,"LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation
  Combining Hierarchical Agents and RAG","The increasing use of smart devices has emphasized the critical role of
maintenance in production activities. Interactive Electronic Technical Manuals
(IETMs) are vital tools that support the maintenance of smart equipment.
However, traditional IETMs face challenges such as transitioning from Graphical
User Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing
complex logical relationships. Additionally, they must meet the current demands
for higher intelligence. This paper proposes a Maintenance Scheme Generation
Method based on Large Language Models (LLM-R). The proposed method includes
several key innovations: We propose the Low Rank Adaptation-Knowledge Retention
(LORA-KR) loss technology to proportionally adjust mixed maintenance data for
fine-tuning the LLM. This method prevents knowledge conflicts caused by mixed
data, improving the model's adaptability and reasoning ability in specific
maintenance domains, Besides, Hierarchical Task-Based Agent and
Instruction-level Retrieval-Augmented Generation (RAG) technologies are adopted
to optimize the generation steps and mitigate the phenomenon of hallucination
caused by the model's Inability to access contextual information. This
enhancement improves the model's flexibility and accuracy in handling known or
unknown maintenance objects and maintenance scheme scenarios. To validate the
proposed method's effectiveness in maintenance tasks, a maintenance scheme
dataset was constructed using objects from different fields. The experimental
results show that the accuracy of the maintenance schemes generated by the
proposed method reached 91.59%, indicating which improvement enhances the
intelligence of maintenance schemes and introduces novel technical approaches
for equipment maintenance.",Laifa Tao
2024-11-07T18:29:38Z,http://arxiv.org/abs/2411.04952v1,"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page
  Multi-document Understanding","Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.",Jaemin Cho
2024-11-07T19:50:28Z,http://arxiv.org/abs/2411.05141v1,"Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with
  Retrieval-Augmented Generation","Current leading Text-To-Audio (TTA) generation models suffer from degraded
performance on zero-shot and few-shot settings. It is often challenging to
generate high-quality audio for audio events that are unseen or uncommon in the
training set. Inspired by the success of Retrieval-Augmented Generation (RAG)
in Large Language Model (LLM)-based knowledge-intensive tasks, we extend the
TTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,
a novel retrieval-augmented TTA approach based on Audiobox, a conditional
flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution
which generates audio conditioned on text, we augmented the conditioning input
with retrieved audio samples that provide additional acoustic information to
generate the target audio. Our retrieval method does not require the external
database to have labeled audio, offering more practical use cases. To evaluate
our proposed method, we curated test sets in zero-shot and few-shot settings.
Our empirical results show that the proposed model can effectively leverage the
retrieved audio samples and significantly improve zero-shot and few-shot TTA
performance, with large margins on multiple evaluation metrics, while
maintaining the ability to generate semantically aligned audio for the
in-domain setting. In addition, we investigate the effect of different
retrieval methods and data sources.",Mu Yang
2024-11-09T02:13:14Z,http://arxiv.org/abs/2411.06037v2,Sufficient Context: A New Lens on Retrieval Augmented Generation Systems,"Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a way to classify instances that have enough information to answer
the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the
context is sufficient, but often output incorrect answers instead of abstaining
when the context is not. On the other hand, open-source LLMs (Llama, Mistral,
Gemma) hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2-10% for Gemini, GPT,
and Gemma.",Hailey Joren
2024-11-09T15:12:28Z,http://arxiv.org/abs/2411.06207v1,"Exploring Knowledge Boundaries in Large Language Models for Retrieval
  Judgment","Large Language Models (LLMs) are increasingly recognized for their practical
applications. However, these models often encounter challenges in dynamically
changing knowledge, as well as in managing unknown static knowledge.
Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a
significant impact on LLMs. Actually, we find that the impact of RAG on the
question answering capabilities of LLMs can be categorized into three groups:
beneficial, neutral, and harmful. By minimizing retrieval requests that yield
neutral or harmful results, we can effectively reduce both time and
computational costs, while also improving the overall performance of LLMs. This
insight motivates us to differentiate between types of questions using certain
metrics as indicators, to decrease the retrieval ratio without compromising
performance. In our work, we propose a method that is able to identify
different types of questions from this view by training a Knowledge Boundary
Model (KBM). Experiments conducted on 11 English and Chinese datasets
illustrate that the KBM effectively delineates the knowledge boundary,
significantly decreasing the proportion of retrievals required for optimal
end-to-end performance. Specifically, we evaluate the effectiveness of KBM in
three complex scenarios: dynamic knowledge, long-tail static knowledge, and
multi-hop problems, as well as its functionality as an external LLM plug-in.",Zhen Zhang
2024-11-14T08:12:36Z,http://arxiv.org/abs/2411.09269v1,"Harnessing multiple LLMs for Information Retrieval: A case study on Deep
  Learning methodologies in Biodiversity publications","Deep Learning (DL) techniques are increasingly applied in scientific studies
across various domains to address complex research questions. However, the
methodological details of these DL models are often hidden in the unstructured
text. As a result, critical information about how these models are designed,
trained, and evaluated is challenging to access and comprehend. To address this
issue, in this work, we use five different open-source Large Language Models
(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,
and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)
approach to extract and process DL methodological details from scientific
publications automatically. We built a voting classifier from the outputs of
five LLMs to accurately report DL methodological information. We tested our
approach using biodiversity publications, building upon our previous research.
To validate our pipeline, we employed two datasets of DL-related biodiversity
publications: a curated set of 100 publications from our prior work and a set
of 364 publications from the Ecological Informatics journal. Our results
demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of
DL methodological information, achieving an accuracy of 69.5% (417 out of 600
comparisons) based solely on textual content from publications. This
performance was assessed against human annotators who had access to code,
figures, tables, and other supplementary information. Although demonstrated in
biodiversity, our methodology is not limited to this field; it can be applied
across other scientific domains where detailed methodological reporting is
essential for advancing knowledge and ensuring reproducibility. This study
presents a scalable and reliable approach for automating information
extraction, facilitating better reproducibility and knowledge transfer across
studies.",Vamsi Krishna Kommineni
2024-11-18T06:33:05Z,http://arxiv.org/abs/2411.11323v1,"SayComply: Grounding Field Robotic Tasks in Operational Compliance
  through Retrieval-Based Language Models","This paper addresses the problem of task planning for robots that must comply
with operational manuals in real-world settings. Task planning under these
constraints is essential for enabling autonomous robot operation in domains
that require adherence to domain-specific knowledge. Current methods for
generating robot goals and plans rely on common sense knowledge encoded in
large language models. However, these models lack grounding of robot plans to
domain-specific knowledge and are not easily transferable between multiple
sites or customers with different compliance needs. In this work, we present
SayComply, which enables grounding robotic task planning with operational
compliance using retrieval-based language models. We design a hierarchical
database of operational, environment, and robot embodiment manuals and
procedures to enable efficient retrieval of the relevant context under the
limited context length of the LLMs. We then design a task planner using a
tree-based retrieval augmented generation (RAG) technique to generate robot
tasks that follow user instructions while simultaneously complying with the
domain knowledge in the database. We demonstrate the benefits of our approach
through simulations and hardware experiments in real-world scenarios that
require precise context retrieval across various types of context,
outperforming the standard RAG method. Our approach bridges the gap in
deploying robots that consistently adhere to operational protocols, offering a
scalable and edge-deployable solution for ensuring compliance across varied and
complex real-world environments. Project website: saycomply.github.io.",Muhammad Fadhil Ginting
2024-11-20T07:44:34Z,http://arxiv.org/abs/2411.13093v2,Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension,"Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.",Yongdong Luo
2024-11-25T15:35:51Z,http://arxiv.org/abs/2411.16495v2,"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous
  Knowledge Reasoning","Recent advancements in large language models (LLMs) have led to significant
improvements in various natural language processing tasks, but it is still
challenging for LLMs to perform knowledge-intensive complex question answering
due to LLMs' inefficacy in reasoning planning and the hallucination problem. A
typical solution is to employ retrieval-augmented generation (RAG) coupled with
chain-of-thought (CoT) reasoning, which decomposes complex questions into
chain-like sub-questions and applies iterative RAG at each sub-question.
However, prior works exhibit sub-optimal reasoning planning and overlook
dynamic knowledge retrieval from heterogeneous sources. In this paper, we
propose AtomR, a novel heterogeneous knowledge reasoning framework that
conducts multi-source reasoning at the atomic level. Drawing inspiration from
the graph modeling of knowledge, AtomR leverages large language models (LLMs)
to decompose complex questions into combinations of three atomic knowledge
operators, significantly enhancing the reasoning process at both the planning
and execution stages. We also introduce BlendQA, a novel evaluation benchmark
tailored to assess complex heterogeneous knowledge reasoning. Experiments show
that AtomR significantly outperforms state-of-the-art baselines across three
single-source and two multi-source reasoning benchmarks, with notable
performance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",Amy Xin
2024-11-27T18:27:07Z,http://arxiv.org/abs/2411.18583v1,"Automated Literature Review Using NLP Techniques and LLM-Based
  Retrieval-Augmented Generation","This research presents and compares multiple approaches to automate the
generation of literature reviews using several Natural Language Processing
(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language
Model (LLM). The ever-increasing number of research articles provides a huge
challenge for manual literature review. It has resulted in an increased demand
for automation. Developing a system capable of automatically generating the
literature reviews from only the PDF files as input is the primary objective of
this research work. The effectiveness of several Natural Language Processing
(NLP) strategies, such as the frequency-based method (spaCy), the transformer
model (Simple T5), and retrieval-augmented generation (RAG) with Large Language
Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR
dataset is chosen for this research experiment and three distinct techniques
are utilized to implement three different systems for auto-generating the
literature reviews. The ROUGE scores are used for the evaluation of all three
systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo
achieved the highest ROUGE-1 score, 0.364. The transformer model comes in
second place and spaCy is at the last position. Finally, a graphical user
interface is created for the best system based on the large language model.",Nurshat Fateh Ali
2024-11-29T07:57:32Z,http://arxiv.org/abs/2411.19528v1,"RAGDiffusion: Faithful Cloth Generation via External Knowledge
  Assimilation","Standard clothing asset generation involves creating forward-facing flat-lay
garment images displayed on a clear background by extracting clothing
information from diverse real-world contexts, which presents significant
challenges due to highly standardized sampling distributions and precise
structural requirements in the generated images. Existing models have limited
spatial perception and often exhibit structural hallucinations in this
high-specification generative task. To address this issue, we propose a novel
Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance
structure determinacy and mitigate hallucinations by assimilating external
knowledge from LLM and databases. RAGDiffusion consists of two core processes:
(1) Retrieval-based structure aggregation, which employs contrastive learning
and a Structure Locally Linear Embedding (SLLE) to derive global structure and
spatial landmarks, providing both soft and hard guidance to counteract
structural ambiguities; and (2) Omni-level faithful garment generation, which
introduces a three-level alignment that ensures fidelity in structural,
pattern, and decoding components within the diffusing. Extensive experiments on
challenging real-world datasets demonstrate that RAGDiffusion synthesizes
structurally and detail-faithful clothing assets with significant performance
improvements, representing a pioneering effort in high-specification faithful
generation with RAG to confront intrinsic hallucinations and enhance fidelity.",Xianfeng Tan
2024-11-29T09:07:21Z,http://arxiv.org/abs/2411.19554v1,"Unimib Assistant: designing a student-friendly RAG-based chatbot for all
  their needs","Natural language processing skills of Large Language Models (LLMs) are
unprecedented, having wide diffusion and application in different tasks. This
pilot study focuses on specializing ChatGPT behavior through a
Retrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs
feature. The purpose of our chatbot, called Unimib Assistant, is to provide
information and solutions to the specific needs of University of Milano-Bicocca
(Unimib) students through a question-answering approach. We provided the system
with a prompt highlighting its specific purpose and behavior, as well as
university-related documents and links obtained from an initial need-finding
phase, interviewing six students. After a preliminary customization phase, a
qualitative usability test was conducted with six other students to identify
the strengths and weaknesses of the chatbot, with the goal of improving it in a
subsequent redesign phase. While the chatbot was appreciated for its
user-friendly experience, perceived general reliability, well-structured
responses, and conversational tone, several significant technical and
functional limitations emerged. In particular, the satisfaction and overall
experience of the users was impaired by the system's inability to always
provide fully accurate information. Moreover, it would often neglect to report
relevant information even if present in the materials uploaded and prompt
given. Furthermore, it sometimes generated unclickable links, undermining its
trustworthiness, since providing the source of information was an important
aspect for our users. Further in-depth studies and feedback from other users as
well as implementation iterations are planned to refine our Unimib Assistant.",Chiara Antico
2024-12-03T21:00:10Z,http://arxiv.org/abs/2412.02835v1,"CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural
  Networks","We present CAISSON, a novel hierarchical approach to Retrieval-Augmented
Generation (RAG) that transforms traditional single-vector search into a
multi-view clustering framework. At its core, CAISSON leverages dual
Self-Organizing Maps (SOMs) to create complementary organizational views of the
document space, where each view captures different aspects of document
relationships through specialized embeddings. The first view processes combined
text and metadata embeddings, while the second operates on metadata enriched
with concept embeddings, enabling a comprehensive multi-view analysis that
captures both fine-grained semantic relationships and high-level conceptual
patterns. This dual-view approach enables more nuanced document discovery by
combining evidence from different organizational perspectives. To evaluate
CAISSON, we develop SynFAQA, a framework for generating synthetic financial
analyst notes and question-answer pairs that systematically tests different
aspects of information retrieval capabilities. Drawing on HotPotQA's
methodology for constructing multi-step reasoning questions, SynFAQA generates
controlled test cases where each question is paired with the set of notes
containing its ground-truth answer, progressing from simple single-entity
queries to complex multi-hop retrieval tasks involving multiple entities and
concepts. Our experimental results demonstrate substantial improvements over
both basic and enhanced RAG implementations, particularly for complex
multi-entity queries, while maintaining practical response times suitable for
interactive applications.",Igor Halperin
2024-12-06T16:22:32Z,http://arxiv.org/abs/2412.05159v1,"Enhancing Cross-Language Code Translation via Task-Specific Embedding
  Alignment in Retrieval-Augmented Generation","We introduce a novel method to enhance cross-language code translation from
Fortran to C++ by integrating task-specific embedding alignment into a
Retrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval
approaches that utilize generic embeddings agnostic to the downstream task, our
strategy aligns the retrieval model directly with the objective of maximizing
translation quality, as quantified by the CodeBLEU metric. This alignment
ensures that the embeddings are semantically and syntactically meaningful for
the specific code translation task. Our methodology involves constructing a
dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and
generating their corresponding C++ translations using the LLaMA 3.1-8B language
model. We compute pairwise CodeBLEU scores between the generated translations
and ground truth examples to capture fine-grained similarities. These scores
serve as supervision signals in a contrastive learning framework, where we
optimize the embedding model to retrieve Fortran-C++ pairs that are most
beneficial for improving the language model's translation performance. By
integrating these CodeBLEU-optimized embeddings into the RAG framework, our
approach significantly enhances both retrieval accuracy and code generation
quality over methods employing generic embeddings. On the HPC Fortran2C++
dataset, our method elevates the average CodeBLEU score from 0.64 to 0.73,
achieving a 14% relative improvement. On the Numerical Recipes dataset, we
observe an increase from 0.52 to 0.60, marking a 15% relative improvement.
Importantly, these gains are realized without any fine-tuning of the language
model, underscoring the efficiency and practicality of our approach.",Manish Bhattarai
2024-12-09T04:56:43Z,http://arxiv.org/abs/2412.06206v1,SiReRAG: Indexing Similar and Related Information for Multihop Reasoning,"Indexing is an important step towards strong performance in
retrieval-augmented generation (RAG) systems. However, existing methods
organize data based on either semantic similarity (similarity) or related
information (relatedness), but do not cover both perspectives comprehensively.
Our analysis reveals that modeling only one perspective results in insufficient
knowledge synthesis, leading to suboptimal performance on complex tasks
requiring multihop reasoning. In this paper, we propose SiReRAG, a novel RAG
indexing approach that explicitly considers both similar and related
information. On the similarity side, we follow existing work and explore some
variances to construct a similarity tree based on recursive summarization. On
the relatedness side, SiReRAG extracts propositions and entities from texts,
groups propositions via shared entities, and generates recursive summaries to
construct a relatedness tree. We index and flatten both similarity and
relatedness trees into a unified retrieval pool. Our experiments demonstrate
that SiReRAG consistently outperforms state-of-the-art indexing methods on
three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an
average 1.9% improvement in F1 scores. As a reasonably efficient solution,
SiReRAG enhances existing reranking methods significantly, with up to 7.8%
improvement in average F1 scores.",Nan Zhang
2024-12-10T14:39:51Z,http://arxiv.org/abs/2412.07548v1,"Automatic Database Configuration Debugging using Retrieval-Augmented
  Language Models","Database management system (DBMS) configuration debugging, e.g., diagnosing
poorly configured DBMS knobs and generating troubleshooting recommendations, is
crucial in optimizing DBMS performance. However, the configuration debugging
process is tedious and, sometimes challenging, even for seasoned database
administrators (DBAs) with sufficient experience in DBMS configurations and
good understandings of the DBMS internals (e.g., MySQL or Oracle). To address
this difficulty, we propose Andromeda, a framework that utilizes large language
models (LLMs) to enable automatic DBMS configuration debugging. Andromeda
serves as a natural surrogate of DBAs to answer a wide range of natural
language (NL) questions on DBMS configuration issues, and to generate
diagnostic suggestions to fix these issues. Nevertheless, directly prompting
LLMs with these professional questions may result in overly generic and often
unsatisfying answers. To this end, we propose a retrieval-augmented generation
(RAG) strategy that effectively provides matched domain-specific contexts for
the question from multiple sources. They come from related historical
questions, troubleshooting manuals and DBMS telemetries, which significantly
improve the performance of configuration debugging. To support the RAG
strategy, we develop a document retrieval mechanism addressing heterogeneous
documents and design an effective method for telemetry analysis. Extensive
experiments on real-world DBMS configuration debugging datasets show that
Andromeda significantly outperforms existing solutions.",Sibei Chen
2024-12-10T15:56:03Z,http://arxiv.org/abs/2412.07618v1,"Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced
  Retrieval-Augmented Generation on Knowledge Graphs","Despite the superior performance of Large language models on many NLP tasks,
they still face significant limitations in memorizing extensive world
knowledge. Recent studies have demonstrated that leveraging the
Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs
that encapsulate extensive factual data in a structured format, robustly
enhances the reasoning capabilities of LLMs. However, deploying such systems in
real-world scenarios presents challenges: the continuous evolution of
non-stationary environments may lead to performance degradation and user
satisfaction requires a careful balance of performance and responsiveness. To
address these challenges, we introduce a Multi-objective Multi-Armed Bandit
enhanced RAG framework, supported by multiple retrieval methods with diverse
capabilities under rich and evolving retrieval contexts in practice. Within
this framework, each retrieval method is treated as a distinct ``arm''. The
system utilizes real-time user feedback to adapt to dynamic environments, by
selecting the appropriate retrieval method based on input queries and the
historical multi-objective performance of each arm. Extensive experiments
conducted on two benchmark KGQA datasets demonstrate that our method
significantly outperforms baseline methods in non-stationary settings while
achieving state-of-the-art performance in stationary environments. Code and
data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",Xiaqiang Tang
1996-03-27T18:38:18Z,http://arxiv.org/abs/cond-mat/9603173v1,"Dynamical Generation of Extended Objects in a $1+1$ Dimensional Chiral
  Field Theory: Non-Perturbative Dirac Operator Resolvent Analysis","We analyze the $1+1$ dimensional Nambu-Jona-Lasinio model non-perturbatively.
In addition to its simple ground state saddle points, the effective action of
this model has a rich collection of non-trivial saddle points in which the
composite fields $\sigx=\lag\bar\psi\psi\rag$ and $\pix=\lag\bar\psi
i\gam_5\psi\rag$ form static space dependent configurations because of
non-trivial dynamics. These configurations may be viewed as one dimensional
chiral bags that trap the original fermions (``quarks"") into stable extended
entities (``hadrons""). We provide explicit expressions for the profiles of
these objects and calculate their masses. Our analysis of these saddle points
is based on an explicit representation we find for the diagonal resolvent of
the Dirac operator in a $\{\sigx, \pix\}$ background which produces a
prescribed number of bound states. We analyse in detail the cases of a single
as well as two bound states. We find that bags that trap $N$ fermions are the
most stable ones, because they release all the fermion rest mass as binding
energy and become massless. Our explicit construction of the diagonal resolvent
is based on elementary Sturm-Liouville theory and simple dimensional analysis
and does not depend on the large $N$ approximation. These facts make it, in our
view, simpler and more direct than the calculations previously done by Shei,
using the inverse scattering method following Dashen, Hasslacher, and Neveu.
Our method of finding such non-trivial static configurations may be applied to
other $1+1$ dimensional field theories.",J. Feinberg
2024-01-05T15:09:57Z,http://arxiv.org/abs/2401.02851v2,"Natural Language Programming in Medicine: Administering Evidence Based
  Clinical Workflows with Autonomous Agents Powered by Generative Large
  Language Models","Generative Large Language Models (LLMs) hold significant promise in
healthcare, demonstrating capabilities such as passing medical licensing exams
and providing clinical knowledge. However, their current use as information
retrieval tools is limited by challenges like data staleness, resource demands,
and occasional generation of incorrect information. This study assessed the
potential of LLMs to function as autonomous agents in a simulated tertiary care
medical center, using real-world clinical cases across multiple specialties.
Both proprietary and open-source LLMs were evaluated, with Retrieval Augmented
Generation (RAG) enhancing contextual relevance. Proprietary models,
particularly GPT-4, generally outperformed open-source models, showing improved
guideline adherence and more accurate responses with RAG. The manual evaluation
by expert clinicians was crucial in validating models' outputs, underscoring
the importance of human oversight in LLM operation. Further, the study
emphasizes Natural Language Programming (NLP) as the appropriate paradigm for
modifying model behavior, allowing for precise adjustments through tailored
prompts and real-world interactions. This approach highlights the potential of
LLMs to significantly enhance and supplement clinical decision-making, while
also emphasizing the value of continuous expert involvement and the flexibility
of NLP to ensure their reliability and effectiveness in healthcare settings.",Akhil Vaid
2024-01-24T06:50:20Z,http://arxiv.org/abs/2401.13256v3,"UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for
  Personalized Dialogue Systems","Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.",Hongru Wang
2024-01-27T10:50:11Z,http://arxiv.org/abs/2401.15378v4,"A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM","Challenges exist in learning and understanding religions, such as the
complexity and depth of religious doctrines and teachings. Chatbots as
question-answering systems can help in solving these challenges. LLM chatbots
use NLP techniques to establish connections between topics and accurately
respond to complex questions. These capabilities make it perfect for
enlightenment on religion as a question-answering chatbot. However, LLMs also
tend to generate false information, known as hallucination. Also, the chatbots'
responses can include content that insults personal religious beliefs,
interfaith conflicts, and controversial or sensitive topics. It must avoid such
cases without promoting hate speech or offending certain groups of people or
their beliefs. This study uses a vector database-based Retrieval Augmented
Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our
question-answering system is called ""MufassirQAS"". We created a database
consisting of several open-access books that include Turkish context. These
books contain Turkish translations and interpretations of Islam. This database
is utilized to answer religion-related questions and ensure our answers are
trustworthy. The relevant part of the dataset, which LLM also uses, is
presented along with the answer. We have put careful effort into creating
system prompts that give instructions to prevent harmful, offensive, or
disrespectful responses to respect people's values and provide reliable
results. The system answers and shares additional information, such as the page
number from the respective book and the articles referenced for obtaining the
information. MufassirQAS and ChatGPT are also tested with sensitive questions.
We got better performance with our system. Study and enhancements are still in
progress. Results and future works are given.",Ahmet Yusuf Alan
2024-01-30T00:21:41Z,http://arxiv.org/abs/2402.01748v2,"Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems","Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.",Shengzhe Xu
2024-03-28T03:09:42Z,http://arxiv.org/abs/2403.19113v1,FACTOID: FACtual enTailment fOr hallucInation Detection,"The widespread adoption of Large Language Models (LLMs) has facilitated
numerous benefits. However, hallucination is a significant concern. In
response, Retrieval Augmented Generation (RAG) has emerged as a highly
promising paradigm to improve LLM outputs by grounding them in factual
information. RAG relies on textual entailment (TE) or similar methods to check
if the text produced by LLMs is supported or contradicted, compared to
retrieved documents. This paper argues that conventional TE methods are
inadequate for spotting hallucinations in content generated by LLMs. For
instance, consider a prompt about the 'USA's stance on the Ukraine war''. The
AI-generated text states, ...U.S. President Barack Obama says the U.S. will not
put troops in Ukraine...'' However, during the war the U.S. president is Joe
Biden which contradicts factual reality. Moreover, current TE systems are
unable to accurately annotate the given text and identify the exact portion
that is contradicted. To address this, we introduces a new type of TE called
``Factual Entailment (FE).'', aims to detect factual inaccuracies in content
generated by LLMs while also highlighting the specific text segment that
contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation
Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL)
framework for FE, incorporating state-of-the-art (SoTA) long text embeddings
such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The
proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy
on the FACTOID benchmark compared to SoTA TE methods. As FE automatically
detects hallucinations, we assessed 15 modern LLMs and ranked them using our
proposed Auto Hallucination Vulnerability Index (HVI_auto). This index
quantifies and offers a comparative scale to evaluate and rank LLMs according
to their hallucinations.",Vipula Rawte
2024-05-01T05:39:07Z,http://arxiv.org/abs/2405.00330v1,"Integrating A.I. in Higher Education: Protocol for a Pilot Study with
  'SAMCares: An Adaptive Learning Hub'","Learning never ends, and there is no age limit to grow yourself. However, the
educational landscape may face challenges in effectively catering to students'
inclusion and diverse learning needs. These students should have access to
state-of-the-art methods for lecture delivery, online resources, and technology
needs. However, with all the diverse learning sources, it becomes harder for
students to comprehend a large amount of knowledge in a short period of time.
Traditional assistive technologies and learning aids often lack the dynamic
adaptability required for individualized education plans. Large Language Models
(LLM) have been used in language translation, text summarization, and content
generation applications. With rapid growth in AI over the past years,
AI-powered chatbots and virtual assistants have been developed. This research
aims to bridge this gap by introducing an innovative study buddy we will be
calling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in
our case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation
(RAG) to offer real-time, context-aware, and adaptive educational support. The
context of the model will be limited to the knowledge base of Sam Houston State
University (SHSU) course notes. The LLM component enables a chat-like
environment to interact with it to meet the unique learning requirements of
each student. For this, we will build a custom web-based GUI. At the same time,
RAG enhances real-time information retrieval and text generation, in turn
providing more accurate and context-specific assistance. An option to upload
additional study materials in the web GUI is added in case additional knowledge
support is required. The system's efficacy will be evaluated through controlled
trials and iterative feedback mechanisms.",Syed Hasib Akhter Faruqui
2024-05-01T11:06:31Z,http://arxiv.org/abs/2405.00449v1,"RAG-based Explainable Prediction of Road Users Behaviors for Automated
  Driving using Knowledge Graphs and Large Language Models","Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.",Mohamed Manzour Hussien
2024-05-30T17:56:05Z,http://arxiv.org/abs/2405.20362v1,"Hallucination-Free? Assessing the Reliability of Leading AI Legal
  Research Tools","Legal practice has witnessed a sharp rise in products incorporating
artificial intelligence (AI). Such tools are designed to assist with a wide
range of core legal tasks, from search and summarization of caselaw to document
drafting. But the large language models used in these tools are prone to
""hallucinate,"" or make up false information, making their use risky in
high-stakes domains. Recently, certain legal research providers have touted
methods such as retrieval-augmented generation (RAG) as ""eliminating""
(Casetext, 2023) or ""avoid[ing]"" hallucinations (Thomson Reuters, 2023), or
guaranteeing ""hallucination-free"" legal citations (LexisNexis, 2023). Because
of the closed nature of these systems, systematically assessing these claims is
challenging. In this article, we design and report on the first preregistered
empirical evaluation of AI-driven legal research tools. We demonstrate that the
providers' claims are overstated. While hallucinations are reduced relative to
general-purpose chatbots (GPT-4), we find that the AI research tools made by
LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and
Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also
document substantial differences between systems in responsiveness and
accuracy. Our article makes four key contributions. It is the first to assess
and report the performance of RAG-based proprietary legal AI tools. Second, it
introduces a comprehensive, preregistered dataset for identifying and
understanding vulnerabilities in these systems. Third, it proposes a clear
typology for differentiating between hallucinations and accurate legal
responses. Last, it provides evidence to inform the responsibilities of legal
professionals in supervising and verifying AI outputs, which remains a central
open question for the responsible integration of AI into law.",Varun Magesh
2024-06-11T19:20:27Z,http://arxiv.org/abs/2406.10273v5,"Beyond Words: On Large Language Models Actionability in Mission-Critical
  Risk Analysis","Context. Risk analysis assesses potential risks in specific scenarios. Risk
analysis principles are context-less; the same methodology can be applied to a
risk connected to health and information technology security. Risk analysis
requires a vast knowledge of national and international regulations and
standards and is time and effort-intensive. A large language model can quickly
summarize information in less time than a human and can be fine-tuned to
specific tasks.
  Aim. Our empirical study aims to investigate the effectiveness of
Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our
knowledge, no prior study has explored its capabilities in risk analysis.
  Method. We manually curated 193 unique scenarios leading to 1283
representative samples from over 50 mission-critical analyses archived by the
industrial context team in the last five years. We compared the base GPT-3.5
and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned
counterparts. We employ two human experts as competitors of the models and
three other human experts to review the models and the former human experts'
analysis. The reviewers analyzed 5,000 scenario analyses.
  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs
are quicker and more actionable. Moreover, our findings show that RAG-assisted
LLMs have the lowest hallucination rates, effectively uncovering hidden risks
and complementing human expertise. Thus, the choice of model depends on
specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and
base models for comprehensiveness and actionability. Therefore, experts can
leverage LLMs as an effective complementing companion in risk analysis within a
condensed timeframe. They can also save costs by averting unnecessary expenses
associated with implementing unwarranted countermeasures.",Matteo Esposito
2024-07-10T02:33:09Z,http://arxiv.org/abs/2407.07321v2,"Examining Long-Context Large Language Models for Environmental Review
  Document Comprehension","As LLMs become increasingly ubiquitous, researchers have tried various
techniques to augment the knowledge provided to these models. Long context and
retrieval-augmented generation (RAG) are two such methods that have recently
gained popularity. In this work, we examine the benefits of both of these
techniques by utilizing question answering (QA) task in a niche domain. While
the effectiveness of LLM-based QA systems has already been established at an
acceptable level in popular domains such as trivia and literature, it has not
often been established in niche domains that traditionally require specialized
expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance
of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and
Mistral -- when answering questions originating from Environmental Impact
Statements prepared by U.S. federal government agencies in accordance with the
National Environmental Environmental Act (NEPA). We specifically measure the
ability of LLMs to understand the nuances of legal, technical, and
compliance-related information present in NEPA documents in different
contextual scenarios. We test the LLMs' internal prior NEPA knowledge by
providing questions without any context, as well as assess how LLMs synthesize
the contextual information present in long NEPA documents to facilitate the
question/answering task. We compare the performance of the models in handling
different types of questions (e.g., problem-solving, divergent, etc.). Our
results suggest that RAG powered models significantly outperform those provided
with only the PDF context in terms of answer accuracy, regardless of the choice
of the LLM. Our further analysis reveals that many models perform better
answering closed type questions (Yes/No) than divergent and problem-solving
questions.",Hung Phan
2024-09-06T14:58:30Z,http://arxiv.org/abs/2409.13709v1,"Column Vocabulary Association (CVA): semantic interpretation of dataless
  tables","Traditional Semantic Table Interpretation (STI) methods rely primarily on the
underlying table data to create semantic annotations. This year's SemTab
challenge introduced the ``Metadata to KG'' track, which focuses on performing
STI by using only metadata information, without access to the underlying data.
In response to this new challenge, we introduce a new term: Column Vocabulary
Association (CVA). This term refers to the task of semantic annotation of
column headers solely based on metadata information. In this study, we evaluate
the performance of various methods in executing the CVA task, including a Large
Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as
well as a more traditional similarity approach with SemanticBERT. Our
methodology uses a zero-shot setting, with no pretraining or examples passed to
the Large Language Models (LLMs), as we aim to avoid a domain-specific setting.
  We investigate a total of 7 different LLMs, of which three commercial GPT
models (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source
models (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate
this models with RAG systems, and we explore how variations in temperature
settings affect performances. Moreover, we continue our investigation by
performing the CVA task utilizing SemanticBERT, analyzing how various metadata
information influence its performance.
  Initial findings indicate that LLMs generally perform well at temperatures
below 1.0, achieving an accuracy of 100\% in certain cases. Nevertheless, our
investigation also reveal that the nature of the data significantly influences
CVA task outcomes. In fact, in cases where the input data and glossary are
related (for example by being created by the same organizations) traditional
methods appear to surpass the performance of LLMs.",Margherita Martorana
2024-09-27T17:17:15Z,http://arxiv.org/abs/2409.18924v2,"AIPatient: Simulating Patients with EHRs and LLM Powered Agentic
  Workflow","Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.",Huizi Yu
1995-10-18T21:07:20Z,http://arxiv.org/abs/astro-ph/9510092v1,"Spiral Structure In The Circum-nuclear Disk At The Center Of The Galaxy
  NGC 4258","Observations of line emission from water masers near the center of the galaxy
NGC 4258 have recently provided compelling evidence for rotating disk of gas,
viewed nearly edge-on, surrounding a massive black hole (Miyoshi etal. 1995).
  We show that the disk is only marginally stable to radial perturbations - a
stability regime where weak non-axisymmetric disturbances tend to grow via the
swing amplification effect, forming a ragged, multi-armed spiral pattern
similar to that observed in Sc galaxies.
  This suggests a natural explanation for the apparent clustering of the
high-velocity emission sources into several distinct clumps, and for the
observed regularity in the distance intervals between them. The clumps of
sources appear at the intersections of the spiral arms and the radial line of
longest coherent gain path, and are thus spaced apart at the characteristic
crest-to-crest radial distance between the arms. This interpretation implies an
H_2 density and disk thickness of about 1.8x10^{10} (Q/1.6)^{-1} cm^{-3}, and
0.003(Q/1.6) pc at a radius of 0.2 pc, respectively, where the local value of
the stability parameter is 1.2 < Q < 2.",Eyal Maoz
2002-08-05T23:34:51Z,http://arxiv.org/abs/astro-ph/0208105v1,"PSR B 1706-44 and the SNR G 343.1-2.3 as the remnants of a cavity
  supernova explosion","The possible association of the supernova remnant (SNR) G 343.1-2.3 with the
pulsar PSR B 1706-44 (superposed on the arclike ""shell"" of the SNR) has been
questioned by some authors on the basis of an inconsistency between the implied
and measured (scintillation) transverse velocities of the pulsar, the absence
of any apparent interaction between the pulsar and the SNR's ""shell"", and some
other indirect arguments. We suggest, however, that this association could be
real if both objects are the remnants of a supernova (SN) which exploded within
a mushroom-like cavity (created by the SN progenitor wind breaking out of the
parent molecular cloud). This suggestion implies that the actual shape of the
SNR's shell is similar to that of the well-known SNR VRO 42.05.01 and that the
observed bright arc corresponds to the ""half"" of the SNR located inside the
cloud. We report the discovery in archival radio data of an extended ragged
radio arc to the southeast of the bright arc which we interpret as the ""half""
of the SN blast wave expanding in the intercloud medium.",D. C. -J. Bock
2006-10-17T22:47:58Z,http://arxiv.org/abs/astro-ph/0610533v1,On the structure of giant HII regions and HII galaxies,"We review the structural properties of giant extragalactic HII regions and
HII galaxies based on two dimensional hydrodynamic calculations, and propose an
evolutionary sequence that accounts for their observed detailed structure. The
model assumes a massive and young stellar cluster surrounded by a large
collection of clouds. These are thus exposed to the most important
star-formation feedback mechanisms: photoionization and the cluster wind. The
models show how the two feedback mechanisms compete with each other in the
disruption of clouds and lead to two different hydrodynamic solutions: The
storage of clouds into a long lasting ragged shell that inhibits the expansion
of the thermalized wind, and the steady filtering of the shocked wind gas
through channels carved within the cloud stratum that results into the creation
of large-scale superbubbles. Both solutions are here claimed to be concurrently
at work in giant HII regions and HII galaxies, causing their detailed inner
structure. A full description of the calculations can be found in The
Astrophysical Journal 643, 186. Animated version of the models presented can be
found at http://www.iaa.csic.es/\~{}eperez/ssc/ssc.html.",G. Tenorio-Tagle
1994-08-22T22:07:49Z,http://arxiv.org/abs/hep-th/9408120v2,On Kinks and Bound States in the Gross-Neveu Model,"We investigate static space dependent $\sigx=\lag\bar\psi\psi\rag$ saddle
point configurations in the two dimensional Gross-Neveu model in the large N
limit. We solve the saddle point condition for $\sigx$ explicitly by employing
supersymmetric quantum mechanics and using simple properties of the diagonal
resolvent of one dimensional Schr\""odinger operators rather than inverse
scattering techniques. The resulting solutions in the sector of unbroken
supersymmetry are the Callan-Coleman-Gross-Zee kink configurations. We thus
provide a direct and clean construction of these kinks. In the sector of broken
supersymmetry we derive the DHN saddle point configurations. Our method of
finding such non-trivial static configurations may be applied also in other two
dimensional field theories.",Joshua Feinberg
2010-07-08T13:51:35Z,http://arxiv.org/abs/1007.1378v3,On independent sets in random graphs,"The independence number of a sparse random graph G(n,m) of average degree
d=2m/n is well-known to be \alpha(G(n,m))~2n ln(d)/d with high probability.
Moreover, a trivial greedy algorithm w.h.p. finds an independent set of size
(1+o(1)) n ln(d)/d, i.e. half the maximum size. Yet in spite of 30 years of
extensive research no efficient algorithm has emerged to produce an independent
set with (1+c)n ln(d)/d, for any fixed c>0. In this paper we prove that the
combinatorial structure of the independent set problem in random graphs
undergoes a phase transition as the size k of the independent sets passes the
point k nln(d)/d. Roughly speaking, we prove that independent sets of size
k>(1+c)n ln(d)/d form an intricately ragged landscape, in which local search
algorithms are bound to get stuck. We illustrate this phenomenon by providing
an exponential lower bound for the Metropolis process, a Markov chain for
sampling independents sets.",Amin Coja-Oghlan
2012-04-02T20:00:00Z,http://arxiv.org/abs/1204.0513v2,Self-Perpetuating Spiral Arms in Disk Galaxies,"The causes of spiral structure in galaxies remain uncertain. Leaving aside
the grand bisymmetric spirals with their own well-known complications, here we
consider the possibility that multi-armed spiral features originate from
density inhomogeneities orbiting within disks. Using high-resolution N-body
simulations, we follow the motions of stars under the influence of gravity, and
show that mass concentrations with properties similar to those of giant
molecular clouds can induce the development of spiral arms through a process
termed swing amplification. However, unlike in earlier work, we demonstrate
that the eventual response of the disk can be highly non-linear, significantly
modifying the formation and longevity of the resulting patterns. Contrary to
expectations, ragged spiral structures can thus survive at least in a
statistical sense long after the original perturbing influence has been
removed.",Elena D'Onghia
2015-01-01T09:47:02Z,http://arxiv.org/abs/1501.00265v1,On the complexity of finite valued functions,"The essential variables in a finite function $f$ are defined as variables
which occur in $f$ and weigh with the values of that function.
  The number of essential variables is an important measure of complexity for
discrete functions.
  When replacing some variables in a function with constants the resulting
functions are called subfunctions, and when replacing all essential variables
in a function with constants we obtain an implementation of this function.
  Such an implementation corresponds with a path in an ordered decision diagram
(ODD) of the function which connects the root with a leaf of the diagram. The
sets of essential variables in subfunctions of $f$ are called separable in $f$.
In this paper we study several properties of separable sets of variables in
functions which directly impact on the number of implementations and
subfunctions in these functions.
  We define equivalence relations which classify the functions of $k$-valued
logic into classes with same number of implementations, subfunctions or
separable sets. These relations induce three transformation groups which are
compared with the lattice of all subgroups of restricted affine group (RAG).
This allows us to solve several important computational and combinatorial
problems.",Sl. Shtrakov
2018-05-19T18:32:14Z,http://arxiv.org/abs/1805.07637v1,Continuum limits of sparse coupling patterns,"We exhibit simple lattice systems, motivated by recently proposed cold atom
experiments, whose continuum limits interpolate between real and $p$-adic
smoothness as a spectral exponent is varied. A real spatial dimension emerges
in the continuum limit if the spectral exponent is negative, while a $p$-adic
extra dimension emerges if the spectral exponent is positive. We demonstrate
Holder continuity conditions, both in momentum space and in position space,
which quantify how smooth or ragged the two-point Green's function is as a
function of the spectral exponent. The underlying discrete dynamics of our
model is defined in terms of a Gaussian partition function as a classical
statistical mechanical lattice model. The couplings between lattice sites are
sparse in the sense that as the number of sites becomes large, a vanishing
fraction of them couple to one another. This sparseness property is useful for
possible experimental realizations of related systems.",Steven S. Gubser
2018-11-14T18:06:57Z,http://arxiv.org/abs/1811.05935v1,Navigating the Cryptocurrency Landscape: An Islamic Perspective,"Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to
generate headlines and intense debate. What started as an underground
experiment by a rag tag group of programmers armed with a Libertarian manifesto
has now resulted in a thriving $230 billion ecosystem, with constant on-going
innovation. Scholars and researchers alike are realizing that cryptocurrencies
are far more than mere technical innovation; they represent a distinct and
revolutionary new economic paradigm tending towards decentralization.
Unfortunately, this bold new universe is little explored from the perspective
of Islamic economics and finance. Our work aims to address these deficiencies.
Our paper makes the following distinct contributions We significantly expand
the discussion on whether cryptocurrencies qualify as ""money"" from an Islamic
perspective and we argue that this debate necessitates rethinking certain
fundamental definitions. We conclude that the cryptocurrency phenomenon, with
its radical new capabilities, may hold considerable opportunity which merits
deeper investigation.",Hina Binte Haq
2019-09-15T17:26:56Z,http://arxiv.org/abs/1909.06840v3,"Comparison of UNet, ENet, and BoxENet for Segmentation of Mast Cells in
  Scans of Histological Slices","Deep neural networks show high accuracy in theproblem of semantic and
instance segmentation of biomedicaldata. However, this approach is
computationally expensive. Thecomputational cost may be reduced with network
simplificationafter training or choosing the proper architecture, which
providessegmentation with less accuracy but does it much faster. In thepresent
study, we analyzed the accuracy and performance ofUNet and ENet architectures
for the problem of semantic imagesegmentation. In addition, we investigated the
ENet architecture by replacing of some convolution layers with
box-convolutionlayers. The analysis performed on the original dataset consisted
of histology slices with mast cells. These cells provide a region
forsegmentation with different types of borders, which vary fromclearly visible
to ragged. ENet was less accurate than UNet byonly about 1-2%, but ENet
performance was 8-15 times faster than UNet one.",Alexander Karimov
2021-02-21T16:50:17Z,http://arxiv.org/abs/2102.10643v2,Safe Reinforcement Learning Using Robust Action Governor,"Reinforcement Learning (RL) is essentially a trial-and-error learning
procedure which may cause unsafe behavior during the
exploration-and-exploitation process. This hinders the application of RL to
real-world control problems, especially to those for safety-critical systems.
In this paper, we introduce a framework for safe RL that is based on
integration of a RL algorithm with an add-on safety supervision module, called
the Robust Action Governor (RAG), which exploits set-theoretic techniques and
online optimization to manage safety-related requirements during learning. We
illustrate this proposed safe RL framework through an application to automotive
adaptive cruise control.",Yutong Li
2022-05-17T06:22:00Z,http://arxiv.org/abs/2205.08112v1,The Fairness of Machine Learning in Insurance: New Rags for an Old Man?,"Since the beginning of their history, insurers have been known to use data to
classify and price risks. As such, they were confronted early on with the
problem of fairness and discrimination associated with data. This issue is
becoming increasingly important with access to more granular and behavioural
data, and is evolving to reflect current technologies and societal concerns. By
looking into earlier debates on discrimination, we show that some algorithmic
biases are a renewed version of older ones, while others show a reversal of the
previous order. Paradoxically, while the insurance practice has not deeply
changed nor are most of these biases new, the machine learning era still deeply
shakes the conception of insurance fairness.",Laurence Barry
2022-09-08T20:24:04Z,http://arxiv.org/abs/2209.04023v1,"An Efficient, Scalable IO Framework for Sparse Data: larcv3","Neutrino physics is one of the fundamental areas of research into the origins
and properties of the Universe. Many experimental neutrino projects use
sophisticated detectors to observe properties of these particles, and have
turned to deep learning and artificial intelligence techniques to analyze their
data. From this, we have developed \texttt{larcv}, a \texttt{C++} and
\texttt{Python} based framework for efficient IO of sparse data with particle
physics applications in mind. We describe in this paper the \texttt{larcv}
framework and some benchmark IO performance tests. \texttt{larcv} is designed
to enable fast and efficient IO of ragged and irregular data, at scale on
modern HPC systems, and is compatible with the most popular open source data
analysis tools in the Python ecosystem.",Corey Adams
2022-10-24T09:20:17Z,http://arxiv.org/abs/2210.13054v1,PARAFAC2-based Coupled Matrix and Tensor Factorizations,"Coupled matrix and tensor factorizations (CMTF) have emerged as an effective
data fusion tool to jointly analyze data sets in the form of matrices and
higher-order tensors. The PARAFAC2 model has shown to be a promising
alternative to the CANDECOMP/PARAFAC (CP) tensor model due to its flexibility
and capability to handle irregular/ragged tensors. While fusion models based on
a PARAFAC2 model coupled with matrix/tensor decompositions have been recently
studied, they are limited in terms of possible regularizations and/or types of
coupling between data sets. In this paper, we propose an algorithmic framework
for fitting PARAFAC2-based CMTF models with the possibility of imposing various
constraints on all modes and linear couplings, using Alternating Optimization
(AO) and the Alternating Direction Method of Multipliers (ADMM). Through
numerical experiments, we demonstrate that the proposed algorithmic approach
accurately recovers the underlying patterns using various constraints and
linear couplings.",Carla Schenker
2023-02-11T04:30:40Z,http://arxiv.org/abs/2302.05598v1,Multi-class Brain Tumor Segmentation using Graph Attention Network,"Brain tumor segmentation from magnetic resonance imaging (MRI) plays an
important role in diagnostic radiology. To overcome the practical issues in
manual approaches, there is a huge demand for building automatic tumor
segmentation algorithms. This work introduces an efficient brain tumor
summation model by exploiting the advancement in MRI and graph neural networks
(GNNs). The model represents the volumetric MRI as a region adjacency graph
(RAG) and learns to identify the type of tumors through a graph attention
network (GAT) -- a variant of GNNs. The ablation analysis conducted on two
benchmark datasets proves that the proposed model can produce competitive
results compared to the leading-edge solutions. It achieves mean dice scores of
0.91, 0.86, 0.79, and mean Hausdorff distances in the 95th percentile (HD95) of
5.91, 6.08, and 9.52 mm, respectively, for whole tumor, core tumor, and
enhancing tumor segmentation on BraTS2021 validation dataset. On average, these
performances are >6\% and >50%, compared to a GNN-based baseline model,
respectively, on dice score and HD95 evaluation metrics.",Dhrumil Patel
2023-05-02T15:33:01Z,http://arxiv.org/abs/2305.01526v1,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset","In this paper, we release a largest ever medical Question Answering (QA)
dataset with 26 million QA pairs. We benchmark many existing approaches in our
dataset in terms of both retrieval and generation. Experimental results show
that the existing models perform far lower than expected and the released
dataset is still challenging in the pre-trained language model era. Moreover,
we also experimentally show the benefit of the proposed dataset in many
aspects: (i) trained models for other QA datasets in a zero-shot fashion; and
(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)
improving existing pre-trained language models by using the QA pairs as a
pre-training corpus in continued training manner. We believe that this dataset
will not only contribute to medical research but also facilitate both the
patients and clinical doctors. See
\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",Jianquan Li
2023-05-05T16:28:03Z,http://arxiv.org/abs/2305.03660v1,"Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT
  models","We propose Retrieval Augmented Generation (RAG) as an approach for automated
radiology report writing that leverages multimodally aligned embeddings from a
contrastively pretrained vision language model for retrieval of relevant
candidate radiology text for an input radiology image and a general domain
generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for
report generation using the relevant radiology text retrieved. This approach
keeps hallucinated generations under check and provides capabilities to
generate report content in the format we desire leveraging the instruction
following capabilities of these generative models. Our approach achieves better
clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score
of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different
clinical settings as it allows to augment the automated radiology report
generation process with content relevant for that setting while also having the
ability to inject user intents and requirements in the prompts as part of the
report generation process to modulate the content and format of the generated
reports as applicable for that clinical setting.",Mercy Ranjit
2023-06-12T03:54:04Z,http://arxiv.org/abs/2306.06851v2,"UniPoll: A Unified Social Media Poll Generation Framework via
  Multi-Objective Optimization","Social media platforms are vital for expressing opinions and understanding
public sentiment, yet many analytical tools overlook passive users who mainly
consume content without engaging actively. To address this, we introduce
UniPoll, an advanced framework designed to automatically generate polls from
social media posts using sophisticated natural language generation (NLG)
techniques. Unlike traditional methods that struggle with social media's
informal and context-sensitive nature, UniPoll leverages enriched contexts from
user comments and employs multi-objective optimization to enhance poll
relevance and engagement. To tackle the inherently noisy nature of social media
data, UniPoll incorporates Retrieval-Augmented Generation (RAG) and synthetic
data generation, ensuring robust performance across real-world scenarios. The
framework surpasses existing models, including T5, ChatGLM3, and GPT-3.5, in
generating coherent and contextually appropriate question-answer pairs.
Evaluated on the Chinese WeiboPolls dataset and the newly introduced English
RedditPolls dataset, UniPoll demonstrates superior cross-lingual and
cross-platform capabilities, making it a potent tool to boost user engagement
and create a more inclusive environment for interaction.",Yixia Li
2023-08-18T13:56:03Z,http://arxiv.org/abs/2308.09564v1,Deep Equilibrium Object Detection,"Query-based object detectors directly decode image features into object
instances with a set of learnable queries. These query vectors are
progressively refined to stable meaningful representations through a sequence
of decoder layers, and then used to directly predict object locations and
categories with simple FFN heads. In this paper, we present a new query-based
object detector (DEQDet) by designing a deep equilibrium decoder. Our DEQ
decoder models the query vector refinement as the fixed point solving of an
{implicit} layer and is equivalent to applying {infinite} steps of refinement.
To be more specific to object decoding, we use a two-step unrolled equilibrium
equation to explicitly capture the query vector refinement. Accordingly, we are
able to incorporate refinement awareness into the DEQ training with the inexact
gradient back-propagation (RAG). In addition, to stabilize the training of our
DEQDet and improve its generalization ability, we devise the deep supervision
scheme on the optimization path of DEQ with refinement-aware
perturbation~(RAP). Our experiments demonstrate DEQDet converges faster,
consumes less memory, and achieves better results than the baseline counterpart
(AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries
achieves the $49.5$ mAP and $33.0$ AP$_s$ on the MS COCO benchmark under
$2\times$ training scheme (24 epochs).",Shuai Wang
2023-10-05T18:01:04Z,http://arxiv.org/abs/2310.03812v2,"Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs","Set-based learning is an essential component of modern deep learning and
network science. Graph Neural Networks (GNNs) and their edge-free counterparts
Deepsets have proven remarkably useful on ragged and topologically challenging
datasets. The key to learning informative embeddings for set members is a
specified aggregation function, usually a sum, max, or mean. We propose
Fishnets, an aggregation strategy for learning information-optimal embeddings
for sets of data for both Bayesian inference and graph aggregation. We
demonstrate that i) Fishnets neural summaries can be scaled optimally to an
arbitrary number of data objects, ii) Fishnets aggregations are robust to
changes in data distribution, unlike standard deepsets, iii) Fishnets saturate
Bayesian information content and extend to regimes where MCMC techniques fail
and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We
show that by adopting a Fishnets aggregation scheme for message passing, GNNs
can achieve state-of-the-art performance versus architecture size on
ogbn-protein data over existing benchmarks with a fraction of learnable
parameters and faster training time.",T. Lucas Makinen
2023-11-10T15:10:36Z,http://arxiv.org/abs/2311.06102v1,"Making LLMs Worth Every Penny: Resource-Limited Text Classification in
  Banking","Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",Lefteris Loukas
2023-11-27T05:27:13Z,http://arxiv.org/abs/2311.15548v1,"Deficiency of Large Language Models in Finance: An Empirical Examination
  of Hallucination","The hallucination issue is recognized as a fundamental deficiency of large
language models (LLMs), especially when applied to fields such as finance,
education, and law. Despite the growing concerns, there has been a lack of
empirical investigation. In this paper, we provide an empirical examination of
LLMs' hallucination behaviors in financial tasks. First, we empirically
investigate LLM model's ability of explaining financial concepts and
terminologies. Second, we assess LLM models' capacity of querying historical
stock prices. Third, to alleviate the hallucination issue, we evaluate the
efficacy of four practical methods, including few-shot learning, Decoding by
Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method
and the prompt-based tool learning method for a function to generate a query
command. Finally, our major finding is that off-the-shelf LLMs experience
serious hallucination behaviors in financial tasks. Therefore, there is an
urgent need to call for research efforts in mitigating LLMs' hallucination.",Haoqiang Kang
2023-11-27T19:17:39Z,http://arxiv.org/abs/2311.16267v2,"Novel Preprocessing Technique for Data Embedding in Engineering Code
  Generation Using Large Language Model","We present four main contributions to enhance the performance of Large
Language Models (LLMs) in generating domain-specific code: (i) utilizing
LLM-based data splitting and data renovation techniques to improve the semantic
representation of embeddings' space; (ii) introducing the Chain of Density for
Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text
Renovation (ATR) algorithm for assessing data renovation reliability; (iii)
developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt
technique; and (iv) effectively refactoring existing scripts to generate new
and high-quality scripts with LLMs. By using engineering simulation software
RedHawk-SC as a case study, we demonstrate the effectiveness of our data
pre-processing method for expanding and categorizing scripts. When combined
with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG)
method in retrieving more relevant information, ultimately achieving a 73.33%
""Percentage of Correct Lines"" for code generation problems in MapReduce
applications.",Yu-Chen Lin
2023-11-28T06:18:54Z,http://arxiv.org/abs/2311.16543v3,"RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language
  Models","This paper presents RTLFixer, a novel framework enabling automatic syntax
errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's
promising capabilities, our analysis indicates that approximately 55% of errors
in LLM-generated Verilog are syntax-related, leading to compilation failures.
To tackle this issue, we introduce a novel debugging framework that employs
Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act
as autonomous agents in interactively debugging the code with feedback. This
framework demonstrates exceptional proficiency in resolving syntax errors,
successfully correcting about 98.5% of compilation errors in our debugging
dataset, comprising 212 erroneous implementations derived from the VerilogEval
benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates
in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.",Yun-Da Tsai
2023-11-29T15:02:46Z,http://arxiv.org/abs/2311.17696v4,"How to Build an AI Tutor that Can Adapt to Any Course and Provide
  Accurate Answers Using Large Language Model and Retrieval-Augmented
  Generation","This paper proposes a low-code solution to build an AI tutor that leverages
advanced AI techniques to provide accurate and contextually relevant responses
in a personalized learning environment. The OpenAI Assistants API allows AI
Tutor to easily embed, store, retrieve, and manage files and chat history,
enabling a low-code solution. Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG) technology generate sophisticated answers
based on course-specific materials. The application efficiently organizes and
retrieves relevant information through vector embedding and similarity-based
retrieval algorithms. The AI Tutor prototype demonstrates its ability to
generate relevant, accurate answers with source citations. It represents a
significant advancement in technology-enhanced tutoring systems, democratizing
access to high-quality, customized educational support in higher education.",Chenxi Dong
2023-12-19T18:56:52Z,http://arxiv.org/abs/2312.12430v3,Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP,"In recent RAG approaches, rerankers play a pivotal role in refining retrieval
accuracy with the ability of revealing logical relations for each pair of query
and text. However, existing rerankers are required to repeatedly encode the
query and a large number of long retrieved text. This results in high
computational costs and limits the number of retrieved text, hindering
accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker
via Broadcasting Query Encoder, a novel technique for title reranking that
achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we
introduce Sigmoid Trick, a novel loss function customized for title reranking.
Combining both techniques, we empirically validated their effectiveness,
achieving state-of-the-art results on all four datasets we experimented with
from the KILT knowledge benchmark.",Ziyi Chen
2024-01-03T00:09:34Z,http://arxiv.org/abs/2401.01469v1,"Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation","Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.",Walid Saba
2024-01-16T02:11:35Z,http://arxiv.org/abs/2401.10286v3,"Code-Based English Models Surprising Performance on Chinese QA Pair
  Extraction Task","In previous studies, code-based models have consistently outperformed
text-based models in reasoning-intensive scenarios. When generating our
knowledge base for Retrieval-Augmented Generation (RAG), we observed that
code-based models also perform exceptionally well in Chinese QA Pair Extraction
task. Further, our experiments and the metrics we designed discovered that
code-based models containing a certain amount of Chinese data achieve even
better performance. Additionally, the capabilities of code-based English models
in specified Chinese tasks offer a distinct perspective for discussion on the
philosophical ""Chinese Room"" thought experiment.",Linghan Zheng
2024-02-06T18:01:29Z,http://arxiv.org/abs/2402.04206v1,"Explaining Autonomy: Enhancing Human-Robot Interaction through
  Explanation Generation with Large Language Models","This paper introduces a system designed to generate explanations for the
actions performed by an autonomous robot in Human-Robot Interaction (HRI).
Explainability in robotics, encapsulated within the concept of an eXplainable
Autonomous Robot (XAR), is a growing research area. The work described in this
paper aims to take advantage of the capabilities of Large Language Models
(LLMs) in performing natural language processing tasks. This study focuses on
the possibility of generating explanations using such models in combination
with a Retrieval Augmented Generation (RAG) method to interpret data gathered
from the logs of autonomous systems. In addition, this work also presents a
formalization of the proposed explanation system. It has been evaluated through
a navigation test from the European Robotics League (ERL), a Europe-wide social
robotics competition. Regarding the obtained results, a validation
questionnaire has been conducted to measure the quality of the explanations
from the perspective of technical users. The results obtained during the
experiment highlight the potential utility of LLMs in achieving explanatory
capabilities in robots.",David SobrÃ­n-Hidalgo
2024-02-15T12:12:19Z,http://arxiv.org/abs/2402.09906v2,Generative Representational Instruction Tuning,"All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.",Niklas Muennighoff
2024-02-16T16:15:01Z,http://arxiv.org/abs/2402.10790v2,"In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs
  Miss","This paper addresses the challenge of processing long documents using
generative transformer models. To evaluate different approaches, we introduce
BABILong, a new benchmark designed to assess model capabilities in extracting
and processing distributed facts within extensive texts. Our evaluation, which
includes benchmarks for GPT-4 and RAG, reveals that common methods are
effective only for sequences up to $10^4$ elements. In contrast, fine-tuning
GPT-2 with recurrent memory augmentations enables it to handle tasks involving
up to $11\times 10^6$ elements. This achievement marks a substantial leap, as
it is by far the longest input processed by any neural network model to date,
demonstrating a significant improvement in the processing capabilities for long
sequences.",Yuri Kuratov
2024-02-16T19:28:52Z,http://arxiv.org/abs/2402.11035v3,Dense Passage Retrieval: Is it Retrieving?,"Dense passage retrieval (DPR) is the first step in the retrieval augmented
generation (RAG) paradigm for improving the performance of large language
models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of
the embeddings between queries and relevant textual data. A deeper
understanding of DPR fine-tuning will be required to fundamentally unlock the
full potential of this approach. In this work, we explore DPR-trained models
mechanistically by using a combination of probing, layer activation analysis,
and model editing. Our experiments show that DPR training decentralizes how
knowledge is stored in the network, creating multiple access pathways to the
same information. We also uncover a limitation in this training style: the
internal knowledge of the pre-trained model bounds what the retrieval model can
retrieve. These findings suggest a few possible directions for dense retrieval:
(1) expose the DPR training process to more knowledge so more can be
decentralized, (2) inject facts as decentralized representations, (3) model and
incorporate knowledge uncertainty in the retrieval process, and (4) directly
map internal model knowledge to a knowledge base.",Benjamin Reichman
2024-02-17T02:21:44Z,http://arxiv.org/abs/2402.11166v1,"GenDec: A robust generative Question-decomposition method for Multi-hop
  reasoning","Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex
questions and find multiple relevant supporting facts. However, Existing large
language models'(LLMs) reasoning ability in multi-hop question answering
remains exploration, which is inadequate in answering multi-hop questions.
Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach
the right final answer. In this paper, we propose a \textbf{gen}erative
question \textbf{dec}omposition method (GenDec) from the perspective of
explainable QA by generating independent and complete sub-questions based on
incorporating additional extracted evidence for enhancing LLMs' reasoning
ability in RAG. To demonstrate the impact, generalization, and robustness of
Gendec, we conduct two experiments, the first is combining GenDec with small QA
systems on paragraph retrieval and QA tasks. We secondly examine the reasoning
capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5
combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,
MuSiQue, and PokeMQA datasets.",Jian Wu
2024-02-19T02:15:34Z,http://arxiv.org/abs/2402.11782v2,What Evidence Do Language Models Find Convincing?,"Retrieval-augmented language models are being increasingly tasked with
subjective, contentious, and conflicting queries such as ""is aspartame linked
to cancer"". To resolve these ambiguous queries, one must search through a large
range of websites and consider ""which, if any, of this evidence do I find
convincing?"". In this work, we study how LLMs answer this question. In
particular, we construct ConflictingQA, a dataset that pairs controversial
queries with a series of real-world evidence documents that contain different
facts (e.g., quantitative results), argument styles (e.g., appeals to
authority), and answers (Yes or No). We use this dataset to perform sensitivity
and counterfactual analyses to explore which text features most affect LLM
predictions. Overall, we find that current models rely heavily on the relevance
of a website to the query, while largely ignoring stylistic features that
humans find important such as whether a text contains scientific references or
is written with a neutral tone. Taken together, these results highlight the
importance of RAG corpus quality (e.g., the need to filter misinformation), and
possibly even a shift in how LLMs are trained to better align with human
judgements.",Alexander Wan
2024-02-28T17:29:27Z,http://arxiv.org/abs/2402.18502v1,"Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware
  Classification","Employing Large Language Models (LLM) in various downstream applications such
as classification is crucial, especially for smaller companies lacking the
expertise and resources required for fine-tuning a model. Fairness in LLMs
helps ensure inclusivity, equal representation based on factors such as race,
gender and promotes responsible AI deployment. As the use of LLMs has become
increasingly prevalent, it is essential to assess whether LLMs can generate
fair outcomes when subjected to considerations of fairness. In this study, we
introduce a framework outlining fairness regulations aligned with various
fairness definitions, with each definition being modulated by varying degrees
of abstraction. We explore the configuration for in-context learning and the
procedure for selecting in-context demonstrations using RAG, while
incorporating fairness rules into the process. Experiments conducted with
different LLMs indicate that GPT-4 delivers superior results in terms of both
accuracy and fairness compared to other models. This work is one of the early
attempts to achieve fairness in prediction tasks by utilizing LLMs through
in-context learning.",Garima Chhikara
2024-02-28T17:38:06Z,http://arxiv.org/abs/2402.18510v4,"RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval","This paper investigates the gap in representation powers of Recurrent Neural
Networks (RNNs) and Transformers in the context of solving algorithmic
problems. We focus on understanding whether RNNs, known for their memory
efficiency in handling long sequences, can match the performance of
Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.
Our theoretical analysis reveals that CoT improves RNNs but is insufficient to
close the gap with Transformers. A key bottleneck lies in the inability of RNNs
to perfectly retrieve information from the context, even with CoT: for several
tasks that explicitly or implicitly require this capability, such as
associative recall and determining if a graph is a tree, we prove that RNNs are
not expressive enough to solve the tasks while Transformers can solve them with
ease. Conversely, we prove that adopting techniques to enhance the in-context
retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)
and adding a single Transformer layer, can elevate RNNs to be capable of
solving all polynomial-time solvable problems with CoT, hence closing the
representation gap with Transformers.",Kaiyue Wen
2024-03-06T15:40:30Z,http://arxiv.org/abs/2403.03792v2,"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt
  Injection Attacks","We introduce a new family of prompt injection attacks, termed Neural Exec.
Unlike known attacks that rely on handcrafted strings (e.g., ""Ignore previous
instructions and...""), we show that it is possible to conceptualize the
creation of execution triggers as a differentiable search problem and use
learning-based methods to autonomously generate them.
  Our results demonstrate that a motivated adversary can forge triggers that
are not only drastically more effective than current handcrafted ones but also
exhibit inherent flexibility in shape, properties, and functionality. In this
direction, we show that an attacker can design and generate Neural Execs
capable of persisting through multi-stage preprocessing pipelines, such as in
the case of Retrieval-Augmented Generation (RAG)-based applications. More
critically, our findings show that attackers can produce triggers that deviate
markedly in form and shape from any known attack, sidestepping existing
blacklist-based detection and sanitation approaches.",Dario Pasquini
2024-03-06T17:48:06Z,http://arxiv.org/abs/2403.03888v3,FaaF: Facts as a Function for the evaluation of generated text,"The demand for accurate and efficient verification of information in texts
generated by large language models (LMs) is at an all-time high, but remains
unresolved. Recent efforts have focused on extracting and verifying atomic
facts from these texts via prompting LM evaluators. However, we demonstrate
that this method of prompting is unreliable when faced with incomplete or
inaccurate reference information. We introduce Facts as a Function (FaaF), a
new approach to the fact verification task that leverages the function-calling
capabilities of LMs. FaaF significantly enhances the ability of LMs to identify
unsupported facts in texts, while also improving efficiency and significantly
lowering costs compared to prompt-based methods. Additionally, we propose a
framework for evaluating factual recall in Retrieval Augmented Generation (RAG)
systems, which we employ to compare prompt-based and FaaF methods using various
LMs under challenging conditions.",Vasileios Katranidis
2024-03-07T08:25:46Z,http://arxiv.org/abs/2403.04307v3,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,"Hallucinations pose a significant challenge to the reliability of large
language models (LLMs) in critical domains. Recent benchmarks designed to
assess LLM hallucinations within conventional NLP tasks, such as
knowledge-intensive question answering (QA) and summarization, are insufficient
for capturing the complexities of user-LLM interactions in dynamic, real-world
settings. To address this gap, we introduce HaluEval-Wild, the first benchmark
specifically designed to evaluate LLM hallucinations in the wild. We
meticulously collect challenging (adversarially filtered by Alpaca) user
queries from ShareGPT, an existing real-world user-LLM interaction datasets, to
evaluate the hallucination rates of various LLMs. Upon analyzing the collected
queries, we categorize them into five distinct types, which enables a
fine-grained analysis of the types of hallucinations LLMs exhibit, and
synthesize the reference answers with the powerful GPT-4 model and
retrieval-augmented generation (RAG). Our benchmark offers a novel approach
towards enhancing our comprehension of and improving LLM reliability in
scenarios reflective of real-world interactions. Our benchmark is available at
https://github.com/HaluEval-Wild/HaluEval-Wild.",Zhiying Zhu
2024-03-11T16:01:05Z,http://arxiv.org/abs/2403.06840v2,"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation
  via Iterative Self-Feedback","Large language models (LLMs) demonstrate exceptional performance in numerous
tasks but still heavily rely on knowledge stored in their parameters. Moreover,
updating this knowledge incurs high training costs. Retrieval-augmented
generation (RAG) methods address this issue by integrating external knowledge.
The model can answer questions it couldn't previously by retrieving knowledge
relevant to the query. This approach improves performance in certain scenarios
for specific tasks. However, if irrelevant texts are retrieved, it may impair
model performance. In this paper, we propose Retrieval Augmented Iterative
Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and
processes them in three submodules to enhance the model's problem-solving
capabilities. Experiments show that our method outperforms existing benchmarks,
performing well on models like GPT3.5, Llama2, significantly enhancing factual
reasoning capabilities and reducing hallucinations.",Yanming Liu
2024-03-14T06:17:20Z,http://arxiv.org/abs/2403.09125v5,"Exploring the Capabilities and Limitations of Large Language Models in
  the Electric Energy Sector","Large Language Models (LLMs) as chatbots have drawn remarkable attention
thanks to their versatile capability in natural language processing as well as
in a wide range of tasks. While there has been great enthusiasm towards
adopting such foundational model-based artificial intelligence tools in all
sectors possible, the capabilities and limitations of such LLMs in improving
the operation of the electric energy sector need to be explored, and this
article identifies fruitful directions in this regard. Key future research
directions include data collection systems for fine-tuning LLMs, embedding
power system-specific tools in the LLMs, and retrieval augmented generation
(RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in
safety-critical use cases.",Subir Majumder
2024-03-15T09:54:04Z,http://arxiv.org/abs/2403.10153v3,"Improving Medical Multi-modal Contrastive Learning with Expert
  Annotations","We introduce eCLIP, an enhanced version of the CLIP model that integrates
expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key
challenges in contrastive multi-modal medical imaging analysis, notably data
scarcity and the ""modality gap"" -- a significant disparity between image and
text embeddings that diminishes the quality of representations and hampers
cross-modal interoperability. eCLIP integrates a heatmap processor and
leverages mixup augmentation to efficiently utilize the scarce expert
annotations, thus boosting the model's learning effectiveness. eCLIP is
designed to be generally applicable to any variant of CLIP without requiring
any modifications of the core architecture. Through detailed evaluations across
several tasks, including zero-shot inference, linear probing, cross-modal
retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using
a frozen Large Language Model, eCLIP showcases consistent improvements in
embedding quality. The outcomes reveal enhanced alignment and uniformity,
affirming eCLIP's capability to harness high-quality annotations for enriched
multi-modal analysis in the medical imaging domain.",Yogesh Kumar
2024-04-02T09:01:32Z,http://arxiv.org/abs/2404.01744v5,Octopus v2: On-device language model for super agent,"Language models have shown effectiveness in a variety of software
applications, particularly in tasks related to automatic workflow. These models
possess the crucial ability to call functions, which is essential in creating
AI agents. Despite the high performance of large-scale language models in cloud
environments, they are often associated with concerns over privacy and cost.
Current on-device models for function calling face issues with latency and
accuracy. Our research presents a new method that empowers an on-device model
with 2 billion parameters to surpass the performance of GPT-4 in both accuracy
and latency, and decrease the context length by 95\%. When compared to Llama-7B
with a RAG-based function calling mechanism, our method enhances latency by
35-fold. This method reduces the latency to levels deemed suitable for
deployment across a variety of edge devices in production environments,
aligning with the performance requisites for real-world applications.",Wei Chen
2024-04-02T21:35:54Z,http://arxiv.org/abs/2404.02319v2,"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization","In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .",Tobias Schnabel
2024-04-03T05:31:59Z,http://arxiv.org/abs/2404.02474v1,uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?,"Inspired by human cognition, Jiang et al.(2023c) create a benchmark for
assessing LLMs' lateral thinking-thinking outside the box. Building upon this
benchmark, we investigate how different prompting methods enhance LLMs'
performance on this task to reveal their inherent power for outside-the-box
thinking ability. Through participating in SemEval-2024, task 9, Sentence
Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)
and direct prompting, enhancing with informative descriptions, and employing
contextualizing prompts using a retrieval augmented generation (RAG) pipeline.
Our experiments involve three LLMs including GPT-3.5, GPT-4, and
Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and
options using GPT-4, validated by humans for quality. Findings indicate that
compressed informative prompts enhance performance. Dynamic in-context learning
enhances model performance significantly. Furthermore, fine-tuning Zephyr on
our dataset enhances performance across other commonsense datasets,
underscoring the value of innovative thinking.",Pouya Sadeghi
2024-04-06T05:44:53Z,http://arxiv.org/abs/2404.04510v1,"IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe
  Biomedical Natural Language Inference for Clinical Trials","Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.",Shreyasi Mandal
2024-04-08T15:00:36Z,http://arxiv.org/abs/2404.05587v2,"Enhancing Software-Related Information Extraction via Single-Choice
  Question Answering with Large Language Models","This paper describes our participation in the Shared Task on Software
Mentions Disambiguation (SOMD), with a focus on improving relation extraction
in scholarly texts through generative Large Language Models (LLMs) using
single-choice question-answering. The methodology prioritises the use of
in-context learning capabilities of GLMs to extract software-related entities
and their descriptive attributes, such as distributive information. Our
approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for
Named Entity Recognition (NER) and Attributive NER to identify relationships
between extracted software entities, providing a structured solution for
analysing software citations in academic literature. The paper provides a
detailed description of our approach, demonstrating how using GLMs in a
single-choice QA paradigm can greatly enhance IE methodologies. Our
participation in the SOMD shared task highlights the importance of precise
software citation practices and showcases our system's ability to overcome the
challenges of disambiguating and extracting relationships between software
mentions. This sets the groundwork for future research and development in this
field.",Wolfgang Otto
2024-04-09T04:20:27Z,http://arxiv.org/abs/2404.06004v1,"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free
  Information Retrieval","In approximate nearest neighbor search (ANNS) methods based on approximate
proximity graphs, DiskANN achieves good recall-speed balance for large-scale
datasets using both of RAM and storage. Despite it claims to save memory usage
by loading compressed vectors by product quantization (PQ), its memory usage
increases in proportion to the scale of datasets. In this paper, we propose
All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the
compressed vectors to storage. Our method achieves $\sim$10 MB memory usage in
query search even with billion-scale datasets with minor performance
degradation. AiSAQ also reduces the index load time before query search, which
enables the index switch between muitiple billion-scale datasets and
significantly enhances the flexibility of retrieval-augmented generation (RAG).
This method is applicable to all graph-based ANNS algorithms and can be
combined with higher-spec ANNS methods in the future.",Kento Tatsuno
2024-04-09T13:02:22Z,http://arxiv.org/abs/2404.06278v1,"Dimensionality Reduction in Sentence Transformer Vector Databases with
  Fast Fourier Transform","Dimensionality reduction in vector databases is pivotal for streamlining AI
data management, enabling efficient storage, faster computation, and improved
model performance. This paper explores the benefits of reducing vector database
dimensions, with a focus on computational efficiency and overcoming the curse
of dimensionality. We introduce a novel application of Fast Fourier Transform
(FFT) to dimensionality reduction, a method previously underexploited in this
context. By demonstrating its utility across various AI domains, including
Retrieval-Augmented Generation (RAG) models and image processing, this
FFT-based approach promises to improve data retrieval processes and enhance the
efficiency and scalability of AI solutions. The incorporation of FFT may not
only optimize operations in real-time processing and recommendation systems but
also extend to advanced image processing techniques, where dimensionality
reduction can significantly improve performance and analysis efficiency. This
paper advocates for the broader adoption of FFT in vector database management,
marking a significant stride towards addressing the challenges of data volume
and complexity in AI research and applications. Unlike many existing
approaches, we directly handle the embedding vectors produced by the model
after processing a test input.",Vitaly Bulgakov
2024-04-10T02:02:34Z,http://arxiv.org/abs/2404.06680v1,"Onco-Retriever: Generative Classifier for Retrieval of EHR Records in
  Oncology","Retrieving information from EHR systems is essential for answering specific
questions about patient journeys and improving the delivery of clinical care.
Despite this fact, most EHR systems still rely on keyword-based searches. With
the advent of generative large language models (LLMs), retrieving information
can lead to better search and summarization capabilities. Such retrievers can
also feed Retrieval-augmented generation (RAG) pipelines to answer any query.
However, the task of retrieving information from EHR real-world clinical data
contained within EHR systems in order to solve several downstream use cases is
challenging due to the difficulty in creating query-document support pairs. We
provide a blueprint for creating such datasets in an affordable manner using
large language models. Our method results in a retriever that is 30-50 F-1
points better than propriety counterparts such as Ada and Mistral for oncology
data elements. We further compare our model, called Onco-Retriever, against
fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation
on real-world EHR data along with latency analysis of the different models and
provide a path forward for healthcare organizations to build domain-specific
retrievers.",Shashi Kant Gupta
2024-04-10T22:26:26Z,http://arxiv.org/abs/2404.07376v2,LLMs in Biomedicine: A study on clinical Named Entity Recognition,"Large Language Models (LLMs) demonstrate remarkable versatility in various
NLP tasks but encounter distinct challenges in biomedical due to the
complexities of language and data scarcity. This paper investigates LLMs
application in the biomedical domain by exploring strategies to enhance their
performance for the NER task. Our study reveals the importance of meticulously
designed prompts in the biomedical. Strategic selection of in-context examples
yields a marked improvement, offering ~15-20\% increase in F1 score across all
benchmark datasets for biomedical few-shot NER. Additionally, our results
indicate that integrating external biomedical knowledge via prompting
strategies can enhance the proficiency of general-purpose LLMs to meet the
specialized needs of biomedical NER. Leveraging a medical knowledge base, our
proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can
boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at
\url{https://github.com/masoud-monajati/LLM_Bio_NER}",Masoud Monajatipoor
2024-04-17T10:00:56Z,http://arxiv.org/abs/2404.11216v2,"Position Engineering: Boosting Large Language Models through Positional
  Information Manipulation","The performance of large language models (LLMs) is significantly influenced
by the quality of the prompts provided. In response, researchers have developed
enormous prompt engineering strategies aimed at modifying the prompt text to
enhance task performance. In this paper, we introduce a novel technique termed
position engineering, which offers a more efficient way to guide large language
models. Unlike prompt engineering, which requires substantial effort to modify
the text provided to LLMs, position engineering merely involves altering the
positional information in the prompt without modifying the text itself. We have
evaluated position engineering in two widely-used LLM scenarios:
retrieval-augmented generation (RAG) and in-context learning (ICL). Our
findings show that position engineering substantially improves upon the
baseline in both cases. Position engineering thus represents a promising new
strategy for exploiting the capabilities of large language models.",Zhiyuan He
2024-04-17T18:13:16Z,http://arxiv.org/abs/2404.11672v1,MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory,"While current large language models (LLMs) demonstrate some capabilities in
knowledge-intensive tasks, they are limited by relying on their parameters as
an implicit storage mechanism. As a result, they struggle with infrequent
knowledge and temporal degradation. In addition, the uninterpretable nature of
parametric memorization makes it challenging to understand and prevent
hallucination. Parametric memory pools and model editing are only partial
solutions. Retrieval Augmented Generation (RAG) $\unicode{x2013}$ though
non-parametric $\unicode{x2013}$ has its own limitations: it lacks structure,
complicates interpretability and makes it hard to effectively manage stored
knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs
by integrating a structured and explicit read-and-write memory module. MemLLM
tackles the aforementioned challenges by enabling dynamic interaction with the
memory and improving the LLM's capabilities in using stored knowledge. Our
experiments indicate that MemLLM enhances the LLM's performance and
interpretability, in language modeling in general and knowledge-intensive tasks
in particular. We see MemLLM as an important step towards making LLMs more
grounded and factual through memory augmentation.",Ali Modarressi
2024-04-22T05:46:40Z,http://arxiv.org/abs/2404.13892v2,Retrieval-Augmented Audio Deepfake Detection,"With recent advances in speech synthesis including text-to-speech (TTS) and
voice conversion (VC) systems enabling the generation of ultra-realistic audio
deepfakes, there is growing concern about their potential misuse. However, most
deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a
single model, resulting in performance bottlenecks and transparency issues.
Inspired by retrieval-augmented generation (RAG), we propose a
retrieval-augmented detection (RAD) framework that augments test samples with
similar retrieved samples for enhanced detection. We also extend the
multi-fusion attentive classifier to integrate it with our proposed RAD
framework. Extensive experiments show the superior performance of the proposed
RAD framework over baseline methods, achieving state-of-the-art results on the
ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.
Further sample analysis indicates that the retriever consistently retrieves
samples mostly from the same speaker with acoustic characteristics highly
consistent with the query audio, thereby improving detection performance.",Zuheng Kang
2024-05-02T15:06:18Z,http://arxiv.org/abs/2405.01359v1,GAIA: A General AI Assistant for Intelligent Accelerator Operations,"Large-scale machines like particle accelerators are usually run by a team of
experienced operators. In case of a particle accelerator, these operators
possess suitable background knowledge on both accelerator physics and the
technology comprising the machine. Due to the complexity of the machine,
particular subsystems of the machine are taken care of by experts, who the
operators can turn to. In this work the reasoning and action (ReAct) prompting
paradigm is used to couple an open-weights large language model (LLM) with a
high-level machine control system framework and other tools, e.g. the
electronic logbook or machine design documentation. By doing so, a multi-expert
retrieval augmented generation (RAG) system is implemented, which assists
operators in knowledge retrieval tasks, interacts with the machine directly if
needed, or writes high level control system scripts. This consolidation of
expert knowledge and machine interaction can simplify and speed up machine
operation tasks for both new and experienced human operators.",Frank Mayet
2024-05-07T23:44:09Z,http://arxiv.org/abs/2405.04717v1,Remote Diffusion,"I explored adapting Stable Diffusion v1.5 for generating domain-specific
satellite and aerial images in remote sensing. Recognizing the limitations of
existing models like Midjourney and Stable Diffusion, trained primarily on
natural RGB images and lacking context for remote sensing, I used the RSICD
dataset to train a Stable Diffusion model with a loss of 0.2. I incorporated
descriptive captions from the dataset for text-conditioning. Additionally, I
created a synthetic dataset for a Land Use Land Classification (LULC) task,
employing prompting techniques with RAG and ChatGPT and fine-tuning a
specialized remote sensing LLM. However, I faced challenges with prompt quality
and model performance. I trained a classification model (ResNet18) on the
synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a
baseline. Quantitative evaluation through FID scores and qualitative feedback
from domain experts assessed the realism and quality of the generated images
and dataset. Despite extensive fine-tuning and dataset iterations, results
indicated subpar image quality and realism, as indicated by high FID scores and
domain-expert evaluation. These findings call attention to the potential of
diffusion models in remote sensing while highlighting significant challenges
related to insufficient pretraining data and computational resources.",Kunal Sunil Kasodekar
2024-05-08T04:07:38Z,http://arxiv.org/abs/2405.06697v1,Automated Conversion of Static to Dynamic Scheduler via Natural Language,"In this paper, we explore the potential application of Large Language Models
(LLMs) that will automatically model constraints and generate code for dynamic
scheduling problems given an existing static model. Static scheduling problems
are modelled and coded by optimization experts. These models may be easily
obsoleted as the underlying constraints may need to be fine-tuned in order to
reflect changes in the scheduling rules. Furthermore, it may be necessary to
turn a static model into a dynamic one in order to cope with disturbances in
the environment. In this paper, we propose a Retrieval-Augmented Generation
(RAG) based LLM model to automate the process of implementing constraints for
Dynamic Scheduling (RAGDyS), without seeking help from an optimization modeling
expert. Our framework aims to minimize technical complexities related to
mathematical modelling and computational workload for end-users, thereby
allowing end-users to quickly obtain a new schedule close to the original
schedule with changes reflected by natural language constraint descriptions.",Paul Mingzheng Tang
2024-05-13T17:44:05Z,http://arxiv.org/abs/2405.07963v2,"PyZoBot: A Platform for Conversational Information Extraction and
  Synthesis from Curated Zotero Reference Libraries through Advanced
  Retrieval-Augmented Generation","The exponential growth of scientific literature has resulted in information
overload, challenging researchers to effectively synthesize relevant
publications. This paper explores the integration of traditional reference
management software with advanced computational techniques, including Large
Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an
AI-driven platform developed in Python, incorporating Zoteros reference
management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge
extraction and synthesis from extensive human-curated scientific literature
databases. It demonstrates proficiency in handling complex natural language
queries, integrating data from multiple sources, and meticulously presenting
references to uphold research integrity and facilitate further exploration. By
leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot
offers an effective solution to manage information overload and keep pace with
rapid scientific advancements. The development of such AI-enhanced tools
promises significant improvements in research efficiency and effectiveness
across various disciplines.",Suad Alshammari
2024-05-15T07:48:10Z,http://arxiv.org/abs/2405.09161v2,"Exploring the Potential of Large Language Models for Automation in
  Technical Customer Service","Purpose: The purpose of this study is to investigate the potential of Large
Language Models (LLMs) in transforming technical customer service (TCS) through
the automation of cognitive tasks. Design/Methodology/Approach: Using a
prototyping approach, the research assesses the feasibility of automating
cognitive tasks in TCS with LLMs, employing real-world technical incident data
from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks
such as translation, summarization, and content generation can be effectively
automated with LLMs like GPT-4, while higher-level tasks such as reasoning
require more advanced technological approaches such as Retrieval-Augmented
Generation (RAG) or finetuning ; furthermore, the study underscores the
significance of data ecosystems in enabling more complex cognitive tasks by
fostering data sharing among various actors involved. Originality/Value: This
study contributes to the emerging theory on LLM potential and technical
feasibility in service management, providing concrete insights for operators of
TCS units and highlighting the need for further research to address limitations
and validate the applicability of LLMs across different domains.",Jochen Wulf
2024-05-16T10:53:31Z,http://arxiv.org/abs/2405.09980v1,FinTextQA: A Dataset for Long-form Financial Question Answering,"Accurate evaluation of financial question answering (QA) systems necessitates
a comprehensive dataset encompassing diverse question types and contexts.
However, current financial QA datasets lack scope diversity and question
complexity. This work introduces FinTextQA, a novel dataset for long-form
question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,
source-attributed QA pairs extracted and selected from finance textbooks and
government agency websites.Moreover, we developed a Retrieval-Augmented
Generation (RAG)-based LFQA system, comprising an embedder, retriever,
reranker, and generator. A multi-faceted evaluation approach, including human
ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the
performance of different LFQA system configurations under heightened noisy
conditions. The results indicate that: (1) Among all compared generators,
Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The
most effective system configuration on our dataset involved setting the
embedder, retriever, reranker, and generator as Ada2, Automated Merged
Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are
less susceptible to noise after the length of contexts reaching a specific
threshold.",Jian Chen
2024-05-25T05:45:55Z,http://arxiv.org/abs/2405.16072v4,"SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS
  Design Generation","In this paper, we introduce SynthAI, a new method for the automated creation
of High-Level Synthesis (HLS) designs. SynthAI integrates ReAct agents,
Chain-of-Thought (CoT) prompting, web search technologies, and the
Retrieval-Augmented Generation (RAG) framework within a structured decision
graph. This innovative approach enables the systematic decomposition of complex
hardware design tasks into multiple stages and smaller, manageable modules. As
a result, SynthAI produces synthesizable designs that closely adhere to
user-specified design objectives and functional requirements. We further
validate the capabilities of SynthAI through several case studies, highlighting
its proficiency in generating complex, multi-module logic designs from a single
initial prompt. The SynthAI code is provided via the following repo:
\url{https://github.com/sarashs/FPGA_AGI}",Seyed Arash Sheikholeslam
2024-05-27T13:16:29Z,http://arxiv.org/abs/2405.17147v1,"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability","The rapid advancement of Large Language Models (LLMs) has significantly
impacted human-computer interaction, epitomized by the release of GPT-4o, which
introduced comprehensive multi-modality capabilities. In this paper, we first
explored the deployment strategies, economic considerations, and sustainability
challenges associated with the state-of-the-art LLMs. More specifically, we
discussed the deployment debate between Retrieval-Augmented Generation (RAG)
and fine-tuning, highlighting their respective advantages and limitations.
After that, we quantitatively analyzed the requirement of xPUs in training and
inference. Additionally, for the tokenomics of LLM services, we examined the
balance between performance and cost from the quality of experience (QoE)'s
perspective of end users. Lastly, we envisioned the future hybrid architecture
of LLM processing and its corresponding sustainability concerns, particularly
in the environmental carbon footprint impact. Through these discussions, we
provided a comprehensive overview of the operational and strategic
considerations essential for the responsible development and deployment of
LLMs.",Haiwei Dong
2024-05-26T06:45:39Z,http://arxiv.org/abs/2405.19366v2,"ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with
  LLM-Enhanced Cardiological Text","The utilization of deep learning on electrocardiogram (ECG) analysis has
brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.
By leveraging the capabilities of deep learning in semantic understanding,
especially in feature extraction and representation learning, this study
introduces a new multimodal contrastive pretaining framework that aims to
improve the quality and robustness of learned representations of 12-lead ECG
signals. Our framework comprises two key components, including Cardio Query
Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a
retrieval-augmented generation (RAG) pipeline to leverage large language models
(LLMs) and external medical knowledge to generate detailed textual descriptions
of ECGs. The generated text is enriched with information about demographics and
waveform patterns. ESI integrates both contrastive and captioning loss to
pretrain ECG encoders for enhanced representations. We validate our approach
through various downstream tasks, including arrhythmia detection and ECG-based
subject identification. Our experimental results demonstrate substantial
improvements over strong baselines in these tasks. These baselines encompass
supervised and self-supervised learning methods, as well as prior multimodal
pretraining approaches.",Han Yu
2024-05-29T23:11:53Z,http://arxiv.org/abs/2405.19563v1,Unlearning Climate Misinformation in Large Language Models,"Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.",Michael Fore
2024-06-02T06:48:43Z,http://arxiv.org/abs/2406.00638v1,"COS-Mix: Cosine Similarity and Distance Fusion for Improved Information
  Retrieval","This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented
Generation (RAG) that integrates cosine similarity and cosine distance measures
to improve retrieval performance, particularly for sparse data. The traditional
cosine similarity measure is widely used to capture the similarity between
vectors in high-dimensional spaces. However, it has been shown that this
measure can yield arbitrary results in certain scenarios. To address this
limitation, we incorporate cosine distance measures to provide a complementary
perspective by quantifying the dissimilarity between vectors. Our approach is
experimented on proprietary data, unlike recent publications that have used
open-source datasets. The proposed method demonstrates enhanced retrieval
performance and provides a more comprehensive understanding of the semantic
relationships between documents or items. This hybrid strategy offers a
promising solution for efficiently and accurately retrieving relevant
information in knowledge-intensive applications, leveraging techniques such as
BM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based
retrieval to facilitate efficient information retrieval.",Kush Juvekar
2024-05-27T09:52:54Z,http://arxiv.org/abs/2406.01607v2,"Recent advances in text embedding: A Comprehensive Review of
  Top-Performing Methods on the MTEB Benchmark","Text embedding methods have become increasingly popular in both industrial
and academic fields due to their critical role in a variety of natural language
processing tasks. The significance of universal text embeddings has been
further highlighted with the rise of Large Language Models (LLMs) applications
such as Retrieval-Augmented Systems (RAGs). While previous models have
attempted to be general-purpose, they often struggle to generalize across tasks
and domains. However, recent advancements in training data quantity, quality
and diversity; synthetic data generation from LLMs as well as using LLMs as
backbones encourage great improvements in pursuing universal text embeddings.
In this paper, we provide an overview of the recent advances in universal text
embedding models with a focus on the top performing text embeddings on Massive
Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we
highlight the key contributions and limitations in this area, and propose
potentially inspiring future research directions.",Hongliu Cao
2024-06-04T12:43:23Z,http://arxiv.org/abs/2406.02266v1,"Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning
  Compressor","Despite the prevalence of retrieval-augmented language models (RALMs), the
seamless integration of these models with retrieval mechanisms to enhance
performance in document-based tasks remains challenging. While some
post-retrieval processing Retrieval-Augmented Generation (RAG) methods have
achieved success, most still lack the ability to distinguish pertinent from
extraneous information, leading to potential inconsistencies and reduced
precision in the generated output, which subsequently affects the truthfulness
of the language model's responses. To address these limitations, this work
proposes a novel two-stage consistency learning approach for retrieved
information compression in retrieval-augmented language models to enhance
performance. By incorporating consistency learning, the aim is to generate
summaries that maintain coherence and alignment with the intended semantic
representations of a teacher model while improving faithfulness to the original
retrieved documents. The proposed method is empirically validated across
multiple datasets, demonstrating notable enhancements in precision and
efficiency for question-answering tasks. It outperforms existing baselines and
showcases the synergistic effects of combining contrastive and consistency
learning paradigms within the retrieval-augmented generation framework.",Chuankai Xu
2024-06-04T16:42:17Z,http://arxiv.org/abs/2406.02472v1,"Analyzing Temporal Complex Events with Large Language Models? A
  Benchmark towards Temporal, Long Context Understanding","The digital landscape is rapidly evolving with an ever-increasing volume of
online news, emphasizing the need for swift and precise analysis of complex
events. We refer to the complex events composed of many news articles over an
extended period as Temporal Complex Event (TCE). This paper proposes a novel
approach using Large Language Models (LLMs) to systematically extract and
analyze the event chain within TCE, characterized by their key points and
timestamps. We establish a benchmark, named TCELongBench, to evaluate the
proficiency of LLMs in handling temporal dynamics and understanding extensive
text. This benchmark encompasses three distinct tasks - reading comprehension,
temporal sequencing, and future event forecasting. In the experiment, we
leverage retrieval-augmented generation (RAG) method and LLMs with long context
window to deal with lengthy news articles of TCE. Our findings indicate that
models with suitable retrievers exhibit comparable performance with those
utilizing long context window.",Zhihan Zhang
2024-06-09T14:42:55Z,http://arxiv.org/abs/2406.05804v6,"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use
  (Including RAG), Planning, and Feedback Learning","Tool use, planning, and feedback learning are currently three prominent
paradigms for developing Large Language Model (LLM)-based agents across various
tasks. Although numerous frameworks have been devised for each paradigm, their
intricate workflows and inconsistent taxonomy create challenges in
understanding and reviewing the frameworks across different paradigms. This
survey introduces a unified taxonomy to systematically review and discuss these
frameworks. Specifically, 1) the taxonomy defines environments/tasks, common
LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),
and universally applicable workflows found in prior work, and 2) it enables a
comparison of key perspectives on the implementations of LMPRs and workflow
designs across different agent paradigms and frameworks. 3) Finally, we
identify three limitations in existing workflow designs and systematically
discuss the future work. Resources have been made publicly available at in our
GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",Xinzhe Li
2024-06-10T16:46:22Z,http://arxiv.org/abs/2406.06458v1,"Evaluating the Retrieval Component in LLM-Based Question Answering
  Systems","Question answering systems (QA) utilizing Large Language Models (LLMs)
heavily depend on the retrieval component to provide them with domain-specific
information and reduce the risk of generating inaccurate responses or
hallucinations. Although the evaluation of retrievers dates back to the early
research in Information Retrieval, assessing their performance within LLM-based
chatbots remains a challenge.
  This study proposes a straightforward baseline for evaluating retrievers in
Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate
that this evaluation framework provides a better image of how the retriever
performs and is more aligned with the overall performance of the QA system.
Although conventional metrics such as precision, recall, and F1 score may not
fully capture LLMs' capabilities - as they can yield accurate responses despite
imperfect retrievers - our method considers LLMs' strengths to ignore
irrelevant contexts, as well as potential errors and hallucinations in their
responses.",Ashkan Alinejad
2024-06-11T08:35:23Z,http://arxiv.org/abs/2406.07053v1,"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation
  and LLMs","Large Language Models (LLMs) have immense potential to transform the
telecommunications industry. They could help professionals understand complex
standards, generate code, and accelerate development. However, traditional LLMs
struggle with the precision and source verification essential for telecom work.
To address this, specialized LLM-based solutions tailored to telecommunication
standards are needed. Retrieval-augmented generation (RAG) offers a way to
create precise, fact-based answers. This paper proposes TelecomRAG, a framework
for a Telecommunication Standards Assistant that provides accurate, detailed,
and verifiable responses. Our implementation, using a knowledge base built from
3GPP Release 16 and Release 18 specification documents, demonstrates how this
assistant surpasses generic LLMs, offering superior accuracy, technical depth,
and verifiability, and thus significant value to the telecommunications field.",Girma M. Yilma
2024-06-12T01:19:36Z,http://arxiv.org/abs/2406.07796v2,"Battling Botpoop using GenAI for Higher Education: A Study of a
  Retrieval Augmented Generation Chatbots Impact on Learning","Generative artificial intelligence (GenAI) and large language models (LLMs)
have simultaneously opened new avenues for enhancing human learning and
increased the prevalence of poor-quality information in student response -
termed Botpoop. This study introduces Professor Leodar, a custom-built,
Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to
enhance educational while reducing Botpoop. Deployed at Nanyang Technological
University, Singapore, Professor Leodar offers a glimpse into the future of
AI-assisted learning, offering personalized guidance, 24/7 availability, and
contextually relevant information. Through a mixed-methods approach, we examine
the impact of Professor Leodar on learning, engagement, and exam preparedness,
with 97.1% of participants reporting positive experiences. These findings help
define possible roles of AI in education and highlight the potential of custom
GenAI chatbots. Our combination of chatbot development, in-class deployment and
outcomes study offers a benchmark for GenAI educational tools and is a stepping
stone for redefining the interplay between AI and human learning.",Maung Thway
2024-06-12T08:26:30Z,http://arxiv.org/abs/2406.07990v1,"Blowfish: Topological and statistical signatures for quantifying
  ambiguity in semantic search","This works reports evidence for the topological signatures of ambiguity in
sentence embeddings that could be leveraged for ranking and/or explanation
purposes in the context of vector search and Retrieval Augmented Generation
(RAG) systems. We proposed a working definition of ambiguity and designed an
experiment where we have broken down a proprietary dataset into collections of
chunks of varying size - 3, 5, and 10 lines and used the different collections
successively as queries and answers sets. It allowed us to test the signatures
of ambiguity with removal of confounding factors. Our results show that proxy
ambiguous queries (size 10 queries against size 3 documents) display different
distributions of homologies 0 and 1 based features than proxy clear queries
(size 5 queries against size 10 documents). We then discuss those results in
terms increased manifold complexity and/or approximately discontinuous
embedding submanifolds. Finally we propose a strategy to leverage those
findings as a new scoring strategy of semantic similarities.",Thomas Roland Barillot
2024-06-14T08:21:42Z,http://arxiv.org/abs/2406.09818v3,"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from
  Corporate Climate Disclosures","To handle the vast amounts of qualitative data produced in corporate climate
communication, stakeholders increasingly rely on Retrieval Augmented Generation
(RAG) systems. However, a significant gap remains in evaluating domain-specific
information retrieval - the basis for answer generation. To address this
challenge, this work simulates the typical tasks of a sustainability analyst by
examining 30 sustainability reports with 16 detailed climate-related questions.
As a result, we obtain a dataset with over 8.5K unique question-source-answer
pairs labeled by different levels of relevance. Furthermore, we develop a use
case with the dataset to investigate the integration of expert knowledge into
information retrieval with embeddings. Although we show that incorporating
expert knowledge works, we also outline the critical limitations of embeddings
in knowledge-intensive downstream domains like climate change communication.",Tobias Schimanski
2024-06-16T22:49:11Z,http://arxiv.org/abs/2406.11093v1,"RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation
  Detection Using In-Context Learning based on Emotional Information","Misinformation is prevalent in various fields such as education, politics,
health, etc., causing significant harm to society. However, current methods for
cross-domain misinformation detection rely on time and resources consuming
fine-tuning and complex model structures. With the outstanding performance of
LLMs, many studies have employed them for misinformation detection.
Unfortunately, they focus on in-domain tasks and do not incorporate significant
sentiment and emotion features (which we jointly call affect). In this paper,
we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to
address cross-domain misinformation detection using in-context learning based
on affective information. It accomplishes this by applying an emotion-aware LLM
to construct a retrieval database of affective embeddings. This database is
used by our retrieval module to obtain source-domain samples, which are
subsequently used for the inference module's in-context few-shot learning to
detect target domain misinformation. We evaluate our framework on three
misinformation benchmarks. Results show that RAEmoLLM achieves significant
improvements compared to the zero-shot method on three datasets, with the
highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be
released on https://github.com/lzw108/RAEmoLLM.",Zhiwei Liu
2024-06-17T03:29:14Z,http://arxiv.org/abs/2406.11177v1,TIFG: Text-Informed Feature Generation with Large Language Models,"Textual information of data is of vital importance for data mining and
feature engineering. However, existing methods focus on learning the data
structures and overlook the textual information along with the data.
Consequently, they waste this valuable resource and miss out on the deeper data
relationships embedded within the texts. In this paper, we introduce
Text-Informed Feature Generation (TIFG), a novel LLM-based text-informed
feature generation framework. TIFG utilizes the textual information to generate
features by retrieving possible relevant features within external knowledge
with Retrieval Augmented Generation (RAG) technology. In this approach, the
TIFG can generate new explainable features to enrich the feature space and
further mine feature relationships. We design the TIFG to be an automated
framework that continuously optimizes the feature generation process, adapts to
new data inputs, and improves downstream task performance over iterations. A
broad range of experiments in various downstream tasks showcases that our
approach can generate high-quality and meaningful features, and is
significantly superior to existing methods.",Xinhao Zhang
2024-06-18T06:54:28Z,http://arxiv.org/abs/2406.12331v1,"Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text
  Understanding","Current Large Language Models (LLMs) face inherent limitations due to their
pre-defined context lengths, which impede their capacity for multi-hop
reasoning within extensive textual contexts. While existing techniques like
Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by
sourcing external information, they fall short when direct answers are not
readily available. We introduce a novel approach that re-imagines information
retrieval through dynamic in-context editing, inspired by recent breakthroughs
in knowledge editing. By treating lengthy contexts as malleable external
knowledge, our method interactively gathers and integrates relevant
information, thereby enabling LLMs to perform sophisticated reasoning steps.
Experimental results demonstrate that our method effectively empowers
context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with
improved performance, which outperforms state-of-the-art context window
extrapolation methods and even compares favorably to more advanced commercial
long-context models. Our interactive method not only enhances reasoning
capabilities but also mitigates the associated training and computational
costs, making it a pragmatic solution for enhancing LLMs' reasoning within
expansive contexts.",Weizhi Fei
2024-06-18T07:05:31Z,http://arxiv.org/abs/2406.12338v1,PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints,"Data fusion models based on Coupled Matrix and Tensor Factorizations (CMTF)
have been effective tools for joint analysis of data from multiple sources.
While the vast majority of CMTF models are based on the strictly multilinear
CANDECOMP/PARAFAC (CP) tensor model, recently also the more flexible PARAFAC2
model has been integrated into CMTF models. PARAFAC2 tensor models can handle
irregular/ragged tensors and have shown to be especially useful for modelling
dynamic data with unaligned or irregular time profiles. However, existing
PARAFAC2-based CMTF models have limitations in terms of possible
regularizations on the factors and/or types of coupling between datasets. To
address these limitations, in this paper we introduce a flexible algorithmic
framework that fits PARAFAC2-based CMTF models using Alternating Optimization
(AO) and the Alternating Direction Method of Multipliers (ADMM). The proposed
framework allows to impose various constraints on all modes and linear
couplings to other matrix-, CP- or PARAFAC2-models. Experiments on various
simulated and a real dataset demonstrate the utility and versatility of the
proposed framework as well as its benefits in terms of accuracy and efficiency
in comparison with state-of-the-art methods.",Carla Schenker
2024-06-18T17:22:48Z,http://arxiv.org/abs/2406.12806v1,"Identifying Performance-Sensitive Configurations in Software Systems
  through Code Analysis with LLM Agents","Configuration settings are essential for tailoring software behavior to meet
specific performance requirements. However, incorrect configurations are
widespread, and identifying those that impact system performance is challenging
due to the vast number and complexity of possible settings. In this work, we
present PerfSense, a lightweight framework that leverages Large Language Models
(LLMs) to efficiently identify performance-sensitive configurations with
minimal overhead. PerfSense employs LLM agents to simulate interactions between
developers and performance engineers using advanced prompting techniques such
as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of
seven open-source Java systems demonstrates that PerfSense achieves an average
accuracy of 64.77% in classifying performance-sensitive configurations,
outperforming both our LLM baseline (50.36%) and the previous state-of-the-art
method (61.75%). Notably, our prompt chaining technique improves recall by 10%
to 30% while maintaining similar precision levels. Additionally, a manual
analysis of 362 misclassifications reveals common issues, including LLMs'
misunderstandings of requirements (26.8%). In summary, PerfSense significantly
reduces manual effort in classifying performance-sensitive configurations and
offers valuable insights for future LLM-based code analysis research.",Zehao Wang
2024-06-19T08:29:54Z,http://arxiv.org/abs/2406.13331v1,Improving Zero-shot LLM Re-Ranker with Risk Minimization,"In the Retrieval-Augmented Generation (RAG) system, advanced Large Language
Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an
unsupervised way, which re-rank documents based on the probability of
generating the query given the content of a document. However, directly
prompting LLMs to approximate QLMs inherently is biased, where the estimated
distribution might diverge from the actual document-specific distribution. In
this study, we introduce a novel framework, $\mathrm{UR^3}$, which leverages
Bayesian decision theory to both quantify and mitigate this estimation bias.
Specifically, $\mathrm{UR^3}$ reformulates the problem as maximizing the
probability of document generation, thereby harmonizing the optimization of
query and document generation probabilities under a unified risk minimization
objective. Our empirical results indicate that $\mathrm{UR^3}$ significantly
enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits
the QA tasks by achieving higher accuracy with fewer input documents.",Xiaowei Yuan
2024-06-20T20:55:38Z,http://arxiv.org/abs/2406.14732v2,"TTQA-RS- A break-down prompting approach for Multi-hop Table-Text
  Question Answering with Reasoning and Summarization","Question answering (QA) over tables and text has gained much popularity over
the years. Multi-hop table-text QA requires multiple hops between the table and
text, making it a challenging QA task. Although several works have attempted to
solve the table-text QA task, most involve training the models and requiring
labeled data. In this paper, we have proposed a Retrieval Augmented Generation
(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop
Table-Text Question Answering with Reasoning and Summarization. Our model uses
an enhanced retriever for table-text information retrieval and uses augmented
knowledge, including table-text summary with decomposed sub-questions with
answers for a reasoning-based table-text QA. Using open-source language models,
our model outperformed all existing prompting methods for table-text QA tasks
on existing table-text QA datasets, such as HybridQA and OTT-QA's development
set. Our experiments demonstrate the potential of prompt-based approaches using
open-source LLMs. Additionally, by using LLaMA3-70B, our model achieved
state-of-the-art performance for prompting-based methods on multi-hop
table-text QA.",Jayetri Bardhan
2024-06-21T10:48:21Z,http://arxiv.org/abs/2406.15045v2,"Integrating Knowledge Retrieval and Large Language Models for Clinical
  Report Correction","This study proposes an approach for error correction in radiology reports,
leveraging large language models (LLMs) and retrieval-augmented generation
(RAG) techniques. The proposed framework employs a novel internal+external
retrieval mechanism to extract relevant medical entities and relations from the
report of interest and an external knowledge source. A three-stage inference
process is introduced, decomposing the task into error detection, localization,
and correction subtasks, which enhances the explainability and performance of
the system. The effectiveness of the approach is evaluated using a benchmark
dataset created by corrupting real-world radiology reports with realistic
errors, guided by domain experts. Experimental results demonstrate the benefits
of the proposed methods, with the combination of internal and external
retrieval significantly improving the accuracy of error detection,
localization, and correction across various state-of-the-art LLMs. The findings
contribute to the development of more robust and reliable error correction
systems for clinical documentation.",Jinge Wu
2024-06-23T17:18:19Z,http://arxiv.org/abs/2406.16167v1,"FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy
  in Large Language Models","We present a novel extension to Retrieval Augmented Generation with the goal
of mitigating factual inaccuracies in the output of large language models.
Specifically, our method draws on the cognitive linguistic theory of frame
semantics for the indexing and retrieval of factual information relevant to
helping large language models answer queries. We conduct experiments to
demonstrate the effectiveness of this method both in terms of retrieval
effectiveness and in terms of the relevance of the frames and frame relations
automatically generated. Our results show that this novel mechanism of Frame
Semantic-based retrieval, designed to improve Retrieval Augmented Generation
(FS-RAG), is effective and offers potential for providing data-driven insights
into frame semantics theory. We provide open access to our program code and
prompts.",Harish Tayyar Madabushi
2024-06-24T07:52:05Z,http://arxiv.org/abs/2406.16383v2,"Context-augmented Retrieval: A Novel Framework for Fast Information
  Retrieval based Response Generation using Large Language Model","Generating high-quality answers consistently by providing contextual
information embedded in the prompt passed to the Large Language Model (LLM) is
dependent on the quality of information retrieval. As the corpus of contextual
information grows, the answer/inference quality of Retrieval Augmented
Generation (RAG) based Question Answering (QA) systems declines. This work
solves this problem by combining classical text classification with the Large
Language Model (LLM) to enable quick information retrieval from the vector
store and ensure the relevancy of retrieved information. For the same, this
work proposes a new approach Context Augmented retrieval (CAR), where
partitioning of vector database by real-time classification of information
flowing into the corpus is done. CAR demonstrates good quality answer
generation along with significant reduction in information retrieval and answer
generation time.",Sai Ganesh
2024-06-24T19:35:11Z,http://arxiv.org/abs/2406.17095v1,Attention Instruction: Amplifying Attention in the Middle via Prompting,"The context window of large language models has been extended to 128k tokens
or more. However, language models still suffer from position bias and have
difficulty in accessing and using the middle part of the context due to the
lack of attention. We examine the relative position awareness of LLMs and the
feasibility of mitigating disproportional attention through prompting. We
augment the original task instruction with $\texttt{attention instructions}$
that direct language models to allocate more attention towards a selected
segment of the context. We conduct a comprehensive investigation on
multi-document question answering task with both position-based and index-based
instructions. We find that language models do not have relative position
awareness of the context. Nevertheless, they demonstrate the capacity to adapt
attention to a specific segment using matching indexes. Our analysis
contributes to a deeper understanding of position bias in LLMs and provides a
pathway to mitigate this bias by instruction, thus benefiting LLMs in locating
and utilizing relevant information from retrieved documents in RAG
applications.",Meiru Zhang
2024-06-25T13:08:35Z,http://arxiv.org/abs/2406.17526v1,LumberChunker: Long-Form Narrative Document Segmentation,"Modern NLP tasks increasingly rely on dense retrieval methods to access
up-to-date and relevant contextual information. We are motivated by the premise
that retrieval benefits from segments that can vary in size such that a
content's semantic independence is better captured. We propose LumberChunker, a
method leveraging an LLM to dynamically segment documents, which iteratively
prompts the LLM to identify the point within a group of sequential passages
where the content begins to shift. To evaluate our method, we introduce
GutenQA, a benchmark with 3000 ""needle in a haystack"" type of question-answer
pairs derived from 100 public domain narrative books available on Project
Gutenberg. Our experiments show that LumberChunker not only outperforms the
most competitive baseline by 7.37% in retrieval performance (DCG@20) but also
that, when integrated into a RAG pipeline, LumberChunker proves to be more
effective than other chunking methods and competitive baselines, such as the
Gemini 1.5M Pro. Our Code and Data are available at
https://github.com/joaodsmarques/LumberChunker",AndrÃ© V. Duarte
2024-06-27T05:14:34Z,http://arxiv.org/abs/2406.18894v1,"Assessing the Effectiveness of LLMs in Android Application Vulnerability
  Analysis","The increasing frequency of attacks on Android applications coupled with the
recent popularity of large language models (LLMs) necessitates a comprehensive
understanding of the capabilities of the latter in identifying potential
vulnerabilities, which is key to mitigate the overall risk. To this end, the
work at hand compares the ability of nine state-of-the-art LLMs to detect
Android code vulnerabilities listed in the latest Open Worldwide Application
Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open
dataset of over 100 vulnerable code samples, including obfuscated ones,
assessing each model's ability to identify key vulnerabilities. Our analysis
reveals the strengths and weaknesses of each LLM, identifying important factors
that contribute to their performance. Additionally, we offer insights into
context augmentation with retrieval-augmented generation (RAG) for detecting
Android code vulnerabilities, which in turn may propel secure application
development. Finally, while the reported findings regarding code vulnerability
analysis show promise, they also reveal significant discrepancies among the
different LLMs.",Vasileios Kouliaridis
2024-06-27T16:33:40Z,http://arxiv.org/abs/2406.19309v2,"Which Neurons Matter in IR? Applying Integrated Gradients-based Methods
  to Understand Cross-Encoders","With the recent addition of Retrieval-Augmented Generation (RAG), the scope
and importance of Information Retrieval (IR) has expanded. As a result, the
importance of a deeper understanding of IR models also increases. However,
interpretability in IR remains under-explored, especially when it comes to the
models' inner mechanisms. In this paper, we explore the possibility of adapting
Integrated Gradient-based methods in an IR context to identify the role of
individual neurons within the model. In particular, we provide new insights
into the role of what we call ""relevance"" neurons, as well as how they deal
with unseen data. Finally, we carry out an in-depth pruning study to validate
our findings.",Mathias Vast
2024-06-27T19:20:09Z,http://arxiv.org/abs/2406.19493v2,"Development and Evaluation of a Retrieval-Augmented Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems","Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.",Anubhab Majumder
2024-07-01T10:26:19Z,http://arxiv.org/abs/2407.01158v1,"Learning to Explore and Select for Coverage-Conditioned
  Retrieval-Augmented Generation","Interactions with billion-scale large language models typically yield
long-form responses due to their extensive parametric capacities, along with
retrieval-augmented features. While detailed responses provide insightful
viewpoint of a specific subject, they frequently generate redundant and less
engaging content that does not meet user interests. In this work, we focus on
the role of query outlining (i.e., selected sequence of queries) in scenarios
that users request a specific range of information, namely coverage-conditioned
($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets
of information-seeking queries decomposed with various perspectives on certain
topics. By utilizing QTree, we train QPlanner, a 7B language model generating
customized query outlines that follow coverage-conditioned queries. We analyze
the effectiveness of generated outlines through automatic and human evaluation,
targeting on retrieval-augmented generation (RAG). Moreover, the experimental
results demonstrate that QPlanner with alignment training can further provide
outlines satisfying diverse user interests. Our resources are available at
https://github.com/youngerous/qtree.",Takyoung Kim
2024-07-06T02:22:25Z,http://arxiv.org/abs/2407.04925v1,RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations,"Massive Open Online Courses (MOOCs) have significantly enhanced educational
accessibility by offering a wide variety of courses and breaking down
traditional barriers related to geography, finance, and time. However, students
often face difficulties navigating the vast selection of courses, especially
when exploring new fields of study. Driven by this challenge, researchers have
been exploring course recommender systems to offer tailored guidance that
aligns with individual learning preferences and career aspirations. These
systems face particular challenges in effectively addressing the ``cold start''
problem for new users. Recent advancements in recommender systems suggest
integrating large language models (LLMs) into the recommendation process to
enhance personalized recommendations and address the ``cold start'' problem.
Motivated by these advancements, our study introduces RAMO (Retrieval-Augmented
Generation for MOOCs), a system specifically designed to overcome the ``cold
start'' challenges of traditional course recommender systems. The RAMO system
leverages the capabilities of LLMs, along with Retrieval-Augmented Generation
(RAG)-facilitated contextual understanding, to provide course recommendations
through a conversational interface, aiming to enhance the e-learning
experience.",Jiarui Rao
2024-07-10T16:16:02Z,http://arxiv.org/abs/2407.07799v2,Attribute or Abstain: Large Language Models as Long Document Assistants,"LLMs can help humans working with long documents, but are known to
hallucinate. Attribution can increase trust in LLM responses: The LLM provides
evidence that supports its response, which enhances verifiability. Existing
approaches to attribution have only been evaluated in RAG settings, where the
initial retrieval confounds LLM performance. This is crucially different from
the long document setting, where retrieval is not needed, but could help. Thus,
a long document specific evaluation of attribution is missing. To fill this
gap, we present LAB, a benchmark of 6 diverse long document tasks with
attribution, and experiments with different approaches to attribution on 5 LLMs
of different sizes.
  We find that citation, i.e. response generation and evidence extraction in
one step, performs best for large and fine-tuned models, while additional
retrieval can help for small, prompted models. We investigate whether the ""Lost
in the Middle'' phenomenon exists for attribution, but do not find this. We
also find that evidence quality can predict response quality on datasets with
simple responses, but not so for complex responses, as models struggle with
providing evidence for complex claims.",Jan Buchmann
2024-07-04T06:26:51Z,http://arxiv.org/abs/2407.07913v1,"CaseGPT: a case reasoning framework based on language models and
  retrieval-augmented generation","This paper presents CaseGPT, an innovative approach that combines Large
Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to
enhance case-based reasoning in the healthcare and legal sectors. The system
addresses the challenges of traditional database queries by enabling fuzzy
searches based on imprecise descriptions, thereby improving data searchability
and usability. CaseGPT not only retrieves relevant case data but also generates
insightful suggestions and recommendations based on patterns discerned from
existing case data. This functionality proves especially valuable for tasks
such as medical diagnostics, legal precedent research, and case strategy
formulation. The paper includes an in-depth discussion of the system's
methodology, its performance in both medical and legal domains, and its
potential for future applications. Our experiments demonstrate that CaseGPT
significantly outperforms traditional keyword-based and simple LLM-based
systems in terms of precision, recall, and efficiency.",Rui Yang
2024-07-11T13:22:17Z,http://arxiv.org/abs/2407.08488v2,Lynx: An Open Source Hallucination Evaluation Model,"Retrieval Augmented Generation (RAG) techniques aim to mitigate
hallucinations in Large Language Models (LLMs). However, LLMs can still produce
information that is unsupported or contradictory to the retrieved contexts. We
introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced
reasoning on challenging real-world hallucination scenarios. To evaluate LYNX,
we present HaluBench, a comprehensive hallucination evaluation benchmark,
consisting of 15k samples sourced from various real-world domains. Our
experiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and
closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,
HaluBench and our evaluation code for public access.",Selvan Sunitha Ravi
2024-07-14T15:25:08Z,http://arxiv.org/abs/2407.10245v1,"GenSco: Can Question Decomposition based Passage Alignment improve
  Question Answering?","Retrieval augmented generation (RAG) with large language models (LLMs) for
Question Answering (QA) entails furnishing relevant context within the prompt
to facilitate the LLM in answer generation. During the generation, inaccuracies
or hallucinations frequently occur due to two primary factors: inadequate or
distracting context in the prompts, and the inability of LLMs to effectively
reason through the facts. In this paper, we investigate whether providing
aligned context via a carefully selected passage sequence leads to better
answer generation by the LLM for multi-hop QA. We introduce, ""GenSco"", a novel
approach of selecting passages based on the predicted decomposition of the
multi-hop questions}. The framework consists of two distinct LLMs: (i)
Generator LLM, which is used for question decomposition and final answer
generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to
semantically guide the Generator for passage selection. The generator is
invoked only once for the answer generation, resulting in a cost-effective and
efficient approach. We evaluate on three broadly established multi-hop question
answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve
an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect
to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.",Barah Fazili
2024-07-15T13:04:09Z,http://arxiv.org/abs/2407.10691v2,"$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity","Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based
retrievers, respectively, averaged on queries containing multiple subqueries
from five scientific retrieval datasets. Moreover, the efficacy of two
downstream scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The
code and experimental datasets are available.",Fengyu Cai
2024-07-15T17:30:31Z,http://arxiv.org/abs/2407.10930v2,"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together","Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai",Dilara Soylu
2024-07-16T14:25:38Z,http://arxiv.org/abs/2407.11765v1,Nowcasting R&D Expenditures: A Machine Learning Approach,"Macroeconomic data are crucial for monitoring countries' performance and
driving policy. However, traditional data acquisition processes are slow,
subject to delays, and performed at a low frequency. We address this
'ragged-edge' problem with a two-step framework. The first step is a supervised
learning model predicting observed low-frequency figures. We propose a
neural-network-based nowcasting model that exploits mixed-frequency,
high-dimensional data. The second step uses the elasticities derived from the
previous step to interpolate unobserved high-frequency figures. We apply our
method to nowcast countries' yearly research and development (R&D) expenditure
series. These series are collected through infrequent surveys, making them
ideal candidates for this task. We exploit a range of predictors, chiefly
Internet search volume data, and document the relevance of these data in
improving out-of-sample predictions. Furthermore, we leverage the high
frequency of our data to derive monthly estimates of R&D expenditures, which
are currently unobserved. We compare our results with those obtained from the
classical regression-based and the sparse temporal disaggregation methods.
Finally, we validate our results by reporting a strong correlation with monthly
R&D employment data.",Atin Aboutorabi
2024-06-28T10:57:50Z,http://arxiv.org/abs/2407.12025v1,"LLM4DESIGN: An Automated Multi-Modal System for Architectural and
  Environmental Design","This study introduces LLM4DESIGN, a highly automated system for generating
architectural and environmental design proposals. LLM4DESIGN, relying solely on
site conditions and design requirements, employs Multi-Agent systems to foster
creativity, Retrieval Augmented Generation (RAG) to ground designs in realism,
and Visual Language Models (VLM) to synchronize all information. This system
resulting in coherent, multi-illustrated, and multi-textual design schemes. The
system meets the dual needs of narrative storytelling and objective drawing
presentation in generating architectural and environmental design proposals.
Extensive comparative and ablation experiments confirm the innovativeness of
LLM4DESIGN's narrative and the grounded applicability of its plans,
demonstrating its superior performance in the field of urban renewal design.
Lastly, we have created the first cross-modal design scheme dataset covering
architecture, landscape, interior, and urban design, providing rich resources
for future research.",Ran Chen
2024-07-01T05:37:17Z,http://arxiv.org/abs/2407.12036v2,Exploring Advanced Large Language Models with LLMsuite,"This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.",Giorgio Roffo
2024-07-17T16:55:42Z,http://arxiv.org/abs/2407.12735v4,EchoSight: Advancing Visual-Language Models with Wiki Knowledge,"Knowledge-based Visual Question Answering (KVQA) tasks require answering
questions about images using extensive background knowledge. Despite
significant advancements, generative models often struggle with these tasks due
to the limited integration of external knowledge. In this paper, we introduce
EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework
that enables large language models (LLMs) to answer visual questions requiring
fine-grained encyclopedic knowledge. To strive for high-performing retrieval,
EchoSight first searches wiki articles by using visual-only information,
subsequently, these candidate articles are further reranked according to their
relevance to the combined text-image query. This approach significantly
improves the integration of multimodal knowledge, leading to enhanced retrieval
outcomes and more accurate VQA responses. Our experimental results on the
Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes
new state-of-the-art results in knowledge-based VQA, achieving an accuracy of
41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",Yibin Yan
2024-07-21T21:04:28Z,http://arxiv.org/abs/2407.15268v1,"Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical
  Radiology Report Generation","Multimodal foundation models hold significant potential for automating
radiology report generation, thereby assisting clinicians in diagnosing cardiac
diseases. However, generated reports often suffer from serious factual
inaccuracy. In this paper, we introduce a fact-aware multimodal
retrieval-augmented pipeline in generating accurate radiology reports
(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then
integrate factual knowledge to train a universal multimodal retriever. Given a
radiology image, our retriever can identify high-quality reference reports to
augment multimodal foundation models, thus enhancing the factual completeness
and correctness of report generation. Experiments on two benchmark datasets
show that our multimodal retriever outperforms state-of-the-art retrievers on
both language generation and radiology-specific metrics, up to 6.5% and 2%
score in F1CheXbert and F1RadGraph. Further analysis indicates that employing
our factually-informed training strategy imposes an effective supervision
signal, without relying on explicit diagnostic label guidance, and successfully
propagates fact-aware capabilities from the multimodal retriever to the
multimodal foundation model in radiology report generation.",Liwen Sun
2024-07-22T07:15:49Z,http://arxiv.org/abs/2407.15428v1,"Decoding BACnet Packets: A Large Language Model Approach for Packet
  Interpretation","The Industrial Control System (ICS) environment encompasses a wide range of
intricate communication protocols, posing substantial challenges for Security
Operations Center (SOC) analysts tasked with monitoring, interpreting, and
addressing network activities and security incidents. Conventional monitoring
tools and techniques often struggle to provide a clear understanding of the
nature and intent of ICS-specific communications. To enhance comprehension, we
propose a software solution powered by a Large Language Model (LLM). This
solution currently focused on BACnet protocol, processes a packet file data and
extracts context by using a mapping database, and contemporary context
retrieval methods for Retrieval Augmented Generation (RAG). The processed
packet information, combined with the extracted context, serves as input to the
LLM, which generates a concise packet file summary for the user. The software
delivers a clear, coherent, and easily understandable summary of network
activities, enabling SOC analysts to better assess the current state of the
control system.",Rashi Sharma
2024-07-22T11:55:14Z,http://arxiv.org/abs/2407.15569v2,"An Empirical Study of Retrieval Augmented Generation with
  Chain-of-Thought","Since the launch of ChatGPT at the end of 2022, generative dialogue models
represented by ChatGPT have quickly become essential tools in daily life. As
user expectations increase, enhancing the capability of generative dialogue
models to solve complex problems has become a focal point of current research.
This paper delves into the effectiveness of the RAFT (Retrieval Augmented
Fine-Tuning) method in improving the performance of Generative dialogue models.
RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and
retrieval augmented generation (RAG), which significantly enhanced the model's
information extraction and logical reasoning abilities. We evaluated the RAFT
method across multiple datasets and analysed its performance in various
reasoning tasks, including long-form QA and short-form QA tasks, tasks in both
Chinese and English, and supportive and comparison reasoning tasks. Notably, it
addresses the gaps in previous research regarding long-form QA tasks and
Chinese datasets. Moreover, we also evaluate the benefit of the
chain-of-thought (CoT) in the RAFT method. This work offers valuable insights
for studies focused on enhancing the performance of generative dialogue models.",Yuetong Zhao
2024-07-22T17:50:31Z,http://arxiv.org/abs/2407.15831v1,"NV-Retriever: Improving text embedding models with effective
  hard-negative mining","Text embedding models have been popular for information retrieval
applications such as semantic search and Question-Answering systems based on
Retrieval-Augmented Generation (RAG). Those models are typically Transformer
models that are fine-tuned with contrastive learning objectives. Many papers
introduced new embedding model architectures and training approaches, however,
one of the key ingredients, the process of mining negative passages, remains
poorly explored or described. One of the challenging aspects of fine-tuning
embedding models is the selection of high quality hard-negative passages for
contrastive learning. In this paper we propose a family of positive-aware
mining methods that leverage the positive relevance score for more effective
false negatives removal. We also provide a comprehensive ablation study on
hard-negative mining methods over their configurations, exploring different
teacher and base models. We demonstrate the efficacy of our proposed methods by
introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval
(BEIR) benchmark and 0.65 points higher than previous methods. The model placed
1st when it was published to MTEB Retrieval on July 07, 2024.",Gabriel de Souza P. Moreira
2024-07-22T17:59:45Z,http://arxiv.org/abs/2407.15847v3,LLMmap: Fingerprinting For Large Language Models,"We introduce LLMmap, a first-generation fingerprinting technique targeted at
LLM-integrated applications. LLMmap employs an active fingerprinting approach,
sending carefully crafted queries to the application and analyzing the
responses to identify the specific LLM version in use. Our query selection is
informed by domain expertise on how LLMs generate uniquely identifiable
responses to thematically varied prompts. With as few as 8 interactions, LLMmap
can accurately identify 42 different LLM versions with over 95% accuracy. More
importantly, LLMmap is designed to be robust across different application
layers, allowing it to identify LLM versions--whether open-source or
proprietary--from various vendors, operating under various unknown system
prompts, stochastic sampling hyperparameters, and even complex generation
frameworks such as RAG or Chain-of-Thought. We discuss potential mitigations
and demonstrate that, against resourceful adversaries, effective
countermeasures may be challenging or even unrealizable.",Dario Pasquini
2024-07-21T16:42:45Z,http://arxiv.org/abs/2407.18333v1,"AutoVCoder: A Systematic Framework for Automated Verilog Code Generation
  using LLMs","Recently, the use of large language models (LLMs) for software code
generation, e.g., C/C++ and Python, has proven a great success. However, LLMs
still suffer from low syntactic and functional correctness when it comes to the
generation of register-transfer level (RTL) code, such as Verilog. To address
this issue, in this paper, we develop AutoVCoder, a systematic open-source
framework that significantly improves the LLMs' correctness of generating
Verilog code and enhances the quality of its output at the same time. Our
framework integrates three novel techniques, including a high-quality hardware
dataset generation approach, a two-round LLM fine-tuning method and a
domain-specific retrieval-augmented generation (RAG) mechanism. Experimental
results demonstrate that AutoVCoder outperforms both industrial and academic
LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%
improvement in functional correctness on the EvalMachine and EvalHuman
benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax
correctness and a 3.4% increase in functional correctness on the RTLLM
benchmark compared with RTLCoder.",Mingzhe Gao
2024-07-26T20:42:40Z,http://arxiv.org/abs/2407.19075v1,"ESAC (EQ-SANS Assisting Chatbot): Application of Large Language Models
  and Retrieval-Augmented Generation for Enhanced User Experience at EQ-SANS","Neutron scattering experiments have played vital roles in exploring materials
properties in the past decades. While user interfaces have been improved over
time, neutron scattering experiments still require specific knowledge or
training by an expert due to the complexity of such advanced instrumentation
and the limited number of experiments each person may perform each year. This
paper introduces an innovative chatbot application that leverages Large
Language Models(LLM) and Retrieval-Augmented Generation (RAG) technologies to
significantly enhance the user experience at the EQ-SANS, a small-angle neutron
scattering instrument at the Spallation Neutron Source of Oak Ridge National
Laboratory. Through a user-centric design approach, the EQ-SANS Assisting
Chatbot (ESAC) serves as an interactive reference for users, thereby
facilitating the use of the instrument by visiting scientists. By bridging the
gap between the users of EQ-SANS and the control systems required to perform
their experiments, the ESAC sets a new standard for interactive learning and
support for the scientific community using large-scale scientific facilities.",Changwoo Do
2024-07-30T17:27:20Z,http://arxiv.org/abs/2407.20990v1,"From Feature Importance to Natural Language Explanations Using LLMs with
  RAG","As machine learning becomes increasingly integral to autonomous
decision-making processes involving human interaction, the necessity of
comprehending the model's outputs through conversational means increases. Most
recently, foundation models are being explored for their potential as post hoc
explainers, providing a pathway to elucidate the decision-making mechanisms of
predictive models. In this work, we introduce traceable question-answering,
leveraging an external knowledge repository to inform the responses of Large
Language Models (LLMs) to user queries within a scene understanding task. This
knowledge repository comprises contextual details regarding the model's output,
containing high-level features, feature importance, and alternative
probabilities. We employ subtractive counterfactual reasoning to compute
feature importance, a method that entails analysing output variations resulting
from decomposing semantic features. Furthermore, to maintain a seamless
conversational flow, we integrate four key characteristics - social, causal,
selective, and contrastive - drawn from social science research on human
explanations into a single-shot prompt, guiding the response generation
process. Our evaluation demonstrates that explanations generated by the LLMs
encompassed these elements, indicating its potential to bridge the gap between
complex model outputs and natural language expressions.",Sule Tekkesinoglu
2024-08-08T09:59:30Z,http://arxiv.org/abs/2408.04342v1,"Towards Explainable Network Intrusion Detection using Large Language
  Models","Large Language Models (LLMs) have revolutionised natural language processing
tasks, particularly as chat agents. However, their applicability to threat
detection problems remains unclear. This paper examines the feasibility of
employing LLMs as a Network Intrusion Detection System (NIDS), despite their
high computational requirements, primarily for the sake of explainability.
Furthermore, considerable resources have been invested in developing LLMs, and
they may offer utility for NIDS. Current state-of-the-art NIDS rely on
artificial benchmarking datasets, resulting in skewed performance when applied
to real-world networking environments. Therefore, we compare the GPT-4 and
LLama3 models against traditional architectures and transformer-based models to
assess their ability to detect malicious NetFlows without depending on
artificially skewed datasets, but solely on their vast pre-trained acquired
knowledge. Our results reveal that, although LLMs struggle with precise attack
detection, they hold significant potential for a path towards explainable NIDS.
Our preliminary exploration shows that LLMs are unfit for the detection of
Malicious NetFlows. Most promisingly, however, these exhibit significant
potential as complementary agents in NIDS, particularly in providing
explanations and aiding in threat response when integrated with Retrieval
Augmented Generation (RAG) and function calling capabilities.",Paul R. B. Houssel
2024-08-07T21:07:13Z,http://arxiv.org/abs/2408.04675v1,ACL Ready: RAG Based Assistant for the ACL Checklist,"The ARR Responsible NLP Research checklist website states that the ""checklist
is designed to encourage best practices for responsible research, addressing
issues of research ethics, societal impact and reproducibility."" Answering the
questions is an opportunity for authors to reflect on their work and make sure
any shared scientific assets follow best practices. Ideally, considering the
checklist before submission can favorably impact the writing of a research
paper. However, the checklist is often filled out at the last moment. In this
work, we introduce ACLReady, a retrieval-augmented language model application
that can be used to empower authors to reflect on their work and assist authors
with the ACL checklist. To test the effectiveness of the system, we conducted a
qualitative study with 13 users which shows that 92% of users found the
application useful and easy to use as well as 77% of the users found that the
application provided the information they expected. Our code is publicly
available under the CC BY-NC 4.0 license on GitHub.",Michael Galarnyk
2024-08-06T22:28:13Z,http://arxiv.org/abs/2408.05242v1,"FLASH: Federated Learning-Based LLMs for Advanced Query Processing in
  Social Networks through RAG","Our paper introduces a novel approach to social network information retrieval
and user engagement through a personalized chatbot system empowered by
Federated Learning GPT. The system is designed to seamlessly aggregate and
curate diverse social media data sources, including user posts, multimedia
content, and trending news. Leveraging Federated Learning techniques, the GPT
model is trained on decentralized data sources to ensure privacy and security
while providing personalized insights and recommendations. Users interact with
the chatbot through an intuitive interface, accessing tailored information and
real-time updates on social media trends and user-generated content. The
system's innovative architecture enables efficient processing of input files,
parsing and enriching text data with metadata, and generating relevant
questions and answers using advanced language models. By facilitating
interactive access to a wealth of social network information, this personalized
chatbot system represents a significant advancement in social media
communication and knowledge dissemination.",Sai Puppala
2024-08-13T14:59:44Z,http://arxiv.org/abs/2408.06941v2,OpenResearcher: Unleashing AI for Accelerated Scientific Research,"The rapid growth of scientific literature imposes significant challenges for
researchers endeavoring to stay updated with the latest advancements in their
fields and delve into new areas. We introduce OpenResearcher, an innovative
platform that leverages Artificial Intelligence (AI) techniques to accelerate
the research process by answering diverse questions from researchers.
OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to
integrate Large Language Models (LLMs) with up-to-date, domain-specific
knowledge. Moreover, we develop various tools for OpenResearcher to understand
researchers' queries, search from the scientific literature, filter retrieved
information, provide accurate and comprehensive answers, and self-refine these
answers. OpenResearcher can flexibly use these tools to balance efficiency and
effectiveness. As a result, OpenResearcher enables researchers to save time and
increase their potential to discover new insights and drive scientific
breakthroughs. Demo, video, and code are available at:
https://github.com/GAIR-NLP/OpenResearcher.",Yuxiang Zheng
2024-08-16T04:32:10Z,http://arxiv.org/abs/2408.08521v1,"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement
  Framework for Multimodal Question Answering","Recent advancements in retrieval-augmented generation (RAG) have demonstrated
impressive performance in the question-answering (QA) task. However, most
previous works predominantly focus on text-based answers. While some studies
address multimodal data, they still fall short in generating comprehensive
multimodal answers, particularly for explaining concepts or providing
step-by-step tutorials on how to accomplish specific goals. This capability is
especially valuable for applications such as enterprise chatbots and settings
such as customer service and educational systems, where the answers are sourced
from multimodal data. In this paper, we introduce a simple and effective
framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR
enhances text-based answers by retrieving relevant multimodal data and refining
the responses to create coherent multimodal answers. This framework can be
easily extended to support multimodal answers in enterprise chatbots with
minimal modifications. Human evaluation results indicate that multimodal
answers generated by MuRAR are more useful and readable compared to plain text
answers.",Zhengyuan Zhu
2024-08-17T19:17:00Z,http://arxiv.org/abs/2408.09277v1,"Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case
  Study at Ericsson","This paper presents our experience developing a Llama-based chatbot for
question answering about continuous integration and continuous delivery (CI/CD)
at Ericsson, a multinational telecommunications company. Our chatbot is
designed to handle the specificities of CI/CD documents at Ericsson, employing
a retrieval-augmented generation (RAG) model to enhance accuracy and relevance.
Our empirical evaluation of the chatbot on industrial CI/CD-related questions
indicates that an ensemble retriever, combining BM25 and embedding retrievers,
yields the best performance. When evaluated against a ground truth of 72 CI/CD
questions and answers at Ericsson, our most accurate chatbot configuration
provides fully correct answers for 61.11% of the questions, partially correct
answers for 26.39%, and incorrect answers for 12.50%. Through an error analysis
of the partially correct and incorrect answers, we discuss the underlying
causes of inaccuracies and provide insights for further refinement. We also
reflect on lessons learned and suggest future directions for further improving
our chatbot's accuracy.",Daksh Chaudhary
2024-08-05T06:31:39Z,http://arxiv.org/abs/2408.11824v3,AppAgent v2: Advanced Agent for Flexible Mobile Interactions,"With the advancement of Multimodal Large Language Models (MLLM), LLM-driven
visual agents are increasingly impacting software interfaces, particularly
those with graphical user interfaces. This work introduces a novel LLM-based
multimodal agent framework for mobile devices. This framework, capable of
navigating mobile devices, emulates human-like interactions. Our agent
constructs a flexible action space that enhances adaptability across various
applications including parser, text and vision descriptions. The agent operates
through two main phases: exploration and deployment. During the exploration
phase, functionalities of user interface elements are documented either through
agent-driven or manual explorations into a customized structured knowledge
base. In the deployment phase, RAG technology enables efficient retrieval and
update from this knowledge base, thereby empowering the agent to perform tasks
effectively and accurately. This includes performing complex, multi-step
operations across various applications, thereby demonstrating the framework's
adaptability and precision in handling customized task workflows. Our
experimental results across various benchmarks demonstrate the framework's
superior performance, confirming its effectiveness in real-world scenarios. Our
code will be open source soon.",Yanda Li
2024-08-24T03:18:42Z,http://arxiv.org/abs/2408.13450v1,vitaLITy 2: Reviewing Academic Literature Using Large Language Models,"Academic literature reviews have traditionally relied on techniques such as
keyword searches and accumulation of relevant back-references, using databases
like Google Scholar or IEEEXplore. However, both the precision and accuracy of
these search techniques is limited by the presence or absence of specific
keywords, making literature review akin to searching for needles in a haystack.
We present vitaLITy 2, a solution that uses a Large Language Model or LLM-based
approach to identify semantically relevant literature in a textual embedding
space. We include a corpus of 66,692 papers from 1970-2023 which are searchable
through text embeddings created by three language models. vitaLITy 2
contributes a novel Retrieval Augmented Generation (RAG) architecture and can
be interacted with through an LLM with augmented prompts, including
summarization of a collection of papers. vitaLITy 2 also provides a chat
interface that allow users to perform complex queries without learning any new
programming language. This also enables users to take advantage of the
knowledge captured in the LLM from its enormous training corpus. Finally, we
demonstrate the applicability of vitaLITy 2 through two usage scenarios.
vitaLITy 2 is available as open-source software at
https://vitality-vis.github.io.",Hongye An
2024-08-25T11:09:15Z,http://arxiv.org/abs/2408.13808v1,"Towards Reliable Medical Question Answering: Techniques and Challenges
  in Mitigating Hallucinations in Language Models","The rapid advancement of large language models (LLMs) has significantly
impacted various domains, including healthcare and biomedicine. However, the
phenomenon of hallucination, where LLMs generate outputs that deviate from
factual accuracy or context, poses a critical challenge, especially in
high-stakes domains. This paper conducts a scoping study of existing techniques
for mitigating hallucinations in knowledge-based task in general and especially
for medical domains. Key methods covered in the paper include
Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback
loops, supervised fine-tuning, and prompt engineering. These techniques, while
promising in general contexts, require further adaptation and optimization for
the medical domain due to its unique demands for up-to-date, specialized
knowledge and strict adherence to medical guidelines. Addressing these
challenges is crucial for developing trustworthy AI systems that enhance
clinical decision-making and patient safety as well as accuracy of biomedical
scientific research.",Duy Khoa Pham
2024-08-26T14:45:03Z,http://arxiv.org/abs/2408.14317v1,Claim Verification in the Age of Large Language Models: A Survey,"The large and ever-increasing amount of data available on the Internet
coupled with the laborious task of manual claim and fact verification has
sparked the interest in the development of automated claim verification
systems. Several deep learning and transformer-based models have been proposed
for this task over the years. With the introduction of Large Language Models
(LLMs) and their superior performance in several NLP tasks, we have seen a
surge of LLM-based approaches to claim verification along with the use of novel
methods such as Retrieval Augmented Generation (RAG). In this survey, we
present a comprehensive account of recent claim verification frameworks using
LLMs. We describe the different components of the claim verification pipeline
used in these frameworks in detail including common approaches to retrieval,
prompting, and fine-tuning. Finally, we describe publicly available English
datasets created for this task.",Alphaeus Dmonte
2024-08-26T16:00:41Z,http://arxiv.org/abs/2408.14380v1,Probing Causality Manipulation of Large Language Models,"Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.",Chenyang Zhang
2024-08-27T09:34:38Z,http://arxiv.org/abs/2408.14906v1,"Writing in the Margins: Better Inference Pattern for Long Context
  Retrieval","In this paper, we introduce Writing in the Margins (WiM), a new inference
pattern for Large Language Models designed to optimize the handling of long
input sequences in retrieval-oriented tasks. This approach leverages the
chunked prefill of the key-value cache to perform segment-wise inference, which
enables efficient processing of extensive contexts along with the generation
and classification of intermediate information (""margins"") that guide the model
towards specific tasks. This method increases computational overhead marginally
while significantly enhancing the performance of off-the-shelf models without
the need for fine-tuning. Specifically, we observe that WiM provides an average
enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)
and more than a 30.0% increase in the F1-score for aggregation tasks (CWE).
Additionally, we show how the proposed pattern fits into an interactive
retrieval design that provides end-users with ongoing updates about the
progress of context processing, and pinpoints the integration of relevant
information into the final response. We release our implementation of WiM using
Hugging Face Transformers library at
https://github.com/writer/writing-in-the-margins.",Melisa Russak
2024-08-27T17:50:03Z,http://arxiv.org/abs/2408.15232v2,"Into the Unknown Unknowns: Engaged Human Learning through Participation
  in Language Model Agent Conversations","While language model (LM)-powered chatbots and generative search engines
excel at answering concrete queries, discovering information in the terrain of
unknown unknowns remains challenging for users. To emulate the common
educational scenario where children/students learn by listening to and
participating in conversations of their parents/teachers, we create
Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all
the questions, Co-STORM lets users observe and occasionally steer the discourse
among several LM agents. The agents ask questions on the user's behalf,
allowing the user to discover unknown unknowns serendipitously. To facilitate
user interaction, Co-STORM assists users in tracking the discourse by
organizing the uncovered information into a dynamic mind map, ultimately
generating a comprehensive report as takeaways. For automatic evaluation, we
construct the WildSeek dataset by collecting real information-seeking records
with user goals. Co-STORM outperforms baseline methods on both discourse trace
and report quality. In a further human evaluation, 70% of participants prefer
Co-STORM over a search engine, and 78% favor it over a RAG chatbot.",Yucheng Jiang
2024-08-28T11:18:06Z,http://arxiv.org/abs/2408.15710v2,"Conan-embedding: General Text Embedding with More and Better Negative
  Samples","With the growing popularity of RAG, the capabilities of embedding models are
gaining increasing attention. Embedding models are primarily trained through
contrastive loss learning, with negative examples being a key component.
Previous work has proposed various hard negative mining strategies, but these
strategies are typically employed as preprocessing steps. In this paper, we
propose the conan-embedding model, which maximizes the utilization of more and
higher-quality negative examples. Specifically, since the model's ability to
handle preprocessed negative examples evolves during training, we propose
dynamic hard negative mining method to expose the model to more challenging
negative examples throughout the training process. Secondly, contrastive
learning requires as many negative examples as possible but is limited by GPU
memory constraints. Therefore, we use a Cross-GPU balancing Loss to provide
more negative examples for embedding training and balance the batch size across
multiple tasks. Moreover, we also discovered that the prompt-response pairs
from LLMs can be used for embedding training. Our approach effectively enhances
the capabilities of embedding models, currently ranking first on the Chinese
leaderboard of Massive text embedding benchmark",Shiyu Li
2024-09-02T15:58:24Z,http://arxiv.org/abs/2409.01344v2,"Pairing Analogy-Augmented Generation with Procedural Memory for
  Procedural Q&A","Large language models struggle to synthesize disparate pieces of information
into a coherent plan when approaching a complex procedural task. In this work,
we introduce a novel formalism and structure for such procedural knowledge.
Based on this formalism, we present a novel procedural knowledge dataset called
LCStep, which we created from LangChain tutorials. To leverage this procedural
knowledge to solve new tasks, we propose analogy-augmented generation (AAG),
which draws inspiration from the human ability to assimilate past experiences
to solve unfamiliar problems. AAG uses a custom procedure memory store to
retrieve and adapt specialized domain knowledge to answer new procedural tasks.
We demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,
RecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,
corroborated by human evaluation in the case of RecipeNLG.",K Roth
2024-09-05T05:34:16Z,http://arxiv.org/abs/2409.03258v3,"GraphInsight: Unlocking Insights in Large Language Models for Graph
  Structure Understanding","Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.",Yukun Cao
2024-09-12T17:48:08Z,http://arxiv.org/abs/2409.08250v1,"OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable
  Personal Question Answering","People often capture memories through photos, screenshots, and videos. While
existing AI-based tools enable querying this data using natural language, they
mostly only support retrieving individual pieces of information like certain
objects in photos and struggle with answering more complex queries that involve
interpreting interconnected memories like event sequences. We conducted a
one-month diary study to collect realistic user queries and generated a
taxonomy of necessary contextual information for integrating with captured
memories. We then introduce OmniQuery, a novel system that is able to answer
complex personal memory-related questions that require extracting and inferring
contextual information. OmniQuery augments single captured memories through
integrating scattered contextual information from multiple interconnected
memories, retrieves relevant memories, and uses a large language model (LLM) to
comprehensive answers. In human evaluations, we show the effectiveness of
OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG
system, winning or tying in 74.5% of the time.",Jiahao Nick Li
2024-09-14T03:11:00Z,http://arxiv.org/abs/2409.09281v1,"Language Models ""Grok"" to Copy","We examine the pre-training dynamics of language models, focusing on their
ability to copy text from preceding context--a fundamental skill for various
LLM applications, including in-context learning (ICL) and retrieval-augmented
generation (RAG). We propose a novel perspective that Transformer-based
language models develop copying abilities similarly to grokking, which refers
to sudden generalization on test set long after the model fit to the training
set. Our experiments yield three arguments: (1) The pre-training loss decreases
rapidly, while the context copying ability of models initially lags and then
abruptly saturates. (2) The speed of developing copying ability is independent
of the number of tokens trained, similarly to how grokking speed is unaffected
by dataset size as long as the data distribution is preserved. (3) Induction
heads, the attention heads responsible for copying, form from shallow to deep
layers during training, mirroring the development of circuits in deeper layers
during grokking. We contend that the connection between grokking and context
copying can provide valuable insights for more effective language model
training, ultimately improving in-context performance. For example, we
demonstrated that techniques that enhance grokking, such as regularization,
either accelerate or enhance the development of context copying.",Ang Lv
2024-09-17T07:44:06Z,http://arxiv.org/abs/2409.10955v1,"Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style","Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.",Yuepei Li
2024-09-17T17:59:33Z,http://arxiv.org/abs/2409.11406v1,"Phidias: A Generative Model for Creating 3D Content from Text, Image,
  and 3D Conditions with Reference-Augmented Diffusion","In 3D modeling, designers often use an existing 3D model as a reference to
create new ones. This practice has inspired the development of Phidias, a novel
generative model that uses diffusion for reference-augmented 3D generation.
Given an image, our method leverages a retrieved or user-provided 3D reference
model to guide the generation process, thereby enhancing the generation
quality, generalization ability, and controllability. Our model integrates
three key components: 1) meta-ControlNet that dynamically modulates the
conditioning strength, 2) dynamic reference routing that mitigates misalignment
between the input image and 3D reference, and 3) self-reference augmentations
that enable self-supervised training with a progressive curriculum.
Collectively, these designs result in a clear improvement over existing
methods. Phidias establishes a unified framework for 3D generation using text,
image, and 3D conditions with versatile applications.",Zhenwei Wang
2024-09-18T09:30:03Z,http://arxiv.org/abs/2409.11831v1,"RaggeDi: Diffusion-based State Estimation of Disordered Rags, Sheets,
  Towels and Blankets","Cloth state estimation is an important problem in robotics. It is essential
for the robot to know the accurate state to manipulate cloth and execute tasks
such as robotic dressing, stitching, and covering/uncovering human beings.
However, estimating cloth state accurately remains challenging due to its high
flexibility and self-occlusion. This paper proposes a diffusion model-based
pipeline that formulates the cloth state estimation as an image generation
problem by representing the cloth state as an RGB image that describes the
point-wise translation (translation map) between a pre-defined flattened mesh
and the deformed mesh in a canonical space. Then we train a conditional
diffusion-based image generation model to predict the translation map based on
an observation. Experiments are conducted in both simulation and the real world
to validate the performance of our method. Results indicate that our method
outperforms two recent methods in both accuracy and speed.",Jikai Ye
2024-09-19T05:14:55Z,http://arxiv.org/abs/2409.12468v2,"Familiarity-Aware Evidence Compression for Retrieval-Augmented
  Generation","Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.",Dongwon Jung
2024-09-19T15:50:22Z,http://arxiv.org/abs/2409.12853v2,"A New Perspective on ADHD Research: Knowledge Graph Construction with
  LLMs and Network Based Insights","Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to
study due to its complex symptomatology and diverse contributing factors. To
explore how we can gain deeper insights on this topic, we performed a network
analysis on a comprehensive knowledge graph (KG) of ADHD, constructed by
integrating scientific literature and clinical data with the help of
cutting-edge large language models. The analysis, including k-core techniques,
identified critical nodes and relationships that are central to understanding
the disorder. Building on these findings, we curated a knowledge graph that is
usable in a context-aware chatbot (Graph-RAG) with Large Language Models
(LLMs), enabling accurate and informed interactions. Our knowledge graph not
only advances the understanding of ADHD but also provides a powerful tool for
research and clinical applications.",Hakan T. Otal
2024-09-10T17:51:21Z,http://arxiv.org/abs/2409.13741v1,Knowing When to Ask -- Bridging Large Language Models and Data,"Large Language Models (LLMs) are prone to generating factually incorrect
information when responding to queries that involve numerical and statistical
data or other timely facts. In this paper, we present an approach for enhancing
the accuracy of LLMs by integrating them with Data Commons, a vast, open-source
repository of public statistics from trusted organizations like the United
Nations (UN), Center for Disease Control and Prevention (CDC) and global census
bureaus. We explore two primary methods: Retrieval Interleaved Generation
(RIG), where the LLM is trained to produce natural language queries to retrieve
data from Data Commons, and Retrieval Augmented Generation (RAG), where
relevant data tables are fetched from Data Commons and used to augment the
LLM's prompt. We evaluate these methods on a diverse set of queries,
demonstrating their effectiveness in improving the factual accuracy of LLM
outputs. Our work represents an early step towards building more trustworthy
and reliable LLMs that are grounded in verifiable statistical data and capable
of complex factual reasoning.",Prashanth Radhakrishnan
2024-09-21T09:36:14Z,http://arxiv.org/abs/2409.14083v1,"SURf: Teaching Large Vision-Language Models to Selectively Utilize
  Retrieved Information","Large Vision-Language Models (LVLMs) have become pivotal at the intersection
of computer vision and natural language processing. However, the full potential
of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains
underutilized. Existing works either focus solely on the text modality or are
limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize
retrieved information and are sensitive to irrelevant or misleading references.
To address these challenges, we propose a self-refinement framework designed to
teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically,
when given questions that are incorrectly answered by the LVLM backbone, we
obtain references that help correct the answers (positive references) and those
that do not (negative references). We then fine-tune the LVLM backbone using a
combination of these positive and negative references. Our experiments across
three tasks and seven datasets demonstrate that our framework significantly
enhances LVLMs ability to effectively utilize retrieved multimodal references
and improves their robustness against irrelevant or misleading information. The
source code is available at https://github.com/GasolSun36/SURf.",Jiashuo Sun
2024-09-21T16:46:15Z,http://arxiv.org/abs/2409.14192v2,"Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic
  Extraction","Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.",Hossein Sholehrasa
2024-09-21T17:41:46Z,http://arxiv.org/abs/2409.14206v1,"AI Assistants for Spaceflight Procedures: Combining Generative
  Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge
  Graphs With Augmented Reality Cues","This paper describes the capabilities and potential of the intelligent
personal assistant (IPA) CORE (Checklist Organizer for Research and
Exploration), designed to support astronauts during procedures onboard the
International Space Station (ISS), the Lunar Gateway station, and beyond. We
reflect on the importance of a reliable and flexible assistant capable of
offline operation and highlight the usefulness of audiovisual interaction using
augmented reality elements to intuitively display checklist information. We
argue that current approaches to the design of IPAs in space operations fall
short of meeting these criteria. Therefore, we propose CORE as an assistant
that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for
a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements
to ensure an intuitive understanding of procedure steps, reliability, offline
availability, and flexibility in terms of response style and procedure updates.",Oliver Bensch
2024-09-23T00:09:34Z,http://arxiv.org/abs/2409.14634v2,"Scideator: Human-LLM Scientific Idea Generation Grounded in
  Research-Paper Facet Recombination","The scientific ideation process often involves blending salient aspects of
existing papers to create new ideas. To see if large language models (LLMs) can
assist this process, we contribute Scideator, a novel mixed-initiative tool for
scientific ideation. Starting from a user-provided set of papers, Scideator
extracts key facets (purposes, mechanisms, and evaluations) from these and
relevant papers, allowing users to explore the idea space by interactively
recombining facets to synthesize inventive ideas. Scideator also helps users to
gauge idea novelty by searching the literature for potential overlaps and
showing automated novelty assessments and explanations. To support these tasks,
Scideator introduces four LLM-powered retrieval-augmented generation (RAG)
modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty
Checker, and Idea Novelty Iterator. In a within-subjects user study, 19
computer-science researchers identified significantly more interesting ideas
using Scideator compared to a strong baseline combining a scientific search
engine with LLM interaction.",Marissa Radensky
2024-09-23T10:23:19Z,http://arxiv.org/abs/2409.14878v1,"InterMind: A Doctor-Patient-Family Interactive Depression Assessment
  System Empowered by Large Language Models","Depression poses significant challenges to patients and healthcare
organizations, necessitating efficient assessment methods. Existing paradigms
typically focus on a patient-doctor way that overlooks multi-role interactions,
such as family involvement in the evaluation and caregiving process. Moreover,
current automatic depression detection (ADD) methods usually model depression
detection as a classification or regression task, lacking interpretability for
the decision-making process. To address these issues, we developed InterMind, a
doctor-patient-family interactive depression assessment system empowered by
large language models (LLMs). Our system enables patients and families to
contribute descriptions, generates assistive diagnostic reports for doctors,
and provides actionable insights, improving diagnostic precision and
efficiency. To enhance LLMs' performance in psychological counseling and
diagnostic interpretability, we integrate retrieval-augmented generation (RAG)
and chain-of-thoughts (CoT) techniques for data augmentation, which mitigates
the hallucination issue of LLMs in specific scenarios after instruction
fine-tuning. Quantitative experiments and professional assessments by
clinicians validate the effectiveness of our system.",Zhiyuan Zhou
2024-09-27T09:20:42Z,http://arxiv.org/abs/2409.18575v1,Corpus-informed Retrieval Augmented Generation of Clarifying Questions,"This study aims to develop models that generate corpus informed clarifying
questions for web search, in a way that ensures the questions align with the
available information in the retrieval corpus. We demonstrate the effectiveness
of Retrieval Augmented Language Models (RAG) in this process, emphasising their
ability to (i) jointly model the user query and retrieval corpus to pinpoint
the uncertainty and ask for clarifications end-to-end and (ii) model more
evidence documents, which can be used towards increasing the breadth of the
questions asked. However, we observe that in current datasets search intents
are largely unsupported by the corpus, which is problematic both for training
and evaluation. This causes question generation models to ``hallucinate'', ie.
suggest intents that are not in the corpus, which can have detrimental effects
in performance. To address this, we propose dataset augmentation methods that
align the ground truth clarifications with the retrieval corpus. Additionally,
we explore techniques to enhance the relevance of the evidence pool during
inference, but find that identifying ground truth intents within the corpus
remains challenging. Our analysis suggests that this challenge is partly due to
the bias of current datasets towards clarification taxonomies and calls for
data that can support generating corpus-informed clarifications.",Antonios Minas Krasakis
2024-09-28T23:59:46Z,http://arxiv.org/abs/2409.19487v3,"HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations","In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.",Ziyu Wang
2024-09-29T16:08:45Z,http://arxiv.org/abs/2409.19753v2,"CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex
  Knowledge Graph Question Answering","Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.",Yike Wu
2024-09-30T07:48:55Z,http://arxiv.org/abs/2409.20042v2,"Beyond Scores: A Modular RAG-Based System for Automatic Short Answer
  Scoring with Feedback","Automatic short answer scoring (ASAS) helps reduce the grading burden on
educators but often lacks detailed, explainable feedback. Existing methods in
ASAS with feedback (ASAS-F) rely on fine-tuning language models with limited
datasets, which is resource-intensive and struggles to generalize across
contexts. Recent approaches using large language models (LLMs) have focused on
scoring without extensive fine-tuning. However, they often rely heavily on
prompt engineering and either fail to generate elaborated feedback or do not
adequately evaluate it. In this paper, we propose a modular retrieval augmented
generation based ASAS-F system that scores answers and generates feedback in
strict zero-shot and few-shot learning scenarios. We design our system to be
adaptable to various educational tasks without extensive prompt engineering
using an automatic prompt generation framework. Results show an improvement in
scoring accuracy by 9\% on unseen questions compared to fine-tuning, offering a
scalable and cost-effective solution.",Menna Fateen
2024-09-30T17:51:15Z,http://arxiv.org/abs/2409.20550v1,"LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,
  and Mitigation","Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",Ziyao Zhang
2024-09-12T23:29:33Z,http://arxiv.org/abs/2410.00004v1,"Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy
  Similarity Searches and Domain Shift Generalization","The retrieval augmented generation (RAG) system such as Retro has been shown
to improve language modeling capabilities and reduce toxicity and
hallucinations by retrieving from a database of non-parametric memory
containing trillions of entries. We introduce Retro-li that shows retrieval can
also help using a small-scale database, but it demands more accurate and better
neighbors when searching in a smaller hence sparser non-parametric memory. This
can be met by using a proper semantic similarity search. We further propose
adding a regularization to the non-parametric memory for the first time: it
significantly reduces perplexity when the neighbor search operations are noisy
during inference, and it improves generalization when a domain shift occurs. We
also show that Retro-li's non-parametric memory can potentially be implemented
on analog in-memory computing hardware, exhibiting O(1) search time while
causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our
code is available at:
https://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",Gentiana Rashiti
2024-09-15T23:09:27Z,http://arxiv.org/abs/2410.01818v1,"Integrating AI's Carbon Footprint into Risk Management Frameworks:
  Strategies and Tools for Sustainable Compliance in Banking Sector","This paper examines the integration of AI's carbon footprint into the risk
management frameworks (RMFs) of the banking sector, emphasising its importance
in aligning with sustainability goals and regulatory requirements. As AI
becomes increasingly central to banking operations, its energy-intensive
processes contribute significantly to carbon emissions, posing environmental,
regulatory, and reputational risks. Regulatory frameworks such as the EU AI
Act, Corporate Sustainability Reporting Directive (CSRD), Corporate
Sustainability Due Diligence Directive (CSDDD), and the Prudential Regulation
Authority's SS1/23 are driving banks to incorporate environmental
considerations into their AI model governance. Recent advancements in AI
research, like the Open Mixture-of-Experts (OLMoE) framework and the Agentic
RAG framework, offer more efficient and dynamic AI models, reducing their
carbon footprint without compromising performance. Using these technological
examples, the paper outlines a structured approach for banks to identify,
assess, and mitigate AI's carbon footprint within their RMFs, including
adopting energy-efficient models, utilising green cloud computing, and
implementing lifecycle management.",Nataliya Tkachenko
2024-10-03T14:55:22Z,http://arxiv.org/abs/2410.02551v1,"ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration","We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.",Zixiang Wang
2024-09-30T06:27:53Z,http://arxiv.org/abs/2410.03727v2,"FaithEval: Can Your Language Model Stay Faithful to Context, Even If
  ""The Moon is Made of Marshmallows""","Ensuring faithfulness to context in large language models (LLMs) and
retrieval-augmented generation (RAG) systems is crucial for reliable deployment
in real-world applications, as incorrect or unsupported information can erode
user trust. Despite advancements on standard benchmarks, faithfulness
hallucination-where models generate responses misaligned with the provided
context-remains a significant challenge. In this work, we introduce FaithEval,
a novel and comprehensive benchmark tailored to evaluate the faithfulness of
LLMs in contextual scenarios across three diverse tasks: unanswerable,
inconsistent, and counterfactual contexts. These tasks simulate real-world
challenges where retrieval mechanisms may surface incomplete, contradictory, or
fabricated information. FaithEval comprises 4.9K high-quality problems in
total, validated through a rigorous four-stage context construction and
validation framework, employing both LLM-based auto-evaluation and human
validation. Our extensive study across a wide range of open-source and
proprietary models reveals that even state-of-the-art models often struggle to
remain faithful to the given context, and that larger models do not necessarily
exhibit improved faithfulness.Project is available at:
\url{https://github.com/SalesforceAIResearch/FaithEval}.",Yifei Ming
2024-10-05T15:13:22Z,http://arxiv.org/abs/2410.04194v1,Consistent Autoformalization for Constructing Mathematical Libraries,"Autoformalization is the task of automatically translating mathematical
content written in natural language to a formal language expression. The
growing language interpretation capabilities of Large Language Models (LLMs),
including in formal languages, are lowering the barriers for autoformalization.
However, LLMs alone are not capable of consistently and reliably delivering
autoformalization, in particular as the complexity and specialization of the
target domain grows. As the field evolves into the direction of systematically
applying autoformalization towards large mathematical libraries, the need to
improve syntactic, terminological and semantic control increases. This paper
proposes the coordinated use of three mechanisms, most-similar retrieval
augmented generation (MS-RAG), denoising steps, and auto-correction with syntax
error feedback (Auto-SEF) to improve autoformalization quality. The empirical
analysis, across different models, demonstrates that these mechanisms can
deliver autoformalizaton results which are syntactically, terminologically and
semantically more consistent. These mechanisms can be applied across different
LLMs and have shown to deliver improve results across different model types.",Lan Zhang
2024-10-06T11:23:56Z,http://arxiv.org/abs/2410.04452v1,"MindScope: Exploring cognitive biases in large language models through
  Multi-Agent Systems","Detecting cognitive biases in large language models (LLMs) is a fascinating
task that aims to probe the existing cognitive biases within these models.
Current methods for detecting cognitive biases in language models generally
suffer from incomplete detection capabilities and a restricted range of
detectable bias types. To address this issue, we introduced the 'MindScope'
dataset, which distinctively integrates static and dynamic elements. The static
component comprises 5,170 open-ended questions spanning 72 cognitive bias
categories. The dynamic component leverages a rule-based, multi-agent
communication framework to facilitate the generation of multi-round dialogues.
This framework is flexible and readily adaptable for various psychological
experiments involving LLMs. In addition, we introduce a multi-agent detection
method applicable to a wide range of detection tasks, which integrates
Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement
learning-based decision module. Demonstrating substantial effectiveness, this
method has shown to improve detection accuracy by as much as 35.10% compared to
GPT-4. Codes and appendix are available at
https://github.com/2279072142/MindScope.",Zhentao Xie
2024-10-07T00:17:37Z,http://arxiv.org/abs/2410.04660v1,"Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in
  Medicine","Biomedical knowledge is uniquely complex and structured, requiring distinct
reasoning strategies compared to other scientific disciplines like physics or
chemistry. Biomedical scientists do not rely on a single approach to reasoning;
instead, they use various strategies, including rule-based, prototype-based,
and case-based reasoning. This diversity calls for flexible approaches that
accommodate multiple reasoning strategies while leveraging in-domain knowledge.
We introduce KGARevion, a knowledge graph (KG) based agent designed to address
the complexity of knowledge-intensive medical queries. Upon receiving a query,
KGARevion generates relevant triplets by using the knowledge base of the LLM.
These triplets are then verified against a grounded KG to filter out erroneous
information and ensure that only accurate, relevant data contribute to the
final answer. Unlike RAG-based models, this multi-step process ensures
robustness in reasoning while adapting to different models of medical
reasoning. Evaluations on four gold-standard medical QA datasets show that
KGARevion improves accuracy by over 5.2%, outperforming 15 models in handling
complex medical questions. To test its capabilities, we curated three new
medical QA datasets with varying levels of semantic complexity, where KGARevion
achieved a 10.4% improvement in accuracy.",Xiaorui Su
2024-10-07T04:15:02Z,http://arxiv.org/abs/2410.04739v1,TableRAG: Million-Token Table Understanding with Language Models,"Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.",Si-An Chen
2024-10-07T05:27:22Z,http://arxiv.org/abs/2410.04759v1,"Driving with Regulation: Interpretable Decision-Making for Autonomous
  Vehicles with Retrieval-Augmented Reasoning via LLM","This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease.",Tianhui Cai
2024-10-07T13:03:45Z,http://arxiv.org/abs/2410.05004v1,Fast State Restoration in LLM Serving with HCache,"The growing complexity of LLM usage today, e.g., multi-round conversation and
retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)
reusable across user requests. Given the capacity constraints of GPU memory,
only a limited number of contexts can be cached on GPU for reusing. Existing
inference systems typically evict part of the KV cache and restore it by
recomputing it from the original tokens or offloading it to host storage for
later retrieval, both of which introduce substantial computational or I/O
overheads. We propose HCache, a novel LLM state restoration method. Its key
idea is to restore LLM states from intermediate activations and thus utilize
computational and I/O resources with low overhead. We enhance HCache with two
techniques, including i) a bubble-free restoration scheduler that integrates
resource-complementary methods to optimize the balance between computation and
IO tasks; and ii) a chunk-based storage manager to address the layout mismatch
issue (i.e., layer-before-token saving versus token-before-layer restoration).
Our evaluations, conducted using real-world tasks, show that HCache reduces the
TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less
storage space; compared to token recomputation, HCache achieves up to 5.73X
reduction in TTFT.",Shiwei Gao
2024-10-08T11:33:09Z,http://arxiv.org/abs/2410.05930v1,"Fortify Your Foundations: Practical Privacy and Security for Foundation
  Model Deployments In The Cloud","Foundation Models (FMs) display exceptional performance in tasks such as
natural language processing and are being applied across a growing range of
disciplines. Although typically trained on large public datasets, FMs are often
fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,
which rely on private data. This access, along with their size and costly
training, heightens the risk of intellectual property theft. Moreover,
multimodal FMs may expose sensitive information. In this work, we examine the
FM threat model and discuss the practicality and comprehensiveness of various
approaches for securing against them, such as ML-based methods and trusted
execution environments (TEEs). We demonstrate that TEEs offer an effective
balance between strong security properties, usability, and performance.
Specifically, we present a solution achieving less than 10\% overhead versus
bare metal for the full Llama2 7B and 13B inference pipelines running inside
\intel\ SGX and \intel\ TDX. We also share our configuration files and insights
from our implementation. To our knowledge, our work is the first to show the
practicality of TEEs for securing FMs.",Marcin Chrapek
2024-10-08T15:22:36Z,http://arxiv.org/abs/2410.06121v1,"Less is More: Making Smaller Language Models Competent Subgraph
  Retrievers for Multi-hop KGQA","Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.",Wenyu Huang
2024-10-10T01:21:48Z,http://arxiv.org/abs/2410.07520v2,News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News,"Large Language Models (LLMs) have fast become an essential tools to many
conversational chatbots due to their ability to provide coherent answers for
varied queries. Datasets used to train these LLMs are often a mix of generic
and synthetic samples, thus lacking the verification needed to provide correct
and verifiable answers for T.V. News.
  We collect and share a large collection of QA pairs extracted from
transcripts of news recordings from various news-channels across the United
States. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM
model. Our model surpasses base models of similar size on several open LLM
benchmarks. We further integrate and propose a RAG method to improve
contextualization of our answers and also point it to a verifiable news
recording.",Tarun Jain
2024-10-10T02:48:06Z,http://arxiv.org/abs/2410.07551v1,KRAG Framework for Enhancing LLMs in the Legal Domain,"This paper introduces Knowledge Representation Augmented Generation (KRAG), a
novel framework designed to enhance the capabilities of Large Language Models
(LLMs) within domain-specific applications. KRAG points to the strategic
inclusion of critical knowledge entities and relationships that are typically
absent in standard data sets and which LLMs do not inherently learn. In the
context of legal applications, we present Soft PROLEG, an implementation model
under KRAG, which uses inference graphs to aid LLMs in delivering structured
legal reasoning, argumentation, and explanations tailored to user inquiries.
The integration of KRAG, either as a standalone framework or in tandem with
retrieval augmented generation (RAG), markedly improves the ability of language
models to navigate and solve the intricate challenges posed by legal texts and
terminologies. This paper details KRAG's methodology, its implementation
through Soft PROLEG, and potential broader applications, underscoring its
significant role in advancing natural language understanding and processing in
specialized knowledge domains.",Nguyen Ha Thanh
2024-10-11T11:41:02Z,http://arxiv.org/abs/2410.08731v1,"Developing a Pragmatic Benchmark for Assessing Korean Legal Language
  Understanding in Large Language Models","Large language models (LLMs) have demonstrated remarkable performance in the
legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However
their efficacy remains limited for non-standardized tasks and tasks in
languages other than English. This underscores the need for careful evaluation
of LLMs within each legal system before application. Here, we introduce KBL, a
benchmark for assessing the Korean legal language understanding of LLMs,
consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning
tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510
examples). First two datasets were developed in close collaboration with
lawyers to evaluate LLMs in practical scenarios in a certified manner.
Furthermore, considering legal practitioners' frequent use of extensive legal
documents for research, we assess LLMs in both a closed book setting, where
they rely solely on internal knowledge, and a retrieval-augmented generation
(RAG) setting, using a corpus of Korean statutes and precedents. The results
indicate substantial room and opportunities for improvement.",Yeeun Kim
2024-10-12T19:24:18Z,http://arxiv.org/abs/2410.09623v1,"Quebec Automobile Insurance Question-Answering With Retrieval-Augmented
  Generation","Large Language Models (LLMs) perform outstandingly in various downstream
tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has
been shown to improve performance for legal question answering (Nuruzzaman and
Hussain, 2020; Louis et al., 2024). However, there are limited applications in
insurance questions-answering, a specific type of legal document. This paper
introduces two corpora: the Quebec Automobile Insurance Expertise Reference
Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance
Questions. Our study leverages both corpora to automatically and manually
assess a GPT4-o, a state-of-the-art LLM, to answer Quebec automobile insurance
questions. Our results demonstrate that, on average, using our expertise
reference corpus generates better responses on both automatic and manual
evaluation metrics. However, they also highlight that LLM QA is unreliable
enough for mass utilization in critical areas. Indeed, our results show that
between 5% to 13% of answered questions include a false statement that could
lead to customer misunderstanding.",David Beauchemin
2024-10-12T19:38:09Z,http://arxiv.org/abs/2410.09629v1,"Synthetic Knowledge Ingestion: Towards Knowledge Refinement and
  Injection for Enhancing Large Language Models","Large language models (LLMs) are proficient in capturing factual knowledge
across various domains. However, refining their capabilities on previously seen
knowledge or integrating new knowledge from external sources remains a
significant challenge. In this work, we propose a novel synthetic knowledge
ingestion method called Ski, which leverages fine-grained synthesis,
interleaved generation, and assemble augmentation strategies to construct
high-quality data representations from raw knowledge sources. We then integrate
Ski and its variations with three knowledge injection techniques: Retrieval
Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual
Pre-training (CPT) to inject and refine knowledge in language models. Extensive
empirical experiments are conducted on various question-answering tasks
spanning finance, biomedicine, and open-generation domains to demonstrate that
Ski significantly outperforms baseline methods by facilitating effective
knowledge injection. We believe that our work is an important step towards
enhancing the factual accuracy of LLM outputs by refining knowledge
representation and injection capabilities.",Jiaxin Zhang
2024-10-13T15:11:31Z,http://arxiv.org/abs/2410.09871v1,"A Comparative Study of PDF Parsing Tools Across Diverse Document
  Categories","PDF is one of the most prominent data formats, making PDF parsing crucial for
information extraction and retrieval, particularly with the rise of RAG
systems. While various PDF parsing tools exist, their effectiveness across
different document types remains understudied, especially beyond academic
papers. Our research aims to address this gap by comparing 10 popular PDF
parsing tools across 6 document categories using the DocLayNet dataset. These
tools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2,
Unstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat
and Table Transformer(TATR). We evaluated both text extraction and table
detection capabilities. For text extraction, PyMuPDF and pypdfium generally
outperformed others, but all parsers struggled with Scientific and Patent
documents. For these challenging categories, learning-based tools like Nougat
demonstrated superior performance. In table detection, TATR excelled in the
Financial, Patent, Law & Regulations, and Scientific categories. Table
detection tool Camelot performed best for tender documents, while PyMuPDF
performed superior in the Manual category. Our findings highlight the
importance of selecting appropriate parsing tools based on document type and
specific tasks, providing valuable insights for researchers and practitioners
working with diverse document sources.",Narayan S. Adhikari
2024-10-14T10:26:57Z,http://arxiv.org/abs/2410.10360v2,"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented
  Language Models with Parameter Decoupling and Tailored Tuning","Retrieval-Augmented Generation (RAG) offers an effective solution to the
issues faced by Large Language Models (LLMs) in hallucination generation and
knowledge obsolescence by incorporating externally retrieved knowledge.
However, existing methods lack effective control mechanisms for integrating
internal and external knowledge. Inspired by human cognitive processes, we
propose Parenting, a novel framework that decouples, identifies, and
purposefully optimizes parameter subspaces related to adherence and robustness.
Specifically, Parenting utilizes a key parameter mining method that combines
forward and backward propagation signals to localize subspaces representing
different capabilities. Then, Parenting employs a type-tailored tuning
strategy, applying specific and appropriate optimizations to different
subspaces, aiming to achieve a balanced enhancement of both adherence and
robustness. Extensive experiments on various datasets and models validate the
effectiveness and generalizability of our method.",Yongxin Xu
2024-10-14T13:18:20Z,http://arxiv.org/abs/2410.10481v1,"Model-Based Differentially Private Knowledge Transfer for Large Language
  Models","As large language models (LLMs) become increasingly prevalent in web
services, effectively leveraging domain-specific knowledge while ensuring
privacy has become critical. Existing methods, such as retrieval-augmented
generation (RAG) and differentially private data synthesis, often compromise
either the utility of domain knowledge or the privacy of sensitive data,
limiting their applicability in specialized domains. To address these
challenges, we propose \textit{Llamdex}, a novel framework that integrates
privacy-preserving, domain-specific models into LLMs. Our approach
significantly enhances the accuracy of domain-specific tasks, achieving up to a
26\% improvement compared to existing methods under the same differential
privacy constraints. Experimental results show that Llamdex not only improves
the accuracy of LLM responses but also maintains comparable inference
efficiency to the original LLM, highlighting its potential for real-world
applications.",Zhaomin Wu
2024-10-16T11:43:17Z,http://arxiv.org/abs/2410.12475v2,"Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional
  Safety Engineering","Functional safety is a critical aspect of automotive engineering,
encompassing all phases of a vehicle's lifecycle, including design,
development, production, operation, and decommissioning. This domain involves
highly knowledge-intensive tasks. This paper introduces Aegis: An Advanced
LLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is
specifically designed to support complex functional safety tasks within the
automotive sector. It is tailored to perform Hazard Analysis and Risk
Assessment(HARA), document Functional Safety Requirements(FSR), and plan test
cases for Automatic Emergency Braking(AEB) systems. The most advanced version,
Aegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective
mechanisms to enhance its capability in managing complex, knowledge-intensive
tasks. Additionally, targeted prompt refinement by professional functional
safety practitioners can significantly optimize Aegis's performance in the
functional safety domain. This paper demonstrates the potential of Aegis to
improve the efficiency and effectiveness of functional safety processes in
automotive engineering.",Lu Shi
2024-10-17T03:38:54Z,http://arxiv.org/abs/2410.13192v2,"Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented
  Generation with Large Language Models","The integration of documents generated by LLMs themselves (Self-Docs)
alongside retrieved documents has emerged as a promising strategy for
retrieval-augmented generation systems. However, previous research primarily
focuses on optimizing the use of Self-Docs, with their inherent properties
remaining underexplored. To bridge this gap, we first investigate the overall
effectiveness of Self-Docs, identifying key factors that shape their
contribution to RAG performance (RQ1). Building on these insights, we develop a
taxonomy grounded in Systemic Functional Linguistics to compare the influence
of various Self-Docs categories (RQ2) and explore strategies for combining them
with external sources (RQ3). Our findings reveal which types of Self-Docs are
most beneficial and offer practical guidelines for leveraging them to achieve
significant improvements in knowledge-intensive question answering tasks.",Jiatao Li
2024-10-17T04:30:46Z,http://arxiv.org/abs/2410.13210v1,"FaithBench: A Diverse Hallucination Benchmark for Summarization by
  Modern LLMs","Summarization is one of the most common tasks performed by large language
models (LLMs), especially in applications like Retrieval-Augmented Generation
(RAG). However, existing evaluations of hallucinations in LLM-generated
summaries, and evaluations of hallucination detection models both suffer from a
lack of diversity and recency in the LLM and LLM families considered. This
paper introduces FaithBench, a summarization hallucination benchmark comprising
challenging hallucinations made by 10 modern LLMs from 8 different families,
with ground truth annotations by human experts. ``Challenging'' here means
summaries on which popular, state-of-the-art hallucination detection models,
including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and
GPT-3.5-Turbo produce the least hallucinations. However, even the best
hallucination detection models have near 50\% accuracies on FaithBench,
indicating lots of room for future improvement. The repo is
https://github.com/vectara/FaithBench",Forrest Sheng Bao
2024-10-17T12:56:52Z,http://arxiv.org/abs/2410.13510v1,"GeoCoder: Solving Geometry Problems by Generating Modular Code through
  Vision-Language Models","Geometry problem-solving demands advanced reasoning abilities to process
multimodal inputs and employ mathematical knowledge effectively.
Vision-language models (VLMs) have made significant progress in various
multimodal tasks. Yet, they still struggle with geometry problems and are
significantly limited by their inability to perform mathematical operations not
seen during pre-training, such as calculating the cosine of an arbitrary angle,
and by difficulties in correctly applying relevant geometry formulas. To
overcome these challenges, we present GeoCoder, which leverages modular
code-finetuning to generate and execute code using a predefined geometry
function library. By executing the code, we achieve accurate and deterministic
calculations, contrasting the stochastic nature of autoregressive token
prediction, while the function library minimizes errors in formula usage. We
also propose a multimodal retrieval-augmented variant of GeoCoder, named
RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving
functions from the geometry library, thereby reducing reliance on parametric
memory. Our modular code-finetuning approach enhances the geometric reasoning
capabilities of VLMs, yielding an average improvement of over 16% across
various question complexities on the GeomVerse dataset compared to other
finetuning methods.",Aditya Sharma
2024-10-17T15:29:57Z,http://arxiv.org/abs/2410.13671v1,"HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World
  Multilingual Settings","Assessing the capabilities and limitations of large language models (LLMs)
has garnered significant interest, yet the evaluation of multiple models in
real-world scenarios remains rare. Multilingual evaluation often relies on
translated benchmarks, which typically do not capture linguistic and cultural
nuances present in the source language. This study provides an extensive
assessment of 24 LLMs on real world data collected from Indian patients
interacting with a medical chatbot in Indian English and 4 other Indic
languages. We employ a uniform Retrieval Augmented Generation framework to
generate responses, which are evaluated using both automated techniques and
human evaluators on four specific metrics relevant to our application. We find
that models vary significantly in their performance and that instruction tuned
Indic models do not always perform well on Indic language queries. Further, we
empirically show that factual correctness is generally lower for responses to
Indic queries compared to English queries. Finally, our qualitative work shows
that code-mixed and culturally relevant queries in our dataset pose challenges
to evaluated models.",Varun Gumma
2024-10-19T01:35:26Z,http://arxiv.org/abs/2410.14931v1,"""Ghost of the past"": identifying and resolving privacy leakage from
  LLM's memory through proactive user interaction","Memories, encompassing past inputs in context window and retrieval-augmented
generation (RAG), frequently surface during human-LLM interactions, yet users
are often unaware of their presence and the associated privacy risks. To
address this, we propose MemoAnalyzer, a system for identifying, visualizing,
and managing private information within memories. A semi-structured interview
(N=40) revealed that low privacy awareness was the primary challenge, while
proactive privacy control emerged as the most common user need. MemoAnalyzer
uses a prompt-based method to infer and identify sensitive information from
aggregated past inputs, allowing users to easily modify sensitive content.
Background color temperature and transparency are mapped to inference
confidence and sensitivity, streamlining privacy adjustments. A 5-day
evaluation (N=36) comparing MemoAnalyzer with the default GPT setting and a
manual modification baseline showed MemoAnalyzer significantly improved privacy
awareness and protection without compromising interaction speed. Our study
contributes to privacy-conscious LLM design, offering insights into privacy
protection for Human-AI interactions.",Shuning Zhang
2024-10-20T04:48:12Z,http://arxiv.org/abs/2410.15284v1,Customized FinGPT Search Agents Using Foundation Models,"Current large language models (LLMs) have proven useful for analyzing
financial data, but most existing models, such as BloombergGPT and FinGPT, lack
customization for specific user needs. In this paper, we address this gap by
developing FinGPT Search Agents tailored for two types of users: individuals
and institutions. For individuals, we leverage Retrieval-Augmented Generation
(RAG) to integrate local documents and user-specified data sources. For
institutions, we employ dynamic vector databases and fine-tune models on
proprietary data. There are several key issues to address, including data
privacy, the time-sensitive nature of financial information, and the need for
fast responses. Experiments show that FinGPT agents outperform existing models
in accuracy, relevance, and response time, making them practical for real-world
applications.",Felix Tian
2024-10-21T06:11:38Z,http://arxiv.org/abs/2410.15667v1,RAC: Efficient LLM Factuality Correction with Retrieval Augmentation,"Large Language Models (LLMs) exhibit impressive results across a wide range
of natural language processing (NLP) tasks, yet they can often produce
factually incorrect outputs. This paper introduces a simple but effective
low-latency post-correction method, \textbf{Retrieval Augmented Correction
(RAC)}, aimed at enhancing the factual performance of LLMs without requiring
additional fine-tuning. Our method is general and can be used with any
instruction-tuned LLM, and has greatly reduced latency compared to prior
approaches. RAC decomposes the LLM's output into atomic facts and applies a
fine-grained verification and correction process with retrieved content to
verify and correct the LLM-generated output. Our extensive experiments show
that RAC yields up to 30\% improvements over state-of-the-art baselines across
two popular factuality evaluation datasets, validating its efficacy and
robustness in both with and without the integration of Retrieval-Augmented
Generation (RAG) across different LLMs.\footnote{Our code is at
\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}",Changmao Li
2024-10-21T11:02:18Z,http://arxiv.org/abs/2410.15884v1,"Using GPT Models for Qualitative and Quantitative News Analytics in the
  2024 US Presidental Election Process","The paper considers an approach of using Google Search API and GPT-4o model
for qualitative and quantitative analyses of news through retrieval-augmented
generation (RAG). This approach was applied to analyze news about the 2024 US
presidential election process. Different news sources for different time
periods have been analyzed. Quantitative scores generated by GPT model have
been analyzed using Bayesian regression to derive trend lines. The
distributions found for the regression parameters allow for the analysis of
uncertainty in the election process. The obtained results demonstrate that
using the GPT models for news analysis, one can get informative analytics and
provide key insights that can be applied in further analyses of election
processes.",Bohdan M. Pavlyshenko
2024-10-22T11:51:09Z,http://arxiv.org/abs/2410.16917v2,DNAHLM -- DNA sequence and Human Language mixed large language Model,"There are already many DNA large language models, but most of them still
follow traditional uses, such as extracting sequence features for
classification tasks. More innovative applications of large language models,
such as prompt engineering, RAG, and zero-shot or few-shot prediction, remain
challenging for DNA-based models. The key issue lies in the fact that DNA
models and human natural language models are entirely separate; however,
techniques like prompt engineering require the use of natural language, thereby
significantly limiting the application of DNA large language models. This paper
introduces a pre-trained model trained on the GPT-2 network, combining DNA
sequences and English text, and uses a unified BPE tokenization method. We then
convert classification and other downstream tasks into Alpaca format
instruction data, and perform instruction fine-tuning on this pre-trained model
to create a fine-tuned model capable of handling multiple tasks. The model has
demonstrated its effectiveness in DNA related zero-shot prediction and
multitask application. This research provides a highly promising direction for
building a unified DNA sequence task framework.",Wang Liang
2024-10-08T16:26:18Z,http://arxiv.org/abs/2410.18104v1,"ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless
  Environment Perception","Large language models (LLMs) hold significant promise in advancing network
management and orchestration in 6G and beyond networks. However, existing LLMs
are limited in domain-specific knowledge and their ability to handle
multi-modal sensory data, which is critical for real-time situational awareness
in dynamic wireless environments. This paper addresses this gap by introducing
ENWAR, an ENvironment-aWARe retrieval augmented generation-empowered
multi-modal LLM framework. ENWAR seamlessly integrates multi-modal sensory
inputs to perceive, interpret, and cognitively process complex wireless
environments to provide human-interpretable situational awareness. ENWAR is
evaluated on the GPS, LiDAR, and camera modality combinations of DeepSense6G
dataset with state-of-the-art LLMs such as Mistral-7b/8x7b and
LLaMa3.1-8/70/405b. Compared to general and often superficial environmental
descriptions of these vanilla LLMs, ENWAR delivers richer spatial analysis,
accurately identifies positions, analyzes obstacles, and assesses line-of-sight
between vehicles. Results show that ENWAR achieves key performance indicators
of up to 70% relevancy, 55% context recall, 80% correctness, and 86%
faithfulness, demonstrating its efficacy in multi-modal perception and
interpretation.",Ahmad M. Nazar
2024-10-08T17:36:48Z,http://arxiv.org/abs/2410.18105v1,"Improving Embedding Accuracy for Document Retrieval Using Entity
  Relationship Maps and Model-Aware Contrastive Sampling","In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic
eXtraction), a 7-billion parameter decoder-only text Feature Extraction Model,
specifically designed for Document Retrieval-Augmented Generation (RAG) tasks.
Our approach employs two training techniques that yield an emergent improvement
in factual focus: (1) Pre-convergence interrupted fine-tuning using Structured
Entity Relationship Maps as training data input: designed to shift the model's
attention and create a bias towards factual content rather than semantic style
- this enhances plain text performance despite not being directly trained for
it; and (2) Model-Aware Contrastive Sampling, creating a balanced and evenly
distributed collation map of hard and soft negatives directly informed by the
base model's competency. This combined methodology yields significant
improvements, enhancing plain text query/document pair retrieval to achieve an
absolute rank@1 accuracy of 90.86% (an increase of 6.26% compared to the next
leading model) in our evaluation, and reducing training data input context size
by an average of 37.71% compared to plain text for both queries and document
texts. Based on our evaluations, our model establishes a new state-of-the-art
standard in text feature extraction for longer context document retrieval
tasks.",Thea Aviss
2024-10-24T00:49:46Z,http://arxiv.org/abs/2410.18344v1,"Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned
  and Retrieval-Augmented Generation Models","This paper introduces a novel approach to enhancing closed-domain Question
Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley
National Laboratory (LBL) Science Information Technology (ScienceIT) domain.
Utilizing a rich dataset derived from the ScienceIT documentation, our study
embarks on a detailed comparison of two fine-tuned large language models and
five retrieval-augmented generation (RAG) models. Through data processing
techniques, we transform the documentation into structured
context-question-answer triples, leveraging the latest Large Language Models
(AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for
data-driven insights. Additionally, we introduce the Aggregated Knowledge Model
(AKM), which synthesizes responses from the seven models mentioned above using
K-means clustering to select the most representative answers. The evaluation of
these models across multiple metrics offers a comprehensive look into their
effectiveness and suitability for the LBL ScienceIT environment. The results
demonstrate the potential benefits of integrating fine-tuning and
retrieval-augmented strategies, highlighting significant performance
improvements achieved with the AKM. The insights gained from this study can be
applied to develop specialized QA systems tailored to specific domains.",Fengchen Liu
2024-10-24T09:16:09Z,http://arxiv.org/abs/2410.18565v1,"Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and
  Evaluation","We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for
Polish language processing. Trained on curated Polish corpora, this model
addresses key challenges in language model development through innovative
techniques. These include Weighted Instruction Cross-Entropy Loss, which
balances the learning of different instruction types, and Adaptive Learning
Rate, which dynamically adjusts the learning rate based on training progress.
To evaluate performance, we created the Open PL LLM Leaderboard and Polish
MT-Bench, novel frameworks assessing various NLP tasks and conversational
abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9
percentage point increase in average score compared to Mistral-7B-v0.1 on the
RAG Reader task. It also excels in the Polish MT-Bench, particularly in
Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model
represents a substantial advancement in Polish language AI, offering a powerful
tool for diverse linguistic applications and setting new benchmarks in the
field.",Krzysztof Ociepa
2024-10-24T17:13:39Z,http://arxiv.org/abs/2410.18926v1,"LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor
  Search","Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets.",Elias JÃ¤Ã¤saari
2024-10-24T20:07:08Z,http://arxiv.org/abs/2410.19135v1,PDL: A Declarative Prompt Programming Language,"Large language models (LLMs) have taken the world by storm by making many
previously difficult uses of AI feasible. LLMs are controlled via highly
expressive textual prompts and return textual answers. Unfortunately, this
unstructured text as input and output makes LLM-based applications brittle.
This motivates the rise of prompting frameworks, which mediate between LLMs and
the external world. However, existing prompting frameworks either have a high
learning curve or take away control over the exact prompts from the developer.
To overcome this dilemma, this paper introduces the Prompt Declaration Language
(PDL). PDL is a simple declarative data-oriented language that puts prompts at
the forefront, based on YAML. PDL works well with many LLM platforms and LLMs.
It supports writing interactive applications that call LLMs and tools, and
makes it easy to implement common use-cases such as chatbots, RAG, or agents.
We hope PDL will make prompt programming simpler, less brittle, and more
enjoyable.",Mandana Vaziri
2024-10-25T17:53:47Z,http://arxiv.org/abs/2410.19727v1,"FISHNET: Financial Intelligence from Sub-querying, Harmonizing,
  Neural-Conditioning, Expert Swarms, and Task Planning","Financial intelligence generation from vast data sources has typically relied
on traditional methods of knowledge-graph construction or database engineering.
Recently, fine-tuned financial domain-specific Large Language Models (LLMs),
have emerged. While these advancements are promising, limitations such as high
inference costs, hallucinations, and the complexity of concurrently analyzing
high-dimensional financial data, emerge. This motivates our invention FISHNET
(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,
Expert swarming, and Task planning), an agentic architecture that accomplishes
highly complex analytical tasks for more than 98,000 regulatory filings that
vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows
remarkable performance for financial insight generation (61.8% success rate
over 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to
empirically prove the success of FISHNET, each agent's importance, and the
optimized performance of assembling all agents. Our modular architecture can be
leveraged for a myriad of use-cases, enabling scalability, flexibility, and
data integrity that are critical for financial tasks.",Nicole Cho
2024-10-30T20:28:10Z,http://arxiv.org/abs/2410.23437v1,Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment,"Retrieval-Augmented Generation (RAG) systems enhance text generation by
incorporating external knowledge but often struggle when retrieving context
across different text modalities due to semantic gaps. We introduce a
generalized projection-based method, inspired by adapter modules in transfer
learning, that efficiently bridges these gaps between various text types, such
as programming code and pseudocode, or English and French sentences. Our
approach emphasizes speed, accuracy, and data efficiency, requiring minimal
resources for training and inference. By aligning embeddings from heterogeneous
text modalities into a unified space through a lightweight projection network,
our model significantly outperforms traditional retrieval methods like the
Okapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while
approaching the accuracy of Sentence Transformers. Extensive evaluations
demonstrate the effectiveness and generalizability of our method across
different tasks, highlighting its potential for real-time, resource-constrained
applications.",Arihan Yadav
2024-11-04T18:21:53Z,http://arxiv.org/abs/2411.02353v1,"Social-RAG: Retrieving from Group Interactions to Socially Ground
  Proactive AI Generation to Group Preferences","AI agents are increasingly tasked with making proactive suggestions in online
spaces where groups collaborate, but can be unhelpful or even annoying, due to
not fitting the group's preferences or behaving in socially inappropriate ways.
Fortunately, group spaces have a rich history of prior social interactions and
affordances for social feedback to support creating agents that align to a
group's interests and norms. We present Social-RAG, a workflow for grounding
agents to social information about a group, which retrieves from prior group
interactions, selects relevant social signals, and then feeds the context into
a large language model to generate messages to the group. We implement this
into PaperPing, our system that posts academic paper recommendations in group
chat, leveraging social signals determined from formative studies with 39
researchers. From a three-month deployment in 18 channels, we observed
PaperPing posted relevant messages in groups without disrupting their existing
social practices, fostering group common ground.",Ruotong Wang
2024-11-06T13:51:42Z,http://arxiv.org/abs/2411.03920v1,"RAGulator: Lightweight Out-of-Context Detectors for Grounded Text
  Generation","Real-time detection of out-of-context LLM outputs is crucial for enterprises
looking to safely adopt RAG applications. In this work, we train lightweight
models to discriminate LLM-generated text that is semantically out-of-context
from retrieved text documents. We preprocess a combination of summarisation and
semantic textual similarity datasets to construct training data using minimal
resources. We find that DeBERTa is not only the best-performing model under
this pipeline, but it is also fast and does not require additional text
preprocessing or feature engineering. While emerging work demonstrates that
generative LLMs can also be fine-tuned and used in complex data pipelines to
achieve state-of-the-art performance, we note that speed and resource limits
are important considerations for on-premise deployment.",Ian Poey
2024-11-07T06:12:38Z,http://arxiv.org/abs/2411.04459v1,"GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial
  Fraud Detection","With the increasing number of financial services available online, the rate
of financial fraud has also been increasing. The traffic and transaction rates
on the internet have increased considerably, leading to a need for fast
decision-making. Financial institutions also have stringent regulations that
often require transparency and explainability of the decision-making process.
However, most state-of-the-art algorithms currently used in the industry are
highly parameterized black-box models that rely on complex computations to
generate a score. These algorithms are inherently slow and lack the
explainability and speed of traditional rule-based learners. This work
introduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational
GPT model to guide the MCTS, significantly enhancing its convergence speed and
the quality of the generated expressions which are further extracted to rules.
Our experiments show that SR-MCTS can detect fraud more efficiently than widely
used methods in the industry while providing substantial insights into the
decision-making process.",Prashank Kadam
2024-11-07T06:51:24Z,http://arxiv.org/abs/2411.04473v1,ML-Promise: A Multilingual Dataset for Corporate Promise Verification,"Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.",Yohei Seki
2024-11-08T06:12:56Z,http://arxiv.org/abs/2411.05349v1,"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent
  Cluster Diagnosis System and Evaluation Framework","Recent advancements in Large Language Models (LLMs) and related technologies
such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
enabled the creation of autonomous intelligent systems capable of performing
cluster diagnostics and troubleshooting. By integrating these technologies with
self-play methodologies, we have developed an LLM-agent system designed to
autonomously diagnose and resolve issues within AI clusters. Our innovations
include a knowledge base tailored for cluster diagnostics, enhanced LLM
algorithms, practical deployment strategies for agents, and a benchmark
specifically designed for evaluating LLM capabilities in this domain. Through
extensive experimentation across multiple dimensions, we have demonstrated the
superiority of our system in addressing the challenges faced in cluster
diagnostics, particularly in detecting and rectifying performance issues more
efficiently and accurately than traditional methods.",Honghao Shi
2024-11-06T15:32:28Z,http://arxiv.org/abs/2411.05844v1,"LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration","GraphRAG addresses significant challenges in Retrieval-Augmented Generation
(RAG) by leveraging graphs with embedded knowledge to enhance the reasoning
capabilities of Large Language Models (LLMs). Despite its promising potential,
the GraphRAG community currently lacks a unified framework for fine-grained
decomposition of the graph-based knowledge retrieval process. Furthermore,
there is no systematic categorization or evaluation of existing solutions
within the retrieval process. In this paper, we present LEGO-GraphRAG, a
modular framework that decomposes the retrieval process of GraphRAG into three
interconnected modules: subgraph-extraction, path-filtering, and
path-refinement. We systematically summarize and classify the algorithms and
neural network (NN) models relevant to each module, providing a clearer
understanding of the design space for GraphRAG instances. Additionally, we
identify key design factors, such as Graph Coupling and Computational Cost,
that influence the effectiveness of GraphRAG implementations. Through extensive
empirical studies, we construct high-quality GraphRAG instances using a
representative selection of solutions and analyze their impact on retrieval and
reasoning performance. Our findings offer critical insights into optimizing
GraphRAG instance design, ultimately contributing to the advancement of more
accurate and contextually relevant LLM applications.",Yukun Cao
2024-11-10T15:21:30Z,http://arxiv.org/abs/2411.06493v2,LProtector: An LLM-driven Vulnerability Detection System,"This paper presents LProtector, an automated vulnerability detection system
for C/C++ codebases driven by the large language model (LLM) GPT-4o and
Retrieval-Augmented Generation (RAG). As software complexity grows, traditional
methods face challenges in detecting vulnerabilities effectively. LProtector
leverages GPT-4o's powerful code comprehension and generation capabilities to
perform binary classification and identify vulnerabilities within target
codebases. We conducted experiments on the Big-Vul dataset, showing that
LProtector outperforms two state-of-the-art baselines in terms of F1 score,
demonstrating the potential of integrating LLMs with vulnerability detection.",Ze Sheng
2024-11-08T21:03:54Z,http://arxiv.org/abs/2411.07264v1,Multi-Document Financial Question Answering using LLMs,"We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",Shalin Shah
2024-11-13T04:20:20Z,http://arxiv.org/abs/2411.08324v1,"Are LLMs Prescient? A Continuous Evaluation using Daily News as the
  Oracle","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict ""future"" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.",Hui Dai
2024-11-13T05:40:24Z,http://arxiv.org/abs/2411.08348v1,"Refining Translations with LLMs: A Constraint-Aware Iterative Prompting
  Approach","Large language models (LLMs) have demonstrated remarkable proficiency in
machine translation (MT), even without specific training on the languages in
question. However, translating rare words in low-resource or domain-specific
contexts remains challenging for LLMs. To address this issue, we propose a
multi-step prompt chain that enhances translation faithfulness by prioritizing
key terms crucial for semantic accuracy. Our method first identifies these
keywords and retrieves their translations from a bilingual dictionary,
integrating them into the LLM's context using Retrieval-Augmented Generation
(RAG). We further mitigate potential output hallucinations caused by long
prompts through an iterative self-checking mechanism, where the LLM refines its
translations based on lexical and semantic constraints. Experiments using Llama
and Qwen as base models on the FLORES-200 and WMT datasets demonstrate
significant improvements over baselines, highlighting the effectiveness of our
approach in enhancing translation faithfulness and robustness, particularly in
low-resource scenarios.",Shangfeng Chen
2024-11-13T09:11:56Z,http://arxiv.org/abs/2411.08449v2,Towards Evaluating Large Language Models for Graph Query Generation,"Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.",Siraj Munir
2024-11-13T09:40:37Z,http://arxiv.org/abs/2411.08469v2,"Building Trustworthy AI: Transparent AI Systems via Large Language
  Models, Ontologies, and Logical Reasoning (TranspNet)","Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their ""black box"" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.",Fadi Al Machot
2024-11-13T12:44:41Z,http://arxiv.org/abs/2411.08574v1,"Practitioners' Discussions on Building LLM-based Applications for
  Production","\textit{Background}: Large language models (LLMs) have become a paramount
interest of researchers and practitioners alike, yet a comprehensive overview
of key considerations for those developing LLM-based systems is lacking. This
study addresses this gap by collecting and mapping the topics practitioners
discuss online, offering practical insights into where priorities lie in
developing LLM-based applications. \textit{Method}: We collected 189 videos
from 2022 to 2024 from practitioners actively developing such systems and
discussing various aspects they encounter during development and deployment of
LLMs in production. We analyzed the transcripts using BERTopic, then manually
sorted and merged the generated topics into themes, leading to a total of 20
topics in 8 themes. \textit{Results}: The most prevalent topics fall within the
theme Design \& Architecture, with a strong focus on retrieval-augmented
generation (RAG) systems. Other frequently discussed topics include model
capabilities and enhancement techniques (e.g., fine-tuning, prompt
engineering), infrastructure and tooling, and risks and ethical challenges.
\textit{Implications}: Our results highlight current discussions and challenges
in deploying LLMs in production. This way, we provide a systematic overview of
key aspects practitioners should be aware of when developing LLM-based
applications. We further pale off topics of interest for academics where
further research is needed.",Alina Mailach
2024-11-19T07:03:19Z,http://arxiv.org/abs/2411.12280v1,"Large Language Models for Material Property Predictions: elastic
  constant tensor prediction and materials design","Efficient and accurate prediction of material properties is critical for
advancing materials design and applications. The rapid-evolution of large
language models (LLMs) presents a new opportunity for material property
predictions, complementing experimental measurements and multi-scale
computational methods. We focus on predicting the elastic constant tensor, as a
case study, and develop domain-specific LLMs for predicting elastic constants
and for materials discovery. The proposed ElaTBot LLM enables simultaneous
prediction of elastic constant tensors, bulk modulus at finite temperatures,
and the generation of new materials with targeted properties. Moreover, the
capabilities of ElaTBot are further enhanced by integrating with general LLMs
(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized
variant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,
reduces the prediction errors by 33.1% compared with domain-specific, material
science LLMs (Darwin) trained on the same dataset. This natural language-based
approach lowers the barriers to computational materials science and highlights
the broader potential of LLMs for material property predictions and inverse
design.",Siyu Liu
2024-11-19T07:16:48Z,http://arxiv.org/abs/2411.12287v2,"CUE-M: Contextual Understanding and Enhanced Search with Multimodal
  Large Language Model","The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large
Language Models (MLLMs) has revolutionized information retrieval and expanded
the practical applications of AI. However, current systems struggle in
accurately interpreting user intent, employing diverse retrieval strategies,
and effectively filtering unintended or inappropriate responses, limiting their
effectiveness. This paper introduces Contextual Understanding and Enhanced
Search with MLLM (CUE-M), a novel multimodal search framework that addresses
these challenges through a multi-stage pipeline comprising image context
enrichment, intent refinement, contextual query generation, external API
integration, and relevance-based filtering. CUE-M incorporates a robust
filtering pipeline combining image-based, text-based, and multimodal
classifiers, dynamically adapting to instance- and category-specific concern
defined by organizational policies. Evaluations on a multimodal Q&A dataset and
a public safety benchmark demonstrate that CUE-M outperforms baselines in
accuracy, knowledge integration, and safety, advancing the capabilities of
multimodal retrieval systems.",Dongyoung Go
2024-11-19T16:54:45Z,http://arxiv.org/abs/2411.12644v2,"CodeXEmbed: A Generalist Embedding Model Family for Multiligual and
  Multi-task Code Retrieval","Despite the success of text retrieval in many NLP tasks, code retrieval
remains a largely underexplored area. Most text retrieval systems are tailored
for natural language queries, often neglecting the specific challenges of
retrieving code. This gap leaves existing models unable to effectively capture
the diversity of programming languages and tasks across different domains,
highlighting the need for more focused research in code retrieval. To address
this, we introduce CodeXEmbed, a family of large-scale code embedding models
ranging from 400M to 7B parameters. Our novel training pipeline unifies
multiple programming languages and transforms various code-related tasks into a
common retrieval framework, enhancing model generalizability and retrieval
performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,
outperforming the previous leading model, Voyage-Code, by over 20% on CoIR
benchmark. In addition to excelling in code retrieval, our models demonstrate
competitive performance on the widely adopted BeIR text retrieval benchmark,
offering versatility across domains. Experimental results demonstrate that
improving retrieval performance significantly enhances end-to-end
Retrieval-Augmented Generation (RAG) performance for code-related tasks.",Ye Liu
2024-11-20T11:41:08Z,http://arxiv.org/abs/2411.13226v1,"AIDBench: A benchmark for evaluating the authorship identification
  capability of large language models","As large language models (LLMs) rapidly advance and integrate into daily
life, the privacy risks they pose are attracting increasing attention. We focus
on a specific privacy risk where LLMs may help identify the authorship of
anonymous texts, which challenges the effectiveness of anonymity in real-world
systems such as anonymous peer review systems. To investigate these risks, we
present AIDBench, a new benchmark that incorporates several author
identification datasets, including emails, blogs, reviews, articles, and
research papers. AIDBench utilizes two evaluation methods: one-to-one
authorship identification, which determines whether two texts are from the same
author; and one-to-many authorship identification, which, given a query text
and a list of candidate texts, identifies the candidate most likely written by
the same author as the query text. We also introduce a Retrieval-Augmented
Generation (RAG)-based method to enhance the large-scale authorship
identification capabilities of LLMs, particularly when input lengths exceed the
models' context windows, thereby establishing a new baseline for authorship
identification using LLMs. Our experiments with AIDBench demonstrate that LLMs
can correctly guess authorship at rates well above random chance, revealing new
privacy risks posed by these powerful models. The source code and data will be
made publicly available after acceptance.",Zichen Wen
2024-11-20T15:45:08Z,http://arxiv.org/abs/2411.13405v1,"On the Way to LLM Personalization: Learning to Remember User
  Conversations","Large Language Models (LLMs) have quickly become an invaluable assistant for
a variety of tasks. However, their effectiveness is constrained by their
ability to tailor responses to human preferences and behaviors via
personalization. Prior work in LLM personalization has largely focused on style
transfer or incorporating small factoids about the user, as knowledge injection
remains an open challenge. In this paper, we explore injecting knowledge of
prior conversations into LLMs to enable future work on less redundant,
personalized conversations. We identify two real-world constraints: (1)
conversations are sequential in time and must be treated as such during
training, and (2) per-user personalization is only viable in
parameter-efficient settings. To this aim, we propose PLUM, a pipeline
performing data augmentation for up-sampling conversations as question-answer
pairs, that are then used to finetune a low-rank adaptation adapter with a
weighted cross entropy loss. Even in this first exploration of the problem, we
perform competitively with baselines such as RAG, attaining an accuracy of
81.5% across 100 conversations.",Lucie Charlotte Magister
2024-11-21T19:01:07Z,http://arxiv.org/abs/2411.16707v1,"Enhancing LLMs for Power System Simulations: A Feedback-driven
  Multi-agent Framework","The integration of experimental technologies with large language models
(LLMs) is transforming scientific research, positioning AI as a versatile
research assistant rather than a mere problem-solving tool. In the field of
power systems, however, managing simulations -- one of the essential
experimental technologies -- remains a challenge for LLMs due to their limited
domain-specific knowledge, restricted reasoning capabilities, and imprecise
handling of simulation parameters. To address these limitations, we propose a
feedback-driven, multi-agent framework that incorporates three proposed
modules: an enhanced retrieval-augmented generation (RAG) module, an improved
reasoning module, and a dynamic environmental acting module with an
error-feedback mechanism. Validated on 69 diverse tasks from Daline and
MATPOWER, this framework achieves success rates of 93.13% and 96.85%,
respectively, significantly outperforming the latest LLMs (ChatGPT 4o and
o1-preview), which achieved a 27.77% success rate on standard simulation tasks
and 0% on complex tasks. Additionally, our framework also supports rapid,
cost-effective task execution, completing each simulation in approximately 30
seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable
framework lays a foundation for developing intelligent LLM-based assistants for
human researchers, facilitating power system research and beyond.",Mengshuo Jia
2024-11-23T09:56:21Z,http://arxiv.org/abs/2411.16732v1,"Multi-Reranker: Maximizing performance of retrieval-augmented generation
  in the FinanceRAG challenge","As Large Language Models (LLMs) increasingly address domain-specific
problems, their application in the financial sector has expanded rapidly. Tasks
that are both highly valuable and time-consuming, such as analyzing financial
statements, disclosures, and related documents, are now being effectively
tackled using LLMs. This paper details the development of a high-performance,
finance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF
'24 FinanceRAG competition. We optimized performance through ablation studies
on query expansion and corpus refinement during the pre-retrieval phase. To
enhance retrieval accuracy, we employed multiple reranker models. Notably, we
introduced an efficient method for managing long context sizes during the
generation phase, significantly improving response quality without sacrificing
performance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our
key contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced
retrieval algorithm, and (3) a novel approach for long-context management. This
work demonstrates the potential of LLMs in effectively processing and analyzing
complex financial data to generate accurate and valuable insights. The source
code and further details are available at https://github.com/cv-lee/FinanceRAG.",Joohyun Lee
2024-11-28T06:28:45Z,http://arxiv.org/abs/2411.18947v1,ICLERB: In-Context Learning Embedding and Reranker Benchmark,"In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new
tasks by conditioning on prompts with relevant information. Retrieval-Augmented
Generation (RAG) enhances ICL by incorporating retrieved documents into the
LLM's context at query time. However, traditional retrieval methods focus on
semantic relevance, treating retrieval as a search problem. In this paper, we
propose reframing retrieval for ICL as a recommendation problem, aiming to
select documents that maximize utility in ICL tasks. We introduce the
In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel
evaluation framework that compares retrievers based on their ability to enhance
LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement
Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune
retrieval models using minimal feedback from the LLM. Our experimental results
reveal notable differences between ICLERB and existing benchmarks, and
demonstrate that small models fine-tuned with our RLRAIF algorithm outperform
large state-of-the-art retrieval models. These findings highlight the
limitations of existing evaluation methods and the need for specialized
benchmarks and training strategies adapted to ICL.",Marie Al Ghossein
2024-12-02T15:25:02Z,http://arxiv.org/abs/2412.01605v1,"Medchain: Bridging the Gap Between LLM Agents and Clinical Practice
  through Interactive Sequential Benchmarking","Clinical decision making (CDM) is a complex, dynamic process crucial to
healthcare delivery, yet it remains a significant challenge for artificial
intelligence systems. While Large Language Model (LLM)-based agents have been
tested on general medical knowledge using licensing exams and knowledge
question-answering tasks, their performance in the CDM in real-world scenarios
is limited due to the lack of comprehensive testing datasets that mirror actual
medical practice. To address this gap, we present MedChain, a dataset of 12,163
clinical cases that covers five key stages of clinical workflow. MedChain
distinguishes itself from existing benchmarks with three key features of
real-world clinical practice: personalization, interactivity, and
sequentiality. Further, to tackle real-world CDM challenges, we also propose
MedChain-Agent, an AI system that integrates a feedback mechanism and a
MCase-RAG module to learn from previous cases and adapt its responses.
MedChain-Agent demonstrates remarkable adaptability in gathering information
dynamically and handling sequential clinical tasks, significantly outperforming
existing approaches. The relevant dataset and code will be released upon
acceptance of this paper.",Jie Liu
2024-12-02T16:55:07Z,http://arxiv.org/abs/2412.01709v1,"Query Performance Explanation through Large Language Model for HTAP
  Systems","In hybrid transactional and analytical processing (HTAP) systems, users often
struggle to understand why query plans from one engine (OLAP or OLTP) perform
significantly slower than those from another. Although optimizers provide plan
details via the EXPLAIN function, these explanations are frequently too
technical for non-experts and offer limited insights into performance
differences across engines. To address this, we propose a novel framework that
leverages large language models (LLMs) to explain query performance in HTAP
systems. Built on Retrieval-Augmented Generation (RAG), our framework
constructs a knowledge base that stores historical query executions and
expert-curated explanations. To enable efficient retrieval of relevant
knowledge, query plans are embedded using a lightweight tree-CNN classifier.
This augmentation allows the LLM to generate clear, context-aware explanations
of performance differences between engines. Our approach demonstrates the
potential of LLMs in hybrid engine systems, paving the way for further
advancements in database optimization and user support.",Haibo Xiu
2024-12-03T00:59:56Z,http://arxiv.org/abs/2412.02065v1,"Leveraging Large Language Models to Democratize Access to Costly
  Financial Datasets for Academic Research","Unequal access to costly datasets essential for empirical research has long
hindered researchers from disadvantaged institutions, limiting their ability to
contribute to their fields and advance their careers. Recent breakthroughs in
Large Language Models (LLMs) have the potential to democratize data access by
automating data collection from unstructured sources. We develop and evaluate a
novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation
(RAG) framework to collect data from corporate disclosures. Our approach
achieves human-level accuracy in collecting CEO pay ratios from approximately
10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000
10-K filings, with LLM processing times of 9 and 40 minutes respectively, each
at a cost under $10. This stands in stark contrast to the hundreds of hours
needed for manual collection or the thousands of dollars required for
commercial database subscriptions. To foster a more inclusive research
community by empowering researchers with limited resources to explore new
avenues of inquiry, we share our methodology and the resulting datasets.",Julian Junyan Wang
2024-12-03T19:37:00Z,http://arxiv.org/abs/2412.02788v2,Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset,"Existing Scholarly Question Answering (QA) methods typically target
homogeneous data sources, relying solely on either text or Knowledge Graphs
(KGs). However, scholarly information often spans heterogeneous sources,
necessitating the development of QA systems that integrate information from
multiple heterogeneous data sources. To address this challenge, we introduce
Hybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale
QA dataset designed to facilitate answering questions incorporating both text
and KG facts. The dataset consists of 10.5K question-answer pairs generated by
a large language model, leveraging the KGs DBLP and SemOpenAlex alongside
corresponding text from Wikipedia. In addition, we propose a RAG-based baseline
hybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD
test set.",Tilahun Abedissa Taffa
2024-12-05T14:24:07Z,http://arxiv.org/abs/2412.04185v1,"Leveraging Large Language Models to Generate Course-specific
  Semantically Annotated Learning Objects","Background: Over the past few decades, the process and methodology of
automated question generation (AQG) have undergone significant transformations.
Recent progress in generative natural language models has opened up new
potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs)
for generating computer science questions that are sufficiently annotated for
automatic learner model updates, are fully situated in the context of a
particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT,
our approach involves more targeted strategies such as retrieval-augmented
generation (RAG) to produce contextually relevant and pedagogically meaningful
learning objects.
  Results and Conclusions: Our results show that generating structural,
semantic annotations works well. However, this success was not reflected in the
case of relational annotations. The quality of the generated questions often
did not meet educational standards, highlighting that although LLMs can
contribute to the pool of learning materials, their current level of
performance requires significant human intervention to refine and validate the
generated content.",Dominic Lohr
2024-12-05T17:00:32Z,http://arxiv.org/abs/2412.04342v1,Retrieval-Augmented Machine Translation with Unstructured Knowledge,"Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",Jiaan Wang
2024-12-05T18:38:30Z,http://arxiv.org/abs/2412.04415v1,"Targeting the Core: A Simple and Effective Method to Attack RAG-based
  Agents via Direct LLM Manipulation","AI agents, powered by large language models (LLMs), have transformed
human-computer interactions by enabling seamless, natural, and context-aware
communication. While these advancements offer immense utility, they also
inherit and amplify inherent safety risks such as bias, fairness,
hallucinations, privacy breaches, and a lack of transparency. This paper
investigates a critical vulnerability: adversarial attacks targeting the LLM
core within AI agents. Specifically, we test the hypothesis that a deceptively
simple adversarial prefix, such as \textit{Ignore the document}, can compel
LLMs to produce dangerous or unintended outputs by bypassing their contextual
safeguards. Through experimentation, we demonstrate a high attack success rate
(ASR), revealing the fragility of existing LLM defenses. These findings
emphasize the urgent need for robust, multi-layered security measures tailored
to mitigate vulnerabilities at the LLM level and within broader agent-based
architectures.",Xuying Li
2024-12-06T17:54:54Z,http://arxiv.org/abs/2412.05223v1,100% Hallucination Elimination Using Acurai,"The issue of hallucinations in large language models (LLMs) remains a
critical barrier to the adoption of AI in enterprise and other high-stakes
applications. Despite advancements in retrieval-augmented generation (RAG)
systems, current state-of-the-art methods fail to achieve more than 80%
accuracy in generating faithful and factually correct outputs, even when
provided with relevant and accurate context. In this work, we introduce Acurai,
a novel systematic approach that achieves 100% hallucination-free responses in
LLMs by reformatting queries and context data prior to input. Leveraging a deep
understanding of LLM internal representations, the importance of noun-phrase
dominance, and the role of discrete functional units (DFUs), Acurai ensures
alignment between input context and generated output. We validate this method
using the RAGTruth corpus, demonstrating its ability to eliminate 100%
hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for
achieving consistent, accurate, and faithful AI responses, marking a
significant step forward in the development of trustworthy AI systems.",Michael C. Wood
2024-12-08T13:36:42Z,http://arxiv.org/abs/2412.05937v1,"Accelerating Manufacturing Scale-Up from Material Discovery Using
  Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering
  Schematics Design","Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (PIDs)
are critical tools for industrial process design, control, and safety. However,
the generation of precise and regulation-compliant diagrams remains a
significant challenge, particularly in scaling breakthroughs from material
discovery to industrial production in an era of automation and digitalization.
This paper introduces an autonomous agentic framework to address these
challenges through a twostage approach involving knowledge acquisition and
generation. The framework integrates specialized sub-agents for retrieving and
synthesizing multimodal data from publicly available online sources and
constructs ontological knowledge graphs using a Graph Retrieval-Augmented
Generation (Graph RAG) paradigm. These capabilities enable the automation of
diagram generation and open-domain question answering (ODQA) tasks with high
contextual accuracy. Extensive empirical experiments demonstrate the frameworks
ability to deliver regulation-compliant diagrams with minimal expert
intervention, highlighting its practical utility for industrial applications.",Sakhinana Sagar Srinivas
2024-12-10T11:05:26Z,http://arxiv.org/abs/2412.07412v1,"Generating Knowledge Graphs from Large Language Models: A Comparative
  Study of GPT-4, LLaMA 2, and BERT","Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a
form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks
requiring structured reasoning and semantic understanding. However, creating
KGs for GraphRAGs remains a significant challenge due to accuracy and
scalability limitations of traditional methods. This paper introduces a novel
approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and
BERT to generate KGs directly from unstructured data, bypassing traditional
pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit
Distance, and Semantic Similarity, we evaluate the models' ability to generate
high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic
fidelity and structural accuracy, LLaMA 2 excels in lightweight,
domain-specific graphs, and BERT provides insights into challenges in
entity-relationship modeling. This study underscores the potential of LLMs to
streamline KG creation and enhance GraphRAG accessibility for real-world
applications, while setting a foundation for future advancements.",Ahan Bhatt
2024-12-11T16:32:41Z,http://arxiv.org/abs/2412.08519v1,"Bridging Relevance and Reasoning: Rationale Distillation in
  Retrieval-Augmented Generation","The reranker and generator are two critical components in the
Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking
relevant documents and generating responses. However, due to differences in
pre-training data and objectives, there is an inevitable gap between the
documents ranked as relevant by the reranker and those required by the
generator to support answering the query. To address this gap, we propose
RADIO, a novel and practical preference alignment framework with RAtionale
DIstillatiOn. Specifically, We first propose a rationale extraction method that
leverages the reasoning capabilities of Large Language Models (LLMs) to extract
the rationales necessary for answering the query. Subsequently, a
rationale-based alignment process is designed to rerank the documents based on
the extracted rationales, and fine-tune the reranker to align the preferences.
We conduct extensive experiments on two tasks across three datasets to
demonstrate the effectiveness of our approach compared to baseline methods. Our
code is released online to ease reproduction.",Pengyue Jia
2024-12-14T17:30:33Z,http://arxiv.org/abs/2412.10906v1,"SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability
  Report Generation","The rapid growth of the financial sector and the rising focus on
Environmental, Social, and Governance (ESG) considerations highlight the need
for advanced NLP tools. However, open-source LLMs proficient in both finance
and ESG domains remain scarce. To address this gap, we introduce SusGen-30K, a
category-balanced dataset comprising seven financial NLP tasks and ESG report
generation, and propose TCFD-Bench, a benchmark for evaluating sustainability
report generation. Leveraging this dataset, we developed SusGen-GPT, a suite of
models achieving state-of-the-art performance across six adapted and two
off-the-shelf tasks, trailing GPT-4 by only 2% despite using 7-8B parameters
compared to GPT-4's 1,700B. Based on this, we propose the SusGen system,
integrated with Retrieval-Augmented Generation (RAG), to assist in
sustainability report generation. This work demonstrates the efficiency of our
approach, advancing research in finance and ESG.",Qilong Wu
2024-12-15T23:52:35Z,http://arxiv.org/abs/2412.11342v1,One-Shot Multilingual Font Generation Via ViT,"Font design poses unique challenges for logographic languages like Chinese,
Japanese, and Korean (CJK), where thousands of unique characters must be
individually crafted. This paper introduces a novel Vision Transformer
(ViT)-based model for multi-language font generation, effectively addressing
the complexities of both logographic and alphabetic scripts. By leveraging ViT
and pretraining with a strong visual pretext task (Masked Autoencoding, MAE),
our model eliminates the need for complex design components in prior frameworks
while achieving comprehensive results with enhanced generalizability.
Remarkably, it can generate high-quality fonts across multiple languages for
unseen, unknown, and even user-crafted characters. Additionally, we integrate a
Retrieval-Augmented Guidance (RAG) module to dynamically retrieve and adapt
style references, improving scalability and real-world applicability. We
evaluated our approach in various font generation tasks, demonstrating its
effectiveness, adaptability, and scalability.",Zhiheng Wang
2024-12-16T03:12:13Z,http://arxiv.org/abs/2412.11404v1,"Attention with Dependency Parsing Augmentation for Fine-Grained
  Attribution","To assist humans in efficiently validating RAG-generated content, developing
a fine-grained attribution mechanism that provides supporting evidence from
retrieved documents for every answer span is essential. Existing fine-grained
attribution methods rely on model-internal similarity metrics between responses
and documents, such as saliency scores and hidden state similarity. However,
these approaches suffer from either high computational complexity or
coarse-grained representations. Additionally, a common problem shared by the
previous works is their reliance on decoder-only Transformers, limiting their
ability to incorporate contextual information after the target span. To address
the above problems, we propose two techniques applicable to all
model-internals-based methods. First, we aggregate token-wise evidence through
set union operations, preserving the granularity of representations. Second, we
enhance the attributor by integrating dependency parsing to enrich the semantic
completeness of target spans. For practical implementation, our approach
employs attention weights as the similarity metric. Experimental results
demonstrate that the proposed method consistently outperforms all prior works.",Qiang Ding
2024-12-16T08:13:14Z,http://arxiv.org/abs/2412.11536v1,"Let your LLM generate a few tokens and you will reduce the need for
  retrieval","In this paper, we investigate how efficiently large language models (LLM) can
be trained to check whether an answer is already stored in their parametric
memory. We distill an LLM-as-a-judge to compute the IK (I Know) score. We found
that this method is particularly beneficial in the context of
retrieval-assisted augmented generation (RAG), with a respectable accuracy of
80%. It enables a significant reduction (more than 50%) in the number of search
and reranking steps required for certain data sets. We have also introduced the
IK score, which serves as a useful tool for characterising datasets by
facilitating the classification task. Interestingly, through the inclusion of
response tokens as input, our results suggest that only about 20,000 training
samples are required to achieve good performance. The central element of this
work is the use of a teacher model - the LLM as a judge - to generate training
data. We also assess the robustness of the IK classifier by evaluating it with
various types of teachers, including both string-based methods and LLMs, with
the latter providing better results.",HervÃ© DÃ©jean
2024-12-16T12:44:42Z,http://arxiv.org/abs/2412.11722v2,"GHIssuemarket: A Sandbox Environment for SWE-Agents Economic
  Experimentation","Software engineering agents (swe-agents), as key innovations in intelligent
software engineering, are poised in the industry's end-of-programming debate to
transcend from assistance to primary roles. we argue the importance of
swe-agents' economic viability to their transcendence -- defined as their
capacity to maintain efficient operations in constrained environments -- and
propose its exploration via software engineering economics experimentation.we
introduce ghissuemarket sandbox, a controlled virtual environment for
swe-agents' economic experimentation, simulating the environment of an
envisioned peer-to-peer multiagent system for github issues outsourcing
auctions. in this controlled setting, autonomous swe-agents auction and bid on
github issues, leveraging real-time communication, a built-in
retrieval-augmented generation (rag) interface for effective decision-making,
and instant cryptocurrency micropayments. we open-source our software
artifacts, discuss our sandbox engineering decisions, and advocate towards
swe-agents' economic exploration -- an emerging field we intend to pursue under
the term intelligent software engineering economics (isee).",Mohamed A. Fouad
2024-12-16T16:03:25Z,http://arxiv.org/abs/2412.11919v1,"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained
  Evidence within Generation","Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.",Xiaoxi Li
2024-12-16T21:36:03Z,http://arxiv.org/abs/2412.12364v1,"LogBabylon: A Unified Framework for Cross-Log File Integration and
  Analysis","Logs are critical resources that record events, activities, or messages
produced by software applications, operating systems, servers, and network
devices. However, consolidating the heterogeneous logs and cross-referencing
them is challenging and complicated. Manually analyzing the log data is
time-consuming and prone to errors. LogBabylon is a centralized log data
consolidating solution that leverages Large Language Models (LLMs) integrated
with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the
log data in a human-readable way and adds insight analysis of the system
performance and anomaly alerts. It provides a paramount view of the system
landscape, enabling proactive management and rapid incident response.
LogBabylon consolidates diverse log sources and enhances the extracted
information's accuracy and relevancy. This facilitates a deeper understanding
of log data, supporting more effective decision-making and operational
efficiency. Furthermore, LogBabylon streamlines the log analysis process,
significantly reducing the time and effort required to interpret complex
datasets. Its capabilities extend to generating context-aware insights,
offering an invaluable tool for continuous monitoring, performance
optimization, and security assurance in dynamic computing environments.",Rabimba Karanjai
2024-12-17T14:44:27Z,http://arxiv.org/abs/2412.12961v1,"Adaptations of AI models for querying the LandMatrix database in natural
  language","The Land Matrix initiative (https://landmatrix.org) and its global
observatory aim to provide reliable data on large-scale land acquisitions to
inform debates and actions in sectors such as agriculture, extraction, or
energy in low- and middle-income countries. Although these data are recognized
in the academic world, they remain underutilized in public policy, mainly due
to the complexity of access and exploitation, which requires technical
expertise and a good understanding of the database schema.
  The objective of this work is to simplify access to data from different
database systems. The methods proposed in this article are evaluated using data
from the Land Matrix. This work presents various comparisons of Large Language
Models (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,
RAG, Agents) to query different database systems (GraphQL and REST queries).
The experiments are reproducible, and a demonstration is available online:
https://github.com/tetis-nlp/landmatrix-graphql-python.",Fatiha Ait Kbir
2024-12-18T15:07:23Z,http://arxiv.org/abs/2412.13924v1,Language verY Rare for All,"In the quest to overcome language barriers, encoder-decoder models like NLLB
have expanded machine translation to rare languages, with some models (e.g.,
NLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform
well in translation, open LLMs prove highly competitive when fine-tuned for
specific tasks involving unknown corpora. We introduce LYRA (Language verY Rare
for All), a novel approach that combines open LLM fine-tuning,
retrieval-augmented generation (RAG), and transfer learning from related
high-resource languages. This study is exclusively focused on single-GPU
training to facilitate ease of adoption. Our study focuses on two-way
translation between French and Mon\'egasque, a rare language unsupported by
existing translation tools due to limited corpus availability. Our results
demonstrate LYRA's effectiveness, frequently surpassing and consistently
matching state-of-the-art encoder-decoder models in rare language translation.",Ibrahim Merad
2024-12-19T13:28:42Z,http://arxiv.org/abs/2412.14838v1,"DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context
  LLMs","Efficient KV cache management in LLMs is crucial for long-context tasks like
RAG and summarization. Existing KV cache compression methods enforce a fixed
pattern, neglecting task-specific characteristics and reducing the retention of
essential information. However, we observe distinct activation patterns across
layers in various tasks, highlighting the need for adaptive strategies tailored
to each task's unique demands. Based on this insight, we propose DynamicKV, a
method that dynamically optimizes token retention by adjusting the number of
tokens retained at each layer to adapt to the specific task. DynamicKV
establishes global and per-layer maximum KV cache budgets, temporarily
retaining the maximum budget for the current layer, and periodically updating
the KV cache sizes of all preceding layers during inference. Our method retains
only 1.7% of the KV cache size while achieving ~85% of the Full KV cache
performance on LongBench. Notably, even under extreme compression (0.9%),
DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the
Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be
released.",Xiabin Zhou
2024-06-29T22:39:20Z,http://arxiv.org/abs/2407.00541v1,"Answering real-world clinical questions using large language model based
  systems","Evidence to guide healthcare decisions is often limited by a lack of relevant
and trustworthy literature as well as difficulty in contextualizing existing
research for a specific patient. Large language models (LLMs) could potentially
address both challenges by either summarizing published literature or
generating new studies based on real-world data (RWD). We evaluated the ability
of five LLM-based systems in answering 50 clinical questions and had nine
independent physicians review the responses for relevance, reliability, and
actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus,
Gemini Pro 1.5) rarely produced answers that were deemed relevant and
evidence-based (2% - 10%). In contrast, retrieval augmented generation
(RAG)-based and agentic LLM systems produced relevant and evidence-based
answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic
ChatRWD was able to answer novel questions compared to other LLMs (65% vs.
0-9%). These results suggest that while general-purpose LLMs should not be used
as-is, a purpose-built system for evidence summarization based on RAG and one
for generating novel evidence working synergistically would improve
availability of pertinent evidence for patient care.",Yen Sia Low
2004-06-06T18:34:37Z,http://arxiv.org/abs/astro-ph/0406143v1,Density-Wave Spiral Theories in the 1960s. II,"By the 1970s the spiral subject was in considerable disarray. The
semiempirical theory by Lin and Shu was confronted with serious problems. They
were put on the defensive over their tightly wrapped steady modes on two
principal fronts: from the radial propagation at the group velocity that would
tend to wind them almost at the material rate, and from the tendencies of
galaxy disks toward a strong global instability that appeared likely to
overwhelm them. Of course, one might claim that such threats were imaginary and
only of academic interest, on the ground that nature itself had overcome them.
One might also be confident that the QSSS hypothesis must be correct as
illuminated by the everlasting truth of Hubble's classification. One might even
take pride in the fact that a very promising concept developed, although not
connected to the wave steadiness, on spiral shocks in interstellar gas and
their induced star formation. But such a heuristic approach did not stimulate
very strong progress in understanding dynamical principles of the spiral
phenomenon; moreover, it often misled, and a rich irony was already that the
supposed QSSS favorites M51 and M81 turned out most probably not to be
quasi-steady at all. A further irony was the continuing failure of Lin and Shu
to account the trailing character of their 'modes', while that was already
grasped by their direct 'deductive' opponents. But the greatest irony lay in
the fact that the concept later known as swing amplification, worked out by the
mid-1960s, was originally denigrated by Lin's camp as relating exclusively to
'material arms', whereas it turned out in the end to be of vital importance to
this entire spiral enterprise including the variants of chaotic ragged
patterns, tidal transient grand designs and growing or quasi-steady modes.",I. I. Pasha
2006-01-27T00:20:29Z,http://arxiv.org/abs/astro-ph/0601631v1,"On the feedback from super stellar clusters. I. The structure of giant
  HII regions and HII galaxies","We review the structural properties of giant extragalactic HII regions and
HII galaxies based on 2D hydrodynamic calculations, and propose an evolutionary
sequence that accounts for their observed detailed structure. The model assumes
a massive and young stellar cluster surrounded by a large collection of clouds.
These are thus exposed to the most important star-formation feedback
mechanisms: photoionization and the cluster wind. The models show how the two
feedback mechanisms compete in the disruption of clouds and lead to two
different hydrodynamic solutions: The storage of clouds into a long lasting
ragged shell that inhibits the expansion of the thermalized wind, and the
steady filtering of the shocked wind gas through channels carved within the
cloud stratum. Both solutions are claimed to be concurrently at work in giant
HII regions and HII galaxies, causing their detailed inner structure. This
includes multiple large-scale shells, filled with an X-ray emitting gas, that
evolve to finally merge with each other, giving the appearance of shells within
shells. The models also show how the inner filamentary structure of the giant
superbubbles is largely enhanced with matter ablated from clouds and how cloud
ablation proceeds within the original cloud stratum. The calculations point at
the initial contrast density between the cloud and the intercloud media as the
factor that defines which of the two feedback mechanisms becomes dominant
throughout the evolution. Animated version of the models can be found at
http://www.iaa.csic.es/\~{}eperez/ssc/ssc.html.",G. Tenorio-Tagle
2008-08-28T21:20:59Z,http://arxiv.org/abs/0808.3998v1,Emergence of a Quasar Outflow,"We report the first discovery of the emergence of a high-velocity broad-line
outflow in a luminous quasar, J105400.40+034801.2 at redshift z ~ 2.1. The
outflow is evident in ultraviolet CIV and SiIV absorption lines with velocity
shifts v ~ 26,300 km/s and deblended widths FWHM ~ 4000 km/s. These features
are marginally strong and broad enough to be considered broad absorption lines
(BALs), but their large velocities exclude them from the standard BAL
definition. The outflow lines appeared between two observations in the years
2002.18 and 2006.96. A third observation in 2008.48 showed the lines becoming
~40% weaker and 10% to 15% narrower. There is no evidence for acceleration or
for any outflow gas at velocities <23,000 km/s. The lines appear to be
optically thick, with the absorber covering just 20% of the quasar continuum
source. This indicates a characteristic absorber size of ~4 x 10^15 cm, but
with a BAL-like total column density log N_H (cm^-2) > 21.2 and average space
density n_H > 2 x 10^5 cm^-3. We attribute the emergence of the outflow lines
to a substantial flow structure moving across our line of sight, possibly near
the ragged edge of the main BAL flow or possibly related to the onset of a BAL
evolutionary phase.",F. Hamann
2009-02-25T06:36:34Z,http://arxiv.org/abs/0902.4292v1,"Generalized Relativistic Magnetohydrodynamic Equations for Pair and
  Electron-Ion Plasmas","We derived one-fluid equations based on a relativistic two-fluid
approximation of e$^\pm$ pair plasma and electron-ion plasma to reveal the
specific relativistic nature of their behavior. Assuming simple condition on
the relativistic one-fluid equations, we propose generalized relativistic
magnetohydrodynamic (RMHD) equations which satisfy causality. We show the
linear analyses of these equations regarding various plasma waves to show the
validity of the generalized RMHD equations derived here and to reveal the
distinct properties of the pair plasma and electron-ion plasma. The distinct
properties relate to (i) the inertia effect of electric charge, (ii) the
momentum of electric current, (iii) the relativistic Hall effect, (iv) the
thermal electromotive force, and (v) the thermalized energy exchange between
the two fluids. Using the generalized RMHD equations, we also clarify the
condition that we can use standard RMHD equations and that we need the distinct
RMHD equations of pair and electron-ion plasmas. The standard RMHD is available
only when the relative velocity of the two fluids is nonrelativistic, a
difference of the enthalpy densities of the two fluids is much smaller than the
total enthalpy density, and the above distinct properties of the
pair/electron-ion plasma are negligible. We discuss a general relativistic
version of the equations applicable to the pair and electron-ion plasmas in
black hole magnetospheres. We find the effective resistivity due to shear of
frame ragging around a rotating black hole.",Shinji Koide
2015-07-06T16:46:07Z,http://arxiv.org/abs/1507.01538v1,Real Functions for Physics,"A new classification of real functions and other related real objects defined
within a compact interval is proposed. The scope of the classification includes
normal real functions and distributions in the sense of Schwartz, referred to
jointly as ""generalized functions"". This classification is defined in terms of
the behavior of these generalized functions under the action of a linear low
pass-filter, which can be understood as an integral operator acting in the
space of generalized functions. The classification criterion defines a class of
generalized functions which we will name ""combed functions"", leaving out a
complementary class of ""ragged functions"". While the classification as combed
functions leaves out many pathological objects, it includes in the same footing
such diverse objects as real analytic functions, the Dirac delta ""function"",
and its derivatives of arbitrarily high orders, as well as many others in
between these two extremes. We argue that the set of combed functions is
sufficient for all the needs of physics, as tools for the description of
nature. This includes the whole of classical physics and all the observable
quantities in quantum mechanics and quantum field theory. The focusing of
attention on this smaller set of generalized functions greatly simplifies the
mathematical arguments needed to deal with them.",Jorge L. deLyra
2016-10-03T04:25:18Z,http://arxiv.org/abs/1610.00402v2,Dynamic Polygon Clouds: Representation and Compression for VR/AR,"We introduce the {\em polygon cloud}, also known as a polygon set or {\em
soup}, as a compressible representation of 3D geometry (including its
attributes, such as color texture) intermediate between polygonal meshes and
point clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal
meshes and dynamic point clouds, can take advantage of temporal redundancy for
compression, if certain challenges are addressed. In this paper, we propose
methods for compressing both static and dynamic polygon clouds, specifically
triangle clouds. We compare triangle clouds to both triangle meshes and point
clouds in terms of compression, for live captured dynamic colored geometry. We
find that triangle clouds can be compressed nearly as well as triangle meshes,
while being far more robust to noise and other structures typically found in
live captures, which violate the assumption of a smooth surface manifold, such
as lines, points, and ragged boundaries. We also find that triangle clouds can
be used to compress point clouds with significantly better performance than
previously demonstrated point cloud compression methods. In particular, for
intra-frame coding of geometry, our method improves upon octree-based
intra-frame coding by a factor of 5-10 in bit rate. Inter-frame coding improves
this by another factor of 2-5. Overall, our dynamic triangle cloud compression
improves over the previous state-of-the-art in dynamic point cloud compression
by 33\% or more.",Philip A. Chou
2016-11-17T10:59:53Z,http://arxiv.org/abs/1611.05633v1,Minor complexities of finite operations,"In this paper we present a new class of complexity measures, induced by a new
data structure for representing $k$-valued functions (operations), called minor
decision diagram. The results are presented in terms of Multi-Valued Logic
circuits (MVL-circuits), ordered decision diagrams, formulas and minor
decomposition trees.
  When assigning values to some variables in a function $f$ the resulting
function is a subfunction of $f$, and when identifying some variables the
resulting function is a minor of $f$. A set $M$ of essential variables in $f$
is separable if there is a subfunction of $f$, whose set of essential variables
is $M$. The essential arity gap $gap(f)$ of the function $f$ is the minimum
number of essential variables in $f$ which become fictive when identifying
distinct essential variables in $f$. We prove that, if a function $f$ has
non-trivial arity gap ($gap(f)\ge 2$), then all sets of essential variables in
$f$ are separable. We define equivalence relations which classify the functions
of $k$-valued logic into classes with the same minor complexities. These
relations induce transformation groups which are compared with the subgroups of
the restricted affine group (RAG) and the groups determined by the equivalence
relations with respect to the subfunctions, implementations and separable sets
in functions. These methods provide a detailed classification of $n$-ary
$k$-valued functions for small values of $n$ and $k$.",Slavcho Shtrakov
2017-08-21T12:27:32Z,http://arxiv.org/abs/1708.06181v1,Haloes at the ragged edge: The importance of the splashback radius,"We have explored the outskirts of dark matter haloes out to 2.5 times the
virial radius using a large sample of halos drawn from Illustris, along with a
set of zoom simulations (MUGS). Using these, we make a systematic exploration
of the shape profile beyond R$_{vir}$. In the mean sphericity profile of
Illustris halos we identify a dip close to the virial radius, which is robust
across a broad range of masses and infall rates. The inner edge of this feature
may be related to the virial radius and the outer edge with the splashback
radius. Due to the high halo-to-halo variation this result is visible only on
average. However, in four individual halos in the MUGS sample, a decrease in
the sphericity and a subsequent recovery is evident close to the splashback
radius. We find that this feature persists for several Gyr, growing with the
halo. This feature appears at the interface between the spherical halo density
distribution and the filamentary structure in the environment. The shape
feature is strongest when there is a high rate of infall, implying that the
effect is due to the mixing of accreting and virializing material. The
filamentary velocity field becomes rapidly mixed in the halo region inside the
virial radius, with the area between this and the splashback radius serving as
the transition region. We also identify a long-lasting and smoothly evolving
splashback region in the radial density gradient in many of the MUGS halos.",O. N. Snaith
2018-11-29T04:57:02Z,http://arxiv.org/abs/1811.11965v1,"RCW 120: A possible case of hit and run, elucidated by multi temperature
  dust mapping","We present resolution-enhanced images of warm dust at multiple temperatures
and opacity index values in the star-forming bubble/HII region, RCW 120. The
image set, representing a 4D hypercube of differential column density, was
obtained using our Bayesian procedure, ppmap. The cool peripheral material
($\sim16$-22 K) exhibits ragged clumpy structure as noted previously by others.
However, at higher temperatures ($\stackrel{>}{_\sim}26$ K) the geometry
changes dramatically, showing a bubble boundary which is accurately circular in
projection, except for the previously-reported opening in the north. Comparison
with Spitzer 8 $\mu$m data suggests that the $\sim26$-30 K dust seen by
Herschel resides in the photodissociation region (PDR) surrounding the HII
region. Its projected radial profile is consistent with that of a spherical
shell, thus arguing against previous suggestions of cylindrical or planar
geometry. The inferred geometry is, in fact, consistent with previous
interpretations of the HII region as a classical Str\""omgren sphere, except for
the fact that the ionising star (CD -38.11636; O8V) is displaced by more than
half a radius from its geometric centre. None of the previously published
models has satisfactorily accounted for that displacement. It could, however,
be explained by proper motion of the O star at $\sim2$-4 km s$^{-1}$ since its
formation, possibly due to a cloud-cloud collision. We suggest that the current
spherical bubble constitutes the fossilised remnant of the initial expansion of
the HII region following the formation of the star, which now continues to flee
its formation site.",K. A. Marsh
2019-03-22T15:08:04Z,http://arxiv.org/abs/1904.12577v2,Table understanding in structured documents,"Abstract--- Table detection and extraction has been studied in the context of
documents like reports, where tables are clearly outlined and stand out from
the document structure visually. We study this topic in a rather more
challenging domain of layout-heavy business documents, particularly invoices.
Invoices present the novel challenges of tables being often without outlines -
either in the form of borders or surrounding text flow - with ragged columns
and widely varying data content. We will also show, that we can extract
specific information from structurally different tables or table-like
structures with one model. We present a comprehensive representation of a page
using graph over word boxes, positional embeddings, trainable textual features
and rephrase the table detection as a text box labeling problem. We will work
on our newly presented dataset of pro forma invoices, invoices and debit note
documents using this representation and propose multiple baselines to solve
this labeling problem. We then propose a novel neural network model that
achieves strong, practical results on the presented dataset and analyze the
model performance and effects of graph convolutions and self-attention in
detail.",Martin HoleÄek
2020-01-13T23:10:09Z,http://arxiv.org/abs/2001.04558v1,Modelling Stochastic Signatures in Classical Pulsators,"We consider the impact of stochastic perturbations on otherwise coherent
oscillations of classical pulsators. The resulting dynamics are modelled by a
driven damped harmonic oscillator subject to either an external or an internal
forcing and white noise velocity fluctuations. We characterize the phase and
relative amplitude variations using analytical and numerical tools. When the
forcing is internal the phase variation displays a random walk behaviour and a
red noise power spectrum with a ragged erratic appearance. We determine the
dependence of the root mean square phase and relative amplitude variations
($\sigma_{\Delta \varphi}$ and $\sigma_{\Delta A/A}$, respectively) on the
amplitude of the stochastic perturbations, the damping constant $\eta$, and the
total observation time $t_{\rm obs}$ for this case, under the assumption that
the relative amplitude variations remain small, showing that $\sigma_{\Delta
\varphi}$ increases with $t_{\rm obs}^{1/2}$ becoming much larger than
$\sigma_{\Delta A/A}$ for $t_{\rm obs} \gg \eta^{-1}$. In the case of an
external forcing the phase and relative amplitude variations remain of the same
order, independent of the observing time. In the case of an internal forcing,
we find that $\sigma_{\Delta \varphi}$ does not depend on $\eta$. Hence, the
damping time cannot be inferred from fitting the power of the signal, as done
for solar-like pulsators, but the amplitude of the stochastic perturbations may
be constrained from the observations. Our results imply that, given sufficient
time, the variation of the phase associated to the stochastic perturbations in
internally driven classical pulsators will become sufficiently large to be
probed observationally.",P. P. Avelino
2020-01-09T00:52:27Z,http://arxiv.org/abs/2001.05293v1,"Lazy object copy as a platform for population-based probabilistic
  programming","This work considers dynamic memory management for population-based
probabilistic programs, such as those using particle methods for inference.
Such programs exhibit a pattern of allocating, copying, potentially mutating,
and deallocating collections of similar objects through successive generations.
These objects may assemble data structures such as stacks, queues, lists,
ragged arrays, and trees, which may be of random, and possibly unbounded, size.
For the simple case of $N$ particles, $T$ generations, $D$ objects, and
resampling at each generation, dense representation requires $O(DNT)$ memory,
while sparse representation requires only $O(DT+DN\log DN)$ memory, based on
existing theoretical results. This work describes an object copy-on-write
platform to automate this saving for the programmer. The core idea is
formalized using labeled directed multigraphs, where vertices represent
objects, edges the pointers between them, and labels the necessary bookkeeping.
A specific labeling scheme is proposed for high performance under the
motivating pattern. The platform is implemented for the Birch probabilistic
programming language, using smart pointers, hash tables, and reference-counting
garbage collection. It is tested empirically on a number of realistic
probabilistic programs, and shown to significantly reduce memory use and
execution time in a manner consistent with theoretical expectations. This
enables copy-on-write for the imperative programmer, lazy deep copies for the
object-oriented programmer, and in-place write optimizations for the functional
programmer.",Lawrence M. Murray
2021-01-02T09:05:34Z,http://arxiv.org/abs/2101.00408v2,"End-to-End Training of Neural Retrievers for Open-Domain Question
  Answering","Recent work on training neural retrievers for open-domain question answering
(OpenQA) has employed both supervised and unsupervised approaches. However, it
remains unclear how unsupervised and supervised methods can be used most
effectively for neural retrievers. In this work, we systematically study
retriever pre-training. We first propose an approach of unsupervised
pre-training with the Inverse Cloze Task and masked salient spans, followed by
supervised finetuning using question-context pairs. This approach leads to
absolute gains of 2+ points over the previous best result in the top-20
retrieval accuracy on Natural Questions and TriviaQA datasets.
  We also explore two approaches for end-to-end supervised training of the
reader and retriever components in OpenQA models. In the first approach, the
reader considers each retrieved document separately while in the second
approach, the reader considers all the retrieved documents together. Our
experiments demonstrate the effectiveness of these approaches as we obtain new
state-of-the-art results. On the Natural Questions dataset, we obtain a top-20
retrieval accuracy of 84, an improvement of 5 points over the recent DPR model.
In addition, we achieve good results on answer extraction, outperforming recent
models like REALM and RAG by 3+ points. We further scale up end-to-end training
to large models and show consistent gains in performance over smaller models.",Devendra Singh Sachan
2021-11-23T13:41:03Z,http://arxiv.org/abs/2112.00653v4,Variational Learning for Unsupervised Knowledge Grounded Dialogs,"Recent methods for knowledge grounded dialogs generate responses by
incorporating information from an external textual document. These methods do
not require the exact document to be known during training and rely on the use
of a retrieval system to fetch relevant documents from a large index. The
documents used to generate the responses are modeled as latent variables whose
prior probabilities need to be estimated. Models such as RAG and REALM,
marginalize the document probabilities over the documents retrieved from the
index to define the log likelihood loss function which is optimized end-to-end.
  In this paper, we develop a variational approach to the above technique
wherein, we instead maximize the Evidence Lower bound (ELBO). Using a
collection of three publicly available open-conversation datasets, we
demonstrate how the posterior distribution, that has information from the
ground-truth response, allows for a better approximation of the objective
function during training. To overcome the challenges associated with sampling
over a large knowledge collection, we develop an efficient approach to
approximate the ELBO. To the best of our knowledge we are the first to apply
variational training for open-scale unsupervised knowledge grounded dialog
systems.",Mayank Mishra
2021-12-09T08:05:02Z,http://arxiv.org/abs/2112.04744v4,"Superpixel-Based Building Damage Detection from Post-earthquake Very
  High Resolution Imagery Using Deep Neural Networks","Building damage detection after natural disasters like earthquakes is crucial
for initiating effective emergency response actions. Remotely sensed very high
spatial resolution (VHR) imagery can provide vital information due to their
ability to map the affected buildings with high geometric precision. Many
approaches have been developed to detect damaged buildings due to earthquakes.
However, little attention has been paid to exploiting rich features represented
in VHR images using Deep Neural Networks (DNN). This paper presents a novel
superpixel based approach combining DNN and a modified segmentation method, to
detect damaged buildings from VHR imagery. Firstly, a modified Fast Scanning
and Adaptive Merging method is extended to create initial over-segmentation.
Secondly, the segments are merged based on the Region Adjacent Graph (RAG),
considered an improved semantic similarity criterion composed of Local Binary
Patterns (LBP) texture, spectral, and shape features. Thirdly, a pre-trained
DNN using Stacked Denoising Auto-Encoders called SDAE-DNN is presented, to
exploit the rich semantic features for building damage detection. Deep-layer
feature abstraction of SDAE-DNN could boost detection accuracy through learning
more intrinsic and discriminative features, which outperformed other methods
using state-of-the-art alternative classifiers. We demonstrate the feasibility
and effectiveness of our method using a subset of WorldView-2 imagery, in the
complex urban areas of Bhaktapur, Nepal, which was affected by the Nepal
Earthquake of April 25, 2015.",Jun Wang
2022-07-13T15:51:40Z,http://arxiv.org/abs/2207.06300v1,"Re2G: Retrieve, Rerank, Generate","As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g.",Michael Glass
2022-10-06T13:58:03Z,http://arxiv.org/abs/2210.02928v2,"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question
  Answering over Images and Text","While language Models store a massive amount of world knowledge implicitly in
their parameters, even very large models often fail to encode information about
rare entities and events, while incurring huge computational costs. Recently,
retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated
world knowledge into language generation by leveraging an external
non-parametric index and have demonstrated impressive performance with
constrained model sizes. However, these methods are restricted to retrieving
only textual knowledge, neglecting the ubiquitous amount of knowledge in other
modalities like images -- much of which contains information not covered by any
text. To address this limitation, we propose the first Multimodal
Retrieval-Augmented Transformer (MuRAG), which accesses an external
non-parametric multimodal memory to augment language generation. MuRAG is
pre-trained with a mixture of large-scale image-text and text-only corpora
using a joint contrastive and generative loss. We perform experiments on two
different datasets that require retrieving and reasoning over both images and
text to answer a given query: WebQA, and MultimodalQA. Our results show that
MuRAG achieves state-of-the-art accuracy, outperforming existing models by
10-20\% absolute on both datasets and under both distractor and full-wiki
settings.",Wenhu Chen
2023-07-13T17:25:28Z,http://arxiv.org/abs/2307.06985v10,Retrieval Augmented Generation using Engineering Design Knowledge,"Aiming to support Retrieval Augmented Generation (RAG) in the design process,
we present a method to identify explicit, engineering design facts - {head
entity :: relationship :: tail entity} from patented artefact descriptions.
Given a sentence with a pair of entities (based on noun phrases) marked in a
unique manner, our method extracts the relationship that is explicitly
communicated in the sentence. For this task, we create a dataset of 375,084
examples and fine-tune language models for relation identification (token
classification) and elicitation (sequence-to-sequence). The token
classification approach achieves up to 99.7 % accuracy. Upon applying the
method to a domain of 4,870 fan system patents, we populate a knowledge base of
over 2.93 million facts. Using this knowledge base, we demonstrate how Large
Language Models (LLMs) are guided by explicit facts to synthesise knowledge and
generate technical and cohesive responses when sought out for knowledge
retrieval tasks in the design process.",L. Siddharth
2023-08-09T02:02:46Z,http://arxiv.org/abs/2308.04662v3,"VulLibGen: Generating Names of Vulnerability-Affected Packages via a
  Large Language Model","Security practitioners maintain vulnerability reports (e.g., GitHub Advisory)
to help developers mitigate security risks. An important task for these
databases is automatically extracting structured information mentioned in the
report, e.g., the affected software packages, to accelerate the defense of the
vulnerability ecosystem.
  However, it is challenging for existing work on affected package
identification to achieve a high accuracy. One reason is that all existing work
focuses on relatively smaller models, thus they cannot harness the knowledge
and semantic capabilities of large language models.
  To address this limitation, we propose VulLibGen, the first method to use LLM
for affected package identification. In contrast to existing work, VulLibGen
proposes the novel idea to directly generate the affected package. To improve
the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval
augmented generation (RAG) and a local search algorithm. The local search
algorithm is a novel postprocessing algorithm we introduce for reducing the
hallucination of the generated packages. Our evaluation results show that
VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages
in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go)
while the best average accuracy in previous work is 0.721. Additionally,
VulLibGen has high value to security practice: we submitted 60 <vulnerability,
affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them
have been accepted and merged and 20 are pending approval. Our code and dataset
can be found in the attachments.",Tianyu Chen
2023-08-21T00:30:43Z,http://arxiv.org/abs/2308.10401v1,"Model-Free Large-Scale Cloth Spreading With Mobile Manipulation: Initial
  Feasibility Study","Cloth manipulation is common in domestic and service tasks, and most studies
use fixed-base manipulators to manipulate objects whose sizes are relatively
small with respect to the manipulators' workspace, such as towels, shirts, and
rags. In contrast, manipulation of large-scale cloth, such as bed making and
tablecloth spreading, poses additional challenges of reachability and
manipulation control. To address them, this paper presents a novel framework to
spread large-scale cloth, with a single-arm mobile manipulator that can solve
the reachability issue, for an initial feasibility study. On the manipulation
control side, without modeling highly deformable cloth, a vision-based
manipulation control scheme is applied and based on an online-update Jacobian
matrix mapping from selected feature points to the end-effector motion. To
coordinate the control of the manipulator and mobile platform, Behavior Trees
(BTs) are used because of their modularity. Finally, experiments are conducted,
including validation of the model-free manipulation control for cloth spreading
in different conditions and the large-scale cloth spreading framework. The
experimental results demonstrate the large-scale cloth spreading task
feasibility with a single-arm mobile manipulator and the model-free deformation
controller.",Xiangyu Chu+
2023-09-28T15:32:36Z,http://arxiv.org/abs/2310.01429v1,Chatmap : Large Language Model Interaction with Cartographic Data,"The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.",Eren Unlu
2023-10-09T11:34:41Z,http://arxiv.org/abs/2310.05628v3,"Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models","Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Publicly released information on sustainability
practices is often disclosed in diverse, unstructured, and multi-modal
documentation. This poses a challenge in efficiently gathering and aligning the
data into a unified framework to derive insights related to Corporate Social
Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes
an intuitive choice for delivering insightful and actionable data to
stakeholders. In this study, we employ Large Language Models (LLMs), In-Context
Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract
structured insights related to ESG aspects from companies' sustainability
reports. We then leverage graph-based representations to conduct statistical
analyses concerning the extracted insights. These analyses revealed that ESG
criteria cover a wide range of topics, exceeding 500, often beyond those
considered in existing categorizations, and are addressed by companies through
a variety of initiatives. Moreover, disclosure similarities emerged among
companies from the same region or sector, validating ongoing hypotheses in the
ESG literature. Lastly, by incorporating additional company attributes into our
analyses, we investigated which factors impact the most on companies' ESG
ratings, showing that ESG disclosure affects the obtained ratings more than
other financial or company data.",Marco Bronzini
2023-11-14T08:07:11Z,http://arxiv.org/abs/2311.07976v1,"Detailing secondary frontal bore of internal tides breaking above
  deep-ocean topography","Above steep deep-sea topography internal tidal waves may break vigorously.
The associated turbulent mixing is important for resuspending matter, bringing
it tens of meters away from the seafloor for redistribution. While intense
turbulence-generation occurs around a primary (frontal) bore during each
transition from warming downslope to cooling upslope phase of the internal
(tidal) carrier wave, a secondary bore can appear about half a wave-period
later before the turn to the warming phase. As will be demonstrated from a
100-day mooring array consisting of 200 high-resolution temperature sensors
between h = 6-404 m above a steep slope of a large North-Atlantic seamount and
a low-resolution acoustic Doppler current profiler sampling between 50 and 450
m, secondary bores show about the same turbulence intensity as around primary
bores but they generally show larger overturns that always reach the seafloor.
The secondary bores associate with a sudden drop in along-isobath flow speed, a
(renewed) increase in upslope flow of up to |0.2| m s-1, and with
first-harmonic quarter-diurnal periodicity which provides a spectral peak for
turbulence dissipation rate. While each bore is different in appearance,
varying from curved like a primary bore to almost straight upward with a ragged
bore, secondary bores are in a first approximation forward breaking in contrast
with backward breaking primary bores.",Hans van Haren
2023-11-29T15:21:35Z,http://arxiv.org/abs/2311.17722v1,SenTest: Evaluating Robustness of Sentence Encoders,"Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.",Tanmay Chavan
2023-12-05T21:21:01Z,http://arxiv.org/abs/2312.03141v2,"NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest
  Neighbor Search through Near Data Processing","Approximate nearest neighbor search (ANNS) is a key retrieval technique for
vector database and many data center applications, such as person
re-identification and recommendation systems. It is also fundamental to
retrieval augmented generation (RAG) for large language models (LLM) now. Among
all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall
rate. However, as the size of dataset increases, the graph may require hundreds
of gigabytes of memory, exceeding the main memory capacity of a single
workstation node. Although we can do partitioning and use solid-state drive
(SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades
the performance of the system. To address this challenge, we present NDSEARCH,
a hardware-software co-designed near-data processing (NDP) solution for ANNS
processing. NDSEARCH consists of a novel in-storage computing architecture,
namely, SEARSSD, that supports the ANNS kernels and leverages logic unit
(LUN)-level parallelism inside the NAND flash chips. NDSEARCH also includes a
processing model that is customized for NDP and cooperates with SEARSSD. The
processing model enables us to apply a two-level scheduling to improve the data
locality and exploit the internal bandwidth in NDSEARCH, and a speculative
searching mechanism to further accelerate the ANNS workload. Our results show
that NDSEARCH improves the throughput by up to 31.7x, 14.6x, 7.4x 2.9x over
CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively.
NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than
CPU and GPU.",Yitu Wang
2023-12-07T17:24:51Z,http://arxiv.org/abs/2312.04455v4,"Fortify the Shortest Stave in Attention: Enhancing Context Awareness of
  Large Language Models for Effective Tool Use","In this paper, we demonstrate that an inherent waveform pattern in the
attention allocation of large language models (LLMs) significantly affects
their performance in tasks demanding a high degree of context awareness, such
as utilizing LLMs for tool-use. Specifically, the crucial information in the
context will be potentially overlooked by model when it is positioned in the
trough zone of the attention waveform, leading to decreased performance. To
address this issue, we propose a novel inference method named Attention
Buckets. It allows LLMs to process their input through multiple parallel
processes. Each process utilizes a distinct base angle for the rotary position
embedding, thereby creating a unique attention waveform. By compensating an
attention trough of a particular process with an attention peak of another
process, our approach enhances LLM's awareness to various contextual positions,
thus mitigating the risk of overlooking crucial information. In the largest
tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art
performance, comparable to that of GPT-4. On other benchmarks and some RAG
tasks, which also demand a thorough understanding of contextual content,
Attention Buckets also exhibited notable enhancements in performance.",Yuhan Chen
2023-12-21T23:42:13Z,http://arxiv.org/abs/2312.14335v2,"Context-aware Decoding Reduces Hallucination in Query-focused
  Summarization","Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",Zhichao Xu
2023-12-26T04:49:56Z,http://arxiv.org/abs/2312.15883v2,"HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and
  Reliable Medical LLMs Responses","In this paper, we investigate the retrieval-augmented generation (RAG) based
on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large
Language Models (LLMs). Recent approaches suffer from insufficient and
repetitive knowledge retrieval, tedious and time-consuming query parsing, and
monotonous knowledge utilization. To this end, we develop a Hypothesis
Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful
reasoning capacity to compensate for the incompleteness of user queries,
optimizes the interaction process with LLMs, and provides diverse retrieved
knowledge. Specifically, HyKGE explores the zero-shot capability and the rich
knowledge of LLMs with Hypothesis Outputs to extend feasible exploration
directions in the KGs, as well as the carefully curated prompt to enhance the
density and efficiency of LLMs' responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise while ensuring the
balance between diversity and relevance in retrieved knowledge. Experiments on
two Chinese medical multiple-choice question datasets and one Chinese
open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority
of HyKGE in terms of accuracy and explainability.",Xinke Jiang
2023-12-25T06:44:32Z,http://arxiv.org/abs/2312.17264v1,"ESGReveal: An LLM-based approach for extracting structured data from ESG
  reports","ESGReveal is an innovative method proposed for efficiently extracting and
analyzing Environmental, Social, and Governance (ESG) data from corporate
reports, catering to the critical need for reliable ESG information retrieval.
This approach utilizes Large Language Models (LLM) enhanced with Retrieval
Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG
metadata module for targeted queries, a preprocessing module for assembling
databases, and an LLM agent for data extraction. Its efficacy was appraised
using ESG reports from 166 companies across various sectors listed on the Hong
Kong Stock Exchange in 2022, ensuring comprehensive industry and market
capitalization representation. Utilizing ESGReveal unearthed significant
insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in
data extraction and 83.7% in disclosure analysis, which is an improvement over
baseline models. This highlights the framework's capacity to refine ESG data
analysis precision. Moreover, it revealed a demand for reinforced ESG
disclosures, with environmental and social data disclosures standing at 69.5%
and 57.2%, respectively, suggesting a pursuit for more corporate transparency.
While current iterations of ESGReveal do not process pictorial information, a
functionality intended for future enhancement, the study calls for continued
research to further develop and compare the analytical capabilities of various
LLMs. In summary, ESGReveal is a stride forward in ESG data processing,
offering stakeholders a sophisticated tool to better evaluate and advance
corporate sustainability efforts. Its evolution is promising in promoting
transparency in corporate reporting and aligning with broader sustainable
development aims.",Yi Zou
2023-12-31T17:15:25Z,http://arxiv.org/abs/2401.00544v2,"A Reliable Knowledge Processing Framework for Combustion Science using
  Foundation Models","This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.",Vansh Sharma
2024-01-03T12:09:43Z,http://arxiv.org/abs/2401.01701v3,"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks
  via Iterative Grounding","Large language models (LLMs) trained on datasets of publicly available source
code have established a new state of the art in code generation tasks. However,
these models are mostly unaware of the code that exists within a specific
project, preventing the models from making good use of existing APIs. Instead,
LLMs often invent, or ""hallucinate"", non-existent APIs or produce variants of
already existing code. This paper presents De-Hallucinator, a technique that
grounds the predictions of an LLM through a novel combination of retrieving
suitable API references and iteratively querying the model with increasingly
suitable context information in the prompt. The approach exploits the
observation that predictions by LLMs often resemble the desired code, but they
fail to correctly refer to already existing APIs. De-Hallucinator automatically
identifies project-specific API references related to the model's initial
predictions and adds these references into the prompt. Unlike
retrieval-augmented generation (RAG), our approach uses the initial
prediction(s) by the model to iteratively retrieve increasingly suitable API
references. Our evaluation applies the approach to two tasks: predicting API
usages in Python and generating tests in JavaScript. We show that
De-Hallucinator consistently improves the generated code across five LLMs. In
particular, the approach improves the edit distance by 23.3-50.6% and the
recall of correctly predicted API usages by 23.9-61.0% for code completion, and
improves the number of fixed tests that initially failed because of
hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage
for test generation.",Aryaz Eghbali
2024-01-21T03:46:00Z,http://arxiv.org/abs/2401.11391v1,"Interactive AI with Retrieval-Augmented Generation for Next Generation
  Networking","With the advance of artificial intelligence (AI), the emergence of Google
Gemini and OpenAI Q* marks the direction towards artificial general
intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has
been introduced, which can interactively understand and respond not only to
human user input but also to dynamic system and network conditions. In this
article, we explore an integration and enhancement of IAI in networking. We
first comprehensively review recent developments and future perspectives of AI
and then introduce the technology and components of IAI. We then explore the
integration of IAI into the next-generation networks, focusing on how implicit
and explicit interactions can enhance network functionality, improve user
experience, and promote efficient network management. Subsequently, we propose
an IAI-enabled network management and optimization framework, which consists of
environment, perception, action, and brain units. We also design the pluggable
large language model (LLM) module and retrieval augmented generation (RAG)
module to build the knowledge base and contextual memory for decision-making in
the brain unit. We demonstrate the effectiveness of the framework through case
studies. Finally, we discuss potential research directions for IAI-based
networks.",Ruichen Zhang
2024-01-20T03:41:23Z,http://arxiv.org/abs/2401.12998v1,"Evaluating and Enhancing Large Language Models Performance in
  Domain-specific Medicine: Osteoarthritis Management with DocOA","The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.",Xi Chen
2024-01-30T18:37:45Z,http://arxiv.org/abs/2401.17244v3,"LLaMP: Large Language Model Made Powerful for High-fidelity Materials
  Knowledge Retrieval and Distillation","Reducing hallucination of Large Language Models (LLMs) is imperative for use
in the sciences, where reliability and reproducibility are crucial. However,
LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and
inevitably biased task to fine-tune them on domain-specific literature and
data. Here we introduce LLaMP, a multimodal retrieval-augmented generation
(RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can
dynamically and recursively interact with computational and experimental data
on Materials Project (MP) and run atomistic simulations via high-throughput
workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage
ability to comprehend and integrate various modalities of materials science
concepts, fetch relevant data stores on the fly, process higher-order data
(such as crystal structure and elastic tensor), and streamline complex tasks in
computational materials and chemistry. We propose a simple metric combining
uncertainty and confidence estimates to evaluate the self-consistency of
responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively
mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli,
electronic bandgaps, and formation energies that seem to derive from mixed data
sources. We also demonstrate LLaMP's capability to edit crystal structures and
run annealing molecular dynamics simulations using pre-trained machine-learning
force fields. The framework offers an intuitive and nearly hallucination-free
approach to exploring and scaling materials informatics, and establishes a
pathway for knowledge distillation and fine-tuning other language models. Code
and live demo are available at https://github.com/chiang-yuan/llamp",Yuan Chiang
2024-02-01T16:40:32Z,http://arxiv.org/abs/2402.00746v7,Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,"Recent advancements in artificial intelligence (AI), especially large
language models (LLMs), have significantly advanced healthcare applications and
demonstrated potentials in intelligent medical treatment. However, there are
conspicuous challenges such as vast data volumes and inconsistent symptom
characterization standards, preventing full integration of healthcare AI
systems with individual patients' needs. To promote professional and
personalized healthcare, we propose an innovative framework, Heath-LLM, which
combines large-scale feature extraction and medical knowledge trade-off
scoring. Compared to traditional health management applications, our system has
three main advantages: (1) It integrates health reports and medical knowledge
into a large model to ask relevant questions to large language model for
disease prediction; (2) It leverages a retrieval augmented generation (RAG)
mechanism to enhance feature extraction; (3) It incorporates a semi-automated
feature updating framework that can merge and delete features to improve
accuracy of disease prediction. We experiment on a large number of health
reports to assess the effectiveness of Health-LLM system. The results indicate
that the proposed system surpasses the existing ones and has the potential to
significantly advance disease prediction and personalized health management.",Mingyu Jin
2024-02-02T02:41:28Z,http://arxiv.org/abs/2402.01788v1,LitLLM: A Toolkit for Scientific Literature Review,"Conducting literature reviews for scientific papers is essential for
understanding research, its limitations, and building on existing work. It is a
tedious task which makes an automatic literature review generator appealing.
Unfortunately, many existing works that generate such reviews using Large
Language Models (LLMs) have significant limitations. They tend to
hallucinate-generate non-actual information-and ignore the latest research they
have not been trained on. To address these limitations, we propose a toolkit
that operates on Retrieval Augmented Generation (RAG) principles, specialized
prompting and instructing techniques with the help of LLMs. Our system first
initiates a web search to retrieve relevant papers by summarizing user-provided
abstracts into keywords using an off-the-shelf LLM. Authors can enhance the
search by supplementing it with relevant papers or keywords, contributing to a
tailored retrieval process. Second, the system re-ranks the retrieved papers
based on the user-provided abstract. Finally, the related work section is
generated based on the re-ranked results and the abstract. There is a
substantial reduction in time and effort for literature review compared to
traditional methods, establishing our toolkit as an efficient alternative. Our
open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM
and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)
with the video demo at https://youtu.be/E2ggOZBAFw0.",Shubham Agarwal
2024-02-02T18:23:09Z,http://arxiv.org/abs/2402.01828v1,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed SLM, a joint speech and language model, which fuses a
pretrained foundational speech model and a large language model (LLM), while
preserving the in-context learning capability intrinsic to the pretrained LLM.
In this paper, we apply SLM to speech dialog applications where the dialog
states are inferred directly from the audio signal.
  Task-oriented dialogs often contain domain-specific entities, i.e.,
restaurants, hotels, train stations, and city names, which are difficult to
recognize, however, critical for the downstream applications. Inspired by the
RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented
SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to
retrieve text entities mentioned in the audio. The retrieved entities are then
added as text inputs to the underlying SLM to bias model predictions. We
evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that
this retrieval augmentation boosts model performance, achieving joint goal
accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error
rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach
is broadly applicable to other speech tasks requiring contextual information or
domain-specific entities, such as contextual ASR with biasing capability.",Mingqiu Wang
2024-02-03T03:44:57Z,http://arxiv.org/abs/2402.02008v1,"How well do LLMs cite relevant medical references? An evaluation
  framework and analyses","Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.",Kevin Wu
2024-02-15T13:39:55Z,http://arxiv.org/abs/2402.09939v1,Generative AI in the Construction Industry: A State-of-the-art Analysis,"The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.",Ridwan Taiwo
2024-02-16T19:26:09Z,http://arxiv.org/abs/2402.11034v2,"PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal
  Question-Answering","Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. ""Who was the US
president in 1970?""). Little work has studied questions whose temporal context
is relative to the present time (e.g. ""Who was the previous US president?""). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.",Jannat Ara Meem
2024-02-16T06:29:16Z,http://arxiv.org/abs/2402.12170v2,"Where is the answer? Investigating Positional Bias in Language Model
  Knowledge Extraction","Large language models require updates to remain up-to-date or adapt to new
domains by fine-tuning them with new documents. One key is memorizing the
latest information in a way that the memorized information is extractable with
a query prompt. However, LLMs suffer from a phenomenon called perplexity curse;
despite minimizing document perplexity during fine-tuning, LLMs struggle to
extract information through a prompt sentence. In this new knowledge
acquisition and extraction, we find a very intriguing fact that LLMs can
accurately answer questions about the first sentence, but they struggle to
extract information described in the middle or end of the documents used for
fine-tuning. Our study suggests that the auto-regressive training causes this
issue; each token is prompted by reliance on all previous tokens, which hinders
the model from recalling information from training documents by question
prompts. To conduct the in-depth study, we publish both synthetic and real
datasets, enabling the evaluation of the QA performance w.r.t. the position of
the corresponding answer in a document. Our investigation shows that even a
large model suffers from the perplexity curse, but regularization such as
denoising auto-regressive loss can enhance the information extraction from
diverse positions. These findings will be (i) a key to improving knowledge
extraction from LLMs and (ii) new elements to discuss the trade-off between RAG
and fine-tuning in adapting LLMs to a new domain.",Kuniaki Saito
2024-02-19T17:37:28Z,http://arxiv.org/abs/2402.12317v2,EVOR: Evolving Retrieval for Code Generation,"Recently the retrieval-augmented generation (RAG) has been successfully
applied in code generation. However, existing pipelines for retrieval-augmented
code generation (RACG) employ static knowledge bases with a single source,
limiting the adaptation capabilities of Large Language Models (LLMs) to domains
they have insufficient knowledge of. In this work, we develop a novel pipeline,
EVOR, that employs the synchronous evolution of both queries and diverse
knowledge bases. On two realistic settings where the external knowledge is
required to solve code generation tasks, we compile four new datasets
associated with frequently updated libraries and long-tail programming
languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR
achieves two to four times of execution accuracy compared to other methods such
as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We
demonstrate that EVOR is flexible and can be easily combined with them to
achieve further improvement. Further analysis reveals that EVOR benefits from
the synchronous evolution of queries and documents and the diverse information
sources in the knowledge base. We hope that our studies will inspire more
insights into the design of advanced RACG pipelines in future research. Our
model, code, and data are available at https://arks-codegen.github.io.",Hongjin Su
2024-02-20T10:00:58Z,http://arxiv.org/abs/2402.12869v2,"Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based
  Question Answering with Domain Hybrid Data","Augmenting Large Language Models (LLMs) for Question Answering (QA) with
domain specific data has attracted wide attention. However, domain data often
exists in a hybrid format, including text and semi-structured tables, posing
challenges for the seamless integration of information. Table-to-Text
Generation is a promising solution by facilitating the transformation of hybrid
data into a uniformly text-formatted corpus. Although this technique has been
widely studied by the NLP community, there is currently no comparative analysis
on how corpora generated by different table-to-text methods affect the
performance of QA systems. In this paper, we address this research gap in two
steps. First, we innovatively integrate table-to-text generation into the
framework of enhancing LLM-based QA systems with domain hybrid data. Then, we
utilize this framework in real-world industrial data to conduct extensive
experiments on two types of QA systems (DSFT and RAG frameworks) with four
representative methods: Markdown format, Template serialization, TPLM-based
method, and LLM-based method. Based on the experimental results, we draw some
empirical findings and explore the underlying reasons behind the success of
some methods. We hope the findings of this work will provide a valuable
reference for the academic and industrial communities in developing robust QA
systems.",Dehai Min
2024-02-22T12:13:35Z,http://arxiv.org/abs/2402.14480v1,"MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems
  in LLM Augmented Generation","Augmented generation techniques such as Retrieval-Augmented Generation (RAG)
and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing
large language model (LLM) outputs with external knowledge and cached
information. However, the integration of vector databases, which serve as a
backbone for these augmentations, introduces critical challenges, particularly
in ensuring accurate vector matching. False vector matching in these databases
can significantly compromise the integrity and reliability of LLM outputs,
leading to misinformation or erroneous responses. Despite the crucial impact of
these issues, there is a notable research gap in methods to effectively detect
and address false vector matches in LLM-augmented generation. This paper
presents MeTMaP, a metamorphic testing framework developed to identify false
vector matching in LLM-augmented generation systems. We derive eight
metamorphic relations (MRs) from six NLP datasets, which form our method's
core, based on the idea that semantically similar texts should match and
dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets
for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over
203 vector matching configurations, involving 29 embedding models and 7
distance metrics, uncovers significant inaccuracies. The results, showing a
maximum accuracy of only 41.51\% on our tests compared to the original
datasets, emphasize the widespread issue of false matches in vector matching
methods and the critical need for effective detection and mitigation in
LLM-augmented applications.",Guanyu Wang
2024-02-23T02:24:15Z,http://arxiv.org/abs/2402.15061v2,"Fine-tuning Large Language Models for Domain-specific Machine
  Translation","Large language models (LLMs) have shown great potential in domain-specific
machine translation (MT). However, one major issue is that LLMs pre-trained on
general domain corpus might not generalize well to specific domains due to the
lack of domain-specific knowledge. To address this issue, this paper focuses on
enhancing the domain-specific MT capability of LLMs, by providing high-quality
training datasets and proposing a novel fine-tuning framework denoted by
DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced
prompting integrates dictionary information into prompts to improve the
translation of domain-specific terminology.; (ii) RAG-based few-shot example
selection provides high-quality examples that simulate both the domain and
style characteristics; (iii) Fine-tuning with few-shot examples further
enhances performance when using in-domain examples. We deploy DragFT on three
well-known LLM backbones with 13B training parameters to validate its
effectiveness. The results on three domain-specific datasets show that DragFT
achieves a significant performance boost and shows superior performance
compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance
improvement of DragFT over existing LLMs can be attributed to incorporating
relevant knowledge while mitigating noise.",Jiawei Zheng
2024-03-01T07:14:45Z,http://arxiv.org/abs/2403.00872v1,"DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy
  in Large-Scale Databases","The task of converting natural language queries into SQL queries is
intricate, necessitating a blend of precise techniques for an accurate
translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a
significant development in this domain. This paper introduces DFIN (Decomposed
Focused-In-Context), an innovative extension of DIN-SQL that enhances
Text-to-SQL conversion by addressing schema linking errors, which are a major
source of inaccuracies. DFIN uniquely alternates between prompting techniques
and Retrieval-Augmented Generation (RAG), adapting to the size and complexity
of the database schema. A preprocessing phase embeds database definitions and
leverages annotated files, akin to those in the BIRD dataset, facilitating the
runtime retrieval of pertinent schema information. This strategy significantly
reduces the token count for schema linking prompts, enabling the use of a
standard GPT-4 model over its larger context variant, thus handling large-scale
databases more effectively and economically. Our evaluation on the BIRD
dataset, a challenging real-world benchmark, demonstrates that DFIN not only
scales efficiently but also improves accuracy, achieving a score of 51.69. This
improvement surpasses DIN-SQL method (the current third-place), which is the
highest-ranked model employing in-context learning rather than fine-tuning,
previously scoring 50.72. The advancement of DFIN underscores the evolving
capabilities of in-context learning methodologies combined with advanced
language models, offering a promising avenue for future research in complex
Text-to-SQL conversion tasks.",Shai Volvovsky
2024-03-07T08:34:57Z,http://arxiv.org/abs/2403.04317v2,Online Adaptation of Language Models with a Memory of Amortized Contexts,"Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.",Jihoon Tack
2024-03-13T08:50:15Z,http://arxiv.org/abs/2403.08345v1,"From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction","The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.",Vamsi Krishna Kommineni
2024-03-15T17:04:27Z,http://arxiv.org/abs/2403.10588v1,"S3LLM: Large-Scale Scientific Software Understanding with LLMs using
  Source, Metadata, and Document","The understanding of large-scale scientific software poses significant
challenges due to its diverse codebase, extensive code length, and target
computing architectures. The emergence of generative AI, specifically large
language models (LLMs), provides novel pathways for understanding such complex
scientific codes. This paper presents S3LLM, an LLM-based framework designed to
enable the examination of source code, code metadata, and summarized
information in conjunction with textual technical reports in an interactive,
conversational manner through a user-friendly interface. S3LLM leverages
open-source LLaMA-2 models to enhance code analysis through the automatic
transformation of natural language queries into domain-specific language (DSL)
queries. Specifically, it translates these queries into Feature Query Language
(FQL), enabling efficient scanning and parsing of entire code repositories. In
addition, S3LLM is equipped to handle diverse metadata types, including DOT,
SQL, and customized formats. Furthermore, S3LLM incorporates retrieval
augmented generation (RAG) and LangChain technologies to directly query
extensive documents. S3LLM demonstrates the potential of using locally deployed
open-source LLMs for the rapid understanding of large-scale scientific
computing software, eliminating the need for extensive coding expertise, and
thereby making the process more efficient and effective. S3LLM is available at
https://github.com/ResponsibleAILab/s3llm.",Kareem Shaik
2024-03-19T09:45:33Z,http://arxiv.org/abs/2403.12582v1,"AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented
  Stock-Chain Framework","The task of financial analysis primarily encompasses two key areas: stock
trend prediction and the corresponding financial question answering. Currently,
machine learning and deep learning algorithms (ML&DL) have been widely applied
for stock trend predictions, leading to significant progress. However, these
methods fail to provide reasons for predictions, lacking interpretability and
reasoning processes. Also, they can not integrate textual information such as
financial news or reports. Meanwhile, large language models (LLMs) have
remarkable textual understanding and generation ability. But due to the
scarcity of financial training datasets and limited integration with real-time
knowledge, LLMs still suffer from hallucinations and are unable to keep up with
the latest information. To tackle these challenges, we first release AlphaFin
datasets, combining traditional research datasets, real-time financial data,
and handwritten chain-of-thought (CoT) data. It has a positive impact on
training LLMs for completing financial analysis. We then use AlphaFin datasets
to benchmark a state-of-the-art method, called Stock-Chain, for effectively
tackling the financial analysis task, which integrates retrieval-augmented
generation (RAG) techniques. Extensive experiments are conducted to demonstrate
the effectiveness of our framework on financial analysis.",Xiang Li
2024-03-21T13:52:30Z,http://arxiv.org/abs/2403.14403v2,"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
  Models through Question Complexity","Retrieval-Augmented Large Language Models (LLMs), which incorporate the
non-parametric knowledge from external knowledge bases into LLMs, have emerged
as a promising approach to enhancing response accuracy in several tasks, such
as Question-Answering (QA). However, even though there are various approaches
dealing with queries of different complexities, they either handle simple
queries with unnecessary computational overhead or fail to adequately address
complex multi-step queries; yet, not all user requests fall into only one of
the simple or complex categories. In this work, we propose a novel adaptive QA
framework, that can dynamically select the most suitable strategy for
(retrieval-augmented) LLMs from the simplest to the most sophisticated ones
based on the query complexity. Also, this selection process is operationalized
with a classifier, which is a smaller LM trained to predict the complexity
level of incoming queries with automatically collected labels, obtained from
actual predicted outcomes of models and inherent inductive biases in datasets.
This approach offers a balanced strategy, seamlessly adapting between the
iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval
methods, in response to a range of query complexities. We validate our model on
a set of open-domain QA datasets, covering multiple query complexities, and
show that ours enhances the overall efficiency and accuracy of QA systems,
compared to relevant baselines including the adaptive retrieval approaches.
Code is available at: https://github.com/starsuzi/Adaptive-RAG.",Soyeong Jeong
2024-03-23T06:03:36Z,http://arxiv.org/abs/2403.15736v2,"General LLMs as Instructors for Domain-Specific LLMs: A Sequential
  Fusion Method to Integrate Extraction and Editing","The substantial interest in updating Large Language Models (LLMs) without
retraining from scratch is accompanied by several challenges. This is
particularly true when updating LLMs with datasets that necessitate
domain-expert reasoning across extensive texts, despite limited samples. We
termed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs
(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval
Augmented Generation (RAG) are inadequate for addressing this critical issue,
particularly evident in our exploration of a specific medical dataset that
epitomizes the distinct needs of FDoR-UL. To tackle this challenge, we
introduce a Sequential Fusion method to integrate knowledge from complex
contexts into LLMs. This method employs a two-stage framework: initially
leveraging general LLMs to perform relation extraction for knowledge
acquisition from complex texts, followed by updating domain-specific LLMs
through Knowledge Editing (KE). Employing our method, domain-specific LLMs
achieved a 71.7% accuracy (an average gain of 39.1%) in question-answering
tasks. Furthermore, we expanded our evaluation to a novel economics-management
dataset we developed, where our method achieved a 75.0% accuracy (an average
gain of 45.0%). These findings underscore the effectiveness and flexibility of
our approach in FDoR-UL across various domains.",Xin Zhang
2024-03-28T03:14:18Z,http://arxiv.org/abs/2403.19116v1,MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,"In today's fast-paced industry, professionals face the challenge of
summarizing a large number of documents and extracting vital information from
them on a daily basis. These metrics are frequently hidden away in tables
and/or their nested hyperlinks. To address this challenge, the approach of
Table Question Answering (QA) has been developed to extract the relevant
information. However, traditional Table QA training tasks that provide a table
and an answer(s) from a gold cell coordinate(s) for a question may not always
ensure extracting the accurate answer(s). Recent advancements in Large Language
Models (LLMs) have opened up new possibilities for extracting information from
tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot
Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The
first step involves Few-Shot Learning (FSL), where relevant tables and
associated contexts of hyperlinks are retrieved based on a given question. The
retrieved content is then used to construct few-shot prompts as inputs to an
LLM, such as ChatGPT. To tackle the challenge of answering complex questions,
the second step leverages Chain-of-thought (CoT) prompting to decompose the
complex question into a sequential chain of questions and reasoning thoughts in
a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process
by retrieving relevant tables and contexts of hyperlinks that are relevant to
the resulting reasoning thoughts and questions. These additional contexts are
then used to supplement the prompt used in the first step, resulting in more
accurate answers from an LLM. Empirical results from OTT-QA demonstrate that
our abstractive QA approach significantly improves the accuracy of extractive
Table QA methods.",Che Guan
2024-03-30T22:41:05Z,http://arxiv.org/abs/2404.00486v1,"Dialectical Alignment: Resolving the Tension of 3H and Security Threats
  of LLMs","With the rise of large language models (LLMs), ensuring they embody the
principles of being helpful, honest, and harmless (3H), known as Human
Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,
etc., effectively fine-tune LLMs to match preferences in the preference
dataset, they often lead LLMs to highly receptive human input and external
evidence, even when this information is poisoned. This leads to a tendency for
LLMs to be Adaptive Chameleons when external evidence conflicts with their
parametric memory. This exacerbates the risk of LLM being attacked by external
poisoned data, which poses a significant security risk to LLM system
applications such as Retrieval-augmented generation (RAG). To address the
challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)
utilizes AI feedback to identify optimal strategies for LLMs to navigate
inter-context conflicts and context-memory conflicts with different external
evidence in context window (i.e., different ratios of poisoned factual
contexts); (2) constructs the SFT dataset as well as the preference dataset
based on the AI feedback and strategies above; (3) uses the above datasets for
LLM alignment to defense poisoned context attack while preserving the
effectiveness of in-context knowledge editing. Our experiments show that the
dialectical alignment model improves poisoned data attack defense by 20 and
does not require any additional prompt engineering or prior declaration of
``you may be attacked`` to the LLMs' context window.",Shu Yang
2024-04-05T11:55:52Z,http://arxiv.org/abs/2404.04044v2,A Comparison of Methods for Evaluating Generative IR,"Information retrieval systems increasingly incorporate generative components.
For example, in a retrieval augmented generation (RAG) system, a retrieval
component might provide a source of ground truth, while a generative component
summarizes and augments its responses. In other systems, a large language model
(LLM) might directly generate responses without consulting a retrieval
component. While there are multiple definitions of generative information
retrieval (Gen-IR) systems, in this paper we focus on those systems where the
system's response is not drawn from a fixed collection of documents or
passages. The response to a query may be entirely new text. Since traditional
IR evaluation methods break down under this model, we explore various methods
that extend traditional offline evaluation approaches to the Gen-IR context.
Offline IR evaluation traditionally employs paid human assessors, but
increasingly LLMs are replacing human assessment, demonstrating capabilities
similar or superior to crowdsourced labels. Given that Gen-IR systems do not
generate responses from a fixed set, we assume that methods for Gen-IR
evaluation must largely depend on LLM-generated labels. Along with methods
based on binary and graded relevance, we explore methods based on explicit
subtopics, pairwise preferences, and embeddings. We first validate these
methods against human assessments on several TREC Deep Learning Track tasks; we
then apply these methods to evaluate the output of several purely generative
systems. For each method we consider both its ability to act autonomously,
without the need for human labels or other input, and its ability to support
human auditing. To trust these methods, we must be assured that their results
align with human assessments. In order to do so, evaluation criteria must be
transparent, so that outcomes can be audited by human assessors.",Negar Arabzadeh
2024-04-09T14:34:48Z,http://arxiv.org/abs/2404.06347v2,RAR-b: Reasoning as Retrieval Benchmark,"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks
have been the two major avenues to record the progress of embedding models in
the past few years. Under the emerging Retrieval-augmented Generation (RAG)
paradigm, we envision the need to evaluate next-level language understanding
abilities of embedding models, and take a conscious look at the reasoning
abilities stored in them. Addressing this, we pose the question: Can retrievers
solve reasoning problems? By transforming reasoning tasks into retrieval tasks,
we find that without specifically trained for reasoning-level language
understanding, current state-of-the-art retriever models may still be far from
being competent for playing the role of assisting LLMs, especially in
reasoning-intensive tasks. Moreover, albeit trained to be aware of
instructions, instruction-aware IR models are often better off without
instructions in inference time for reasoning tasks, posing an overlooked
retriever-LLM behavioral gap for the research community to align. However,
recent decoder-based embedding models show great promise in narrowing the gap,
highlighting the pathway for embedding models to achieve reasoning-level
language understanding. We also show that, although current off-the-shelf
re-ranker models fail on these tasks, injecting reasoning abilities into them
through fine-tuning still appears easier than doing so to bi-encoders, and we
are able to achieve state-of-the-art performance across all tasks by
fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark
(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning
abilities stored in retriever models. RAR-b is available at
https://github.com/gowitheflow-1998/RAR-b.",Chenghao Xiao
2024-04-10T16:12:50Z,http://arxiv.org/abs/2404.07135v2,"Towards Robustness of Text-to-Visualization Translation against Lexical
  and Phrasal Variability","Text-to-Vis is an emerging task in the natural language processing (NLP) area
that aims to automatically generate data visualizations from natural language
questions (NLQs). Despite their progress, existing text-to-vis models often
heavily rely on lexical matching between words in the questions and tokens in
data schemas. This overreliance on lexical matching may lead to a diminished
level of model robustness against input variations. In this study, we
thoroughly examine the robustness of current text-to-vis models, an area that
has not previously been explored. In particular, we construct the first
robustness dataset nvBench-Rob, which contains diverse lexical and phrasal
variations based on the original text-to-vis benchmark nvBench. Then, we found
that the performance of existing text-to-vis models on this new dataset
dramatically drops, implying that these methods exhibit inadequate robustness
overall. Finally, we propose a novel framework based on Retrieval-Augmented
Generation (RAG) technique, named GRED, specifically designed to address input
perturbations in these two variants. The framework consists of three parts:
NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and
Annotation-based Debugger, which are used to tackle the challenges posed by
natural language variants, programming style differences and data schema
variants, respectively. Extensive experimental evaluations show that, compared
to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs
better in terms of model robustness, with a 32% increase in accuracy on the
proposed nvBench-Rob dataset.",Jinwei Lu
2024-04-13T02:39:36Z,http://arxiv.org/abs/2404.08878v1,"Generative AI Agent for Next-Generation MIMO Design: Fundamentals,
  Challenges, and Vision","Next-generation multiple input multiple output (MIMO) is expected to be
intelligent and scalable. In this paper, we study generative artificial
intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we
provide an overview of the development, fundamentals, and challenges of the
next-generation MIMO. Then, we propose the concept of the generative AI agent,
which is capable of generating tailored and specialized contents with the aid
of large language model (LLM) and retrieval augmented generation (RAG). Next,
we comprehensively discuss the features and advantages of the generative AI
agent framework. More importantly, to tackle existing challenges of
next-generation MIMO, we discuss generative AI agent-enabled next-generation
MIMO design, from the perspective of performance analysis, signal processing,
and resource allocation. Furthermore, we present two compelling case studies
that demonstrate the effectiveness of leveraging the generative AI agent for
performance analysis in complex configuration scenarios. These examples
highlight how the integration of generative AI agents can significantly enhance
the analysis and design of next-generation MIMO systems. Finally, we discuss
important potential research future directions.",Zhe Wang
2024-04-14T03:44:54Z,http://arxiv.org/abs/2404.09134v2,"Generative AI Agents with Large Language Model for Satellite Networks
  via a Mixture of Experts Transmission","In response to the needs of 6G global communications, satellite communication
networks have emerged as a key solution. However, the large-scale development
of satellite communication networks is constrained by the complex system
models, whose modeling is challenging for massive users. Moreover, transmission
interference between satellites and users seriously affects communication
performance. To solve these problems, this paper develops generative artificial
intelligence (AI) agents for model formulation and then applies a mixture of
experts (MoE) approach to design transmission strategies. Specifically, we
leverage large language models (LLMs) to build an interactive modeling paradigm
and utilize retrieval-augmented generation (RAG) to extract satellite expert
knowledge that supports mathematical modeling. Afterward, by integrating the
expertise of multiple specialized components, we propose an MoE-proximal policy
optimization (PPO) approach to solve the formulated problem. Each expert can
optimize the optimization variables at which it excels through specialized
training through its own network and then aggregates them through the gating
network to perform joint optimization. The simulation results validate the
accuracy and effectiveness of employing a generative agent for problem
formulation. Furthermore, the superiority of the proposed MoE-ppo approach over
other benchmarks is confirmed in solving the formulated problem. The
adaptability of MoE-PPO to various customized modeling problems has also been
demonstrated.",Ruichen Zhang
2024-04-14T16:34:31Z,http://arxiv.org/abs/2404.09296v2,"Cross-Data Knowledge Graph Construction for LLM-enabled Educational
  Question-Answering System: A Case Study at HCMUT","In today's rapidly evolving landscape of Artificial Intelligence, large
language models (LLMs) have emerged as a vibrant research topic. LLMs find
applications in various fields and contribute significantly. Despite their
powerful language capabilities, similar to pre-trained language models (PLMs),
LLMs still face challenges in remembering events, incorporating new
information, and addressing domain-specific issues or hallucinations. To
overcome these limitations, researchers have proposed Retrieval-Augmented
Generation (RAG) techniques, some others have proposed the integration of LLMs
with Knowledge Graphs (KGs) to provide factual context, thereby improving
performance and delivering more accurate feedback to user queries.
  Education plays a crucial role in human development and progress. With the
technology transformation, traditional education is being replaced by digital
or blended education. Therefore, educational data in the digital environment is
increasing day by day. Data in higher education institutions are diverse,
comprising various sources such as unstructured/structured text, relational
databases, web/app-based API access, etc. Constructing a Knowledge Graph from
these cross-data sources is not a simple task. This article proposes a method
for automatically constructing a Knowledge Graph from multiple data sources and
discusses some initial applications (experimental trials) of KG in conjunction
with LLMs for question-answering tasks.",Tuan Bui
2024-03-23T13:25:01Z,http://arxiv.org/abs/2404.10779v1,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",Mathav Raj J
2024-04-18T11:29:23Z,http://arxiv.org/abs/2404.12096v3,LongEmbed: Extending Embedding Models for Long Context Retrieval,"Embedding models play a pivot role in modern NLP applications such as IR and
RAG. While the context limit of LLMs has been pushed beyond 1 million tokens,
embedding models are still confined to a narrow context window not exceeding 8k
tokens, refrained from application scenarios requiring long inputs such as
legal contracts. This paper explores context window extension of existing
embedding models, pushing the limit to 32k without requiring additional
training. First, we examine the performance of current embedding models for
long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed
comprises two synthetic tasks and four carefully chosen real-world tasks,
featuring documents of varying length and dispersed target information.
Benchmarking results underscore huge room for improvement in these models.
Based on this, comprehensive experiments show that training-free context window
extension strategies like position interpolation can effectively extend the
context window of existing embedding models by several folds, regardless of
their original context being 512 or beyond 4k. Furthermore, for models
employing absolute position encoding (APE), we show the possibility of further
fine-tuning to harvest notable performance gains while strictly preserving
original behavior for short inputs. For models using rotary position embedding
(RoPE), significant enhancements are observed when employing RoPE-specific
methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for
context window extension. To facilitate future research, we release E5-Base-4k
and E5-RoPE-Base, along with the LongEmbed benchmark.",Dawei Zhu
2024-04-28T05:46:28Z,http://arxiv.org/abs/2404.18077v2,"Generative AI for Low-Carbon Artificial Intelligence of Things with
  Large Language Models","By integrating Artificial Intelligence (AI) with the Internet of Things
(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.
However, AIoT is facing the challenges of energy consumption and carbon
emissions due to the continuous advancement of mobile technology. Fortunately,
Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT
due to its excellent reasoning and generation capabilities. In this article, we
explore the potential of GAI for carbon emissions reduction and propose a novel
GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main
impacts that cause carbon emissions in AIoT, and then introduce GAI techniques
and their relations to carbon emissions. We then explore the application
prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon
emissions of network components. Subsequently, we propose a Large Language
Model (LLM)-enabled carbon emission optimization framework, in which we design
pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more
accurate and reliable optimization problems. Furthermore, we utilize Generative
Diffusion Models (GDMs) to identify optimal strategies for carbon emission
reduction. Numerical results demonstrate the effectiveness of the proposed
framework. Finally, we insightfully provide open research directions for
low-carbon AIoT.",Jinbo Wen
2024-04-29T07:11:39Z,http://arxiv.org/abs/2404.18470v2,"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls
  using Large Language Model for Stock Performance Prediction","In the realm of financial analytics, leveraging unstructured data, such as
earnings conference calls (ECCs), to forecast stock volatility is a critical
challenge that has attracted both academics and investors. While previous
studies have used multimodal deep learning-based models to obtain a general
view of ECCs for volatility predicting, they often fail to capture detailed,
complex information. Our research introduces a novel framework: \textbf{ECC
Analyzer}, which utilizes large language models (LLMs) to extract richer, more
predictive content from ECCs to aid the model's prediction performance. We use
the pre-trained large models to extract textual and audio features from ECCs
and implement a hierarchical information extraction strategy to extract more
fine-grained information. This strategy first extracts paragraph-level general
information by summarizing the text and then extracts fine-grained focus
sentences using Retrieval-Augmented Generation (RAG). These features are then
fused through multimodal feature fusion to perform volatility prediction.
Experimental results demonstrate that our model outperforms traditional
analytical benchmarks, confirming the effectiveness of advanced LLM techniques
in financial analysis.",Yupeng Cao
2024-05-06T20:50:17Z,http://arxiv.org/abs/2405.03845v1,Self-Improving Customer Review Response Generation Based on LLMs,"Previous studies have demonstrated that proactive interaction with user
reviews has a positive impact on the perception of app users and encourages
them to submit revised ratings. Nevertheless, developers encounter challenges
in managing a high volume of reviews, particularly in the case of popular apps
with a substantial influx of daily reviews. Consequently, there is a demand for
automated solutions aimed at streamlining the process of responding to user
reviews. To address this, we have developed a new system for generating
automatic responses by leveraging user-contributed documents with the help of
retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
Our solution, named SCRABLE, represents an adaptive customer review response
automation that enhances itself with self-optimizing prompts and a judging
mechanism based on LLMs. Additionally, we introduce an automatic scoring
mechanism that mimics the role of a human evaluator to assess the quality of
responses generated in customer review domains. Extensive experiments and
analyses conducted on real-world datasets reveal that our method is effective
in producing high-quality responses, yielding improvement of more than 8.5%
compared to the baseline. Further validation through manual examination of the
generated responses underscores the efficacy our proposed system.",Guy Azov
2024-05-13T07:56:15Z,http://arxiv.org/abs/2405.07530v1,Prompt-based Code Completion via Multi-Retrieval Augmented Generation,"Automated code completion, aiming at generating subsequent tokens from
unfinished code, has been significantly benefited from recent progress in
pre-trained Large Language Models (LLMs). However, these models often suffer
from coherence issues and hallucinations when dealing with complex code logic
or extrapolating beyond their training data. Existing Retrieval Augmented
Generation (RAG) techniques partially address these issues by retrieving
relevant code with a separate encoding model where the retrieved snippet serves
as contextual reference for code completion. However, their retrieval scope is
subject to a singular perspective defined by the encoding model, which largely
overlooks the complexity and diversity inherent in code semantics. To address
this limitation, we propose ProCC, a code completion framework leveraging
prompt engineering and the contextual multi-armed bandits algorithm to flexibly
incorporate and adapt to multiple perspectives of code. ProCC first employs a
prompt-based multi-retriever system which crafts prompt templates to elicit LLM
knowledge to understand code semantics with multiple retrieval perspectives.
Then, it adopts the adaptive retrieval selection algorithm to incorporate code
similarity into the decision-making process to determine the most suitable
retrieval perspective for the LLM to complete the code. Experimental results
demonstrate that ProCC outperforms state-of-the-art code completion technique
by 8.6% on our collected open-source benchmark suite and 10.1% on the
private-domain benchmark suite collected from a billion-user e-commerce company
in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in
a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned
model.",Hanzhuo Tan
2024-05-21T13:02:27Z,http://arxiv.org/abs/2405.12750v1,"Generative AI and Large Language Models for Cyber Security: All Insights
  You Need","This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.",Mohamed Amine Ferrag
2024-05-20T11:05:56Z,http://arxiv.org/abs/2405.13057v1,Can Github issues be solved with Tree Of Thoughts?,"While there have been extensive studies in code generation by large language
models (LLM), where benchmarks like HumanEval have been surpassed with an
impressive 96.3% success rate, these benchmarks predominantly judge a model's
performance on basic function-level code generation and lack the critical
thinking and concept of scope required of real-world scenarios such as solving
GitHub issues. This research introduces the application of the Tree of Thoughts
(ToT) language model reasoning framework for enhancing the decision-making and
problem-solving abilities of LLMs for this complex task. Compared to
traditional input-output (IO) prompting and Retrieval Augmented Generation
(RAG) techniques, ToT is designed to improve performance by facilitating a
structured exploration of multiple reasoning trajectories and enabling
self-assessment of potential solutions. We experimentally deploy ToT in
tackling a Github issue contained within an instance of the SWE-bench. However,
our results reveal that the ToT framework alone is not enough to give LLMs the
critical reasoning capabilities to outperform existing methods. In this paper
we analyze the potential causes of these shortcomings and identify key areas
for improvement such as deepening the thought process and introducing agentic
capabilities. The insights of this research are aimed at informing future
directions for refining the application of ToT and better harnessing the
potential of LLMs in real-world problem-solving scenarios.",Ricardo La Rosa
2024-05-22T07:21:32Z,http://arxiv.org/abs/2405.13401v4,"TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in
  Large Language Models","Large language models (LLMs) have raised concerns about potential security
threats despite performing significantly in Natural Language Processing (NLP).
Backdoor attacks initially verified that LLM is doing substantial harm at all
stages, but the cost and robustness have been criticized. Attacking LLMs is
inherently risky in security review, while prohibitively expensive. Besides,
the continuous iteration of LLMs will degrade the robustness of backdoors. In
this paper, we propose TrojanRAG, which employs a joint backdoor attack in the
Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack
scenarios. Specifically, the adversary constructs elaborate target contexts and
trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized
by contrastive learning, thus constraining the triggering conditions to a
parameter subspace to improve the matching. To improve the recall of the RAG
for the target contexts, we introduce a knowledge graph to construct structured
data to achieve hard matching at a fine-grained level. Moreover, we normalize
the backdoor scenarios in LLMs to analyze the real harm caused by backdoors
from both attackers' and users' perspectives and further verify whether the
context is a favorable tool for jailbreaking models. Extensive experimental
results on truthfulness, language understanding, and harmfulness show that
TrojanRAG exhibits versatility threats while maintaining retrieval capabilities
on normal queries.",Pengzhou Cheng
2024-05-23T15:37:06Z,http://arxiv.org/abs/2405.14702v2,"G3: An Effective and Adaptive Framework for Worldwide Geolocalization
  Using Large Multi-Modality Models","Worldwide geolocalization aims to locate the precise location at the
coordinate level of photos taken anywhere on the Earth. It is very challenging
due to 1) the difficulty of capturing subtle location-aware visual semantics,
and 2) the heterogeneous geographical distribution of image data. As a result,
existing studies have clear limitations when scaled to a worldwide context.
They may easily confuse distant images with similar visual contents, or cannot
adapt to various locations worldwide with different amounts of relevant data.
To resolve these limitations, we propose G3, a novel framework based on
Retrieval-Augmented Generation (RAG). In particular, G3 consists of three
steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to
optimize both retrieval and generation phases of worldwide geolocalization.
During Geo-alignment, our solution jointly learns expressive multi-modal
representations for images, GPS and textual descriptions, which allows us to
capture location-aware semantics for retrieving nearby images for a given
query. During Geo-diversification, we leverage a prompt ensembling method that
is robust to inconsistent retrieval performance for different image queries.
Finally, we combine both retrieved and generated GPS candidates in
Geo-verification for location prediction. Experiments on two well-established
datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other
state-of-the-art methods. Our code and data are available online for
reproduction.",Pengyue Jia
2024-05-27T12:48:30Z,http://arxiv.org/abs/2405.17130v1,"Exploiting the Layered Intrinsic Dimensionality of Deep Models for
  Practical Adversarial Training","Despite being a heavily researched topic, Adversarial Training (AT) is
rarely, if ever, deployed in practical AI systems for two primary reasons: (i)
the gained robustness is frequently accompanied by a drop in generalization and
(ii) generating adversarial examples (AEs) is computationally prohibitively
expensive. To address these limitations, we propose SMAAT, a new AT algorithm
that leverages the manifold conjecture, stating that off-manifold AEs lead to
better robustness while on-manifold AEs result in better generalization.
Specifically, SMAAT aims at generating a higher proportion of off-manifold AEs
by perturbing the intermediate deepnet layer with the lowest intrinsic
dimension. This systematically results in better scalability compared to
classical AT as it reduces the PGD chains length required for generating the
AEs. Additionally, our study provides, to the best of our knowledge, the first
explanation for the difference in the generalization and robustness trends
between vision and language models, ie., AT results in a drop in generalization
in vision models whereas, in encoder-based language models, generalization
either improves or remains unchanged. We show that vision transformers and
decoder-based models tend to have low intrinsic dimensionality in the earlier
layers of the network (more off-manifold AEs), while encoder-based models have
low intrinsic dimensionality in the later layers. We demonstrate the efficacy
of SMAAT; on several tasks, including robustifying (i) sentiment classifiers,
(ii) safety filters in decoder-based models, and (iii) retrievers in RAG
setups. SMAAT requires only 25-33% of the GPU time compared to standard AT,
while significantly improving robustness across all applications and
maintaining comparable generalization.",Enes Altinisik
2024-05-28T16:56:42Z,http://arxiv.org/abs/2405.18359v1,"Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual
  Performance in LLMs","Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs without
extensive training or fine-tuning. Through systematic investigation and
evaluation of diverse languages using popular question-answering (QA) datasets,
we present novel techniques that unlock the true potential of LLMs in a
polyglot landscape. Our approach encompasses three key strategies that yield
significant improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval
Augmented Generation (RAG) with multilingual embeddings and achieves improved
multilingual task performance. Finally, we introduce a novel learning approach
that dynamically selects the optimal prompt strategy, LLM model, and embedding
model per query at run-time. This dynamic adaptation maximizes the efficacy of
LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online
settings, and can seamlessly adapt to new languages and datasets, leading to
substantial advancements in multilingual understanding and generation across
diverse languages.",Somnath Kumar
2024-05-30T09:50:38Z,http://arxiv.org/abs/2405.19893v1,"Similarity is Not All You Need: Endowing Retrieval Augmented Generation
  with Multi Layered Thoughts","In recent years, large language models (LLMs) have made remarkable
achievements in various domains. However, the untimeliness and cost of
knowledge updates coupled with hallucination issues of LLMs have curtailed
their applications in knowledge intensive tasks, where retrieval augmented
generation (RAG) can be of help. Nevertheless, existing retrieval augmented
models typically use similarity as a bridge between queries and documents and
follow a retrieve then read procedure. In this work, we argue that similarity
is not always the panacea and totally relying on similarity would sometimes
degrade the performance of retrieval augmented generation. To this end, we
propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented
Generation framework. To begin with, beyond existing similarity oriented
thought, we embrace a small scale utility model that draws supervision from an
LLM for utility oriented thought and further come up with a smarter model by
comprehensively combining the similarity and utility oriented thoughts.
Furthermore, given the fact that the retrieved document set tends to be huge
and using them in isolation makes it difficult to capture the commonalities and
characteristics among them, we propose to make an LLM as a task adaptive
summarizer to endow retrieval augmented generation with compactness-oriented
thought. Finally, with multi layered thoughts from the precedent stages, an LLM
is called for knowledge augmented generation. Extensive experiments on
knowledge-intensive tasks have demonstrated the superiority of MetRag.",Chunjing Gan
2024-05-30T18:00:21Z,http://arxiv.org/abs/2405.20389v1,"Designing an Evaluation Framework for Large Language Models in Astronomy
  Research","Large Language Models (LLMs) are shifting how scientific research is done. It
is imperative to understand how researchers interact with these models and how
scientific sub-communities like astronomy might benefit from them. However,
there is currently no standard for evaluating the use of LLMs in astronomy.
Therefore, we present the experimental design for an evaluation study on how
astronomy researchers interact with LLMs. We deploy a Slack chatbot that can
answer queries from users via Retrieval-Augmented Generation (RAG); these
responses are grounded in astronomy papers from arXiv. We record and anonymize
user questions and chatbot answers, user upvotes and downvotes to LLM
responses, user feedback to the LLM, and retrieved documents and similarity
scores with the query. Our data collection method will enable future dynamic
evaluations of LLM tools for astronomy.",John F. Wu
2024-05-30T20:05:44Z,http://arxiv.org/abs/2405.20455v5,"DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency
  Management","In the era of Large Language Models (LLMs) with their advanced capabilities,
a unique opportunity arises to develop LLM-based digital assistant tools that
can support software developers by facilitating comprehensive reasoning about
software dependencies and open-source libraries before importing them. This
reasoning process is daunting, mandating multiple specialized tools and
dedicated expertise, each focusing on distinct aspects (e.g., security analysis
tools may overlook design flaws such as circular dependencies, which hinder
software maintainability). Creating a significant bottleneck in the software
development lifecycle. In this paper, we introduce DepsRAG, a multi-agent
framework designed to assist developers in reasoning about software
dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG)
that includes both direct and transitive dependencies. Developers can interact
with DepsRAG through a conversational interface, posing queries about the
dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance
these queries by retrieving relevant information from the KG as well as
external sources, such as the Web and vulnerability databases, thus
demonstrating its adaptability to novel scenarios. DepsRAG incorporates a
Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated
responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three
multi-step reasoning tasks, observing a threefold increase in accuracy with the
integration of the Critic-Agent mechanism. DepsRAG demo and implementation are
available: https://github.com/Mohannadcse/DepsRAG.",Mohannad Alhanahnah
2024-06-03T15:26:06Z,http://arxiv.org/abs/2406.01428v2,"Superhuman performance in urology board questions by an explainable
  large language model enabled for context integration of the European
  Association of Urology guidelines: the UroBot study","Large Language Models (LLMs) are revolutionizing medical Question-Answering
(medQA) through extensive use of medical literature. However, their performance
is often hampered by outdated training data and a lack of explainability, which
limits clinical applicability. This study aimed to create and assess UroBot, a
urology-specialized chatbot, by comparing it with state-of-the-art models and
the performance of urologists on urological board questions, ensuring full
clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,
and GPT-4o models, employing retrieval-augmented generation (RAG) and the
latest 2023 guidelines from the European Association of Urology (EAU). The
evaluation included ten runs of 200 European Board of Urology (EBU) In-Service
Assessment (ISA) questions, with performance assessed by the mean Rate of
Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing
GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and
exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).
By comparison, the average performance of urologists on board questions, as
reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and
superior accuracy compared to both existing models and urologists on board
questions highlight its potential for clinical integration. The study also
provides the necessary code and instructions for further development of UroBot.",Martin J. Hetz
2024-06-04T08:36:39Z,http://arxiv.org/abs/2406.02110v1,"UniOQA: A Unified Framework for Knowledge Graph Question Answering with
  Large Language Models","OwnThink stands as the most extensive Chinese open-domain knowledge graph
introduced in recent times. Despite prior attempts in question answering over
OwnThink (OQA), existing studies have faced limitations in model representation
capabilities, posing challenges in further enhancing overall accuracy in
question answering. In this paper, we introduce UniOQA, a unified framework
that integrates two complementary parallel workflows. Unlike conventional
approaches, UniOQA harnesses large language models (LLMs) for precise question
answering and incorporates a direct-answer-prediction process as a
cost-effective complement. Initially, to bolster representation capacity, we
fine-tune an LLM to translate questions into the Cypher query language (CQL),
tackling issues associated with restricted semantic understanding and
hallucinations. Subsequently, we introduce the Entity and Relation Replacement
algorithm to ensure the executability of the generated CQL. Concurrently, to
augment overall accuracy in question answering, we further adapt the
Retrieval-Augmented Generation (RAG) process to the knowledge graph.
Ultimately, we optimize answer accuracy through a dynamic decision algorithm.
Experimental findings illustrate that UniOQA notably advances SpCQL Logical
Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new
state-of-the-art results on this benchmark. Through ablation experiments, we
delve into the superior representation capacity of UniOQA and quantify its
performance breakthrough.",Zhuoyang Li
2024-06-04T20:02:52Z,http://arxiv.org/abs/2406.02746v4,RATT: A Thought Structure for Coherent and Correct LLM Reasoning,"Large Language Models (LLMs) gain substantial reasoning and decision-making
capabilities from thought structures. However, existing methods such as Tree of
Thought and Retrieval Augmented Thoughts often fall short in complex tasks due
to the limitations of insufficient local retrieval of factual knowledge and
inadequate global selection of strategies. These limitations make it
challenging for these methods to balance factual accuracy and comprehensive
logical optimization effectively. To address these limitations, we introduce
the Retrieval Augmented Thought Tree (RATT), a novel thought structure that
considers both overall logical soundness and factual correctness at each step
of the thinking process. Specifically, at every point of a thought branch, RATT
performs planning and lookahead to explore and evaluate multiple potential
reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented
Generation (RAG) with LLM's ability to assess overall strategy. Through this
combination of factual knowledge and strategic feasibility, the RATT adjusts
and integrates the thought tree structure to search for the most promising
branches within the search space. This thought structure significantly enhances
the model's coherence in logical inference and efficiency in decision-making,
and thus increases the limit of the capacity of LLM to generate reliable
inferences and decisions based on thought structures. A broad range of
experiments on different types of tasks showcases that the RATT structure
significantly outperforms existing methods in factual correctness and logical
coherence.",Jinghan Zhang
2024-06-06T06:41:53Z,http://arxiv.org/abs/2406.03777v3,"Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge
  Devices","The scaling laws have become the de facto guidelines for designing large
language models (LLMs), but they were studied under the assumption of unlimited
computing resources for both training and inference. As LLMs are increasingly
used as personalized intelligent assistants, their customization (i.e.,
learning through fine-tuning) and deployment onto resource-constrained edge
devices will become more and more prevalent. An urging but open question is how
a resource-constrained computing environment would affect the design choices
for a personalized LLM. We study this problem empirically in this work. In
particular, we consider the tradeoffs among a number of key design factors and
their intertwined impacts on learning efficiency and accuracy. The factors
include the learning methods for LLM customization, the amount of personalized
data used for learning customization, the types and sizes of LLMs, the
compression methods of LLMs, the amount of time afforded to learn, and the
difficulty levels of the target use cases. Through extensive experimentation
and benchmarking, we draw a number of surprisingly insightful guidelines for
deploying LLMs onto resource-constrained devices. For example, an optimal
choice between parameter learning and RAG may vary depending on the difficulty
of the downstream task, the longer fine-tuning time does not necessarily help
the model, and a compressed LLM may be a better choice than an uncompressed LLM
to learn from limited personalized data.",Ruiyang Qin
2024-06-06T11:14:27Z,http://arxiv.org/abs/2406.03963v1,"A + B: A General Generator-Reader Framework for Optimizing LLMs to
  Unleash Synergy Potential","Retrieval-Augmented Generation (RAG) is an effective solution to supplement
necessary knowledge to large language models (LLMs). Targeting its bottleneck
of retriever performance, ""generate-then-read"" pipeline is proposed to replace
the retrieval stage with generation from the LLM itself. Although promising,
this research direction is underexplored and still cannot work in the scenario
when source knowledge is given. In this paper, we formalize a general ""A + B""
framework with varying combinations of foundation models and types for
systematic investigation. We explore the efficacy of the base and chat versions
of LLMs and found their different functionalities suitable for generator A and
reader B, respectively. Their combinations consistently outperform single
models, especially in complex scenarios. Furthermore, we extend the application
of the ""A + B"" framework to scenarios involving source documents through
continuous learning, enabling the direct integration of external knowledge into
LLMs. This approach not only facilitates effective acquisition of new knowledge
but also addresses the challenges of safety and helpfulness post-adaptation.
The paper underscores the versatility of the ""A + B"" framework, demonstrating
its potential to enhance the practical application of LLMs across various
domains.",Wei Tang
2024-06-08T16:24:24Z,http://arxiv.org/abs/2406.05514v3,RAG-Enhanced Commit Message Generation,"Commit message is one of the most important textual information in software
development and maintenance. However, it is time-consuming to write commit
messages manually. Commit Message Generation (CMG) has become a research
hotspot. Recently, several pre-trained language models (PLMs) and large
language models (LLMs) with code capabilities have been introduced,
demonstrating impressive performance on code-related tasks. Meanwhile, prior
studies have explored the utilization of retrieval techniques for CMG, but it
is still unclear what effects would emerge from combining advanced retrieval
techniques with various generation models. This paper proposed REACT, a
REtrieval-Augmented framework for CommiT message generation. It integrates
advanced retrieval techniques with different PLMs and LLMs, to enhance the
performance of these models on the CMG task. Specifically, a hybrid retriever
is designed and used to retrieve the most relevant code diff and commit message
pair as an exemplar. Then, the retrieved pair is utilized to guide and enhance
the CMG task by PLMs and LLMs through fine-tuning and in-context learning. The
experimental results show that REACT significantly enhances these models'
performance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,
boosting Llama 3's BLEU score by 102%, and substantially surpassing all
baselines.",Linghao Zhang
2024-06-10T17:58:29Z,http://arxiv.org/abs/2406.06519v1,"UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance
  Assessor","Copious amounts of relevance judgments are necessary for the effective
training and accurate evaluation of retrieval systems. Conventionally, these
judgments are made by human assessors, rendering this process expensive and
laborious. A recent study by Thomas et al. from Microsoft Bing suggested that
large language models (LLMs) can accurately perform the relevance assessment
task and provide human-quality judgments, but unfortunately their study did not
yield any reusable software artifacts. Our work presents UMBRELA (a recursive
acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source
toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o
model and adds more nuance to the original paper. Across Deep Learning Tracks
from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate
highly with rankings generated by effective multi-stage retrieval systems. Our
toolkit is designed to be easily extensible and can be integrated into existing
multi-stage retrieval and evaluation pipelines, offering researchers a valuable
resource for studying retrieval evaluation methodologies. UMBRELA will be used
in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our
toolkit becoming a foundation for further innovation in the field. UMBRELA is
available at https://github.com/castorini/umbrela.",Shivani Upadhyay
2024-06-04T08:34:19Z,http://arxiv.org/abs/2406.06577v1,"RAG-based Crowdsourcing Task Decomposition via Masked Contrastive
  Learning with Prompts","Crowdsourcing is a critical technology in social manufacturing, which
leverages an extensive and boundless reservoir of human resources to handle a
wide array of complex tasks. The successful execution of these complex tasks
relies on task decomposition (TD) and allocation, with the former being a
prerequisite for the latter. Recently, pre-trained language models (PLMs)-based
methods have garnered significant attention. However, they are constrained to
handling straightforward common-sense tasks due to their inherent restrictions
involving limited and difficult-to-update knowledge as well as the presence of
hallucinations. To address these issues, we propose a retrieval-augmented
generation-based crowdsourcing framework that reimagines TD as event detection
from the perspective of natural language understanding. However, the existing
detection methods fail to distinguish differences between event types and
always depend on heuristic rules and external semantic analyzing tools.
Therefore, we present a Prompt-Based Contrastive learning framework for TD
(PBCT), which incorporates a prompt-based trigger detector to overcome
dependence. Additionally, trigger-attentive sentinel and masked contrastive
learning are introduced to provide varying attention to trigger and contextual
features according to different event types. Experiment results demonstrate the
competitiveness of our method in both supervised and zero-shot detection. A
case study on printed circuit board manufacturing is showcased to validate its
adaptability to unknown professional domains.",Jing Yang
2024-05-09T18:15:12Z,http://arxiv.org/abs/2406.07561v1,"Artificial Intelligence as the New Hacker: Developing Agents for
  Offensive Security","In the vast domain of cybersecurity, the transition from reactive defense to
offensive has become critical in protecting digital infrastructures. This paper
explores the integration of Artificial Intelligence (AI) into offensive
cybersecurity, particularly through the development of an autonomous AI agent,
ReaperAI, designed to simulate and execute cyberattacks. Leveraging the
capabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI
demonstrates the potential to identify, exploit, and analyze security
vulnerabilities autonomously.
  This research outlines the core methodologies that can be utilized to
increase consistency and performance, including task-driven penetration testing
frameworks, AI-driven command generation, and advanced prompting techniques.
The AI agent operates within a structured environment using Python, enhanced by
Retrieval Augmented Generation (RAG) for contextual understanding and memory
retention. ReaperAI was tested on platforms including, Hack The Box, where it
successfully exploited known vulnerabilities, demonstrating its potential
power.
  However, the deployment of AI in offensive security presents significant
ethical and operational challenges. The agent's development process revealed
complexities in command execution, error handling, and maintaining ethical
constraints, highlighting areas for future enhancement.
  This study contributes to the discussion on AI's role in cybersecurity by
showcasing how AI can augment offensive security strategies. It also proposes
future research directions, including the refinement of AI interactions with
cybersecurity tools, enhancement of learning mechanisms, and the discussion of
ethical guidelines for AI in offensive roles. The findings advocate for a
unique approach to AI implementation in cybersecurity, emphasizing innovation.",Leroy Jacob Valencia
2024-06-15T17:07:31Z,http://arxiv.org/abs/2406.10690v3,"Automating Pharmacovigilance Evidence Generation: Using Large Language
  Models to Produce Context-Aware SQL","Objective: To enhance the efficiency and accuracy of information retrieval
from pharmacovigilance (PV) databases by employing Large Language Models (LLMs)
to convert natural language queries (NLQs) into Structured Query Language (SQL)
queries, leveraging a business context document.
  Materials and Methods: We utilized OpenAI's GPT-4 model within a
retrieval-augmented generation (RAG) framework, enriched with a business
context document, to transform NLQs into syntactically precise SQL queries.
Each NLQ was presented to the LLM randomly and independently to prevent
memorization. The study was conducted in three phases, varying query
complexity, and assessing the LLM's performance both with and without the
business context document.
  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing
from 8.3\% with the database schema alone to 78.3\% with the business context
document. This enhancement was consistent across low, medium, and high
complexity queries, indicating the critical role of contextual knowledge in
query generation.
  Discussion: The integration of a business context document markedly improved
the LLM's ability to generate accurate and contextually relevant SQL queries.
Performance achieved a maximum of 85\% when high complexity queries are
excluded, suggesting promise for routine deployment.
  Conclusion: This study presents a novel approach to employing LLMs for safety
data retrieval and analysis, demonstrating significant advancements in query
generation accuracy. The methodology offers a framework applicable to various
data-intensive domains, enhancing the accessibility and efficiency of
information retrieval for non-technical users.",Jeffery L. Painter
2024-06-16T22:04:10Z,http://arxiv.org/abs/2406.12934v1,Current state of LLM Risks and AI Guardrails,"Large language models (LLMs) have become increasingly sophisticated, leading
to widespread deployment in sensitive applications where safety and reliability
are paramount. However, LLMs have inherent risks accompanying them, including
bias, potential for unsafe actions, dataset poisoning, lack of explainability,
hallucinations, and non-reproducibility. These risks necessitate the
development of ""guardrails"" to align LLMs with desired behaviors and mitigate
potential harm.
  This work explores the risks associated with deploying LLMs and evaluates
current approaches to implementing guardrails and model alignment techniques.
We examine intrinsic and extrinsic bias evaluation methods and discuss the
importance of fairness metrics for responsible AI development. The safety and
reliability of agentic LLMs (those capable of real-world actions) are explored,
emphasizing the need for testability, fail-safes, and situational awareness.
  Technical strategies for securing LLMs are presented, including a layered
protection model operating at external, secondary, and internal levels. System
prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to
minimize bias and protect privacy are highlighted.
  Effective guardrail design requires a deep understanding of the LLM's
intended use case, relevant regulations, and ethical considerations. Striking a
balance between competing requirements, such as accuracy and privacy, remains
an ongoing challenge. This work underscores the importance of continuous
research and development to ensure the safe and responsible use of LLMs in
real-world applications.",Suriya Ganesh Ayyamperumal
2024-06-21T01:52:37Z,http://arxiv.org/abs/2406.14825v4,"TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction
  in RAG-based Crowdsourcing Systems","Temporal relation extraction (TRE) aims to grasp the evolution of events or
actions, and thus shape the workflow of associated tasks, so it holds promise
in helping understand task requests initiated by requesters in crowdsourcing
systems. However, existing methods still struggle with limited and unevenly
distributed annotated data. Therefore, inspired by the abundant global
knowledge stored within pre-trained language models (PLMs), we propose a
multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt
tuning and contrastive learning to tackle these issues. To elicit more
effective prompts for PLMs, we introduce a task-oriented prompt construction
approach that thoroughly takes the myriad factors of TRE into consideration for
automatic prompt generation. In addition, we design temporal event reasoning in
the form of masked language modeling as auxiliary tasks to bolster the model's
focus on events and temporal cues. The experimental results demonstrate that
TemPrompt outperforms all compared baselines across the majority of metrics
under both standard and few-shot settings. A case study on designing and
manufacturing printed circuit boards is provided to validate its effectiveness
in crowdsourcing scenarios.",Jing Yang
2024-06-21T08:45:52Z,http://arxiv.org/abs/2406.14979v2,"Retrieve-Plan-Generation: An Iterative Planning and Answering Framework
  for Knowledge-Intensive LLM Generation","Despite the significant progress of large language models (LLMs) in various
tasks, they often produce factual errors due to their limited internal
knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with
external knowledge sources, offers a promising solution. However, these methods
can be misled by irrelevant paragraphs in retrieved documents. Due to the
inherent uncertainty in LLM generation, inputting the entire document may
introduce off-topic information, causing the model to deviate from the central
topic and affecting the relevance of the generated content. To address these
issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates
plan tokens to guide subsequent generation in the plan stage. In the answer
stage, the model selects relevant fine-grained paragraphs based on the plan and
uses them for further answer generation. This plan-answer process is repeated
iteratively until completion, enhancing generation relevance by focusing on
specific topics. To implement this framework efficiently, we utilize a simple
but effective multi-task prompt-tuning method, enabling the existing LLMs to
handle both planning and answering. We comprehensively compare RPG with
baselines across 5 knowledge-intensive generation tasks, demonstrating the
effectiveness of our approach.",Yuanjie Lyu
2024-06-23T04:35:42Z,http://arxiv.org/abs/2406.16008v2,"Found in the Middle: Calibrating Positional Attention Bias Improves Long
  Context Utilization","Large language models (LLMs), even when specifically trained to process long
input contexts, struggle to capture relevant information located in the middle
of their input. This phenomenon has been known as the lost-in-the-middle
problem. In this work, we make three contributions. First, we set out to
understand the factors that cause this phenomenon. In doing so, we establish a
connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs
exhibit a U-shaped attention bias where the tokens at the beginning and at the
end of its input receive higher attention, regardless of their relevance.
Second, we mitigate this positional bias through a calibration mechanism,
found-in-the-middle, that allows the model to attend to contexts faithfully
according to their relevance, even though when they are in the middle. Third,
we show found-in-the-middle not only achieves better performance in locating
relevant information within a long context, but also eventually leads to
improved retrieval-augmented generation (RAG) performance across various tasks,
outperforming existing methods by up to 15 percentage points. These findings
open up future directions in understanding LLM attention bias and its potential
consequences.",Cheng-Yu Hsieh
2024-06-24T01:22:54Z,http://arxiv.org/abs/2406.16252v2,"Graph-Augmented LLMs for Personalized Health Insights: A Case Study in
  Sleep Analysis","Health monitoring systems have revolutionized modern healthcare by enabling
the continuous capture of physiological and behavioral data, essential for
preventive measures and early health intervention. While integrating this data
with Large Language Models (LLMs) has shown promise in delivering interactive
health advice, traditional methods like Retrieval-Augmented Generation (RAG)
and fine-tuning often fail to fully utilize the complex, multi-dimensional, and
temporally relevant data from wearable devices. These conventional approaches
typically provide limited actionable and personalized health insights due to
their inadequate capacity to dynamically integrate and interpret diverse health
data streams. In response, this paper introduces a graph-augmented LLM
framework designed to significantly enhance the personalization and clarity of
health insights. Utilizing a hierarchical graph structure, the framework
captures inter and intra-patient relationships, enriching LLM prompts with
dynamic feature importance scores derived from a Random Forest Model. The
effectiveness of this approach is demonstrated through a sleep analysis case
study involving 20 college students during the COVID-19 lockdown, highlighting
the potential of our model to generate actionable and personalized health
insights efficiently. We leverage another LLM to evaluate the insights for
relevance, comprehensiveness, actionability, and personalization, addressing
the critical need for models that process and interpret complex health data
effectively. Our findings show that augmenting prompts with our framework
yields significant improvements in all 4 criteria. Through our framework, we
can elicit well-crafted, more thoughtful responses tailored to a specific
patient.",Ajan Subramanian
2024-06-24T23:57:57Z,http://arxiv.org/abs/2406.17186v2,"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented
  Analysis Generation","Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.",Abe Bohan Hou
2024-06-25T09:42:56Z,http://arxiv.org/abs/2406.17419v2,"Leave No Document Behind: Benchmarking Long-Context LLMs with Extended
  Multi-Doc QA","Long-context modeling capabilities have garnered widespread attention,
leading to the emergence of Large Language Models (LLMs) with ultra-context
windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually
catching up. However, existing benchmarks employ irrelevant noise texts to
artificially extend the length of test cases, diverging from the real-world
scenarios of long-context applications. To bridge this gap, we propose a novel
long-context benchmark, Loong, aligning with realistic scenarios through
extended multi-document question answering (QA). Unlike typical document QA, in
Loong's test cases, each document is relevant to the final answer, ignoring any
document will lead to the failure of the answer. Furthermore, Loong introduces
four types of tasks with a range of context lengths: Spotlight Locating,
Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic
and comprehensive evaluation of long-context understanding. Extensive
experiments indicate that existing long-context language models still exhibit
considerable potential for enhancement. Retrieval augmented generation (RAG)
achieves poor performance, demonstrating that Loong can reliably assess the
model's long-context modeling capabilities.",Minzheng Wang
2024-06-28T01:14:43Z,http://arxiv.org/abs/2406.19593v1,"SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs","Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.",Xin Su
2024-06-30T15:38:48Z,http://arxiv.org/abs/2407.00731v2,"Large Language Models Struggle in Token-Level Clinical Named Entity
  Recognition","Large Language Models (LLMs) have revolutionized various sectors, including
healthcare where they are employed in diverse applications. Their utility is
particularly significant in the context of rare diseases, where data scarcity,
complexity, and specificity pose considerable challenges. In the clinical
domain, Named Entity Recognition (NER) stands out as an essential task and it
plays a crucial role in extracting relevant information from clinical texts.
Despite the promise of LLMs, current research mostly concentrates on
document-level NER, identifying entities in a more general context across
entire documents, without extracting their precise location. Additionally,
efforts have been directed towards adapting ChatGPT for token-level NER.
However, there is a significant research gap when it comes to employing
token-level NER for clinical texts, especially with the use of local
open-source LLMs. This study aims to bridge this gap by investigating the
effectiveness of both proprietary and local LLMs in token-level clinical NER.
Essentially, we delve into the capabilities of these models through a series of
experiments involving zero-shot prompting, few-shot prompting,
retrieval-augmented generation (RAG), and instruction-fine-tuning. Our
exploration reveals the inherent challenges LLMs face in token-level NER,
particularly in the context of rare diseases, and suggests possible
improvements for their application in healthcare. This research contributes to
narrowing a significant gap in healthcare informatics and offers insights that
could lead to a more refined application of LLMs in the healthcare sector.",Qiuhao Lu
2024-07-01T09:19:50Z,http://arxiv.org/abs/2407.01110v1,"SecGenAI: Enhancing Security of Cloud-based Generative AI Applications
  within Australian Critical Technologies of National Interest","The rapid advancement of Generative AI (GenAI) technologies offers
transformative opportunities within Australia's critical technologies of
national interest while introducing unique security challenges. This paper
presents SecGenAI, a comprehensive security framework for cloud-based GenAI
applications, with a focus on Retrieval-Augmented Generation (RAG) systems.
SecGenAI addresses functional, infrastructure, and governance requirements,
integrating end-to-end security analysis to generate specifications emphasizing
data privacy, secure deployment, and shared responsibility models. Aligned with
Australian Privacy Principles, AI Ethics Principles, and guidelines from the
Australian Cyber Security Centre and Digital Transformation Agency, SecGenAI
mitigates threats such as data leakage, adversarial attacks, and model
inversion. The framework's novel approach combines advanced machine learning
techniques with robust security measures, ensuring compliance with Australian
regulations while enhancing the reliability and trustworthiness of GenAI
systems. This research contributes to the field of intelligent systems by
providing actionable strategies for secure GenAI implementation in industry,
fostering innovation in AI applications, and safeguarding national interests.",Christoforus Yoga Haryanto
2024-07-02T07:52:30Z,http://arxiv.org/abs/2407.02028v1,"Why does in-context learning fail sometimes? Evaluating in-context
  learning on open and closed questions","We measure the performance of in-context learning as a function of task
novelty and difficulty for open and closed questions. For that purpose, we
created a novel benchmark consisting of hard scientific questions, each paired
with a context of various relevancy. We show that counter-intuitively, a
context that is more aligned with the topic does not always help more than a
less relevant context. This effect is especially visible for open questions and
questions of high difficulty or novelty. This result reveals a fundamental
difference between the treatment of close-form and open-form questions by
large-language models and shows a need for a more robust evaluation of
in-context learning on the variety of different types of questions. It also
poses a new question of how to optimally select a context for large language
models, especially in the context of Retrieval Augmented Generation (RAG)
systems. Our results suggest that the answer to this question can be highly
application-dependent and might be contingent on factors including the format
of the question, the perceived difficulty level of the questions, and the
novelty or popularity of the information we seek.",Xiang Li
2024-07-04T12:29:06Z,http://arxiv.org/abs/2407.03889v1,"Automated C/C++ Program Repair for High-Level Synthesis via Large
  Language Models","In High-Level Synthesis (HLS), converting a regular C/C++ program into its
HLS-compatible counterpart (HLS-C) still requires tremendous manual effort.
Various program scripts have been introduced to automate this process. But the
resulting codes usually contain many issues that should be manually repaired by
developers. Since Large Language Models (LLMs) have the ability to automate
code generation, they can also be used for automated program repair in HLS.
However, due to the limited training of LLMs considering hardware and software
simultaneously, hallucinations may occur during program repair using LLMs,
leading to compilation failures. Besides, using LLMs for iterative repair also
incurs a high cost. To address these challenges, we propose an LLM-driven
program repair framework that takes regular C/C++ code as input and
automatically generates its corresponding HLS-C code for synthesis while
minimizing human repair effort. To mitigate the hallucinations in LLMs and
enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is
introduced to guide the LLMs toward correct repair. In addition, we use LLMs to
create a static bit width optimization program to identify the optimized bit
widths for variables. Moreover, LLM-driven HLS optimization strategies are
introduced to add/tune pragmas in HLS-C programs for circuit optimization.
Experimental results demonstrate that the proposed LLM-driven automated
framework can achieve much higher repair pass rates in 24 real-world
applications compared with the traditional scripts and the direct application
of LLMs for program repair.",Kangwei Xu
2024-07-05T17:43:30Z,http://arxiv.org/abs/2407.04681v1,"Rethinking Visual Prompting for Multimodal Large Language Models with
  External Knowledge","In recent years, multimodal large language models (MLLMs) have made
significant strides by training on vast high-quality image-text datasets,
enabling them to generally understand images well. However, the inherent
difficulty in explicitly conveying fine-grained or spatially dense information
in text, such as masks, poses a challenge for MLLMs, limiting their ability to
answer questions requiring an understanding of detailed or localized visual
elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)
concept, this paper proposes a new visual prompt approach to integrate
fine-grained external knowledge, gleaned from specialized vision models (e.g.,
instance segmentation/OCR models), into MLLMs. This is a promising yet
underexplored direction for enhancing MLLMs' performance. Our approach diverges
from concurrent works, which transform external knowledge into additional text
prompts, necessitating the model to indirectly learn the correspondence between
visual content and text coordinates. Instead, we propose embedding fine-grained
knowledge information directly into a spatial embedding map as a visual prompt.
This design can be effortlessly incorporated into various MLLMs, such as LLaVA
and Mipha, considerably improving their visual understanding performance.
Through rigorous experiments, we demonstrate that our method can enhance MLLM
performance across nine benchmarks, amplifying their fine-grained context-aware
capabilities.",Yuanze Lin
2024-07-06T09:10:05Z,http://arxiv.org/abs/2407.05015v1,"How do you know that? Teaching Generative Language Models to Reference
  Answers to Biomedical Questions","Large language models (LLMs) have recently become the leading source of
answers for users' questions online. Despite their ability to offer eloquent
answers, their accuracy and reliability can pose a significant challenge. This
is especially true for sensitive domains such as biomedicine, where there is a
higher need for factually correct answers. This paper introduces a biomedical
retrieval-augmented generation (RAG) system designed to enhance the reliability
of generated responses. The system is based on a fine-tuned LLM for the
referenced question-answering, where retrieved relevant abstracts from PubMed
are passed to LLM's context as input through a prompt. Its output is an answer
based on PubMed abstracts, where each statement is referenced accordingly,
allowing the users to verify the answer. Our retrieval system achieves an
absolute improvement of 23% compared to the PubMed search engine. Based on the
manual evaluation on a small sample, our fine-tuned LLM component achieves
comparable results to GPT-4 Turbo in referencing relevant abstracts. We make
the dataset used to fine-tune the models and the fine-tuned models based on
Mistral-7B-instruct-v0.1 and v0.2 publicly available.",Bojana BaÅ¡aragin
2024-07-09T15:59:28Z,http://arxiv.org/abs/2407.06985v4,"PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and
  Tuning Methods","In domain-specific applications, GPT-4, augmented with precise prompts or
Retrieval-Augmented Generation (RAG), shows notable potential but faces the
critical tri-lemma of performance, cost, and data privacy. High performance
requires sophisticated processing techniques, yet managing multiple agents
within a complex workflow often proves costly and challenging. To address this,
we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.
This systematizes domain-specific tasks by integrating precise question
decomposition, advanced information retrieval, comprehensive summarization, and
rigorous self-assessment. Given the concerns of cost and data privacy,
enterprises are shifting from proprietary models like GPT-4 to custom models,
striking a balance between cost, security, and performance. We developed
industrial practices leveraging online data and user feedback for efficient
model tuning. This study provides best practice guidelines for applying
multi-agent systems in domain-specific problem-solving and implementing
effective agent tuning strategies. Our empirical studies, particularly in the
financial question-answering domain, demonstrate that our approach achieves
95.0% of GPT-4's performance, while effectively managing costs and ensuring
data privacy.",Yiying Wang
2024-07-12T17:34:03Z,http://arxiv.org/abs/2407.09450v2,Human-like Episodic Memory for Infinite Context LLMs,"Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs
with no fine-tuning, enabling them to handle practically infinite context
lengths while maintaining computational efficiency. EM-LLM organises sequences
of tokens into coherent episodic events using a combination of Bayesian
surprise and graph-theoretic boundary refinement in an online fashion. When
needed, these events are retrieved through a two-stage memory process,
combining similarity-based and temporally contiguous retrieval for efficient
and human-like access to relevant information. Experiments on the LongBench and
InfiniteBench benchmarks demonstrate EM-LLM's superior performance,
consistently outperforming the state-of-the-art retrieval model InfLLM across
various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,
RAG, in a wide range of tasks, while requiring similar resources. Notably,
EM-LLM's performance even surpasses full-context models in most tasks, while
successfully performing retrieval across 10 million tokens - a scale
computationally infeasible for such models. Finally, our analysis reveals
strong correlations between EM-LLM's event segmentation and human-perceived
events, suggesting a bridge between this artificial system and its biological
counterpart, thereby offering a novel computational framework for exploring
human memory mechanisms.",Zafeirios Fountas
2024-07-13T21:13:55Z,http://arxiv.org/abs/2407.10005v1,"Fine-grained Analysis of In-context Linear Estimation: Data,
  Architecture, and Beyond","Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through
gradient descent steps. However, the existing results on the optimization
landscape apply under stylized settings where task and feature vectors are
assumed to be IID and the attention weights are fully parameterized. In this
work, we develop a stronger characterization of the optimization and
generalization landscape of ICL through contributions on architectures,
low-rank parameterization, and correlated designs: (1) We study the landscape
of 1-layer linear attention and 1-layer H3, a state-space model. Under a
suitable correlated design assumption, we prove that both implement 1-step
preconditioned gradient descent. We show that thanks to its native convolution
filters, H3 also has the advantage of implementing sample weighting and
outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank
parameterized attention weights in terms of covariance spectrum. Through this,
we also shed light on how LoRA can adapt to a new distribution by capturing the
shift between task covariances. Experimental results corroborate our
theoretical findings. Overall, this work explores the optimization and risk
landscape of ICL in practically meaningful settings and contributes to a more
thorough understanding of its mechanics.",Yingcong Li
2024-07-18T17:59:30Z,http://arxiv.org/abs/2407.13766v2,Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark,"Large Multimodal Models (LMMs) have made significant strides in visual
question-answering for single images. Recent advancements like long-context
LMMs have allowed them to ingest larger, or even multiple, images. However, the
ability to process a large number of visual tokens does not guarantee effective
retrieval and reasoning for multi-image question answering (MIQA), especially
in real-world applications like photo album searches or satellite imagery
analysis. In this work, we first assess the limitations of current benchmarks
for long-context LMMs. We address these limitations by introducing a new
vision-centric, long-context benchmark, ""Visual Haystacks (VHs)"". We
comprehensively evaluate both open-source and proprietary models on VHs, and
demonstrate that these models struggle when reasoning across potentially
unrelated images, perform poorly on cross-image reasoning, as well as exhibit
biases based on the placement of key information within the context window.
Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented
Generation), an open-source, lightweight visual-RAG framework that processes up
to 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of
contemporary models. MIRAGE demonstrates up to 13% performance improvement over
existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA
multi-image QA benchmark, and achieves competitive performance on single-image
QA with state-of-the-art LMMs.",Tsung-Han Wu
2024-07-14T00:42:39Z,http://arxiv.org/abs/2407.15718v1,Integrating AI Tutors in a Programming Course,"RAGMan is an LLM-powered tutoring system that can support a variety of
course-specific and homework-specific AI tutors. RAGMan leverages Retrieval
Augmented Generation (RAG), as well as strict instructions, to ensure the
alignment of the AI tutors' responses. By using RAGMan's AI tutors, students
receive assistance with their specific homework assignments without directly
obtaining solutions, while also having the ability to ask general
programming-related questions.
  RAGMan was deployed as an optional resource in an introductory programming
course with an enrollment of 455 students. It was configured as a set of five
homework-specific AI tutors. This paper describes the interactions the students
had with the AI tutors, the students' feedback, and a comparative grade
analysis. Overall, about half of the students engaged with the AI tutors, and
the vast majority of the interactions were legitimate homework questions. When
students posed questions within the intended scope, the AI tutors delivered
accurate responses 98% of the time. Within the students used AI tutors, 78%
reported that the tutors helped their learning. Beyond AI tutors' ability to
provide valuable suggestions, students reported appreciating them for fostering
a safe learning environment free from judgment.",Iris Ma
2024-07-23T07:40:41Z,http://arxiv.org/abs/2407.16252v3,"LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese
  Legal Consultation","Legal Large Language Models (LLMs) have shown promise in providing legal
consultations to non-experts. However, most existing Chinese legal consultation
models are based on single-agent systems, which differ from real-world legal
consultations, where multiple professionals collaborate to offer more tailored
responses. To better simulate real consultations, we propose LawLuo, a
multi-agent framework for multi-turn Chinese legal consultations. LawLuo
includes four agents: the receptionist agent, which assesses user intent and
selects a lawyer agent; the lawyer agent, which interacts with the user; the
secretary agent, which organizes conversation records and generates
consultation reports; and the boss agent, which evaluates the performance of
the lawyer and secretary agents to ensure optimal results. These agents'
interactions mimic the operations of real law firms. To train them to follow
different legal instructions, we developed distinct fine-tuning datasets. We
also introduce a case graph-based RAG to help the lawyer agent address vague
user inputs. Experimental results show that LawLuo outperforms baselines in
generating more personalized and professional responses, handling ambiguous
queries, and following legal instructions in multi-turn conversations. Our full
code and constructed datasets will be open-sourced upon paper acceptance.",Jingyun Sun
2024-07-30T09:04:45Z,http://arxiv.org/abs/2407.20668v1,"Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion
  Prediction for Social Media Influencers","Predicting influencers' views and public sentiment on social media is crucial
for anticipating societal trends and guiding strategic responses. This study
introduces a novel computational framework to predict opinion leaders'
perspectives and the emotive reactions of the populace, addressing the inherent
challenges posed by the unstructured, context-sensitive, and heterogeneous
nature of online communication. Our research introduces an innovative module
that starts with the automatic 5W1H (Where, Who, When, What, Why, and How)
questions formulation engine, tailored to emerging news stories and trending
topics. We then build a total of 60 anonymous opinion leader agents in six
domains and realize the views generation based on an enhanced large language
model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we
synthesize the potential views of opinion leaders and predicted the emotional
responses to different events. The efficacy of our automated 5W1H module is
corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity.
The influencer agents exhibit a consistent performance, achieving an average
GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the
'Russia-Ukraine War' as a case study, our methodology accurately foresees key
influencers' perspectives and aligns emotional predictions with real-world
sentiment trends in various domains.",Qinglan Wei
2024-07-31T08:43:17Z,http://arxiv.org/abs/2407.21439v2,"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities. However, a significant drawback of MLLMs is their reliance on
static training data, leading to outdated information and limited contextual
awareness. This static nature hampers their ability to provide accurate and
up-to-date responses, particularly in dynamic or rapidly evolving contexts.
Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)
offers a promising solution, the system would inevitably encounter the
multi-granularity noisy correspondence (MNC) problem, which hinders accurate
retrieval and generation. In this work, we propose RagVL, a novel framework
with knowledge-enhanced reranking and noise-injected training, to address these
limitations. We instruction-tune the MLLM with a simple yet effective
instruction template to induce its ranking ability and serve it as a reranker
to precisely filter the top-k retrieved images. For generation, we inject
visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments on the subsets of two datasets
that require retrieving and reasoning over images to answer a given query
verify the effectiveness of our method. Code and models are available at
https://github.com/IDEA-FinAI/RagVL.",Zhanpeng Chen
2024-07-31T09:16:33Z,http://arxiv.org/abs/2407.21459v1,"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government
  Financial Data and Regulations to Enhance Decision Making","Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.",Gilang Fajar Febrian
2024-07-31T11:14:45Z,http://arxiv.org/abs/2408.00030v1,"A New Type of Foundation Model Based on Recordings of People's Emotions
  and Physiology","Foundation models have had a big impact in recent years and billions of
dollars are being invested in them in the current AI boom. The more popular
ones, such as Chat-GPT, are trained on large amounts of data from the Internet,
and then reinforcement learning, RAG, prompt engineering and cognitive
modelling are used to fine-tune and augment their behavior. This technology has
been used to create models of individual people, such as Caryn Marjorie.
However, these chatbots are not based on people's actual emotional and
physiological responses to their environment, so they are, at best,
surface-level approximations to the characters they are imitating. This paper
describes how a new type of foundation model - a first-person foundation model
- could be created from recordings of what a person sees and hears as well as
their emotional and physiological reactions to these stimuli. A first-person
foundation model would map environmental stimuli to a person's emotional and
physiological states, and map a person's emotional and physiological states to
their behavior. First-person foundation models have many exciting applications,
including a new type of recommendation engine, personal assistants, generative
adversarial networks, dating and recruitment. To obtain training data for a
first-person foundation model, we have developed a recording rig that captures
what the wearer is seeing and hearing as well as their emotional and
physiological states. This novel source of data could help to address the
shortage of new data for building the next generation of foundation models.",David Gamez
2024-08-06T16:55:54Z,http://arxiv.org/abs/2408.03297v2,"KnowPO: Knowledge-aware Preference Optimization for Controllable
  Knowledge Selection in Retrieval-Augmented Language Models","By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors such
as contextual ignorance and contextual overinclusion. To this end, we propose a
Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at
achieving adaptive knowledge selection based on contextual relevance in real
retrieval scenarios. Concretely, we proposed a general paradigm for
constructing knowledge conflict datasets, which comprehensively cover various
error types and learn how to avoid these negative signals through preference
optimization methods. Simultaneously, we proposed a rewriting strategy and data
ratio optimization strategy to address preference imbalances. Experimental
results show that KnowPO outperforms previous methods for handling knowledge
conflicts by over 37\%, while also exhibiting robust generalization across
various out-of-distribution datasets.",Ruizhe Zhang
2024-08-07T15:27:22Z,http://arxiv.org/abs/2408.03841v1,"MaxMind: A Memory Loop Network to Enhance Software Productivity based on
  Large Language Models","The application of large language models to facilitate automated software
operations and tool generation (SOTG), thus augmenting software productivity,
mirrors the early stages of human evolution when the ability to create and use
tools accelerated the progress of civilization. These complex tasks require AI
to continuously summarize and improve. Current research often overlooks the
importance of converting real-time task experiences into system memory and
differentiating the value of existing knowledge for future reference. This
paper addresses these issues by evolving external memory models into
Memory-Loop Networks for timely memorization and experience referencing. We
also enhance a RAG mechanism with knowledge precision segmentation to utilize
memory based on value differentiation, and design the MaxMind model for SOTG
accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an
electronic spreadsheet processing system aligned with the MaxMind philosophy.
Comparative experiments with SheetCopilot have demonstrated that the
accumulation and recycling of task memories lead to a steady enhancement in
task success rate, with an improvement rate of approximately 3%-6% per round in
this implementation example. Note that as the memories continue to grow, this
cumulative improvement may be substantial. The inclusion of memory recycling
can also boost the system's task execution efficiency by up to 25%, and it can
address the retraining issue faced by LLMs when handling specialized tasks
through memories transfer.These suggest that MaxMind has significant potential
to enhance the capabilities and productivity of LLM systems in SOTG.",Yuchen Dong
2024-08-06T02:13:15Z,http://arxiv.org/abs/2408.04662v2,Citekit: A Modular Toolkit for Large Language Model Citation Generation,"Enabling Large Language Models (LLMs) to generate citations in
Question-Answering (QA) tasks is an emerging paradigm aimed at enhancing the
verifiability of their responses when LLMs are utilizing external references to
generate an answer. However, there is currently no unified framework to
standardize and fairly compare different citation generation methods, leading
to difficulties in reproducing different methods and a comprehensive
assessment. To cope with the problems above, we introduce \name, an open-source
and modular toolkit designed to facilitate the implementation and evaluation of
existing citation generation methods, while also fostering the development of
new approaches to improve citation quality in LLM outputs. This tool is highly
extensible, allowing users to utilize 4 main modules and 14 components to
construct a pipeline, evaluating an existing method or innovative designs. Our
experiments with two state-of-the-art LLMs and 11 citation generation baselines
demonstrate varying strengths of different modules in answer accuracy and
citation quality improvement, as well as the challenge of enhancing
granularity. Based on our analysis of the effectiveness of components, we
propose a new method, self-RAG \snippet, obtaining a balanced answer accuracy
and citation quality. Citekit is released at
https://github.com/SjJ1017/Citekit.",Jiajun Shen
2024-08-12T03:52:11Z,http://arxiv.org/abs/2408.05911v1,"A New Pipeline For Generating Instruction Dataset via RAG and Self
  Fine-Tuning","With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.",Chih-Wei Song
2024-08-07T19:32:59Z,http://arxiv.org/abs/2408.11061v1,StructuredRAG: JSON Response Formatting with Large Language Models,"The ability of Large Language Models (LLMs) to generate structured outputs,
such as JSON, is crucial for their use in Compound AI Systems. However,
evaluating and improving this capability remains challenging. In this work, we
introduce StructuredRAG, a benchmark of six tasks designed to assess LLMs'
proficiency in following response format instructions. We evaluate two
state-of-the-art LLMs, Gemini 1.5 Pro and Llama 3 8B-instruct with 4-bit
quantization using two distinct prompting strategies. We introduce these
prompting strategies as f-String and Follow the Format (FF) prompting. Across
24 experiments, we find an average success rate of 82.55%. We further find a
high variance in performance across tasks, models, and prompting strategies
with success rates ranging from 0 to 100%. We find that Llama 3 8B-instruct
often performs competitively with Gemini 1.5 Pro. We observe that task
complexity significantly influences performance, with tasks involving lists or
composite object outputs proving more challenging. Our findings highlight the
need for further research into improving the reliability and consistency of
structured output generation in LLMs. We have open-sourced our experimental
code and results at github.com/weaviate/structured-rag.",Connor Shorten
2024-08-21T12:09:37Z,http://arxiv.org/abs/2408.11557v4,"A Quick, trustworthy spectral knowledge Q&A system leveraging
  retrieval-augmented generation on LLM","Large Language Model (LLM) has demonstrated significant success in a range of
natural language processing (NLP) tasks within general domain. The emergence of
LLM has introduced innovative methodologies across diverse fields, including
the natural sciences. Researchers aim to implement automated, concurrent
process driven by LLM to supplant conventional manual, repetitive and
labor-intensive work. In the domain of spectral analysis and detection, it is
imperative for researchers to autonomously acquire pertinent knowledge across
various research objects, which encompasses the spectroscopic techniques and
the chemometric methods that are employed in experiments and analysis.
Paradoxically, despite the recognition of spectroscopic detection as an
effective analytical method, the fundamental process of knowledge retrieval
remains both time-intensive and repetitive. In response to this challenge, we
first introduced the Spectral Detection and Analysis Based Paper(SDAAP)
dataset, which is the first open-source textual knowledge dataset for spectral
analysis and detection and contains annotated literature data as well as
corresponding knowledge instruction data. Subsequently, we also designed an
automated Q\&A framework based on the SDAAP dataset, which can retrieve
relevant knowledge and generate high-quality responses by extracting entities
in the input as retrieval parameters. It is worth noting that: within this
framework, LLM is only used as a tool to provide generalizability, while RAG
technique is used to accurately capture the source of the knowledge.This
approach not only improves the quality of the generated responses, but also
ensures the traceability of the knowledge. Experimental results show that our
framework generates responses with more reliable expertise compared to the
baseline.",Jiheng Liang
2024-08-21T17:00:05Z,http://arxiv.org/abs/2408.11775v1,"Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support: For 3GPP Standards","Recent studies show that large language models (LLMs) struggle with technical
standards in telecommunications. We propose a fine-tuned retrieval-augmented
generation (RAG) system based on the Phi-2 small language model (SLM) to serve
as an oracle for communication networks. Our developed system leverages
forward-looking semantic chunking to adaptively determine parsing breakpoints
based on embedding similarity, enabling effective processing of diverse
document formats. To handle the challenge of multiple similar contexts in
technical standards, we employ a re-ranking algorithm to prioritize the most
relevant retrieved chunks. Recognizing the limitations of Phi-2's small context
window, we implement a recent technique, namely SelfExtend, to expand the
context window during inference, which not only boosts the performance but also
can accommodate a wider range of user queries and design requirements from
customers to specialized technicians. For fine-tuning, we utilize the low-rank
adaptation (LoRA) technique to enhance computational efficiency during training
and enable effective fine-tuning on small datasets. Our comprehensive
experiments demonstrate substantial improvements over existing
question-answering approaches in the telecom domain, achieving performance that
exceeds larger language models such as GPT-4 (which is about 880 times larger
in size). This work presents a novel approach to leveraging SLMs for
communication networks, offering a balance of efficiency and performance. This
work can serve as a foundation towards agentic language models for networks.",Omar Erak
2024-08-18T11:52:24Z,http://arxiv.org/abs/2408.13273v1,"Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach
  for Temporal Knowledge Graph Forecasting","Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.",Geethan Sannidhi
2024-08-27T00:50:14Z,http://arxiv.org/abs/2408.14717v1,Text2SQL is Not Enough: Unifying AI and Databases with TAG,"AI systems that serve natural language questions over databases promise to
unlock tremendous value. Such systems would allow users to leverage the
powerful reasoning and knowledge capabilities of language models (LMs)
alongside the scalable computational power of data management systems. These
combined capabilities would empower users to ask arbitrary natural language
questions over custom data sources. However, existing methods and benchmarks
insufficiently explore this setting. Text2SQL methods focus solely on natural
language questions that can be expressed in relational algebra, representing a
small subset of the questions real users wish to ask. Likewise,
Retrieval-Augmented Generation (RAG) considers the limited subset of queries
that can be answered with point lookups to one or a few data records within the
database. We propose Table-Augmented Generation (TAG), a unified and
general-purpose paradigm for answering natural language questions over
databases. The TAG model represents a wide range of interactions between the LM
and database that have been previously unexplored and creates exciting research
opportunities for leveraging the world knowledge and reasoning capabilities of
LMs over data. We systematically develop benchmarks to study the TAG problem
and find that standard methods answer no more than 20% of queries correctly,
confirming the need for further research in this area. We release code for the
benchmark at https://github.com/TAG-Research/TAG-Bench.",Asim Biswal
2024-08-24T19:34:04Z,http://arxiv.org/abs/2409.00082v1,"Towards Human-Level Understanding of Complex Process Engineering
  Schematics: A Pedagogical, Introspective Multi-Agent Framework for
  Open-Domain Question Answering","In the chemical and process industries, Process Flow Diagrams (PFDs) and
Piping and Instrumentation Diagrams (P&IDs) are critical for design,
construction, and maintenance. Recent advancements in Generative AI, such as
Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in
understanding and interpreting process diagrams for Visual Question Answering
(VQA). However, proprietary models pose data privacy risks, and their
computational complexity prevents knowledge editing for domain-specific
customization on consumer hardware. To overcome these challenges, we propose a
secure, on-premises enterprise solution using a hierarchical, multi-agent
Retrieval Augmented Generation (RAG) framework for open-domain question
answering (ODQA) tasks, offering enhanced data privacy, explainability, and
cost-effectiveness. Our novel multi-agent framework employs introspective and
specialized sub-agents using open-source, small-scale multimodal models with
the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis,
integrating multiple information sources to provide accurate and contextually
relevant answers. Our approach, supported by iterative self-correction, aims to
deliver superior performance in ODQA tasks. We conducted rigorous experimental
studies, and the empirical results validated the proposed approach
effectiveness.",Sagar Srinivas Sakhinana
2024-09-01T07:01:22Z,http://arxiv.org/abs/2409.00636v1,"A Learnable Agent Collaboration Network Framework for Personalized
  Multimodal AI Search Engine","Large language models (LLMs) and retrieval-augmented generation (RAG)
techniques have revolutionized traditional information access, enabling AI
agent to search and summarize information on behalf of users during dynamic
dialogues. Despite their potential, current AI search engines exhibit
considerable room for improvement in several critical areas. These areas
include the support for multimodal information, the delivery of personalized
responses, the capability to logically answer complex questions, and the
facilitation of more flexible interactions. This paper proposes a novel AI
Search Engine framework called the Agent Collaboration Network (ACN). The ACN
framework consists of multiple specialized agents working collaboratively, each
with distinct roles such as Account Manager, Solution Strategist, Information
Manager, and Content Creator. This framework integrates mechanisms for picture
content understanding, user profile tracking, and online evolution, enhancing
the AI search engine's response quality, personalization, and interactivity. A
highlight of the ACN is the introduction of a Reflective Forward Optimization
method (RFO), which supports the online synergistic adjustment among agents.
This feature endows the ACN with online learning capabilities, ensuring that
the system has strong interactive flexibility and can promptly adapt to user
feedback. This learning method may also serve as an optimization approach for
agent-based systems, potentially influencing other domains of agent
applications.",Yunxiao Shi
2024-09-03T16:37:45Z,http://arxiv.org/abs/2409.02038v1,BEAVER: An Enterprise Benchmark for Text-to-SQL,"Existing text-to-SQL benchmarks have largely been constructed using publicly
available tables from the web with human-generated tests containing question
and SQL statement pairs. They typically show very good results and lead people
to think that LLMs are effective at text-to-SQL tasks. In this paper, we apply
off-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In
this environment, LLMs perform poorly, even when standard prompt engineering
and RAG techniques are utilized. As we will show, the reasons for poor
performance are largely due to three characteristics: (1) public LLMs cannot
train on enterprise data warehouses because they are largely in the ""dark web"",
(2) schemas of enterprise tables are more complex than the schemas in public
data, which leads the SQL-generation task innately harder, and (3)
business-oriented questions are often more complex, requiring joins over
multiple tables and aggregations. As a result, we propose a new dataset BEAVER,
sourced from real enterprise data warehouses together with natural language
queries and their correct SQL statements which we collected from actual user
history. We evaluated this dataset using recent LLMs and demonstrated their
poor performance on this task. We hope this dataset will facilitate future
researchers building more sophisticated text-to-SQL systems which can do better
on this important class of data.",Peter Baile Chen
2024-09-04T00:10:36Z,http://arxiv.org/abs/2409.02343v1,"NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
  Retrieval","$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of $k$-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE's non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.",Sepanta Zeighami
2024-09-04T09:46:33Z,http://arxiv.org/abs/2409.02572v3,"Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models","Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within
the field of Digital Forensics (DF). It focuses on examining and analyzing
time-based digital artefacts, such as timestamps derived from event logs, file
metadata, and other relevant data, to correlate events linked to cyber
incidents and reconstruct their chronological sequence. Traditional tools often
struggle to efficiently handle the large volume and variety of data generated
during DF investigations and Incident Response (IR) processes. This paper
introduces a novel framework, GenDFIR, which combines Rule-Based Artificial
Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance
and automate the TA process. The proposed approach consists of two key stages:
(1) R-BAI is used to identify and select anomalous digital artefacts based on
predefined rules. (2) The selected artefacts are then transformed into
embeddings for processing by an LLM with the assistance of a
Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to
perform automated TA on the artefacts and predict potential incident outcomes.
To validate the framework, we evaluated its performance, efficiency, and
reliability. Several metrics were applied to simulated cyber incident
scenarios, which were presented as forensic case documents. Our findings
demonstrate the significant potential of integrating R-BAI and LLMs for TA.
This innovative approach underscores the power of Generative AI (GenAI),
particularly LLMs, and opens up new possibilities for advanced threat detection
and incident reconstruction, marking a significant advancement in the field.",Fatma Yasmine Loumachi
2024-09-04T13:49:19Z,http://arxiv.org/abs/2409.02711v1,"Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for
  PostNL","The developments in the field of generative AI has brought a lot of
opportunities for companies, for instance to improve efficiency in customer
service and automating tasks. PostNL, the biggest parcel and E-commerce
corporation of the Netherlands wants to use generative AI to enhance the
communication around track and trace of parcels. During the internship a
Minimal Viable Product (MVP) is created to showcase the value of using
generative AI technologies, to enhance parcel tracking, analyzing the parcel's
journey and being able to communicate about it in an easy to understand manner.
The primary goal was to develop an in-house LLM-based system, reducing
dependency on external platforms and establishing the feasibility of a
dedicated generative AI team within the company. This multi-agent LLM based
system aimed to construct parcel journey stories and identify logistical
disruptions with heightened efficiency and accuracy. The research involved
deploying a sophisticated AI-driven communication system, employing
Retrieval-Augmented Generation (RAG) for enhanced response precision, and
optimizing large language models (LLMs) tailored to domain specific tasks.
  The MVP successfully implemented a multi-agent open-source LLM system, called
SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of
user inquiries and improving internal knowledge handling. Results and
evaluation demonstrated technological innovation and feasibility, notably in
communication about the track and trace of a parcel, which exceeded initial
expectations. These advancements highlight the potential of AI-driven solutions
in logistics, suggesting many opportunities for further refinement and broader
implementation within PostNL operational framework.",Mohammad Reshadati
2024-09-08T16:35:19Z,http://arxiv.org/abs/2409.05152v2,OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs,"Despite the recent advancements in Large Language Models (LLMs), which have
significantly enhanced the generative capabilities for various NLP tasks, LLMs
still face limitations in directly handling retrieval tasks. However, many
practical applications demand the seamless integration of both retrieval and
generation. This paper introduces a novel and efficient One-pass Generation and
retrieval framework (OneGen), designed to improve LLMs' performance on tasks
that require both generation and retrieval. The proposed framework bridges the
traditionally separate training approaches for generation and retrieval by
incorporating retrieval tokens generated autoregressively. This enables a
single LLM to handle both tasks simultaneously in a unified forward pass. We
conduct experiments on two distinct types of composite tasks, RAG and Entity
Linking, to validate the pluggability, effectiveness, and efficiency of OneGen
in training and inference. Furthermore, our results show that integrating
generation and retrieval within the same context preserves the generative
capabilities of LLMs while improving retrieval performance. To the best of our
knowledge, OneGen is the first to enable LLMs to conduct vector retrieval
during the generation.",Jintian Zhang
2024-09-12T08:25:33Z,http://arxiv.org/abs/2409.07829v1,"Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:
  A Case Study in WeChat","UI automation tests play a crucial role in ensuring the quality of mobile
applications. Despite the growing popularity of machine learning techniques to
generate these tests, they still face several challenges, such as the mismatch
of UI elements. The recent advances in Large Language Models (LLMs) have
addressed these issues by leveraging their semantic understanding capabilities.
However, a significant gap remains in applying these models to industrial-level
app testing, particularly in terms of cost optimization and knowledge
limitation. To address this, we introduce CAT to create cost-effective UI
automation tests for industry apps by combining machine learning and LLMs with
best practices. Given the task description, CAT employs Retrieval Augmented
Generation (RAG) to source examples of industrial app usage as the few-shot
learning context, assisting LLMs in generating the specific sequence of
actions. CAT then employs machine learning techniques, with LLMs serving as a
complementary optimizer, to map the target element on the UI screen. Our
evaluations on the WeChat testing dataset demonstrate the CAT's performance and
cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming
the state-of-the-art. We have also integrated our approach into the real-world
WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and
enhancing the developers' testing process.",Sidong Feng
2024-08-30T13:31:32Z,http://arxiv.org/abs/2409.09052v1,"OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in
  Computed Tomography","Multimodal large language models (MLLMs) have achieved significant success in
the general field of image processing. Their emerging task generalization and
freeform conversational capabilities can greatly facilitate medical diagnostic
assistance, helping patients better understand their conditions and enhancing
doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging
technique used to capture the internal mechanisms of a patient's condition and
is widely utilized. However, in past research, the complex textural features of
this imaging data have made accurate interpretation by algorithms challenging,
impeding the performance of general LLMs in diagnostic assistance. To address
this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is
trained on 120,000 CT images and diagnostic reports and includes a
Retrieval-Augmented Generation (RAG) module capable of effectively mitigating
model hallucinations. This module is informed by extensive medical literature,
textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT
images but also stores, understands, and reasons over medical knowledge and
language. In extensive experiments, OrthoDoc outperforms commercial models led
by GPT-4, demonstrating superior diagnostic capabilities and accuracy.
Specifically, OrthoDoc significantly surpasses existing models in the diagnosis
of common orthopedic conditions such as fractures, arthritis, and tumors.
Additionally, OrthoDoc exhibits robust generalization and stability when
handling rare and complex cases.",Youzhu Jin
2024-09-17T13:44:42Z,http://arxiv.org/abs/2409.11190v2,"SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer","We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.",Anmol Gautam
2024-09-17T15:29:34Z,http://arxiv.org/abs/2409.11279v1,"P-RAG: Progressive Retrieval Augmented Generation For Planning on
  Embodied Everyday Task","Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.",Weiye Xu
2024-09-19T08:26:45Z,http://arxiv.org/abs/2409.12558v1,"RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval
  Augmented Dialogues","In real-world applications with Large Language Models (LLMs), external
retrieval mechanisms - such as Search-Augmented Generation (SAG), tool
utilization, and Retrieval-Augmented Generation (RAG) - are often employed to
enhance the quality of augmented generations in dialogues. These approaches
often come with multi-turn dialogue, where each interaction is enriched by
relevant information retrieved from external sources. Existing benchmarks
either assess LLMs' chat abilities in multi-turn dialogues or their use of
retrieval for augmented responses in single-turn settings. However, there is a
gap in evaluating LLMs' ability to leverage retrieval for more precise
responses across multiple turns. To address this limitation, we introduce
RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate
LLMs' capabilities in multi-turn dialogues following retrievals, essential for
their deployment in context-rich applications. RAD-Bench evaluates two key
abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are
measured using discriminative questions and retrieved contexts, and
corresponding reference answers, assessing how effectively LLMs integrate and
reason with context to maintain and enhance conversation quality over multiple
turns. Our evaluation results on commonly used LLMs reveal that model
performance deteriorates as additional layers of conditions or constraints are
applied across conversation turns, even when accurate retrieved contexts are
provided.",Tzu-Lin Kuo
2024-09-05T02:34:05Z,http://arxiv.org/abs/2409.13699v1,Vietnamese Legal Information Retrieval in Question-Answering System,"In the modern era of rapidly increasing data volumes, accurately retrieving
and recommending relevant documents has become crucial in enhancing the
reliability of Question Answering (QA) systems. Recently, Retrieval Augmented
Generation (RAG) has gained significant recognition for enhancing the
capabilities of large language models (LLMs) by mitigating hallucination issues
in QA systems, which is particularly beneficial in the legal domain. Various
methods, such as semantic search using dense vector embeddings or a combination
of multiple techniques to improve results before feeding them to LLMs, have
been proposed. However, these methods often fall short when applied to the
Vietnamese language due to several challenges, namely inefficient Vietnamese
data processing leading to excessive token length or overly simplistic ensemble
techniques that lead to instability and limited improvement. Moreover, a
critical issue often overlooked is the ordering of final relevant documents
which are used as reference to ensure the accuracy of the answers provided by
LLMs. In this report, we introduce our three main modifications taken to
address these challenges. First, we explore various practical approaches to
data processing to overcome the limitations of the embedding model.
Additionally, we enhance Reciprocal Rank Fusion by normalizing order to combine
results from keyword and vector searches effectively. We also meticulously
re-rank the source pieces of information used by LLMs with Active Retrieval to
improve user experience when refining the information generated. In our
opinion, this technique can also be considered as a new re-ranking method that
might be used in place of the traditional cross encoder. Finally, we integrate
these techniques into a comprehensive QA system, significantly improving its
performance and reliability",Thiem Nguyen Ba
2024-09-06T13:06:29Z,http://arxiv.org/abs/2409.13707v1,"Retrieval Augmented Generation-Based Incident Resolution Recommendation
  System for IT Support","Clients wishing to implement generative AI in the domain of IT Support and
AIOps face two critical issues: domain coverage and model size constraints due
to model choice limitations. Clients might choose to not use larger proprietary
models such as GPT-4 due to cost and privacy concerns and so are limited to
smaller models with potentially less domain coverage that do not generalize to
the client's domain. Retrieval augmented generation is a common solution that
addresses both of these issues: a retrieval system first retrieves the
necessary domain knowledge which a smaller generative model leverages as
context for generation. We present a system developed for a client in the IT
Support domain for support case solution recommendation that combines retrieval
augmented generation (RAG) for answer generation with an encoder-only model for
classification and a generative large language model for query generation. We
cover architecture details, data collection and annotation, development journey
and preliminary validations, expected final deployment process and evaluation
plans, and finally lessons learned.",Paulina Toro Isaza
2024-09-13T16:43:08Z,http://arxiv.org/abs/2409.13749v1,KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models,"Although powerful, current cutting-edge LLMs may not fulfil the needs of
highly specialised sectors. We introduce KodeXv0.1, a family of large language
models that outclass GPT-4 in financial question answering. We utilise the base
variants of Llama 3.1 8B and 70B and adapt them to the financial domain through
a custom training regime. To this end, we collect and process a large number of
publicly available financial documents such as earnings calls and business
reports. These are used to generate a high-quality, synthetic dataset
consisting of Context-Question-Answer triplets which closely mirror real-world
financial tasks. Using the train split of this dataset, we perform RAG-aware
4bit LoRA instruction tuning runs of Llama 3.1 base variants to produce
KodeX-8Bv0.1 and KodeX-70Bv0.1. We then complete extensive model evaluations
using FinanceBench, FinQABench and the withheld test split of our dataset. Our
results show that KodeX-8Bv0.1 is more reliable in financial contexts than
cutting-edge instruct models in the same parameter regime, surpassing them by
up to 9.24%. In addition, it is even capable of outperforming state-of-the-art
proprietary models such as GPT-4 by up to 7.07%. KodeX-70Bv0.1 represents a
further improvement upon this, exceeding GPT-4's performance on every tested
benchmark.",Neel Rajani
2024-09-22T16:20:00Z,http://arxiv.org/abs/2409.14516v1,"Beyond Words: Evaluating Large Language Models in Transportation
  Planning","The resurgence and rapid advancement of Generative Artificial Intelligence
(GenAI) in 2023 has catalyzed transformative shifts across numerous industry
sectors, including urban transportation and logistics. This study investigates
the evaluation of Large Language Models (LLMs), specifically GPT-4 and
Phi-3-mini, to enhance transportation planning. The study assesses the
performance and spatial comprehension of these models through a
transportation-informed evaluation framework that includes general geospatial
skills, general transportation domain skills, and real-world transportation
problem-solving. Utilizing a mixed-methods approach, the research encompasses
an evaluation of the LLMs' general Geographic Information System (GIS) skills,
general transportation domain knowledge as well as abilities to support human
decision-making in the real-world transportation planning scenarios of
congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy
and reliability across various GIS and transportation-specific tasks compared
to Phi-3-mini, highlighting its potential as a robust tool for transportation
planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical
scenarios, suggesting its utility in resource-constrained environments. The
findings underscore the transformative potential of GenAI technologies in urban
transportation planning. Future work could explore the application of newer
LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a
broader set of real-world transportation planning and operations challenges, to
deepen the integration of advanced AI models in transportation management
practices.",Shaowei Ying
2024-09-30T10:48:20Z,http://arxiv.org/abs/2409.20181v2,"Reference Trustable Decoding: A Training-Free Augmentation Paradigm for
  Large Language Models","Large language models (LLMs) have rapidly advanced and demonstrated
impressive capabilities. In-Context Learning (ICL) and Parameter-Efficient
Fine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to
downstream tasks. ICL typically constructs a few-shot learning scenario, either
manually or by setting up a Retrieval-Augmented Generation (RAG) system,
helping models quickly grasp domain knowledge or question-answering patterns
without changing model parameters. However, this approach involves trade-offs,
such as slower inference speed and increased space occupancy. PEFT assists the
model in adapting to tasks through minimal parameter modifications, but the
training process still demands high hardware requirements, even with a small
number of parameters involved. To address these challenges, we propose
Reference Trustable Decoding (RTD), a paradigm that allows models to quickly
adapt to new tasks without fine-tuning, maintaining low inference costs. RTD
constructs a reference datastore from the provided training examples and
optimizes the LLM's final vocabulary distribution by flexibly selecting
suitable references based on the input, resulting in more trustable responses
and enabling the model to adapt to downstream tasks at a low cost. Experimental
evaluations on various LLMs using different benchmarks demonstrate that RTD
establishes a new paradigm for augmenting models to downstream tasks.
Furthermore, our method exhibits strong orthogonality with traditional methods,
allowing for concurrent usage. Our code can be found at
https://github.com/ShiLuohe/ReferenceTrustableDecoding",Luohe Shi
2024-10-02T04:29:08Z,http://arxiv.org/abs/2410.01231v1,"Revisiting the Index Construction of Proximity Graph-Based Approximate
  Nearest Neighbor Search","Proximity graphs (PG) have gained increasing popularity as the
state-of-the-art (SOTA) solutions to $k$-approximate nearest neighbor ($k$-ANN)
search on high-dimensional data, which serves as a fundamental function in
various fields, e.g. information retrieval and retrieval-augmented
generation~(RAG). Although PG-based approaches have the best $k$-ANN search
performance, their index construction cost is superlinear to the number of
points, since they have to identify close neighbors for each point to establish
the edges. Such superlinear cost substantially limits their scalability in the
era of big data. Hence, the goal of this paper is to accelerate the
construction of PG-based methods without compromising their $k$-ANN search
performance.
  To achieve this goal, two mainstream categories of PG are revisited: relative
neighborhood graph (RNG) and navigable small world graph (NSWG). By revisiting
their construction process, we find the issues of construction efficiency. To
address these issues, we propose a new construction framework with a novel
pruning strategy for edge selection, which accelerates RNG construction while
keeping its $k$-ANN search performance. Then, we integrate this framework into
NSWG construction to enhance both the construction efficiency and $k$-ANN
search performance. Moreover, extensive experiments are conducted to validate
our construction framework for both RNG and NSWG. The results demonstrate that
it significantly reduces the PG construction cost, achieving up to 5.6x speedup
while not compromising the $k$-ANN search performance.",Shuo Yang
2024-10-02T11:26:02Z,http://arxiv.org/abs/2410.01428v1,"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with
  Retrieval-Augmentation for Solving Challenging Tasks","State-of-the-art large language models (LLMs) exhibit impressive
problem-solving capabilities but may struggle with complex reasoning and
factual correctness. Existing methods harness the strengths of chain-of-thought
and retrieval-augmented generation (RAG) to decompose a complex problem into
simpler steps and apply retrieval to improve factual correctness. These methods
work well on straightforward reasoning tasks but often falter on challenging
tasks such as competitive programming and mathematics, due to frequent
reasoning errors and irrelevant knowledge retrieval. To address this, we
introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a
novel framework that leverages fine-tuned critic models to guide both reasoning
and retrieval processes through planning. CR-Planner solves a problem by
iteratively selecting and executing sub-goals. Initially, it identifies the
most promising sub-goal from reasoning, query generation, and retrieval, guided
by rewards given by a critic model named sub-goal critic. It then executes this
sub-goal through sampling and selecting the optimal output based on evaluations
from another critic model named execution critic. This iterative process,
informed by retrieved information and critic models, enables CR-Planner to
effectively navigate the solution space towards the final answer. We employ
Monte Carlo Tree Search to collect the data for training the critic models,
allowing for a systematic exploration of action sequences and their long-term
impacts. We validate CR-Planner on challenging domain-knowledge-intensive and
reasoning-heavy tasks, including competitive programming, theorem-driven math
reasoning, and complex domain retrieval problems. Our experiments demonstrate
that CR-Planner significantly outperforms baselines, highlighting its
effectiveness in addressing challenging problems by improving both reasoning
and retrieval.",Xingxuan Li
2024-09-27T23:05:02Z,http://arxiv.org/abs/2410.01841v1,A GEN AI Framework for Medical Note Generation,"The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.",Hui Yi Leong
2024-09-17T18:24:27Z,http://arxiv.org/abs/2410.02779v1,"Learning variant product relationship and variation attributes from
  e-commerce website structures","We introduce VARM, variant relationship matcher strategy, to identify pairs
of variant products in e-commerce catalogs. Traditional definitions of entity
resolution are concerned with whether product mentions refer to the same
underlying product. However, this fails to capture product relationships that
are critical for e-commerce applications, such as having similar, but not
identical, products listed on the same webpage or share reviews. Here, we
formulate a new type of entity resolution in variant product relationships to
capture these similar e-commerce product links. In contrast with the
traditional definition, the new definition requires both identifying if two
products are variant matches of each other and what are the attributes that
vary between them. To satisfy these two requirements, we developed a strategy
that leverages the strengths of both encoding and generative AI models. First,
we construct a dataset that captures webpage product links, and therefore
variant product relationships, to train an encoding LLM to predict variant
matches for any given pair of products. Second, we use RAG prompted generative
LLMs to extract variation and common attributes amongst groups of variant
products. To validate our strategy, we evaluated model performance using real
data from one of the world's leading e-commerce retailers. The results showed
that our strategy outperforms alternative solutions and paves the way to
exploiting these new type of product relationships.",Pedro Herrero-Vidal
2024-10-04T00:08:46Z,http://arxiv.org/abs/2410.03049v1,"Scalable Frame-based Construction of Sociocultural NormBases for
  Socially-Aware Dialogues","Sociocultural norms serve as guiding principles for personal conduct in
social interactions, emphasizing respect, cooperation, and appropriate
behavior, which is able to benefit tasks including conversational information
retrieval, contextual information retrieval and retrieval-enhanced machine
learning. We propose a scalable approach for constructing a Sociocultural Norm
(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We
construct a comprehensive and publicly accessible Chinese Sociocultural
NormBase. Our approach utilizes socially aware dialogues, enriched with
contextual frames, as the primary data source to constrain the generating
process and reduce the hallucinations. This enables extracting of high-quality
and nuanced natural-language norm statements, leveraging the pragmatic
implications of utterances with respect to the situation. As real dialogue
annotated with gold frames are not readily available, we propose using
synthetic data. Our empirical results show: (i) the quality of the SCNs derived
from synthetic data is comparable to that from real dialogues annotated with
gold frames, and (ii) the quality of the SCNs extracted from real data,
annotated with either silver (predicted) or gold frames, surpasses that without
the frame annotations. We further show the effectiveness of the extracted SCNs
in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple
downstream dialogue tasks.",Shilin Qu
2024-10-04T22:45:26Z,http://arxiv.org/abs/2410.03960v2,"SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving
  Model Transformation","LLM inference for popular enterprise use cases, such as summarization, RAG,
and code-generation, typically observes orders of magnitude longer prompt
lengths than generation lengths. This characteristic leads to high cost of
prefill and increased response latency. In this paper, we present SwiftKV, a
novel model transformation and distillation procedure specifically designed to
reduce the time and cost of processing prompt tokens while preserving high
quality of generated tokens. SwiftKV combines three key mechanisms: i)
SingleInputKV, which prefills later layers' KV cache using a much earlier
layer's output, allowing prompt tokens to skip much of the model computation,
ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the
memory footprint and support larger batch size for higher throughput, and iii)
a knowledge-preserving distillation procedure that can adapt existing LLMs for
SwiftKV with minimal accuracy impact and low compute and data requirement. For
Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%
and the memory requirement of the KV cache by 62.5% while incurring minimum
quality degradation across a wide range of tasks. In the end-to-end inference
serving using an optimized vLLM implementation, SwiftKV realizes up to 2x
higher aggregate throughput and 60% lower time per output token. It can achieve
a staggering 560 TFlops/GPU of normalized inference throughput, which
translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100
GPUs. Our training, inference, and model implementations are open-sourced and
can be found through
https://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",Aurick Qiao
2024-10-06T18:46:28Z,http://arxiv.org/abs/2410.04585v1,"Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community
  Retrieval","Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.",Pengcheng Jiang
2024-10-07T04:59:08Z,http://arxiv.org/abs/2410.04749v2,"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language
  Generation with Knowledge Graph for Explaining Thoracic Pathologies","Generating Natural Language Explanations (NLEs) for model predictions on
medical images, particularly those depicting thoracic pathologies, remains a
critical and challenging task. Existing methodologies often struggle due to
general models' insufficient domain-specific medical knowledge and privacy
concerns associated with retrieval-based augmentation techniques. To address
these issues, we propose a novel Vision-Language framework augmented with a
Knowledge Graph (KG)-based datastore, which enhances the model's understanding
by incorporating additional domain-specific medical knowledge essential for
generating accurate and informative NLEs. Our framework employs a KG-based
retrieval mechanism that not only improves the precision of the generated
explanations but also preserves data privacy by avoiding direct data retrieval.
The KG datastore is designed as a plug-and-play module, allowing for seamless
integration with various model architectures. We introduce and evaluate three
distinct frameworks within this paradigm: KG-LLaVA, which integrates the
pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining
MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts
LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are
validated on the MIMIC-NLE dataset, where they achieve state-of-the-art
results, underscoring the effectiveness of KG augmentation in generating
high-quality NLEs for thoracic pathologies.",Ameer Hamza
2024-10-08T07:28:17Z,http://arxiv.org/abs/2410.05752v1,"Exploring the Meaningfulness of Nearest Neighbor Search in
  High-Dimensional Space","Dense high dimensional vectors are becoming increasingly vital in fields such
as computer vision, machine learning, and large language models (LLMs), serving
as standard representations for multimodal data. Now the dimensionality of
these vector can exceed several thousands easily. Despite the nearest neighbor
search (NNS) over these dense high dimensional vectors have been widely used
for retrieval augmented generation (RAG) and many other applications, the
effectiveness of NNS in such a high-dimensional space remains uncertain, given
the possible challenge caused by the ""curse of dimensionality."" To address
above question, in this paper, we conduct extensive NNS studies with different
distance functions, such as $L_1$ distance, $L_2$ distance and
angular-distance, across diverse embedding datasets, of varied types,
dimensionality and modality. Our aim is to investigate factors influencing the
meaningfulness of NNS. Our experiments reveal that high-dimensional text
embeddings exhibit increased resilience as dimensionality rises to higher
levels when compared to random vectors. This resilience suggests that text
embeddings are less affected to the ""curse of dimensionality,"" resulting in
more meaningful NNS outcomes for practical use. Additionally, the choice of
distance function has minimal impact on the relevance of NNS. Our study shows
the effectiveness of the embedding-based data representation method and can
offer opportunity for further optimization of dense vector-related
applications.",Zhonghan Chen
2024-10-10T18:21:00Z,http://arxiv.org/abs/2410.08289v1,"Increasing the Difficulty of Automatically Generated Questions via
  Reinforcement Learning with Synthetic Preference","As the cultural heritage sector increasingly adopts technologies like
Retrieval-Augmented Generation (RAG) to provide more personalised search
experiences and enable conversations with collections data, the demand for
specialised evaluation datasets has grown. While end-to-end system testing is
essential, it's equally important to assess individual components. We target
the final, answering task, which is well-suited to Machine Reading
Comprehension (MRC). Although existing MRC datasets address general domains,
they lack the specificity needed for cultural heritage information.
Unfortunately, the manual creation of such datasets is prohibitively expensive
for most heritage institutions. This paper presents a cost-effective approach
for generating domain-specific MRC datasets with increased difficulty using
Reinforcement Learning from Human Feedback (RLHF) from synthetic preference
data. Our method leverages the performance of existing question-answering
models on a subset of SQuAD to create a difficulty metric, assuming that more
challenging questions are answered correctly less frequently. This research
contributes: (1) A methodology for increasing question difficulty using PPO and
synthetic data; (2) Empirical evidence of the method's effectiveness, including
human evaluation; (3) An in-depth error analysis and study of emergent
phenomena; and (4) An open-source codebase and set of three llama-2-chat
adapters for reproducibility and adaptation.",William Thorne
2024-10-11T18:22:08Z,http://arxiv.org/abs/2410.09174v1,"Context-Aware SQL Error Correction Using Few-Shot Learning -- A Novel
  Approach Based on NLQ, Error, and SQL Similarity","In recent years, the demand for automated SQL generation has increased
significantly, driven by the need for efficient data querying in various
applications. However, generating accurate SQL queries remains a challenge due
to the complexity and variability of natural language inputs. This paper
introduces a novel few-shot learning-based approach for error correction in SQL
generation, enhancing the accuracy of generated queries by selecting the most
suitable few-shot error correction examples for a given natural language
question (NLQ). In our experiments with the open-source Gretel dataset, the
proposed model offers a 39.2% increase in fixing errors from the baseline
approach with no error correction and a 10% increase from a simple error
correction method. The proposed technique leverages embedding-based similarity
measures to identify the closest matches from a repository of few-shot
examples. Each example comprises an incorrect SQL query, the resulting error,
the correct SQL query, and detailed steps to transform the incorrect query into
the correct one. By employing this method, the system can effectively guide the
correction of errors in newly generated SQL queries. Our approach demonstrates
significant improvements in SQL generation accuracy by providing contextually
relevant examples that facilitate error identification and correction. The
experimental results highlight the effectiveness of embedding-based selection
in enhancing the few-shot learning process, leading to more precise and
reliable SQL query generation. This research contributes to the field of
automated SQL generation by offering a robust framework for error correction,
paving the way for more advanced and user-friendly database interaction tools.",Divyansh Jain
2024-10-13T16:28:38Z,http://arxiv.org/abs/2410.09908v1,"Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble
  for Zero-shot Learning","Foundation models have become a cornerstone in deep learning, with techniques
like Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models.
Similarly, methods such as Retrieval-Augmented Generation (RAG), which leverage
vectorized databases, have further improved model performance by grounding
outputs in external information. While these approaches have demonstrated
notable success, they often require extensive training or labeled data, which
can limit their adaptability in resource-constrained environments. To address
these challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new
method that creates a vectorized database of LoRAs, enabling efficient
retrieval and application of model adaptations to new tasks. RPE minimizes the
need for extensive training and eliminates the requirement for labeled data,
making it particularly effective for zero-shot learning. Additionally, RPE is
well-suited for privacy-sensitive domains like healthcare, as it modifies model
parameters without accessing raw data. When applied to tasks such as medical
report generation and image segmentation, RPE not only proved effective but
also surpassed supervised fine-tuning methods in certain cases, highlighting
its potential to enhance both computational efficiency and privacy in deep
learning applications.",Pengfei Jin
2024-10-02T16:34:32Z,http://arxiv.org/abs/2410.12831v1,"Segment as You Wish -- Free-Form Language-Based Segmentation for Medical
  Images","Medical imaging is crucial for diagnosing a patient's health condition, and
accurate segmentation of these images is essential for isolating regions of
interest to ensure precise diagnosis and treatment planning. Existing methods
primarily rely on bounding boxes or point-based prompts, while few have
explored text-related prompts, despite clinicians often describing their
observations and instructions in natural language. To address this gap, we
first propose a RAG-based free-form text prompt generator, that leverages the
domain corpus to generate diverse and realistic descriptions. Then, we
introduce FLanS, a novel medical image segmentation model that handles various
free-form text prompts, including professional anatomy-informed queries,
anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven
queries. Additionally, our model also incorporates a symmetry-aware
canonicalization module to ensure consistent, accurate segmentations across
varying scan orientations and reduce confusion between the anatomical position
of an organ and its appearance in the scan. FLanS is trained on a large-scale
dataset of over 100k medical images from 7 public datasets. Comprehensive
experiments demonstrate the model's superior language understanding and
segmentation precision, along with a deep comprehension of the relationship
between them, outperforming SOTA baselines on both in-domain and out-of-domain
datasets.",Longchao Da
2024-10-17T08:37:25Z,http://arxiv.org/abs/2410.13326v1,"Comparing the Utility, Preference, and Performance of Course Material
  Search Functionality and Retrieval-Augmented Generation Large Language Model
  (RAG-LLM) AI Chatbots in Information-Seeking Tasks","Providing sufficient support for students requires substantial resources,
especially considering the growing enrollment numbers. Students need help in a
variety of tasks, ranging from information-seeking to requiring support with
course assignments. To explore the utility of recent large language models
(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that
augments the answers that are produced with information from the course
materials. To study the effect of the LLM-powered AI chatbot, we conducted a
lab-based user study (N=14), in which the participants worked on tasks from a
web software development course. The participants were divided into two groups,
where one of the groups first had access to the chatbot and then to a more
traditional search functionality, while another group started with the search
functionality and was then given the chatbot. We assessed the participants'
performance and perceptions towards the chatbot and the search functionality
and explored their preferences towards the support functionalities. Our
findings highlight that both support mechanisms are seen as useful and that
support mechanisms work well for specific tasks, while less so for other tasks.
We also observe that students tended to prefer the second support mechanism
more, where students who were first given the chatbot tended to prefer the
search functionality and vice versa.",Leonardo Pasquarelli
2024-10-18T06:51:13Z,http://arxiv.org/abs/2410.14209v1,"Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents","In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.",Zihan Liu
2024-10-19T07:08:40Z,http://arxiv.org/abs/2410.15016v1,"Transit Pulse: Utilizing Social Media as a Source for Customer Feedback
  and Information Extraction with Large Language Model","Users of the transit system flood social networks daily with messages that
contain valuable insights crucial for improving service quality. These posts
help transit agencies quickly identify emerging issues. Parsing topics and
sentiments is key to gaining comprehensive insights to foster service
excellence. However, the volume of messages makes manual analysis impractical,
and standard NLP techniques like Term Frequency-Inverse Document Frequency
(TF-IDF) fall short in nuanced interpretation. Traditional sentiment analysis
separates topics and sentiments before integrating them, often missing the
interaction between them. This incremental approach complicates classification
and reduces analytical productivity. To address these challenges, we propose a
novel approach to extracting and analyzing transit-related information,
including sentiment and sarcasm detection, identification of unusual system
problems, and location data from social media. Our method employs Large
Language Models (LLM), specifically Llama 3, for a streamlined analysis free
from pre-established topic labels. To enhance the model's domain-specific
knowledge, we utilize Retrieval-Augmented Generation (RAG), integrating
external knowledge sources into the information extraction pipeline. We
validated our method through extensive experiments comparing its performance
with traditional NLP approaches on user tweet data from the real world transit
system. Our results demonstrate the potential of LLMs to transform social media
data analysis in the public transit domain, providing actionable insights and
enhancing transit agencies' responsiveness by extracting a broader range of
information.",Jiahao Wang
2024-10-20T04:24:16Z,http://arxiv.org/abs/2410.15277v1,"BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via
  Compression","Retrieval-augmented generation (RAG) can supplement large language models
(LLMs) by integrating external knowledge. However, as the number of retrieved
documents increases, the input length to LLMs grows linearly, causing a
dramatic increase in latency and a degradation in long-context understanding.
This is particularly serious for multi-hop questions that require a chain of
reasoning across documents. To accelerate inference, reduce costs, and minimize
distractions, this paper presents BRIEF (Bridging Retrieval and Inference
through Evidence Fusion), a lightweight approach that performs query-aware
multi-hop reasoning by compressing retrieved documents into highly dense
textual summaries to integrate into in-context learning. To enable learning
compression for multi-hop reasoning, we curate synthetic data by extracting
atomic proposition expressions that encapsulate distinct factoids from the
source documents to compose synthetic summaries. Based on our synthetic data
built entirely by open-source models, BRIEF generates more concise summaries
and enables a range of LLMs to achieve exceptional open-domain question
answering (QA) performance. For example, on HotpotQA, BRIEF improves the
compression rate by 2 times compared to the state-of-the-art baseline, while
outperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It
also generates more concise summaries than proprietary GPT-3.5, while
demonstrating nearly identical QA performance.",Yuankai Li
2024-10-20T14:31:05Z,http://arxiv.org/abs/2410.15403v2,"MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis
  and Knowledge-based Departmental Consultation","We present MMDS, a system capable of recognizing medical images and patient
facial details, and providing professional medical diagnoses. The system
consists of two core components:The first component is the analysis of medical
images and videos. We trained a specialized multimodal medical model capable of
interpreting medical images and accurately analyzing patients' facial emotions
and facial paralysis conditions. The model achieved an accuracy of 72.59% on
the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in
recognizing the ""happy"" emotion. In facial paralysis recognition, the model
reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on
this model, we developed a parser for analyzing facial movement videos of
patients with facial paralysis, achieving precise grading of the paralysis
severity. In tests on 30 videos of facial paralysis patients, the system
demonstrated a grading accuracy of 83.3%.The second component is the generation
of professional medical responses. We employed a large language model,
integrated with a medical knowledge base, to generate professional diagnoses
based on the analysis of medical images or videos. The core innovation lies in
our development of a department-specific knowledge base routing management
mechanism, in which the large language model categorizes data by medical
departments and, during the retrieval process, determines the appropriate
knowledge base to query. This significantly improves retrieval accuracy in the
RAG (retrieval-augmented generation) process.",Yi Ren
2024-10-20T16:21:25Z,http://arxiv.org/abs/2410.15440v1,"Evaluating Consistencies in LLM responses through a Semantic Clustering
  of Question Answering","In the realm of Large Language Model (LLM) functionalities, providing
reliable information is paramount, yet reports suggest that LLM outputs lack
consistency. This inconsistency, often at-tributed to randomness in token
sampling, under-mines user trust as it leads to varying responses even for
identical queries. In this paper, we present a new approach for evaluating
semantic consistencies of LLM including comparison of alternative tech-niques.
Our approach evaluates whether LLM re-sponses are semantically congruent for a
given question, recognizing that as syntactically different sentences may
convey the same meaning. Here-tofore, To enhance LLM consistency, two main
approaches have been explored: Leverage external knowledge as context like the
RAG pattern or use Zero-shot-CoT to improve performance of LLM itself. We apply
our evaluation approach to these techniques, and demonstrate to compare the
im-pact of these methods on LLM response con-sistency across different domains
of question an-swering tasks. Using the TruthfulQA dataset to assess LLM
responses, the study induces N re-sponses per question from the LLM and
clusters semantically equivalent sentences to measure semantic consistency
across 37 categories. Through this, it quantitatively analyzes the
effectiveness of the aforementioned methods in improving LLM performance before
and after their adoption.",Yanggyu Lee
2024-10-22T12:56:04Z,http://arxiv.org/abs/2410.16977v1,"IPL: Leveraging Multimodal Large Language Models for Intelligent Product
  Listing","Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,
Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are
mainly targeting individual sellers who usually lack sufficient experience in
e-commerce. Individual sellers often struggle to compose proper descriptions
for selling products. With the recent advancement of Multimodal Large Language
Models (MLLMs), we attempt to integrate such state-of-the-art generative AI
technologies into the product listing process. To this end, we develop IPL, an
Intelligent Product Listing tool tailored to generate descriptions using
various product attributes such as category, brand, color, condition, etc. IPL
enables users to compose product descriptions by merely uploading photos of the
selling product. More importantly, it can imitate the content style of our C2C
platform Xianyu. This is achieved by employing domain-specific instruction
tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation
(RAG) process. A comprehensive empirical evaluation demonstrates that the
underlying model of IPL significantly outperforms the base model in
domain-specific tasks while producing less hallucination. IPL has been
successfully deployed in our production system, where 72% of users have their
published product listings based on the generated content, and those product
listings are shown to have a quality score 5.6% higher than those without AI
assistance.",Kang Chen
2024-10-23T06:54:03Z,http://arxiv.org/abs/2410.17600v1,"Graphusion: A RAG Framework for Knowledge Graph Construction with a
  Global Perspective","Knowledge Graphs (KGs) are crucial in the field of artificial intelligence
and are widely used in downstream tasks, such as question-answering (QA). The
construction of KGs typically requires significant effort from domain experts.
Large Language Models (LLMs) have recently been used for Knowledge Graph
Construction (KGC). However, most existing approaches focus on a local
perspective, extracting knowledge triplets from individual sentences or
documents, missing a fusion process to combine the knowledge in a global KG.
This work introduces Graphusion, a zero-shot KGC framework from free text. It
contains three steps: in Step 1, we extract a list of seed entities using topic
modeling to guide the final KG includes the most relevant entities; in Step 2,
we conduct candidate triplet extraction using LLMs; in Step 3, we design the
novel fusion module that provides a global view of the extracted knowledge,
incorporating entity merging, conflict resolution, and novel triplet discovery.
Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for
entity extraction and relation recognition, respectively. Moreover, we showcase
how Graphusion could be applied to the Natural Language Processing (NLP) domain
and validate it in an educational scenario. Specifically, we introduce TutorQA,
a new expert-verified benchmark for QA, comprising six tasks and a total of
1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant
improvement on the benchmark, for example, a 9.2% accuracy improvement on
sub-graph completion.",Rui Yang
2024-10-24T14:47:25Z,http://arxiv.org/abs/2410.18792v2,An LLM Agent for Automatic Geospatial Data Analysis,"Large language models (LLMs) are being used in data science code generation
tasks, but they often struggle with complex sequential tasks, leading to
logical errors. Their application to geospatial data processing is particularly
challenging due to difficulties in incorporating complex data structures and
spatial constraints, effectively utilizing diverse function calls, and the
tendency to hallucinate less-used geospatial libraries. To tackle these
problems, we introduce GeoAgent, a new interactive framework designed to help
LLMs handle geospatial data processing more effectively. GeoAgent pioneers the
integration of a code interpreter, static analysis, and Retrieval-Augmented
Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,
offering a novel approach to geospatial data processing. In addition, we
contribute a new benchmark specifically designed to evaluate the LLM-based
approach in geospatial tasks. This benchmark leverages a variety of Python
libraries and includes both single-turn and multi-turn tasks such as data
acquisition, data analysis, and visualization. By offering a comprehensive
evaluation among diverse geospatial contexts, this benchmark sets a new
standard for developing LLM-based approaches in geospatial data analysis tasks.
Our findings suggest that relying solely on knowledge of LLM is insufficient
for accurate geospatial task programming, which requires coherent multi-step
processes and multiple function calls. Compared to the baseline LLMs, the
proposed GeoAgent has demonstrated superior performance, yielding notable
improvements in function calls and task completion. In addition, these results
offer valuable insights for the future development of LLM agents in automatic
geospatial data analysis task programming.",Yuxing Chen
2024-10-26T19:48:47Z,http://arxiv.org/abs/2410.20263v1,"EfficientEQA: An Efficient Approach for Open Vocabulary Embodied
  Question Answering","Embodied Question Answering (EQA) is an essential yet challenging task for
robotic home assistants. Recent studies have shown that large vision-language
models (VLMs) can be effectively utilized for EQA, but existing works either
focus on video-based question answering without embodied exploration or rely on
closed-form choice sets. In real-world scenarios, a robotic agent must
efficiently explore and accurately answer questions in open-vocabulary
settings. To address these challenges, we propose a novel framework called
EfficientEQA for open-vocabulary EQA, which enables efficient exploration and
accurate answering. In EfficientEQA, the robot actively explores unknown
environments using Semantic-Value-Weighted Frontier Exploration, a strategy
that prioritizes exploration based on semantic importance provided by
calibrated confidence from black-box VLMs to quickly gather relevant
information. To generate accurate answers, we employ Retrieval-Augmented
Generation (RAG), which utilizes BLIP to retrieve useful images from
accumulated observations and VLM reasoning to produce responses without relying
on predefined answer choices. Additionally, we detect observations that are
highly relevant to the question as outliers, allowing the robot to determine
when it has sufficient information to stop exploring and provide an answer.
Experimental results demonstrate the effectiveness of our approach, showing an
improvement in answering accuracy by over 15% and efficiency, measured in
running steps, by over 20% compared to state-of-the-art methods.",Kai Cheng
2024-10-30T13:22:22Z,http://arxiv.org/abs/2410.22996v1,"Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A
  Knowledge Graph Generation Approach","A well structured collection of the various Quantum Cascade Laser (QCL)
design and working properties data provides a platform to analyze and
understand the relationships between these properties. By analyzing these
relationships, we can gain insights into how different design features impact
laser performance properties such as the working temperature. Most of these QCL
properties are captured in scientific text. There is therefore need for
efficient methodologies that can be utilized to extract QCL properties from
text and generate a semantically enriched and interlinked platform where the
properties can be analyzed to uncover hidden relations. There is also the need
to maintain provenance and reference information on which these properties are
based. Semantic Web technologies such as Ontologies and Knowledge Graphs have
proven capability in providing interlinked data platforms for knowledge
representation in various domains. In this paper, we propose an approach for
generating a QCL properties Knowledge Graph (KG) from text for semantic
enrichment of the properties. The approach is based on the QCL ontology and a
Retrieval Augmented Generation (RAG) enabled information extraction pipeline
based on GPT 4-Turbo language model. The properties of interest include:
working temperature, laser design type, lasing frequency, laser optical power
and the heterostructure. The experimental results demonstrate the feasibility
and effectiveness of this approach for efficiently extracting QCL properties
from unstructured text and generating a QCL properties Knowledge Graph, which
has potential applications in semantic enrichment and analysis of QCL data.",Deperias Kerre
2024-10-31T14:22:20Z,http://arxiv.org/abs/2410.23968v1,"EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable
  Robot Task Planning","Recent advances in Large Language Models (LLMs) have helped facilitate
exciting progress for robotic planning in real, open-world environments. 3D
scene graphs (3DSGs) offer a promising environment representation for grounding
such LLM-based planners as they are compact and semantically rich. However, as
the robot's environment scales (e.g., number of entities tracked) and the
complexity of scene graph information increases (e.g., maintaining more
attributes), providing the 3DSG as-is to an LLM-based planner quickly becomes
infeasible due to input token count limits and attentional biases present in
LLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods
that retrieve query-relevant document chunks for LLM question and answering, we
adapt the paradigm for our embodied domain. Specifically, we propose a 3D scene
subgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based
planner with for executing natural language robotic tasks. Notably, our
retrieved subgraphs adapt to changes in the environment as well as changes in
task-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's
ability to significantly reduce input token counts (by an order of magnitude)
and planning time (up to 70% reduction in average time per planning step) while
improving success rates on AI2Thor simulated household tasks with a single-arm,
mobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with
a manipulator to highlight the performance benefits for robot deployment at the
edge in real environments.",Meghan Booker
2024-11-04T05:25:39Z,http://arxiv.org/abs/2411.01807v1,Can Language Models Enable In-Context Database?,"Large language models (LLMs) are emerging as few-shot learners capable of
handling a variety of tasks, including comprehension, planning, reasoning,
question answering, arithmetic calculations, and more. At the core of these
capabilities is LLMs' proficiency in representing and understanding structural
or semi-structural data, such as tables and graphs. Numerous studies have
demonstrated that reasoning on tabular data or graphs is not only feasible for
LLMs but also gives a promising research direction which treats these data as
in-context data. The lightweight and human readable characteristics of
in-context database can potentially make it an alternative for the traditional
database in typical RAG (Retrieval Augmented Generation) settings. However,
almost all current work focuses on static in-context data, which does not allow
dynamic update. In this paper, to enable dynamic database update, delta
encoding of database is proposed. We explore how data stored in traditional
RDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD
(Create, Read, Update and Delete) operations on in-context databases. A
benchmark named InConDB is presented and extensive experiments are conducted to
show the performance of different language models in enabling in-context
database by varying the database encoding method, prompting method, operation
type and input data distribution, revealing both the proficiency and
limitations.",Yu Pan
2024-10-18T05:23:39Z,http://arxiv.org/abs/2411.02404v1,"Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative
  Mining","Ranking consistently emerges as a primary focus in information retrieval
research. Retrieval and ranking models serve as the foundation for numerous
applications, including web search, open domain QA, enterprise domain QA, and
text-based recommender systems. Typically, these models undergo training on
triplets consisting of binary relevance assignments, comprising one positive
and one negative passage. However, their utilization involves a context where a
significantly more nuanced understanding of relevance is necessary, especially
when re-ranking a large pool of potentially relevant passages. Although
collecting positive examples through user feedback like impressions or clicks
is straightforward, identifying suitable negative pairs from a vast pool of
possibly millions or even billions of documents possess a greater challenge.
Generating a substantial number of negative pairs is often necessary to
maintain the high quality of the model. Several approaches have been suggested
in literature to tackle the issue of selecting suitable negative pairs from an
extensive corpus. This study focuses on explaining the crucial role of hard
negatives in the training process of cross-encoder models, specifically aiming
to explain the performance gains observed with hard negative sampling compared
to random sampling. We have developed a robust hard negative mining technique
for efficient training of cross-encoder re-rank models on an enterprise dataset
which has domain specific context. We provide a novel perspective to enhance
retrieval models, ultimately influencing the performance of advanced LLM
systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action
Agents (ReAct). The proposed approach demonstrates that learning both
similarity and dissimilarity simultaneously with cross-encoders improves
performance of retrieval systems.",Hansa Meghwani
2024-11-04T22:45:52Z,http://arxiv.org/abs/2411.02657v1,"Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare
  Disease Knowledge","Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.",Karthik Soman
2024-11-04T00:01:34Z,http://arxiv.org/abs/2411.03349v1,RuAG: Learned-rule-augmented Generation for Large Language Models,"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have
gained attention for their ability to enhance LLMs' reasoning by incorporating
external knowledge but suffer from limited contextual window size, leading to
insufficient information injection. To this end, we propose a novel framework,
RuAG, to automatically distill large volumes of offline data into interpretable
first-order logic rules, which are injected into LLMs to boost their reasoning
capabilities. Our method begins by formulating the search process relying on
LLMs' commonsense, where LLMs automatically define head and body predicates.
Then, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational
searching space and efficiently discover logic rules from data. The resulting
logic rules are translated into natural language, allowing targeted knowledge
injection and seamless integration into LLM prompts for LLM's downstream task
reasoning. We evaluate our framework on public and private industrial tasks,
including natural language processing, time-series, decision-making, and
industrial tasks, demonstrating its effectiveness in enhancing LLM's capability
over diverse tasks.",Yudi Zhang
2024-11-07T21:10:39Z,http://arxiv.org/abs/2411.05185v1,PentestAgent: Incorporating LLM Agents to Automated Penetration Testing,"Penetration testing is a critical technique for identifying security
vulnerabilities, traditionally performed manually by skilled security
specialists. This complex process involves gathering information about the
target system, identifying entry points, exploiting the system, and reporting
findings. Despite its effectiveness, manual penetration testing is
time-consuming and expensive, often requiring significant expertise and
resources that many organizations cannot afford. While automated penetration
testing methods have been proposed, they often fall short in real-world
applications due to limitations in flexibility, adaptability, and
implementation.
  Recent advancements in large language models (LLMs) offer new opportunities
for enhancing penetration testing through increased intelligence and
automation. However, current LLM-based approaches still face significant
challenges, including limited penetration testing knowledge and a lack of
comprehensive automation capabilities. To address these gaps, we propose
PentestAgent, a novel LLM-based automated penetration testing framework that
leverages the power of LLMs and various LLM-based techniques like Retrieval
Augmented Generation (RAG) to enhance penetration testing knowledge and
automate various tasks. Our framework leverages multi-agent collaboration to
automate intelligence gathering, vulnerability analysis, and exploitation
stages, reducing manual intervention. We evaluate PentestAgent using a
comprehensive benchmark, demonstrating superior performance in task completion
and overall efficiency. This work significantly advances the practical
applicability of automated penetration testing systems.",Xiangmin Shen
2024-11-08T09:40:53Z,http://arxiv.org/abs/2411.05442v1,"IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge
  Delivery","In the rapidly evolving landscape of cyber security, intelligent chatbots are
gaining prominence. Artificial Intelligence, Machine Learning, and Natural
Language Processing empower these chatbots to handle user inquiries and deliver
threat intelligence. This helps cyber security knowledge readily available to
both professionals and the public. Traditional rule-based chatbots often lack
flexibility and struggle to adapt to user interactions. In contrast, Large
Language Model-based chatbots offer contextually relevant information across
multiple domains and adapt to evolving conversational contexts. In this work,
we develop IntellBot, an advanced cyber security Chatbot built on top of
cutting-edge technologies like Large Language Models and Langchain alongside a
Retrieval-Augmented Generation model to deliver superior capabilities. This
chatbot gathers information from diverse data sources to create a comprehensive
knowledge base covering known vulnerabilities, recent cyber attacks, and
emerging threats. It delivers tailored responses, serving as a primary hub for
cyber security insights. By providing instant access to relevant information
and resources, this IntellBot enhances threat intelligence, incident response,
and overall security posture, saving time and empowering users with knowledge
of cyber security best practices. Moreover, we analyzed the performance of our
copilot using a two-stage evaluation strategy. We achieved BERT score above 0.8
by indirect approach and a cosine similarity score ranging from 0.8 to 1, which
affirms the accuracy of our copilot. Additionally, we utilized RAGAS to
evaluate the RAG model, and all evaluation metrics consistently produced scores
above 0.77, highlighting the efficacy of our system.",Dincy R. Arikkat
2024-11-11T20:54:54Z,http://arxiv.org/abs/2411.07360v1,"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are
  We There Yet?","Hallucinations, the tendency to produce irrelevant/incorrect responses, are
prevalent concerns in generative AI-based tools like ChatGPT. Although
hallucinations in ChatGPT are studied for textual responses, it is unknown how
ChatGPT hallucinates for technical texts that contain both textual and
technical terms. We surveyed 47 software engineers and produced a benchmark of
412 Q&A pairs from the bug reports of two OSS projects. We find that a
RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is
36.4% correct when producing answers to the questions, due to two reasons 1)
limitations to understand complex technical contents in code snippets like
stack traces, and 2) limitations to integrate contexts denoted in the technical
terms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose
underlying principle is that if we can preprocess the technical reports better
and guide the query validation process in ChatGPT, we can address the observed
limitations. CHIME uses context-free grammar (CFG) to parse stack traces in
technical reports. CHIME then verifies and fixes ChatGPT responses by applying
metamorphic testing and query transformation. In our benchmark, CHIME shows
30.3% more correction over ChatGPT responses. In a user study, we find that the
improved responses with CHIME are considered more useful than those generated
from ChatGPT without CHIME.",Salma Begum Tamanna
2024-11-12T10:12:12Z,http://arxiv.org/abs/2411.07688v1,"Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with
  ImageRAG","Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000
$\times$ 100,000 pixels or more) poses a significant challenge for current
Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize
the UHR image to standard input image size, the extensive spatial and
contextual information that UHR images contain will be neglected. Otherwise,
the original size of these images often exceeds the token limits of standard
RSMLLMs, making it difficult to process the entire image and capture long-range
dependencies to answer the query based on the abundant visual context. In this
paper, we introduce ImageRAG for RS, a training-free framework to address the
complexities of analyzing UHR remote sensing imagery. By transforming UHR
remote sensing image analysis task to image's long context selection task, we
design an innovative image contextual retrieval mechanism based on the
Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's
core innovation lies in its ability to selectively retrieve and focus on the
most relevant portions of the UHR image as visual contexts that pertain to a
given query. Fast path and slow path are proposed in this framework to handle
this task efficiently and effectively. ImageRAG allows RSMLLMs to manage
extensive context and spatial information from UHR RSI, ensuring the analysis
is both accurate and efficient.",Zilun Zhang
2024-10-29T07:25:30Z,http://arxiv.org/abs/2411.08041v1,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured
and unstructured data is a major challenge in many real-world applications.
Pattern matching and querying represent fundamental tasks in modern data
analytics that leverage this curated knowledge. The development of such
applications necessitates overcoming several research challenges, including
data extraction, named entity recognition, data modeling, and designing query
interfaces. Moreover, the explainability of these functionalities is critical
for their broader adoption.
  The emergence of Large Language Models (LLMs) has accelerated the development
lifecycle of new capabilities. Nonetheless, there is an ongoing need for
domain-specific tools tailored to user activities. The creation of digital
assistants has gained considerable traction in recent years, with LLMs offering
a promising avenue to develop such assistants utilizing domain-specific
knowledge and assumptions.
  In this context, we introduce an advanced query and reasoning system,
GraphAide, which constructs a knowledge graph (KG) from diverse sources and
allows to query and reason over the resulting KG. GraphAide harnesses both the
KG and LLMs to rapidly develop domain-specific digital assistants. It
integrates design patterns from retrieval augmented generation (RAG) and the
semantic web to create an agentic LLM application. GraphAide underscores the
potential for streamlined and efficient development of specialized digital
assistants, thereby enhancing their applicability across various domains.",Sumit Purohit
2024-11-13T01:12:35Z,http://arxiv.org/abs/2411.08275v1,"A Large-Scale Study of Relevance Assessments with Large Language Models:
  An Initial Look","The application of large language models to provide relevance assessments
presents exciting opportunities to advance information retrieval, natural
language processing, and beyond, but to date many unknowns remain. This paper
reports on the results of a large-scale evaluation (the TREC 2024 RAG Track)
where four different relevance assessment approaches were deployed in situ: the
""standard"" fully manual process that NIST has implemented for decades and three
different alternatives that take advantage of LLMs to different extents using
the open-source UMBRELA tool. This setup allows us to correlate system rankings
induced by the different approaches to characterize tradeoffs between cost and
quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system
rankings induced by automatically generated relevance assessments from UMBRELA
correlate highly with those induced by fully manual assessments across a
diverse set of 77 runs from 19 teams. Our results suggest that automatically
generated UMBRELA judgments can replace fully manual judgments to accurately
capture run-level effectiveness. Surprisingly, we find that LLM assistance does
not appear to increase correlation with fully manual assessments, suggesting
that costs associated with human-in-the-loop processes do not bring obvious
tangible benefits. Overall, human assessors appear to be stricter than UMBRELA
in applying relevance criteria. Our work validates the use of LLMs in academic
TREC-style evaluations and provides the foundation for future studies.",Shivani Upadhyay
2024-11-16T20:18:57Z,http://arxiv.org/abs/2411.10878v1,"Empowering Meta-Analysis: Leveraging Large Language Models for
  Scientific Synthesis","This study investigates the automation of meta-analysis in scientific
documents using large language models (LLMs). Meta-analysis is a robust
statistical method that synthesizes the findings of multiple studies support
articles to provide a comprehensive understanding. We know that a meta-article
provides a structured analysis of several articles. However, conducting
meta-analysis by hand is labor-intensive, time-consuming, and susceptible to
human error, highlighting the need for automated pipelines to streamline the
process. Our research introduces a novel approach that fine-tunes the LLM on
extensive scientific datasets to address challenges in big data handling and
structured data extraction. We automate and optimize the meta-analysis process
by integrating Retrieval Augmented Generation (RAG). Tailored through prompt
engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for
fine-tuning on large contextual datasets, LLMs efficiently generate structured
meta-analysis content. Human evaluation then assesses relevance and provides
information on model performance in key metrics. This research demonstrates
that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs
generating 87.6% relevant meta-analysis abstracts. The relevance of the
context, based on human evaluation, shows a reduction in irrelevancy from 4.56%
to 1.9%. These experiments were conducted in a low-resource environment,
highlighting the study's contribution to enhancing the efficiency and
reliability of meta-analysis automation.",Jawad Ibn Ahad
2024-11-17T00:09:04Z,http://arxiv.org/abs/2411.10918v1,"LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems
  Anomaly Detection","Modern industrial infrastructures rely heavily on Cyber-Physical Systems
(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic
effects. To reduce these risks, anomaly detection methods based on physical
invariants have been developed. However, these methods often require
domain-specific expertise to manually define invariants, making them costly and
difficult to scale. To address this limitation, we propose a novel approach to
extract physical invariants from CPS testbeds for anomaly detection. Our
insight is that CPS design documentation often contains semantically rich
descriptions of physical procedures, which can profile inter-correlated
dynamics among system components. Leveraging the built-in physics and
engineering knowledge of recent generative AI models, we aim to automate this
traditionally manual process, improving scalability and reducing costs. This
work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)
workflow with a customized prompting system tailored for CPS documentation,
enabling accurate extraction of semantic information and inference of physical
invariants from complex, multimodal content. Then, rather than directly
applying the inferred invariants for anomaly detection, we introduce an
innovative statistics-based learning approach that integrates these invariants
into the training dataset. This method addresses limitations such as
hallucination and concept drift, enhancing the reliability of the model. We
evaluate our approach on real-world public CPS security dataset which contains
86 data points and 58 attacking cases. The results show that our approach
achieves a high precision of 0.923, accurately detecting anomalies while
minimizing false alarms.",Danial Abshari
2024-11-17T23:20:37Z,http://arxiv.org/abs/2411.11913v1,"On-Board Vision-Language Models for Personalized Autonomous Vehicle
  Motion Control: System Design and Real-World Validation","Personalized driving refers to an autonomous vehicle's ability to adapt its
driving behavior or control strategies to match individual users' preferences
and driving styles while maintaining safety and comfort standards. However,
existing works either fail to capture every individual preference precisely or
become computationally inefficient as the user base expands. Vision-Language
Models (VLMs) offer promising solutions to this front through their natural
language understanding and scene reasoning capabilities. In this work, we
propose a lightweight yet effective on-board VLM framework that provides
low-latency personalized driving performance while maintaining strong reasoning
capabilities. Our solution incorporates a Retrieval-Augmented Generation
(RAG)-based memory module that enables continuous learning of individual
driving preferences through human feedback. Through comprehensive real-world
vehicle deployment and experiments, our system has demonstrated the ability to
provide safe, comfortable, and personalized driving experiences across various
scenarios and significantly reduce takeover rates by up to 76.9%. To the best
of our knowledge, this work represents the first end-to-end VLM-based motion
control system in real-world autonomous vehicles.",Can Cui
2024-11-20T10:17:09Z,http://arxiv.org/abs/2411.13173v2,"Writing Style Matters: An Examination of Bias and Fairness in
  Information Retrieval Systems","The rapid advancement of Language Model technologies has opened new
opportunities, but also introduced new challenges related to bias and fairness.
This paper explores the uncharted territory of potential biases in
state-of-the-art universal text embedding models towards specific document and
query writing styles within Information Retrieval (IR) systems. Our
investigation reveals that different embedding models exhibit different
preferences of document writing style, while more informal and emotive styles
are less favored by most embedding models. In terms of query writing styles,
many embedding models tend to match the style of the query with the style of
the retrieved documents, but some show a consistent preference for specific
styles. Text embedding models fine-tuned on synthetic data generated by LLMs
display a consistent preference for certain style of generated data. These
biases in text embedding based IR systems can inadvertently silence or
marginalize certain communication styles, thereby posing a significant threat
to fairness in information retrieval. Finally, we also compare the answer
styles of Retrieval Augmented Generation (RAG) systems based on different LLMs
and find out that most text embedding models are biased towards LLM's answer
styles when used as evaluation metrics for answer correctness. This study sheds
light on the critical issue of writing style based bias in IR systems, offering
valuable insights for the development of more fair and robust models.",Hongliu Cao
2024-11-21T15:28:52Z,http://arxiv.org/abs/2411.14219v1,"Towards Context-Rich Automated Biodiversity Assessments: Deriving
  AI-Powered Insights from Camera Trap Data","Camera traps offer enormous new opportunities in ecological studies, but
current automated image analysis methods often lack the contextual richness
needed to support impactful conservation outcomes. Here we present an
integrated approach that combines deep learning-based vision and language
models to improve ecological reporting using data from camera traps. We
introduce a two-stage system: YOLOv10-X to localise and classify species
(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read
YOLOv10-X binding box labels to identify species, overcoming its limitation
with hard to classify objects in images. Additionally, Phi-3.5 detects broader
variables, such as vegetation type, and time of day, providing rich ecological
and environmental context to YOLO's species detection output. When combined,
this output is processed by the model's natural language system to answer
complex queries, and retrieval-augmented generation (RAG) is employed to enrich
responses with external information, like species weight and IUCN status
(information that cannot be obtained through direct visual analysis). This
information is used to automatically generate structured reports, providing
biodiversity stakeholders with deeper insights into, for example, species
abundance, distribution, animal behaviour, and habitat selection. Our approach
delivers contextually rich narratives that aid in wildlife management
decisions. By providing contextually rich insights, our approach not only
reduces manual effort but also supports timely decision-making in conservation,
potentially shifting efforts from reactive to proactive management.",Paul Fergus
2024-11-21T16:28:32Z,http://arxiv.org/abs/2411.14272v1,"Efficient Aspect-Based Summarization of Climate Change Reports with
  Small Language Models","The use of Natural Language Processing (NLP) for helping decision-makers with
Climate Change action has recently been highlighted as a use case aligning with
a broader drive towards NLP technologies for social good. In this context,
Aspect-Based Summarization (ABS) systems that extract and summarize relevant
information are particularly useful as they provide stakeholders with a
convenient way of finding relevant information in expert-curated reports. In
this work, we release a new dataset for ABS of Climate Change reports and we
employ different Large Language Models (LLMs) and so-called Small Language
Models (SLMs) to tackle this problem in an unsupervised way. Considering the
problem at hand, we also show how SLMs are not significantly worse for the
problem while leading to reduced carbon footprint; we do so by applying for the
first time an existing framework considering both energy efficiency and task
performance to the evaluation of zero-shot generative models for ABS. Overall,
our results show that modern language models, both big and small, can
effectively tackle ABS for Climate Change reports but more research is needed
when we frame the problem as a Retrieval Augmented Generation (RAG) problem and
our work and dataset will help foster efforts in this direction.",Iacopo Ghinassi
2024-11-22T16:15:50Z,http://arxiv.org/abs/2411.15041v1,"mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for
  Knowledge-Based VQA","Advanced Multimodal Large Language Models (MLLMs) struggle with recent
Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their
limited and frozen knowledge scope, often leading to ambiguous and inaccurate
responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally
introduced to provide MLLMs with comprehensive and up-to-date knowledge,
effectively expanding the knowledge scope. However, current mRAG methods have
inherent drawbacks, including: 1) Performing retrieval even when external
knowledge is not needed. 2) Lacking of identification of evidence that supports
the query. 3) Increasing model complexity due to additional information
filtering modules or rules. To address these shortcomings, we propose a novel
generalized framework called \textbf{m}ultimodal
\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration
(mR$^2$AG), which achieves adaptive retrieval and useful information
localization to enable answers through two easy-to-implement reflection
operations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection
is designed to distinguish different user queries and avoids redundant
retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in
locating beneficial evidence of the retrieved content and generating answers
accordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM
with efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset
(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,
GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while
maintaining the exceptional capabilities of base MLLMs across a wide range of
Visual-dependent tasks.",Tao Zhang
2024-11-23T18:14:42Z,http://arxiv.org/abs/2411.16740v3,"Document Haystacks: Vision-Language Reasoning Over Piles of 1000+
  Documents","Large multimodal models (LMMs) have achieved impressive progress in
vision-language understanding, yet they face limitations in real-world
applications requiring complex reasoning over a large number of images.
Existing benchmarks for multi-image question-answering are limited in scope,
each question is paired with only up to 30 images, which does not fully capture
the demands of large-scale retrieval tasks encountered in the real-world
usages. To reduce these gaps, we introduce two document haystack benchmarks,
dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on
large-scale visual document retrieval and understanding. Additionally, we
propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)
framework that leverages a suite of multimodal vision encoders, each optimized
for specific strengths, and a dedicated question-document relevance module.
V-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the
challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,
compared to the previous best baseline models. Additionally, integrating V-RAG
with LMMs enables them to efficiently operate across thousands of images,
yielding significant improvements on our DocHaystack and InfoHaystack
benchmarks. Our code and datasets are available at
https://github.com/Vision-CAIR/dochaystacks",Jun Chen
2024-11-29T14:01:34Z,http://arxiv.org/abs/2411.19713v2,"CantorNet: A Sandbox for Testing Geometrical and Topological Complexity
  Measures","Many natural phenomena are characterized by self-similarity, for example the
symmetry of human faces, or a repetitive motif of a song. Studying of such
symmetries will allow us to gain deeper insights into the underlying mechanisms
of complex systems. Recognizing the importance of understanding these patterns,
we propose a geometrically inspired framework to study such phenomena in
artificial neural networks. To this end, we introduce \emph{CantorNet},
inspired by the triadic construction of the Cantor set, which was introduced by
Georg Cantor in the $19^\text{th}$ century. In mathematics, the Cantor set is a
set of points lying on a single line that is self-similar and has a counter
intuitive property of being an uncountably infinite null set. Similarly, we
introduce CantorNet as a sandbox for studying self-similarity by means of novel
topological and geometrical complexity measures. CantorNet constitutes a family
of ReLU neural networks that spans the whole spectrum of possible Kolmogorov
complexities, including the two opposite descriptions (linear and exponential
as measured by the description length). CantorNet's decision boundaries can be
arbitrarily ragged, yet are analytically known. Besides serving as a testing
ground for complexity measures, our work may serve to illustrate potential
pitfalls in geometry-ignorant data augmentation techniques and adversarial
attacks.",Michal Lewandowski
2024-11-30T14:32:48Z,http://arxiv.org/abs/2412.00495v1,"Rethinking Strategic Mechanism Design In The Age Of Large Language
  Models: New Directions For Communication Systems","This paper explores the application of large language models (LLMs) in
designing strategic mechanisms -- including auctions, contracts, and games --
for specific purposes in communication networks. Traditionally, strategic
mechanism design in telecommunications has relied on human expertise to craft
solutions based on game theory, auction theory, and contract theory. However,
the evolving landscape of telecom networks, characterized by increasing
abstraction, emerging use cases, and novel value creation opportunities, calls
for more adaptive and efficient approaches. We propose leveraging LLMs to
automate or semi-automate the process of strategic mechanism design, from
intent specification to final formulation. This paradigm shift introduces both
semi-automated and fully-automated design pipelines, raising crucial questions
about faithfulness to intents, incentive compatibility, algorithmic stability,
and the balance between human oversight and artificial intelligence (AI)
autonomy. The paper discusses potential frameworks, such as retrieval-augmented
generation (RAG)-based systems, to implement LLM-driven mechanism design in
communication networks contexts. We examine key challenges, including LLM
limitations in capturing domain-specific constraints, ensuring strategy
proofness, and integrating with evolving telecom standards. By providing an
in-depth analysis of the synergies and tensions between LLMs and strategic
mechanism design within the IoT ecosystem, this work aims to stimulate
discussion on the future of AI-driven information economic mechanisms in
telecommunications and their potential to address complex, dynamic network
management scenarios.",Ismail Lotfi
2024-11-30T23:11:44Z,http://arxiv.org/abs/2412.00608v3,"Leveraging LLM for Automated Ontology Extraction and Knowledge Graph
  Generation","Extracting relevant and structured knowledge from large, complex technical
documents within the Reliability and Maintainability (RAM) domain is
labor-intensive and prone to errors. Our work addresses this challenge by
presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge
Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through
an interactive user interface guided by our adaptive iterative Chain of Thought
(CoT) algorithm to ensure that the ontology extraction process and, thus, KG
generation align with user-specific requirements. Although KG generation
follows a clear, structured path based on the confirmed ontology, there is no
universally correct ontology as it is inherently based on the user's
preferences. OntoKGen recommends an ontology grounded in best practices,
minimizing user effort and providing valuable insights that may have been
overlooked, all while giving the user complete control over the final ontology.
Having generated the KG based on the confirmed ontology, OntoKGen enables
seamless integration into schemeless, non-relational databases like Neo4j. This
integration allows for flexible storage and retrieval of knowledge from
diverse, unstructured sources, facilitating advanced querying, analysis, and
decision-making. Moreover, the generated KG serves as a robust foundation for
future integration into Retrieval Augmented Generation (RAG) systems, offering
enhanced capabilities for developing domain-specific intelligent applications.",Mohammad Sadeq Abolhasani
2024-12-01T15:19:23Z,http://arxiv.org/abs/2412.00846v1,"Improving Multimodal LLMs Ability In Geometry Problem Solving,
  Reasoning, And Multistep Scoring","This paper presents GPSM4K, a comprehensive geometry multimodal dataset
tailored to augment the problem-solving capabilities of Large Vision Language
Models (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs
manually extracted from mathematics textbooks spanning grades 7-12 and is
further augmented to 5340 problems, consisting of both numerical and
theorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which
feature only objective-type questions, GPSM4K offers detailed step-by-step
solutions in a consistent format, facilitating a comprehensive evaluation of
problem-solving approaches. This dataset serves as an excellent benchmark for
assessing the geometric reasoning capabilities of LVLMs. Evaluation of our test
set shows that there is scope for improvement needed in open-source language
models in geometry problem-solving. Finetuning on our training set increases
the geometry problem-solving capabilities of models. Further, We also evaluate
the effectiveness of techniques such as image captioning and Retrieval
Augmentation generation (RAG) on model performance. We leveraged LLM to
automate the task of final answer evaluation by providing ground truth and
predicted solutions. This research will help to assess and improve the
geometric reasoning capabilities of LVLMs.",Avinash Anand
2024-12-04T03:02:46Z,http://arxiv.org/abs/2412.02987v1,"Advancing Conversational Psychotherapy: Integrating Privacy,
  Dual-Memory, and Domain Expertise with Large Language Models","Mental health has increasingly become a global issue that reveals the
limitations of traditional conversational psychotherapy, constrained by
location, time, expense, and privacy concerns. In response to these challenges,
we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed
to democratize access to psychotherapy. SoulSpeak improves upon the
capabilities of standard LLM-enabled chatbots by incorporating a novel
dual-memory component that combines short-term and long-term context via
Retrieval Augmented Generation (RAG) to offer personalized responses while
ensuring the preservation of user privacy and intimacy through a dedicated
privacy module. In addition, it leverages a counseling chat dataset of
therapist-client interactions and various prompting techniques to align the
generated responses with psychotherapeutic methods. We introduce two fine-tuned
BERT models to evaluate the system against existing LLMs and human therapists:
the Conversational Psychotherapy Preference Model (CPPM) to simulate human
preference among responses and another to assess response relevance to user
input. CPPM is useful for training and evaluating psychotherapy-focused
language models independent from SoulSpeak, helping with the constrained
resources available for psychotherapy. Furthermore, the effectiveness of the
dual-memory component and the robustness of the privacy module are also
examined. Our findings highlight the potential and challenge of enhancing
mental health care by offering an alternative that combines the expertise of
traditional therapy with the advantages of LLMs, providing a promising way to
address the accessibility and personalization gap in current mental health
services.",XiuYu Zhang
2024-12-04T18:26:13Z,http://arxiv.org/abs/2412.03531v1,"A Review on Scientific Knowledge Extraction using Large Language Models
  in Biomedical Sciences","The rapid advancement of large language models (LLMs) has opened new
boundaries in the extraction and synthesis of medical knowledge, particularly
within evidence synthesis. This paper reviews the state-of-the-art applications
of LLMs in the biomedical domain, exploring their effectiveness in automating
complex tasks such as evidence synthesis and data extraction from a biomedical
corpus of documents. While LLMs demonstrate remarkable potential, significant
challenges remain, including issues related to hallucinations, contextual
understanding, and the ability to generalize across diverse medical tasks. We
highlight critical gaps in the current research literature, particularly the
need for unified benchmarks to standardize evaluations and ensure reliability
in real-world applications. In addition, we propose directions for future
research, emphasizing the integration of state-of-the-art techniques such as
retrieval-augmented generation (RAG) to enhance LLM performance in evidence
synthesis. By addressing these challenges and utilizing the strengths of LLMs,
we aim to improve access to medical literature and facilitate meaningful
discoveries in healthcare.",Gabriel Lino Garcia
2024-12-06T17:07:27Z,http://arxiv.org/abs/2412.05187v1,SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot,"Surgical interventions, particularly in neurology, represent complex and
high-stakes scenarios that impose substantial cognitive burdens on surgical
teams. Although deliberate education and practice can enhance cognitive
capabilities, surgical training opportunities remain limited due to patient
safety concerns. To address these cognitive challenges in surgical training and
operation, we propose SurgBox, an agent-driven sandbox framework to
systematically enhance the cognitive capabilities of surgeons in immersive
surgical simulations. Specifically, our SurgBox leverages large language models
(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically
replicate various surgical roles, enabling realistic training environments for
deliberate practice. In particular, we devise Surgery Copilot, an AI-driven
assistant to actively coordinate the surgical information stream and support
clinical decision-making, thereby diminishing the cognitive workload of
surgical teams during surgery. By incorporating a novel Long-Short Memory
mechanism, our Surgery Copilot can effectively balance immediate procedural
assistance with comprehensive surgical knowledge. Extensive experiments using
real neurosurgical procedure records validate our SurgBox framework in both
enhancing surgical cognitive capabilities and supporting clinical
decision-making. By providing an integrated solution for training and
operational support to address cognitive challenges, our SurgBox framework
advances surgical education and practice, potentially transforming surgical
outcomes and healthcare quality. The code is available at
https://github.com/franciszchen/SurgBox.",Jinlin Wu
2024-12-06T19:00:15Z,http://arxiv.org/abs/2412.05366v1,"ExploraCoder: Advancing code generation for multiple unseen APIs via
  planning and chained exploration","Through training on publicly available source code libraries, large language
models (LLMs) can invoke multiple encapsulated APIs to solve complex
programming problems. However, existing models inherently cannot generalize to
use APIs that are unseen in their training corpora. As libraries continuously
evolve, it becomes impractical to exhaustively retrain LLMs with new API
knowledge. This limitation hampers LLMs from solving problems which require
newly introduced or privately maintained libraries. Human programmers often
explore unfamiliar APIs by writing experimental code before invoking them for a
more complex problem. Inspired by this behavior, we propose , a training-free
framework that empowers LLMs to invoke multiple unseen APIs in code solution by
(1) planning a complex problem into several API invocation subtasks, and (2)
exploring correct API usage through a novel chain-of-API-exploration.
Concretely, ExploraCoder guides the LLM to iteratively generate several
experimental API invocations for each simple subtask, where the promising
execution experience are exploited by subsequent subtasks. This forms a chained
exploration trace that ultimately guides LLM in generating the final solution.
We evaluate ExploraCoder on Torchdata-Github benchmark as well as a newly
constructed benchmark that involves more complex API interactions. Experimental
results demonstrate that ExploraCoder significantly improves performance for
models lacking prior API knowledge, achieving an absolute increase of 11.24%
over niave RAG approaches and 14.07% over pretraining methods in pass@10.
Moreover, the integration of a self-debug mechanism further boosts
ExploraCoder's performance on more challenging tasks. Comprehensive ablation
and case studies provide further insights into the effectiveness of
ExploraCoder.",Yunkun Wang
2024-12-07T08:50:24Z,http://arxiv.org/abs/2412.05587v2,"GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on
  the Google Earth Engine Platform Powered by Large Language Models","As the scale and complexity of spatiotemporal data continue to grow rapidly,
the use of geospatial modeling on the Google Earth Engine (GEE) platform
presents dual challenges: improving the coding efficiency of domain experts and
enhancing the coding capabilities of interdisciplinary users. To address these
challenges and improve the performance of large language models (LLMs) in
geospatial code generation tasks, we propose a framework for building a
geospatial operator knowledge base tailored to the GEE JavaScript API. This
framework consists of an operator syntax knowledge table, an operator
relationship frequency table, an operator frequent pattern knowledge table, and
an operator relationship chain knowledge table. By leveraging Abstract Syntax
Tree (AST) techniques and frequent itemset mining, we systematically extract
operator knowledge from 185,236 real GEE scripts and syntax documentation,
forming a structured knowledge base. Experimental results demonstrate that the
framework achieves over 90% accuracy, recall, and F1 score in operator
knowledge extraction. When integrated with the Retrieval-Augmented Generation
(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge
base improves performance by 20-30%. Ablation studies further quantify the
necessity of each knowledge table in the knowledge base construction. This work
provides robust support for the advancement and application of geospatial code
modeling techniques, offering an innovative approach to constructing
domain-specific knowledge bases that enhance the code generation capabilities
of LLMs, and fostering the deeper integration of generative AI technologies
within the field of geoinformatics.",Shuyang Hou
2024-12-08T23:00:06Z,http://arxiv.org/abs/2412.06099v1,DECO: Life-Cycle Management of Enterprise-Grade Chatbots,"Software engineers frequently grapple with the challenge of accessing
disparate documentation and telemetry data, including Troubleshooting Guides
(TSGs), incident reports, code repositories, and various internal tools
developed by multiple stakeholders. While on-call duties are inevitable,
incident resolution becomes even more daunting due to the obscurity of legacy
sources and the pressures of strict time constraints. To enhance the efficiency
of on-call engineers (OCEs) and streamline their daily workflows, we introduced
DECO -- a comprehensive framework for developing, deploying, and managing
enterprise-grade chatbots tailored to improve productivity in engineering
routines. This paper details the design and implementation of the DECO
framework, emphasizing its innovative NL2SearchQuery functionality and a
hierarchical planner. These features support efficient and customized
retrieval-augmented-generation (RAG) algorithms that not only extract relevant
information from diverse sources but also select the most pertinent toolkits in
response to user queries. This enables the addressing of complex technical
questions and provides seamless, automated access to internal resources.
Additionally, DECO incorporates a robust mechanism for converting unstructured
incident logs into user-friendly, structured guides, effectively bridging the
documentation gap. Feedback from users underscores DECO's pivotal role in
simplifying complex engineering tasks, accelerating incident resolution, and
bolstering organizational productivity. Since its launch in September 2023,
DECO has demonstrated its effectiveness through extensive engagement, with tens
of thousands of interactions from hundreds of active users across multiple
organizations within the company.",Yiwen Zhu
2024-12-06T21:17:47Z,http://arxiv.org/abs/2412.06827v1,"Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning
  with Human-AI Feedback","Large Language Models (LLMs) have demonstrated strong capabilities in
text-based tasks but struggle with the complex reasoning required for physics
problems, particularly in advanced arithmetic and conceptual understanding.
While some research has explored ways to enhance LLMs in physics education
using techniques such as prompt engineering and Retrieval Augmentation
Generation (RAG), not enough effort has been made in addressing their
limitations in physics reasoning. This paper presents a novel approach to
improving LLM performance on physics questions using Reinforcement Learning
with Human and Artificial Intelligence Feedback (RLHAIF). We evaluate several
reinforcement learning methods, including Proximal Policy Optimization (PPO),
Direct Preference Optimization (DPO), and Remax optimization. These methods are
chosen to investigate RL policy performance with different settings on the
PhyQA dataset, which includes challenging physics problems from high school
textbooks. Our RLHAIF model, tested on leading LLMs like LLaMA2 and Mistral,
achieved superior results, notably with the MISTRAL-PPO model, demonstrating
marked improvements in reasoning and accuracy. It achieved high scores, with a
58.67 METEOR score and a 0.74 Reasoning score, making it a strong example for
future physics reasoning research in this area.",Avinash Anand
2024-12-10T16:05:56Z,http://arxiv.org/abs/2412.07626v1,"OmniDocBench: Benchmarking Diverse PDF Document Parsing with
  Comprehensive Annotations","Document content extraction is crucial in computer vision, especially for
meeting the high-quality data needs of large language models (LLMs) and
retrieval-augmented generation (RAG) technologies. However, current document
parsing methods suffer from significant limitations in terms of diversity and
comprehensive evaluation. To address these challenges, we introduce
OmniDocBench, a novel multi-source benchmark designed to advance automated
document content extraction. OmniDocBench includes a meticulously curated and
annotated high-quality evaluation dataset comprising nine diverse document
types, such as academic papers, textbooks, slides, among others. Our benchmark
provides a flexible and comprehensive evaluation framework with 19 layout
category labels and 14 attribute labels, enabling multi-level assessments
across entire datasets, individual modules, or specific data types. Using
OmniDocBench, we perform an exhaustive comparative analysis of existing modular
pipelines and multimodal end-to-end methods, highlighting their limitations in
handling document diversity and ensuring fair evaluation. OmniDocBench
establishes a robust, diverse, and fair evaluation standard for the document
content extraction field, offering crucial insights for future advancements and
fostering the development of document parsing technologies. The codes and
dataset is available in https://github.com/opendatalab/OmniDocBench.",Linke Ouyang
2024-12-10T17:20:47Z,http://arxiv.org/abs/2412.07687v1,"Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions","The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.",Anant Prakash Awasthi
2024-12-11T03:00:24Z,http://arxiv.org/abs/2412.08054v1,Federated In-Context LLM Agent Learning,"Large Language Models (LLMs) have revolutionized intelligent services by
enabling logical reasoning, tool use, and interaction with external systems as
agents. The advancement of LLMs is frequently hindered by the scarcity of
high-quality data, much of which is inherently sensitive. Federated learning
(FL) offers a potential solution by facilitating the collaborative training of
distributed LLMs while safeguarding private data. However, FL frameworks face
significant bandwidth and computational demands, along with challenges from
heterogeneous data distributions. The emerging in-context learning capability
of LLMs offers a promising approach by aggregating natural language rather than
bulky model parameters. Yet, this method risks privacy leakage, as it
necessitates the collection and presentation of data samples from various
clients during aggregation. In this paper, we propose a novel
privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,
which to our best knowledge for the first work unleashes the power of
in-context learning to train diverse LLM agents through FL. In our design,
knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums
Generation (KCG) module are transmitted between clients and the server instead
of model parameters in previous FL methods. Apart from that, an incredible
Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)
module is designed and we incorporate the aggregated global knowledge
compendium as a teacher to teach LLM agents the usage of tools. We conducted
extensive experiments and the results show that FICAL has competitive
performance compared to other SOTA baselines with a significant communication
cost decrease of $\mathbf{3.33\times10^5}$ times.",Panlong Wu
2024-12-13T07:51:32Z,http://arxiv.org/abs/2412.09936v1,"CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven
  Visual Language Models","The obesity phenomenon, known as the heavy issue, is a leading cause of
preventable chronic diseases worldwide. Traditional calorie estimation tools
often rely on specific data formats or complex pipelines, limiting their
practicality in real-world scenarios. Recently, vision-language models (VLMs)
have excelled in understanding real-world contexts and enabling conversational
interactions, making them ideal for downstream tasks such as ingredient
analysis. However, applying VLMs to calorie estimation requires domain-specific
data and alignment strategies. To this end, we curated CalData, a 330K
image-text pair dataset tailored for ingredient recognition and calorie
estimation, combining a large-scale recipe dataset with detailed nutritional
instructions for robust vision-language training. Built upon this dataset, we
present CaLoRAify, a novel VLM framework aligning ingredient recognition and
calorie estimation via training with visual-text pairs. During inference, users
only need a single monocular food image to estimate calories while retaining
the flexibility of agent-based conversational interaction. With Low-rank
Adaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our
system enhances the performance of foundational VLMs in the vertical domain of
calorie estimation. Our code and data are fully open-sourced at
https://github.com/KennyYao2001/16824-CaLORAify.",Dongyu Yao
2024-12-15T04:51:30Z,http://arxiv.org/abs/2412.11050v1,"RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous
  Driving with Vision-Language Models","Understanding and addressing corner cases is essential for ensuring the
safety and reliability of autonomous driving systems. Vision-Language Models
(VLMs) play a crucial role in enhancing scenario comprehension, yet they face
significant challenges, such as hallucination and insufficient real-world
grounding, which compromise their performance in critical driving scenarios. In
this work, we propose RAC3, a novel framework designed to improve VLMs' ability
to handle corner cases effectively. The framework integrates
Retrieval-Augmented Generation (RAG) to mitigate hallucination by dynamically
incorporating context-specific external knowledge. A cornerstone of RAC3 is its
cross-modal alignment fine-tuning, which utilizes contrastive learning to embed
image-text pairs into a unified semantic space, enabling robust retrieval of
similar scenarios. We evaluate RAC3 through extensive experiments using a
curated dataset of corner case scenarios, demonstrating its ability to enhance
semantic alignment, improve hallucination mitigation, and achieve superior
performance metrics, such as Cosine Similarity and ROUGE-L scores. For example,
for the LLaVA-v1.6-34B VLM, the cosine similarity between the generated text
and the reference text has increased by 5.22\%. The F1-score in ROUGE-L has
increased by 39.91\%, the Precision has increased by 55.80\%, and the Recall
has increased by 13.74\%. This work underscores the potential of
retrieval-augmented VLMs to advance the robustness and safety of autonomous
driving in complex environments.",Yujin Wang
2024-12-16T17:32:38Z,http://arxiv.org/abs/2412.12006v2,"Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A
  Novel Weighted Retrieval-Augmented Generation Paradigm","Technical troubleshooting in enterprise environments often involves
navigating diverse, heterogeneous data sources to resolve complex issues
effectively. This paper presents a novel agentic AI solution built on a
Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise
technical troubleshooting. By dynamically weighting retrieval sources such as
product manuals, internal knowledge bases, FAQs, and troubleshooting guides
based on query context, the framework prioritizes the most relevant data. For
instance, it gives precedence to product manuals for SKU-specific queries while
incorporating general FAQs for broader issues. The system employs FAISS for
efficient dense vector search, coupled with a dynamic aggregation mechanism to
seamlessly integrate results from multiple sources. A Llama-based
self-evaluator ensures the contextual accuracy and confidence of the generated
responses before delivering them. This iterative cycle of retrieval and
validation enhances precision, diversity, and reliability in response
generation. Preliminary evaluations on large enterprise datasets demonstrate
the framework's efficacy in improving troubleshooting accuracy, reducing
resolution times, and adapting to varied technical challenges. Future research
aims to enhance the framework by integrating advanced conversational AI
capabilities, enabling more interactive and intuitive troubleshooting
experiences. Efforts will also focus on refining the dynamic weighting
mechanism through reinforcement learning to further optimize the relevance and
precision of retrieved information. By incorporating these advancements, the
proposed framework is poised to evolve into a comprehensive, autonomous AI
solution, redefining technical service workflows across enterprise settings.",Rajat Khanda
2024-12-18T10:33:55Z,http://arxiv.org/abs/2412.13693v2,A2H: A UI Converter from Android to HarmonyOS Platform,"With the growing importance of smartphones, developers face the challenge of
creating separate applications for multiple platforms (e.g., Android, iOS, and
HarmonyOS), leading to increased development costs and longer iteration cycles.
One potential solution is to develop an app on one platform and then
automatically convert it to other platforms, reducing the need for separate
development efforts. However, migrating user interfaces (UIs) between platforms
is particularly challenging due to significant differences in layout structures
and development paradigms, such as the disparity between XML layout files in
Android and ArkUI framework in HarmonyOS. Manual conversion of UIs is
time-consuming, error-prone, and inefficient, necessitating an automated
solution to streamline the process and enable seamless migration from Android
to HarmonyOS. To address this challenge, we propose the A2H Converter, an
automated tool for migrating Android UIs to HarmonyOS. The tool employs an
large language model (LLM)-driven multi-agent framework to convert Android XML
layouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision
rules, the system maps Android UI components to ArkUI equivalents, while a
reflective mechanism continuously improves conversion accuracy. A2H Converter
handles project-level layouts, ensuring consistency across multiple files and
addressing complex UI logic. Experiments on six Android applications collected
from GitHub demonstrate that our A2H Converter achieves a migration success
rate of over 90.1%, 89.3%, and 89.2% at the component, page, and project
levels, respectively. The demo video is available at. The tool is available at
http://124.70.54.129:37860/.",Chen Wang
2024-12-18T12:11:39Z,http://arxiv.org/abs/2412.13774v1,Designing an LLM-Based Copilot for Manufacturing Equipment Selection,"Effective decision-making in automation equipment selection is critical for
reducing ramp-up time and maintaining production quality, especially in the
face of increasing product variation and market demands. However, limited
expertise and resource constraints often result in inefficiencies during the
ramp-up phase when new products are integrated into production lines. Existing
methods often lack structured and tailored solutions to support automation
engineers in reducing ramp-up time, leading to compromises in quality. This
research investigates whether large-language models (LLMs), combined with
Retrieval-Augmented Generation (RAG), can assist in streamlining equipment
selection in ramp-up planning. We propose a factual-driven copilot integrating
LLMs with structured and semi-structured knowledge retrieval for three
component types (robots, feeders and vision systems), providing a guided and
traceable state-machine process for decision-making in automation equipment
selection. The system was demonstrated to an industrial partner, who tested it
on three internal use-cases. Their feedback affirmed its capability to provide
logical and actionable recommendations for automation equipment. More
specifically, among 22 equipment prompts analyzed, 19 involved selecting the
correct equipment while considering most requirements, and in 6 cases, all
requirements were fully met.",Jonas Werheid
2020-10-08T10:01:48Z,http://arxiv.org/abs/2010.03872v1,"Clinically Verified Hybrid Deep Learning System for Retinal Ganglion
  Cells Aware Grading of Glaucomatous Progression","Objective: Glaucoma is the second leading cause of blindness worldwide.
Glaucomatous progression can be easily monitored by analyzing the degeneration
of retinal ganglion cells (RGCs). Many researchers have screened glaucoma by
measuring cup-to-disc ratios from fundus and optical coherence tomography
scans. However, this paper presents a novel strategy that pays attention to the
RGC atrophy for screening glaucomatous pathologies and grading their severity.
Methods: The proposed framework encompasses a hybrid convolutional network that
extracts the retinal nerve fiber layer, ganglion cell with the inner plexiform
layer and ganglion cell complex regions, allowing thus a quantitative screening
of glaucomatous subjects. Furthermore, the severity of glaucoma in screened
cases is objectively graded by analyzing the thickness of these regions.
Results: The proposed framework is rigorously tested on publicly available
Armed Forces Institute of Ophthalmology (AFIO) dataset, where it achieved the
F1 score of 0.9577 for diagnosing glaucoma, a mean dice coefficient score of
0.8697 for extracting the RGC regions and an accuracy of 0.9117 for grading
glaucomatous progression. Furthermore, the performance of the proposed
framework is clinically verified with the markings of four expert
ophthalmologists, achieving a statistically significant Pearson correlation
coefficient of 0.9236. Conclusion: An automated assessment of RGC degeneration
yields better glaucomatous screening and grading as compared to the
state-of-the-art solutions. Significance: An RGC-aware system not only screens
glaucoma but can also grade its severity and here we present an end-to-end
solution that is thoroughly evaluated on a standardized dataset and is
clinically validated for analyzing glaucomatous pathologies.",Hina Raja
2021-07-12T15:37:40Z,http://arxiv.org/abs/2107.05513v1,3D MHD astrospheres: applications to IRC-10414 and Betelgeuse,"A significative fraction of all massive stars in the Milky Way move
supersonically through their local interstellar medium (ISM), producing bow
shock nebulae by wind-ISM interaction. The stability of these observed
astrospheres around cool massive stars challenges precedent two-dimensional
(magneto-)hydrodynamical simulations of their surroundings. We present
three-dimensional magneto-hydrodynamical (3D MHD) simulations of the
circumstellar medium of runaway M-type red supergiant stars moving with
velocity v_star= 50 km/s. We treat the stellar wind with a Parker spiral and
assume a 7 microG magnetisation of the ISM. Our free parameter is the angle
theta_mag between ISM flow and magnetisation, taken to 0, 45 and 90 degrees. It
is found that simulation dimension, coordinate systems and grid effects can
greatly affect the development of the modelled astrospheres. Nevertheless, as
soon as the ISM flow and magnetisation directions differs by more than a few
degrees (theta_mag>5 degree), the bow shock is stabilised, most clumpiness and
ragged structures vanishing. The complex shape of the bowshocks induce
important projection effects, e.g. at optical Ha line, producing complex of
astrospheric morphologies. We speculate that those effects are also at work
around earlier-type massive stars, which would explain their diversity of their
observed arc-like nebula around runaway OB stars. Our 3D MHD models are fitting
well observations of the astrospheres of several runaway red supergiant stars.
The results interpret the smoothed astrosphere of IRC-10414 and Betelgeuse
aOri) are stabilised by an organised, non-parallel ambient magnetic field. Our
findings suggest that IRC-10414 is currently in a steady state of its
evolution, and that Betelgeuse's bar is of interstellar origin.",D. M. -A. Meyer
2023-04-14T15:18:44Z,http://arxiv.org/abs/2304.07197v1,"The Impacts of Neutron-Star Structure and Base Heating on Type I X-Ray
  Bursts and Code Comparison","Type I X-ray bursts are rapidly brightening phenomena triggered by
thermonuclear burning on accreting layer of a neutron star (NS). The light
curves represent the physical properties of NSs and the nuclear reactions on
the proton-rich nuclei. The numerical treatments of the accreting NS and
physics of the NS interior are not established, which shows uncertainty in
modelling for observed X-ray light curves. In this study, we investigate
theoretical X-ray-burst models, compared with burst light curves with
GS~1826-24 observations. We focus on the impacts of the NS mass, the NS radius,
and base-heating on the NS surface using the MESA code. We find a monotonic
correlation between the NS mass and the parameters of the light curve. The
higher the mass, the longer the recurrence time and the greater the peak
luminosity. While the larger the radius, the longer the recurrence time, the
peak luminosity remains nearly constant. In the case of increasing base
heating, both the recurrence time and peak luminosity decrease. We also examine
the above results using with a different numerical code, HERES, based on
general relativity and consider the central NS. We find that the burst rate,
burst energy and burst strength are almost same in two X-ray burst codes by
adjusting the base-heat parameter in MESA (the relative errors $\lesssim5\%$),
while the duration time and the rise time are significantly different between
(the relative error is possibly $\sim50\%$). The peak luminosity and the
e-folding time are ragged between two codes for different accretion rates.",Guoqing Zhen
2023-07-07T07:16:03Z,http://arxiv.org/abs/2307.03427v1,"Merging-Diverging Hybrid Transformer Networks for Survival Prediction in
  Head and Neck Cancer","Survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning. Recently, deep survival models
based on deep learning and medical images have shown promising performance for
survival prediction. However, existing deep survival models are not well
developed in utilizing multi-modality images (e.g., PET-CT) and in extracting
region-specific information (e.g., the prognostic information in Primary Tumor
(PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a
merging-diverging learning framework for survival prediction from
multi-modality images. This framework has a merging encoder to fuse
multi-modality information and a diverging decoder to extract region-specific
information. In the merging encoder, we propose a Hybrid Parallel
Cross-Attention (HPCA) block to effectively fuse multi-modality features via
parallel convolutional layers and cross-attention transformers. In the
diverging decoder, we propose a Region-specific Attention Gate (RAG) block to
screen out the features related to lesion regions. Our framework is
demonstrated on survival prediction from PET-CT images in Head and Neck (H&N)
cancer, by designing an X-shape merging-diverging hybrid transformer network
(named XSurv). Our XSurv combines the complementary information in PET and CT
images and extracts the region-specific prognostic information in PT and MLN
regions. Extensive experiments on the public dataset of HEad and neCK TumOR
segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that
our XSurv outperforms state-of-the-art survival prediction methods.",Mingyuan Meng
2023-12-18T03:19:31Z,http://arxiv.org/abs/2312.10904v2,"Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)","Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.",Sabrina Toro
2023-12-25T02:32:05Z,http://arxiv.org/abs/2312.15591v5,Privacy-Preserved Neural Graph Databases,"In the era of large language models (LLMs), efficient and accurate data
retrieval has become increasingly crucial for the use of domain-specific or
private data in the retrieval augmented generation (RAG). Neural graph
databases (NGDBs) have emerged as a powerful paradigm that combines the
strengths of graph databases (GDBs) and neural networks to enable efficient
storage, retrieval, and analysis of graph-structured data which can be
adaptively trained with LLMs. The usage of neural embedding storage and Complex
neural logical Query Answering (CQA) provides NGDBs with generalization
ability. When the graph is incomplete, by extracting latent patterns and
representations, neural graph databases can fill gaps in the graph structure,
revealing hidden relationships and enabling accurate query answering.
Nevertheless, this capability comes with inherent trade-offs, as it introduces
additional privacy risks to the domain-specific or private databases. Malicious
attackers can infer more sensitive information in the database using
well-designed queries such as from the answer sets of where Turing Award
winners born before 1950 and after 1940 lived, the living places of Turing
Award winner Hinton are probably exposed, although the living places may have
been deleted in the training stage due to the privacy concerns. In this work,
we propose a privacy-preserved neural graph database (P-NGDB) framework to
alleviate the risks of privacy leakage in NGDBs. We introduce adversarial
training techniques in the training stage to enforce the NGDBs to generate
indistinguishable answers when queried with private information, enhancing the
difficulty of inferring sensitive information through combinations of multiple
innocuous queries.",Qi Hu
2023-12-29T03:23:23Z,http://arxiv.org/abs/2312.17449v2,"DB-GPT: Empowering Database Interactions with Private Large Language
  Models","The recent breakthroughs in large language models (LLMs) are positioned to
transition many areas of software. Database technologies particularly have an
important entanglement with LLMs as efficient and intuitive database
interactions are paramount. In this paper, we present DB-GPT, a revolutionary
and production-ready project that integrates LLMs with traditional database
systems to enhance user experience and accessibility. DB-GPT is designed to
understand natural language queries, provide context-aware responses, and
generate complex SQL queries with high accuracy, making it an indispensable
tool for users ranging from novice to expert. The core innovation in DB-GPT
lies in its private LLM technology, which is fine-tuned on domain-specific
corpora to maintain user privacy and ensure data security while offering the
benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which
includes a novel retrieval augmented generation (RAG) knowledge system, an
adaptive learning mechanism to continuously improve performance based on user
feedback and a service-oriented multi-model framework (SMMF) with powerful
data-driven agents. Our extensive experiments and user studies confirm that
DB-GPT represents a paradigm shift in database interactions, offering a more
natural, efficient, and secure way to engage with data repositories. The paper
concludes with a discussion of the implications of DB-GPT framework on the
future of human-database interaction and outlines potential avenues for further
enhancements and applications in the field. The project code is available at
https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by
installing it with the instructions
https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute
video at https://www.youtube.com/watch?v=KYs4nTDzEhk.",Siqiao Xue
2024-01-27T02:29:42Z,http://arxiv.org/abs/2401.15269v3,"Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models","Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.",Minbyul Jeong
2024-02-12T14:53:28Z,http://arxiv.org/abs/2402.07688v2,"CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation
  for Evaluating LLMs in Cybersecurity Knowledge","Large Language Models (LLMs) are increasingly used across various domains,
from software development to cyber threat intelligence. Understanding all the
different fields of cybersecurity, which includes topics such as cryptography,
reverse engineering, and risk assessment, poses a challenge even for human
experts. To accurately test the general knowledge of LLMs in cybersecurity, the
research community needs a diverse, accurate, and up-to-date dataset. To
address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000,
and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets
comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing
GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents,
including NIST standards, research papers, publicly accessible books, RFCs, and
other publications in the cybersecurity domain, to generate questions, each
with four possible answers. The results underwent several rounds of error
checking and refinement. Human experts invested over 200 hours validating the
questions and solutions to ensure their accuracy and relevance, and to filter
out any questions unrelated to cybersecurity. We have evaluated and compared 25
state-of-the-art LLM models on the CyberMetric datasets. In addition to our
primary goal of evaluating LLMs, we involved 30 human participants to solve
CyberMetric-80 in a closed-book scenario. The results can serve as a reference
for comparing the general cybersecurity knowledge of humans and LLMs. The
findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct,
Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs.
Additionally, the top LLMs were more accurate than humans on CyberMetric-80,
although highly experienced human experts still outperformed small models such
as Llama-3-8B, Phi-2 or Gemma-7b.",Norbert Tihanyi
2024-02-20T02:16:16Z,http://arxiv.org/abs/2402.12659v2,FinBen: A Holistic Financial Benchmark for Large Language Models,"LLMs have transformed NLP and shown promise in various fields, yet their
potential in finance is underexplored due to a lack of comprehensive evaluation
benchmarks, the rapid development of LLMs, and the complexity of financial
tasks. In this paper, we introduce FinBen, the first extensive open-source
evaluation benchmark, including 36 datasets spanning 24 financial tasks,
covering seven critical aspects: information extraction (IE), textual analysis,
question answering (QA), text generation, risk management, forecasting, and
decision-making. FinBen offers several key innovations: a broader range of
tasks and datasets, the first evaluation of stock trading, novel agent and
Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source
evaluation datasets for text summarization, question answering, and stock
trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT,
and the latest Gemini, reveals several key findings: While LLMs excel in IE and
textual analysis, they struggle with advanced reasoning and complex tasks like
text generation and forecasting. GPT-4 excels in IE and stock trading, while
Gemini is better at text generation and forecasting. Instruction-tuned LLMs
improve textual analysis but offer limited benefits for complex tasks such as
QA. FinBen has been used to host the first financial LLMs shared task at the
FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel
solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation
in financial LLMs. All datasets, results, and codes are released for the
research community: https://github.com/The-FinAI/PIXIU.",Qianqian Xie
2024-03-12T02:30:50Z,http://arxiv.org/abs/2403.07952v1,"AesopAgent: Agent-driven Evolutionary System on Story-to-Video
  Production","The Agent and AIGC (Artificial Intelligence Generated Content) technologies
have recently made significant progress. We propose AesopAgent, an Agent-driven
Evolutionary System on Story-to-Video Production. AesopAgent is a practical
application of agent technology for multimodal content generation. The system
integrates multiple generative capabilities within a unified framework, so that
individual users can leverage these modules easily. This innovative system
would convert user story proposals into scripts, images, and audio, and then
integrate these multimodal contents into videos. Additionally, the animating
units (e.g., Gen-2 and Sora) could make the videos more infectious. The
AesopAgent system could orchestrate task workflow for video generation,
ensuring that the generated video is both rich in content and coherent. This
system mainly contains two layers, i.e., the Horizontal Layer and the Utility
Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary
system that optimizes the whole video generation workflow and the steps within
the workflow. It continuously evolves and iteratively optimizes workflow by
accumulating expert experience and professional knowledge, including optimizing
the LLM prompts and utilities usage. The Utility Layer provides multiple
utilities, leading to consistent image generation that is visually coherent in
terms of composition, characters, and style. Meanwhile, it provides audio and
special effects, integrating them into expressive and logically arranged
videos. Overall, our AesopAgent achieves state-of-the-art performance compared
with many previous works in visual storytelling. Our AesopAgent is designed for
convenient service for individual users, which is available on the following
page: https://aesopai.github.io/.",Jiuniu Wang
2024-03-25T21:37:30Z,http://arxiv.org/abs/2403.17209v4,"Generation of Asset Administration Shell with Large Language Model
  Agents: Toward Semantic Interoperability in Digital Twins in the Context of
  Industry 4.0","This research introduces a novel approach for achieving semantic
interoperability in digital twins and assisting the creation of Asset
Administration Shell (AAS) as digital twin model within the context of Industry
4.0. The foundational idea of our research is that the communication based on
semantics and the generation of meaningful textual data are directly linked,
and we posit that these processes are equivalent if the exchanged information
can be serialized in text form. Based on this, we construct a ""semantic node""
data structure in our research to capture the semantic essence of textual data.
Then, a system powered by large language models is designed and implemented to
process the ""semantic node"" and generate standardized digital twin models from
raw textual data collected from datasheets describing technical assets. Our
evaluation demonstrates an effective generation rate of 62-79%, indicating a
substantial proportion of the information from the source text can be
translated error-free to the target digital twin instance model with the
generative capability of large language models. This result has a direct
application in the context of Industry 4.0, and the designed system is
implemented as a data model generation tool for reducing the manual effort in
creating AAS model. In our evaluation, a comparative analysis of different LLMs
and an in-depth ablation study of Retrieval-Augmented Generation (RAG)
mechanisms provide insights into the effectiveness of LLM systems for
interpreting technical concepts and translating data. Our findings emphasize
LLMs' capability to automate AAS instance creation and contribute to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are presented
on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.",Yuchen Xia
2024-04-16T00:43:03Z,http://arxiv.org/abs/2404.10198v2,"ClashEval: Quantifying the tug-of-war between an LLM's internal prior
  and external evidence","Retrieval augmented generation (RAG) is frequently used to mitigate
hallucinations and provide up-to-date knowledge for large language models
(LLMs). However, given that document retrieval is an imprecise task and
sometimes results in erroneous or even harmful content being presented in
context, this raises the question of how LLMs handle retrieved information: If
the provided content is incorrect, does the model know to ignore it, or does it
recapitulate the error? Conversely, when the model's initial response is
incorrect, does it always know to use the retrieved information to correct
itself, or does it insist on its wrong prior response? To answer this, we
curate a dataset of over 1200 questions across six domains (e.g., drug dosages,
Olympic records, locations) along with content relevant to answering each
question. We further apply precise perturbations to the answers in the content
that range from subtle to blatant errors. We benchmark six top-performing LLMs,
including GPT-4o, on this dataset and find that LLMs are susceptible to
adopting incorrect retrieved content, overriding their own correct prior
knowledge over 60% of the time. However, the more unrealistic the retrieved
content is (i.e. more deviated from truth), the less likely the model is to
adopt it. Also, the less confident a model is in its initial response (via
measuring token probabilities), the more likely it is to adopt the information
in the retrieved content. We exploit this finding and demonstrate simple
methods for improving model accuracy where there is conflicting retrieved
content. Our results highlight a difficult task and benchmark for LLMs --
namely, their ability to correctly discern when it is wrong in light of correct
retrieved content and to reject cases when the provided content is incorrect.",Kevin Wu
2024-05-06T08:38:14Z,http://arxiv.org/abs/2405.03267v2,"Characterizing the Dilemma of Performance and Index Size in
  Billion-Scale Vector Search and Breaking It with Second-Tier Memory","Vector searches on large-scale datasets are critical to modern online
services like web search and RAG, which necessity storing the datasets and
their index on the secondary storage like SSD. In this paper, we are the first
to characterize the trade-off of performance and index size in existing
SSD-based graph and cluster indexes: to improve throughput by 5.7$\times$ and
1.7$\times$, these indexes have to pay a 5.8$\times$ storage amplification and
7.7$\times$ with respect to the dataset size, respectively. The root cause is
that the coarse-grained access of SSD mismatches the fine-grained random read
required by vector indexes with small amplification.
  This paper argues that second-tier memory, such as remote DRAM/NVM connected
via RDMA or CXL, is a powerful storage for addressing the problem from a
system's perspective, thanks to its fine-grained access granularity. However,
putting existing indexes -- primarily designed for SSD -- directly on
second-tier memory cannot fully utilize its power. Meanwhile, second-tier
memory still behaves more like storage, so using it as DRAM is also
inefficient. To this end, we build a graph and cluster index that centers
around the performance features of second-tier memory. With careful execution
engine and index layout designs, we show that vector indexes can achieve
optimal performance with orders of magnitude smaller index amplification, on a
variety of second-tier memory devices.
  Based on our improved graph and vector indexes on second-tier memory, we
further conduct a systematic study between them to facilitate developers
choosing the right index for their workloads. Interestingly, the findings on
the second-tier memory contradict the ones on SSDs.",Rongxin Cheng
2024-05-23T10:00:14Z,http://arxiv.org/abs/2405.14383v1,"Perception of Knowledge Boundary for Large Language Models through
  Semi-open-ended Question Answering","Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer
from hallucinations. The knowledge boundary (KB) of an LLM limits its factual
understanding, beyond which it may begin to hallucinate. Investigating the
perception of LLMs' KB is crucial for detecting hallucinations and LLMs'
reliable generation. Current studies perceive LLMs' KB on questions with a
concrete answer (close-ended questions) while paying limited attention to
semi-open-ended questions (SoeQ) that correspond to many potential answers.
Some researchers achieve it by judging whether the question is answerable or
not. However, this paradigm is unsuitable for SoeQ, which are usually partially
answerable, containing both answerable and ambiguous (unanswerable) answers.
Ambiguous answers are essential for knowledge-seeking, but they may go beyond
the KB of LLMs. In this paper, we perceive the LLMs' KB with SoeQ by
discovering more ambiguous answers. First, we apply an LLM-based approach to
construct SoeQ and obtain answers from a target LLM. Unfortunately, the output
probabilities of mainstream black-box LLMs are inaccessible to sample for
low-probability ambiguous answers. Therefore, we apply an open-sourced
auxiliary model to explore ambiguous answers for the target LLM. We calculate
the nearest semantic representation for existing answers to estimate their
probabilities, with which we reduce the generation probability of
high-probability answers to achieve a more effective generation. Finally, we
compare the results from the RAG-based evaluation and LLM self-evaluation to
categorize four types of ambiguous answers that are beyond the KB of the target
LLM. Following our method, we construct a dataset to perceive the KB for GPT-4.
We find that GPT-4 performs poorly on SoeQ and is often unaware of its KB.
Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering more
ambiguous answers.",Zhihua Wen
2024-05-23T13:32:07Z,http://arxiv.org/abs/2405.14554v2,"SearchLVLMs: A Plug-and-Play Framework for Augmenting Large
  Vision-Language Models by Searching Up-to-Date Internet Knowledge","Large vision-language models (LVLMs) are ignorant of the up-to-date
knowledge, such as LLaVA series, because they cannot be updated frequently due
to the large amount of resources required, and therefore fail in many cases.
For example, if a LVLM was released on January 2024, and it wouldn't know the
singer of the theme song for the new Detective Conan movie, which wasn't
released until April 2024. To solve the problem, a promising solution motivated
by retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date
knowledge via internet search during inference, i.e., internet-augmented
generation (IAG), which is already integrated in some closed-source commercial
LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain
a mystery. In this paper, we propose a plug-and-play framework, for augmenting
existing LVLMs in handling visual question answering (VQA) about up-to-date
knowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to
effectively and efficiently find the most helpful content from the websites
returned by a search engine to prompt LVLMs with up-to-date knowledge. To train
the model and evaluate our framework's performance, we propose a pipeline to
automatically generate news-related VQA samples to construct a dataset, dubbed
UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness
of website/content for VQA samples to construct the training set. Experimental
results demonstrate the effectiveness of our framework, outperforming GPT-4V by
about 25% in accuracy.",Chuanhao Li
2024-05-26T06:00:17Z,http://arxiv.org/abs/2405.16444v2,"CacheBlend: Fast Large Language Model Serving for RAG with Cached
  Knowledge Fusion","Large language models (LLMs) often incorporate multiple text chunks in their
inputs to provide the necessary contexts. To speed up the prefill of the long
LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache
when the context is reused as the prefix of another LLM input. However, the
reused text chunks are not always the input prefix, and when they are not,
their precomputed KV caches cannot be directly used since they ignore the
text's cross-attention with the preceding text in the LLM input. Thus, the
benefits of reusing KV caches remain largely unrealized.
  This paper tackles just one question: when an LLM input contains multiple
text chunks, how to quickly combine their precomputed KV caches in order to
achieve the same generation quality as the expensive full prefill (i.e.,
without reusing KV cache)? We present CacheBlend, a scheme that reuses the
pre-computed KV caches, regardless prefix or not, and selectively recomputes
the KV values of a small subset of tokens to partially update each reused KV
cache. In the meantime,the small extra delay for recomputing some tokens can be
pipelined with the retrieval of KV caches within the same job,allowing
CacheBlend to store KV caches in slower devices with more storage capacity
while retrieving them without increasing the inference delay. By comparing
CacheBlend with the state-of-the-art KV cache reusing schemes on three
open-source LLMs of various sizes and four popular benchmark datasets of
different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by
2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full
KV recompute, without compromising generation quality or incurring more storage
cost.",Jiayi Yao
2024-06-26T03:32:35Z,http://arxiv.org/abs/2406.18039v1,"Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model
  with Three Types of Knowledge","Liver cancer has a high incidence rate, but primary healthcare settings often
lack experienced doctors. Advances in large models and AI technologies offer
potential assistance. This work aims to address limitations in liver cancer
diagnosis models, such as poor understanding of medical images, insufficient
consideration of liver blood vessels, and ensuring accurate medical
information. We propose a specialized diagnostic assistant to improve the
diagnostic capabilities of less experienced doctors. Our framework combines
large and small models, using optimized small models for precise patient image
perception. Specifically, a segmentation network iteratively removes ambiguous
pixels for liver tumor segmentation, and a multi-scale, multi-level
differential network segments liver vessels. Features from these segmentations
and medical records form a patient's personalized knowledge base. For
diagnosis, Chain of Thought (COT) technology designs prompts mimicking
experienced doctors' thought patterns, and Retrieval-Augmented Generation (RAG)
technology provides answers based on reliable domain knowledge and trusted
cases. Our small model methods improve liver tumor and vessel segmentation
performance, resulting in more accurate information extraction. The large model
component scores over 1 point higher on a 10-point scale in evaluations by
doctors compared to control methods. Our method enhances semantic perception of
medical images, improves classification of ambiguous pixels, and optimizes
small object perception. It considers blood vessel positions for specific
treatments and improves response credibility and interpretability by mimicking
experienced doctors' thought processes using reliable resources. This approach
has been recognized by doctors and benefits liver cancer auxiliary diagnosis.",Xuzhou Wu
2024-06-26T12:51:37Z,http://arxiv.org/abs/2406.18312v4,AI-native Memory: A Pathway from LLMs Towards AGI,"Large language models (LLMs) have demonstrated the world with the sparks of
artificial general intelligence (AGI). One opinion, especially from some
startups working on LLMs, argues that an LLM with nearly unlimited context
length can realize AGI. However, they might be too optimistic about the
long-context capability of (existing) LLMs -- (1) Recent literature has shown
that their effective context length is significantly smaller than their claimed
context length; and (2) Our reasoning-in-a-haystack experiments further
demonstrate that simultaneously finding the relevant information from a long
context and conducting (simple) reasoning is nearly impossible. In this paper,
we envision a pathway from LLMs to AGI through the integration of
\emph{memory}. We believe that AGI should be a system where LLMs serve as core
processors. In addition to raw data, the memory in this system would store a
large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw
data, this approach not only connects semantically related information closer,
but also simplifies complex inferences at the time of querying. As an
intermediate stage, the memory will likely be in the form of natural language
descriptions, which can be directly consumed by users too. Ultimately, every
agent/person should have its own large personal model, a deep neural network
model (thus \emph{AI-native}) that parameterizes and compresses all types of
memory, even the ones cannot be described by natural languages. Finally, we
discuss the significant potential of AI-native memory as the transformative
infrastructure for (proactive) engagement, personalization, distribution, and
social in the AGI era, as well as the incurred privacy and security challenges
with preliminary solutions.",Jingbo Shang
2024-06-29T15:23:28Z,http://arxiv.org/abs/2407.00466v1,"BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for
  Biomedical Science","Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,
draws increasing attention, where one common approach is to build a copilot
agent driven by Large Language Models (LLMs). However, to evaluate such
systems, people either rely on direct Question-Answering (QA) to the LLM
itself, or in a biomedical experimental manner. How to precisely benchmark
biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of
scientists, understanding the literature, and introduce BioKGBench. In contrast
to traditional evaluation benchmark that only focuses on factual QA, where the
LLMs are known to have hallucination issues, we first disentangle
""Understanding Literature"" into two atomic abilities, i) ""Understanding"" the
unstructured text from research papers by performing scientific claim
verification, and ii) Ability to interact with structured Knowledge-Graph
Question-Answering (KGQA) as a form of ""Literature"" grounding. We then
formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based
Retrieval-Augmented Generation (RAG) to identify the factual errors of existing
large-scale knowledge graph databases. We collect over two thousand data for
two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios
and biomedical ones, have either failed or inferior performance on our
benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors
which provide scenarios for agents to make discoveries and demonstrate the
effectiveness of our approach. The code and data are available at
https://github.com/westlake-autolab/BioKGBench.",Xinna Lin
2024-07-05T12:42:31Z,http://arxiv.org/abs/2407.04472v3,"EventChat: Implementation and user-centric evaluation of a large
  language model-driven conversational recommender system for exploring leisure
  events in an SME context","Large language models (LLMs) present an enormous evolution in the strategic
potential of conversational recommender systems (CRS). Yet to date, research
has predominantly focused upon technical frameworks to implement LLM-driven
CRS, rather than end-user evaluations or strategic implications for firms,
particularly from the perspective of a small to medium enterprises (SME) that
makeup the bedrock of the global economy. In the current paper, we detail the
design of an LLM-driven CRS in an SME setting, and its subsequent performance
in the field using both objective system metrics and subjective user
evaluations. While doing so, we additionally outline a short-form revised
ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly
evolving field. Our results reveal good system performance from a user
experience perspective (85.5% recommendation accuracy) but underscore latency,
cost, and quality issues challenging business viability. Notably, with a median
cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and
response time emerge as crucial areas for achieving a more user-friendly and
economically viable LLM-driven CRS for SME settings. One major driver of these
costs is the use of an advanced LLM as a ranker within the retrieval-augmented
generation (RAG) technique. Our results additionally indicate that relying
solely on approaches such as Prompt-based learning with ChatGPT as the
underlying LLM makes it challenging to achieve satisfying quality in a
production environment. Strategic considerations for SMEs deploying an
LLM-driven CRS are outlined, particularly considering trade-offs in the current
technical landscape.",Hannes Kunstmann
2024-07-26T11:00:08Z,http://arxiv.org/abs/2408.00804v1,"ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large
  Language Model","The field of integrated circuit (IC) design is highly specialized, presenting
significant barriers to entry and research and development challenges. Although
large language models (LLMs) have achieved remarkable success in various
domains, existing LLMs often fail to meet the specific needs of students,
engineers, and researchers. Consequently, the potential of LLMs in the IC
design domain remains largely unexplored. To address these issues, we introduce
ChipExpert, the first open-source, instructional LLM specifically tailored for
the IC design field. ChipExpert is trained on one of the current best
open-source base model (Llama-3 8B). The entire training process encompasses
several key stages, including data preparation, continue pre-training,
instruction-guided supervised fine-tuning, preference alignment, and
evaluation. In the data preparation stage, we construct multiple high-quality
custom datasets through manual selection and data synthesis techniques. In the
subsequent two stages, ChipExpert acquires a vast amount of IC design knowledge
and learns how to respond to user queries professionally. ChipExpert also
undergoes an alignment phase, using Direct Preference Optimization, to achieve
a high standard of ethical performance. Finally, to mitigate the hallucinations
of ChipExpert, we have developed a Retrieval-Augmented Generation (RAG) system,
based on the IC design knowledge base. We also released the first IC design
benchmark ChipICD-Bench, to evaluate the capabilities of LLMs across multiple
IC design sub-domains. Through comprehensive experiments conducted on this
benchmark, ChipExpert demonstrated a high level of expertise in IC design
knowledge Question-and-Answer tasks.",Ning Xu
2024-07-31T11:51:26Z,http://arxiv.org/abs/2408.02680v1,"Recording First-person Experiences to Build a New Type of Foundation
  Model","Foundation models have had a big impact in recent years and billions of
dollars are being invested in them in the current AI boom. The more popular
ones, such as Chat-GPT, are trained on large amounts of Internet data. However,
it is becoming apparent that this data is likely to be exhausted soon, and
technology companies are looking for new sources of data to train the next
generation of foundation models.
  Reinforcement learning, RAG, prompt engineering and cognitive modelling are
often used to fine-tune and augment the behaviour of foundation models. These
techniques have been used to replicate people, such as Caryn Marjorie. These
chatbots are not based on people's actual emotional and physiological responses
to their environment, so they are, at best, a surface-level approximation to
the characters they are imitating.
  To address these issues, we have developed a recording rig that captures what
the wearer is seeing and hearing as well as their skin conductance (GSR),
facial expression and brain state (14 channel EEG). AI algorithms are used to
process this data into a rich picture of the environment and internal states of
the subject. Foundation models trained on this data could replicate human
behaviour much more accurately than the personality models that have been
developed so far. This type of model has many potential applications, including
recommendation, personal assistance, GAN systems, dating and recruitment.
  This paper gives some background to this work and describes the recording rig
and preliminary tests of its functionality. It then suggests how a new type of
foundation model could be created from the data captured by the rig and
outlines some applications. Data gathering and model training are expensive, so
we are currently working on the launch of a start-up that could raise funds for
the next stage of the project.",Dionis Barcari
2024-08-06T14:53:25Z,http://arxiv.org/abs/2408.04665v1,"LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations","The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
literature text has been challenging but crucial for the logical design of new
MOFs with desirable functionality. The recent advent of large language models
(LLMs) provides disruptively new solution to this long-standing problem and
latest researches have reported over 90% F1 in extracting correct conditions
from MOFs literature. We argue in this paper that most existing synthesis
extraction practices with LLMs stay with the primitive zero-shot learning,
which could lead to downgraded extraction and application performance due to
the lack of specialized knowledge. This work pioneers and optimizes the
few-shot in-context learning paradigm for LLM extraction of material synthesis
conditions. First, we propose a human-AI joint data curation process to secure
high-quality ground-truth demonstrations for few-shot learning. Second, we
apply a BM25 algorithm based on the retrieval-augmented generation (RAG)
technique to adaptively select few-shot demonstrations for each MOF's
extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the
proposed few-shot method achieves much higher average F1 performance (0.93 vs.
0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
fully automatic evaluation that are more objective than the previous human
evaluation. The proposed method is further validated through real-world
material experiments: compared with the baseline zero-shot LLM, the proposed
few-shot approach increases the MOFs structural inference performance (R^2) by
29.4% in average.",Lei Shi
2024-08-21T13:34:29Z,http://arxiv.org/abs/2408.11609v2,Xinyu: An Efficient LLM-based System for Commentary Generation,"Commentary provides readers with a deep understanding of events by presenting
diverse arguments and evidence. However, creating commentary is a
time-consuming task, even for skilled commentators. Large language models
(LLMs) have simplified the process of natural language generation, but their
direct application in commentary creation still faces challenges due to unique
task requirements. These requirements can be categorized into two levels: 1)
fundamental requirements, which include creating well-structured and logically
consistent narratives, and 2) advanced requirements, which involve generating
quality arguments and providing convincing evidence. In this paper, we
introduce Xinyu, an efficient LLM-based system designed to assist commentators
in generating Chinese commentaries. To meet the fundamental requirements, we
deconstruct the generation process into sequential steps, proposing targeted
strategies and supervised fine-tuning (SFT) for each step. To address the
advanced requirements, we present an argument ranking model for arguments and
establish a comprehensive evidence database that includes up-to-date events and
classic books, thereby strengthening the substantiation of the evidence with
retrieval augmented generation (RAG) technology. To evaluate the generated
commentaries more fairly, corresponding to the two-level requirements, we
introduce a comprehensive evaluation metric that considers five distinct
perspectives in commentary generation. Our experiments confirm the
effectiveness of our proposed system. We also observe a significant increase in
the efficiency of commentators in real-world scenarios, with the average time
spent on creating a commentary dropping from 4 hours to 20 minutes.
Importantly, such an increase in efficiency does not compromise the quality of
the commentaries.",Yiquan Wu
2024-08-31T17:20:27Z,http://arxiv.org/abs/2409.10540v1,"Beyond Flashcards: Designing an Intelligent Assistant for USMLE Mastery
  and Virtual Tutoring in Medical Education (A Study on Harnessing Chatbot
  Technology for Personalized Step 1 Prep)","Traditional medical basic sciences educational approaches follow a
one-size-fits-all model, neglecting the diverse learning styles of individual
students. I propose an intelligent AI companion which will fill this gap by
providing on-the-fly solutions to students' questions in the context of not
only USMLE Step 1 but also other similar examinations in other countries, inter
alia, PLAB Part 1 in United Kingdom, and NEET (PG) and FMGE in India. I have
harnessed Generative AI for dynamic, accurate, human-like responses and for
knowledge retention and application. Users were encouraged to employ prompt
engineering, in particular, in-context learning, for response optimization and
enhancing the model's precision in understanding the intent of the user through
the way the query is framed. The implementation of RAG has enhanced the
chatbot's ability to combine pre-existing medical knowledge with generative
capabilities for efficient and contextually relevant support. Mistral was
employed using Python to perform the needed functions. The digital
conversational agent was implemented and achieved a score of 0.5985 on a
reference-based metric similar to BLEU and ROUGE scores. My approach addresses
a critical gap in traditional medical basic sciences education by introducing
an intelligent AI companion which specializes in helping medical aspirants with
planning and information retention for USMLE Step 1 and other similar exams.
Considering the stress that medical aspirants face in studying for the exam and
in obtaining spontaneous answers to medical basic sciences queries, especially
whose answers are challenging to obtain by searching online, and obviating a
student's need to search bulky medical texts or lengthy indices or appendices,
I have been able to create a quality assistant capable of producing ad-libitum
responses best suited to the user's needs.",Ritwik Raj Saxena
2024-09-26T17:30:28Z,http://arxiv.org/abs/2409.18164v2,Data-Prep-Kit: getting your data ready for LLM application development,"Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).",David Wood
2024-10-03T17:20:11Z,http://arxiv.org/abs/2410.02694v2,"HELMET: How to Evaluate Long-Context Language Models Effectively and
  Thoroughly","There have been many benchmarks for evaluating long-context language models
(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack
(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate
to the diverse downstream applications of LCLMs, and the inconsistency further
complicates model comparison. We investigate the underlying reasons behind
current practices and find that existing benchmarks often provide noisy signals
due to low coverage of applications, insufficient lengths, unreliable metrics,
and incompatibility with base models. In this work, we present HELMET (How to
Evaluate Long-context Models Effectively and Thoroughly), a comprehensive
benchmark encompassing seven diverse, application-centric categories. We also
address many issues in previous benchmarks by adding controllable lengths up to
128k tokens, model-based evaluation for reliable metrics, and few-shot
prompting for robustly evaluating base models. Consequently, we demonstrate
that HELMET offers more reliable and consistent rankings of frontier LCLMs.
Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks
like NIAH are not good predictors of downstream performance; (2) the diverse
categories in HELMET exhibit distinct trends and low correlation with each
other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models
significantly lag behind closed ones when the task requires full-context
reasoning or following complex instructions -- the gap widens with increased
lengths. Finally, we recommend using our RAG tasks for fast model development,
as they are easy to run and more predictive of other downstream performance;
ultimately, we advocate for a holistic evaluation across diverse tasks.",Howard Yen
2024-10-09T00:48:12Z,http://arxiv.org/abs/2410.06440v1,Checker Bug Detection and Repair in Deep Learning Libraries,"Checker bugs in Deep Learning (DL) libraries are critical yet not
well-explored. These bugs are often concealed in the input validation and
error-checking code of DL libraries and can lead to silent failures, incorrect
results, or unexpected program behavior in DL applications. Despite their
potential to significantly impact the reliability and performance of DL-enabled
systems built with these libraries, checker bugs have received limited
attention.
  We present the first comprehensive study of DL checker bugs in two
widely-used DL libraries, i.e., TensorFlow and PyTorch. Initially, we
automatically collected a dataset of 2,418 commits from TensorFlow and PyTorch
repositories on GitHub from Sept. 2016 to Dec. 2023 using specific keywords
related to checker bugs. Through manual inspection, we identified 527 DL
checker bugs. Subsequently, we analyzed these bugs from three perspectives,
i.e., root causes, symptoms, and fixing patterns. Using the knowledge gained
via root cause analysis of checker bugs, we further propose TensorGuard, a
proof-of-concept RAG-based LLM-based tool to detect and fix checker bugs in DL
libraries via prompt engineering a series of ChatGPT prompts. We evaluated
TensorGuard's performance on a test dataset that includes 92 buggy and 135
clean checker-related changes in TensorFlow and PyTorch from January 2024 to
July 2024. Our results demonstrate that TensorGuard has high average recall
(94.51\%) using Chain of Thought prompting, a balanced performance between
precision and recall using Zero-Shot prompting and Few-Shot prompting
strategies. In terms of patch generation, TensorGuard achieves an accuracy of
11.1\%, which outperforms the state-of-the-art bug repair baseline by 2\%. We
have also applied TensorGuard on the latest six months' checker-related changes
(493 changes) of the JAX library from Google, which resulted in the detection
of 64 new checker bugs.",Nima Shiri Harzevili
2024-10-19T21:50:11Z,http://arxiv.org/abs/2410.15222v1,"AutoFLUKA: A Large Language Model Based Framework for Automating Monte
  Carlo Simulations in FLUKA","Monte Carlo (MC) simulations, particularly using FLUKA, are essential for
replicating real-world scenarios across scientific and engineering fields.
Despite the robustness and versatility, FLUKA faces significant limitations in
automation and integration with external post-processing tools, leading to
workflows with a steep learning curve, which are time-consuming and prone to
human errors. Traditional methods involving the use of shell and Python
scripts, MATLAB, and Microsoft Excel require extensive manual intervention and
lack flexibility, adding complexity to evolving scenarios. This study explores
the potential of Large Language Models (LLMs) and AI agents to address these
limitations. AI agents, integrate natural language processing with autonomous
reasoning for decision-making and adaptive planning, making them ideal for
automation. We introduce AutoFLUKA, an AI agent application developed using the
LangChain Python Framework to automate typical MC simulation workflows in
FLUKA. AutoFLUKA can modify FLUKA input files, execute simulations, and
efficiently process results for visualization, significantly reducing human
labor and error. Our case studies demonstrate that AutoFLUKA can handle both
generalized and domain-specific cases, such as Microdosimetry, with an
streamlined automated workflow, showcasing its scalability and flexibility. The
study also highlights the potential of Retrieval Augmentation Generation (RAG)
tools to act as virtual assistants for FLUKA, further improving user
experience, time and efficiency. In conclusion, AutoFLUKA represents a
significant advancement in automating MC simulation workflows, offering a
robust solution to the inherent limitations. This innovation not only saves
time and resources but also opens new paradigms for research and development in
high energy physics, medical physics, nuclear engineering space and
environmental science.",Zavier Ndum Ndum
2024-10-21T03:51:54Z,http://arxiv.org/abs/2410.15621v1,"DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on
  Commercial DRAM-PIMs","Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic
similarity search in large datasets, has become a fundamental component of
critical applications such as information retrieval and retrieval-augmented
generation (RAG). However, ANNS is a well-known I/O-intensive algorithm with a
low compute-to-I/O ratio, often requiring massive storage due to the large
volume of high-dimensional data. This leads to I/O bottlenecks on CPUs and
memory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)
architecture, which offers high bandwidth, large-capacity memory, and the
ability to perform efficient computation in or near the data, presents a
promising solution for ANNS. In this work, we investigate the use of commercial
DRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS
engine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM
exhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup
tables (LUTs) to replace more multiplications with I/O operations. We then
systematically tune ANNS to search optimized configurations with lower
computational load, aligning the compute-to-I/O ratio of ANNS with that of
DRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS
algorithm, we further explore implementation optimizations to fully utilize the
two thousand parallel processing units with private local memory in DRAM-PIMs.
To address the load imbalance caused by ANNS requests distributed across
different clusters of large datasets, we propose a load-balancing strategy that
combines static data layout optimization with dynamic runtime request
scheduling. Experimental results on representative datasets show that DRIM-ANN
achieves an average performance speedup of 2.92x compared to a 32-thread CPU
counterpart.",Mingkai Chen
2024-10-21T18:08:42Z,http://arxiv.org/abs/2410.16397v1,"Towards a Reliable Offline Personal AI Assistant for Long Duration
  Spaceflight","As humanity prepares for new missions to the Moon and Mars, astronauts will
need to operate with greater autonomy, given the communication delays that make
real-time support from Earth difficult. For instance, messages between Mars and
Earth can take up to 24 minutes, making quick responses impossible. This
limitation poses a challenge for astronauts who must rely on in-situ tools to
access the large volume of data from spacecraft sensors, rovers, and
satellites, data that is often fragmented and difficult to use. To bridge this
gap, systems like the Mars Exploration Telemetry-Driven Information System
(METIS) are being developed. METIS is an AI assistant designed to handle
routine tasks, monitor spacecraft systems, and detect anomalies, all while
reducing the reliance on mission control. Current Generative Pretrained
Transformer (GPT) Models, while powerful, struggle in safety-critical
environments. They can generate plausible but incorrect responses, a phenomenon
known as ""hallucination,"" which could endanger astronauts. To overcome these
limitations, this paper proposes enhancing systems like METIS by integrating
GPTs, Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and
Augmented Reality (AR). The idea is to allow astronauts to interact with their
data more intuitively, using natural language queries and visualizing real-time
information through AR. KGs will be used to easily access live telemetry and
multimodal data, ensuring that astronauts have the right information at the
right time. By combining AI, KGs, and AR, this new system will empower
astronauts to work more autonomously, safely, and efficiently during future
space missions.",Oliver Bensch
2024-11-20T09:59:12Z,http://arxiv.org/abs/2411.13163v1,"Unlocking Historical Clinical Trial Data with ALIGN: A Compositional
  Large Language Model System for Medical Coding","The reuse of historical clinical trial data has significant potential to
accelerate medical research and drug development. However, interoperability
challenges, particularly with missing medical codes, hinders effective data
integration across studies. While Large Language Models (LLMs) offer a
promising solution for automated coding without labeled data, current
approaches face challenges on complex coding tasks. We introduce ALIGN, a novel
compositional LLM-based system for automated, zero-shot medical coding. ALIGN
follows a three-step process: (1) diverse candidate code generation; (2)
self-evaluation of codes and (3) confidence scoring and uncertainty estimation
enabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing
medication terms into Anatomical Therapeutic Chemical (ATC) and medical history
terms into Medical Dictionary for Regulatory Activities (MedDRA) codes
extracted from 22 immunology trials. ALIGN outperformed the LLM baselines,
while also providing capabilities for trustworthy deployment. For MedDRA
coding, ALIGN achieved high accuracy across all levels, matching RAG and
excelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN
demonstrated superior performance, particularly at lower hierarchy levels (ATC
Level 4), with 72-73% overall accuracy and 86-89% accuracy for common
medications, outperforming baselines by 7-22%. ALIGN's uncertainty-based
deferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably
enhancing performance on uncommon medications. ALIGN achieves this
cost-efficiently at \$0.0007 and \$0.02 per code for GPT-4o-mini and GPT-4o,
reducing barriers to clinical adoption. ALIGN advances automated medical coding
for clinical trial data, contributing to enhanced data interoperability and
reusability, positioning it as a promising tool to improve clinical research
and accelerate drug development.",Nabeel Seedat
2024-11-07T02:49:53Z,http://arxiv.org/abs/2411.13560v1,"AMSnet-KG: A Netlist Dataset for LLM-based AMS Circuit Auto-Design Using
  Knowledge Graph RAG","High-performance analog and mixed-signal (AMS) circuits are mainly
full-custom designed, which is time-consuming and labor-intensive. A
significant portion of the effort is experience-driven, which makes the
automation of AMS circuit design a formidable challenge. Large language models
(LLMs) have emerged as powerful tools for Electronic Design Automation (EDA)
applications, fostering advancements in the automatic design process for
large-scale AMS circuits. However, the absence of high-quality datasets has led
to issues such as model hallucination, which undermines the robustness of
automatically generated circuit designs. To address this issue, this paper
introduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and
netlists. We construct a knowledge graph with annotations on detailed
functional and performance characteristics. Facilitated by AMSnet-KG, we
propose an automated AMS circuit generation framework that utilizes the
comprehensive knowledge embedded in LLMs. We first formulate a design strategy
(e.g., circuit architecture using a number of circuit components) based on
required specifications. Next, matched circuit components are retrieved and
assembled into a complete topology, and transistor sizing is obtained through
Bayesian optimization. Simulation results of the netlist are fed back to the
LLM for further topology refinement, ensuring the circuit design specifications
are met. We perform case studies of operational amplifier and comparator design
to verify the automatic design flow from specifications to netlists with
minimal human effort. The dataset used in this paper will be open-sourced upon
publishing of this paper.",Yichen Shi
2024-11-23T08:18:55Z,http://arxiv.org/abs/2411.15490v1,"Improving Factuality of 3D Brain MRI Report Generation with Paired
  Image-domain Retrieval and Text-domain Augmentation","Acute ischemic stroke (AIS) requires time-critical management, with hours of
delayed intervention leading to an irreversible disability of the patient.
Since diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)
plays a crucial role in the detection of AIS, automated prediction of AIS from
DWI has been a research topic of clinical importance. While text radiology
reports contain the most relevant clinical information from the image findings,
the difficulty of mapping across different modalities has limited the
factuality of conventional direct DWI-to-report generation methods. Here, we
propose paired image-domain retrieval and text-domain augmentation (PIRTA), a
cross-modal retrieval-augmented generation (RAG) framework for providing
clinician-interpretative AIS radiology reports with improved factuality. PIRTA
mitigates the need for learning cross-modal mapping, which poses difficulty in
image-to-text generation, by casting the cross-modal mapping problem as an
in-domain retrieval of similar DWI images that have paired ground-truth text
radiology reports. By exploiting the retrieved radiology reports to augment the
report generation process of the query image, we show by experiments with
extensive in-house and public datasets that PIRTA can accurately retrieve
relevant reports from 3D DWI images. This approach enables the generation of
radiology reports with significantly higher accuracy compared to direct
image-to-text generation using state-of-the-art multimodal language models.",Junhyeok Lee
2024-12-18T20:18:03Z,http://arxiv.org/abs/2412.14304v1,"Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing
  LLM Ophthalmological QA in LMICs","Current ophthalmology clinical workflows are plagued by over-referrals, long
waits, and complex and heterogeneous medical records. Large language models
(LLMs) present a promising solution to automate various procedures such as
triaging, preliminary tests like visual acuity assessment, and report
summaries. However, LLMs have demonstrated significantly varied performance
across different languages in natural language question-answering tasks,
potentially exacerbating healthcare disparities in Low and Middle-Income
Countries (LMICs). This study introduces the first multilingual
ophthalmological question-answering benchmark with manually curated questions
parallel across languages, allowing for direct cross-lingual comparisons. Our
evaluation of 6 popular LLMs across 7 different languages reveals substantial
bias across different languages, highlighting risks for clinical deployment of
LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought
or Retrieval-augmented generation (RAG) by themselves fall short of closing
this performance gap, often failing to improve performance across all languages
and lacking specificity for the medical domain. To address this issue, We
propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time
de-biasing method leveraging retrieval augmented generation and
self-verification. Our approach not only improves performance across all
languages but also significantly reduces the multilingual bias gap,
facilitating equitable LLM application across the globe.",David Restrepo
2024-01-30T18:58:43Z,http://arxiv.org/abs/2401.17268v1,Weaver: Foundation Models for Creative Writing,"This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.",Tiannan Wang
