Published Date,Link,Title,Summary,First Author
2024-06-06T07:01:50Z,http://arxiv.org/abs/2406.03790v2,"End-to-End Trainable Retrieval-Augmented Generation for Relation
  Extraction","This paper addresses a crucial challenge in retrieval-augmented
generation-based relation extractors; the end-to-end training is not applicable
to conventional retrieval-augmented generation due to the non-differentiable
nature of instance retrieval. This problem prevents the instance retrievers
from being optimized for the relation extraction task, and conventionally it
must be trained with an objective different from that for relation extraction.
To address this issue, we propose a novel End-to-end Trainable
Retrieval-Augmented Generation (ETRAG), which allows end-to-end optimization of
the entire model, including the retriever, for the relation extraction
objective by utilizing a differentiable selection of the $k$ nearest instances.
We evaluate the relation extraction performance of ETRAG on the TACRED dataset,
which is a standard benchmark for relation extraction. ETRAG demonstrates
consistent improvements against the baseline model as retrieved instances are
added. Furthermore, the analysis of instances retrieved by the end-to-end
trained retriever confirms that the retrieved instances contain common relation
labels or entities with the query and are specialized for the target task. Our
findings provide a promising foundation for future research on
retrieval-augmented generation and the broader applications of text generation
in Natural Language Processing.",Kohei Makino
2024-05-12T09:48:28Z,http://arxiv.org/abs/2405.13002v1,DuetRAG: Collaborative Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) methods augment the input of Large
Language Models (LLMs) with relevant retrieved passages, reducing factual
errors in knowledge-intensive tasks. However, contemporary RAG approaches
suffer from irrelevant knowledge retrieval issues in complex domain questions
(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to
low-quality generations. To address this issue, we propose a novel
Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our
bootstrapping philosophy is to simultaneously integrate the domain fintuning
and RAG models to improve the knowledge retrieval quality, thereby enhancing
generation quality. Finally, we demonstrate DuetRAG' s matches with expert
human researchers on HotPot QA.",Dian Jiao
2024-07-04T14:20:12Z,http://arxiv.org/abs/2407.03955v1,Meta-prompting Optimized Retrieval-augmented Generation,"Retrieval-augmented generation resorts to content retrieved from external
sources in order to leverage the performance of large language models in
downstream tasks. The excessive volume of retrieved content, the possible
dispersion of its parts, or their out of focus range may happen nevertheless to
eventually have a detrimental rather than an incremental effect. To mitigate
this issue and improve retrieval-augmented generation, we propose a method to
refine the retrieved content before it is included in the prompt by resorting
to meta-prompting optimization. Put to empirical test with the demanding
multi-hop question answering task from the StrategyQA dataset, the evaluation
results indicate that this method outperforms a similar retrieval-augmented
system but without this method by over 30%.",Jo√£o Rodrigues
2024-12-19T21:14:54Z,http://arxiv.org/abs/2412.15404v1,"A Retrieval-Augmented Generation Framework for Academic Literature
  Navigation in Data Science","In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.",Ahmet Yasin Aytar
2023-09-04T08:28:44Z,http://arxiv.org/abs/2309.01431v2,Benchmarking Large Language Models in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) is a promising approach for mitigating
the hallucination of large language models (LLMs). However, existing research
lacks rigorous evaluation of the impact of retrieval-augmented generation on
different large language models, which make it challenging to identify the
potential bottlenecks in the capabilities of RAG for different LLMs. In this
paper, we systematically investigate the impact of Retrieval-Augmented
Generation on large language models. We analyze the performance of different
large language models in 4 fundamental abilities required for RAG, including
noise robustness, negative rejection, information integration, and
counterfactual robustness. To this end, we establish Retrieval-Augmented
Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and
Chinese. RGB divides the instances within the benchmark into 4 separate
testbeds based on the aforementioned fundamental abilities required to resolve
the case. Then we evaluate 6 representative LLMs on RGB to diagnose the
challenges of current LLMs when applying RAG. Evaluation reveals that while
LLMs exhibit a certain degree of noise robustness, they still struggle
significantly in terms of negative rejection, information integration, and
dealing with false information. The aforementioned assessment outcomes indicate
that there is still a considerable journey ahead to effectively apply RAG to
LLMs.",Jiawei Chen
2024-05-30T09:50:38Z,http://arxiv.org/abs/2405.19893v1,"Similarity is Not All You Need: Endowing Retrieval Augmented Generation
  with Multi Layered Thoughts","In recent years, large language models (LLMs) have made remarkable
achievements in various domains. However, the untimeliness and cost of
knowledge updates coupled with hallucination issues of LLMs have curtailed
their applications in knowledge intensive tasks, where retrieval augmented
generation (RAG) can be of help. Nevertheless, existing retrieval augmented
models typically use similarity as a bridge between queries and documents and
follow a retrieve then read procedure. In this work, we argue that similarity
is not always the panacea and totally relying on similarity would sometimes
degrade the performance of retrieval augmented generation. To this end, we
propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented
Generation framework. To begin with, beyond existing similarity oriented
thought, we embrace a small scale utility model that draws supervision from an
LLM for utility oriented thought and further come up with a smarter model by
comprehensively combining the similarity and utility oriented thoughts.
Furthermore, given the fact that the retrieved document set tends to be huge
and using them in isolation makes it difficult to capture the commonalities and
characteristics among them, we propose to make an LLM as a task adaptive
summarizer to endow retrieval augmented generation with compactness-oriented
thought. Finally, with multi layered thoughts from the precedent stages, an LLM
is called for knowledge augmented generation. Extensive experiments on
knowledge-intensive tasks have demonstrated the superiority of MetRag.",Chunjing Gan
2023-08-01T12:04:50Z,http://arxiv.org/abs/2308.00479v1,"Retrieval Augmented Generation and Representative Vector Summarization
  for large unstructured textual data in Medical Education","Large Language Models are increasingly being used for various tasks including
content generation and as chatbots. Despite their impressive performances in
general tasks, LLMs need to be aligned when applying for domain specific tasks
to mitigate the problems of hallucination and producing harmful answers.
Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a
non-parametric knowledgebases to LLMs. Applications of RAG in the field of
medical education are discussed in this paper. A combined extractive and
abstractive summarization method for large unstructured textual data using
representative vectors is proposed.",S. S. Manathunga
2023-12-12T23:22:57Z,http://arxiv.org/abs/2312.07796v1,"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge
  Gaps","The paper presents a methodology for uncovering knowledge gaps on the
internet using the Retrieval Augmented Generation (RAG) model. By simulating
user search behaviour, the RAG system identifies and addresses gaps in
information retrieval systems. The study demonstrates the effectiveness of the
RAG system in generating relevant suggestions with a consistent accuracy of
93%. The methodology can be applied in various fields such as scientific
discovery, educational enhancement, research development, market analysis,
search engine optimisation, and content development. The results highlight the
value of identifying and understanding knowledge gaps to guide future
endeavours.",Joan Figuerola Hurtado
2024-03-03T21:24:35Z,http://arxiv.org/abs/2403.01616v2,"Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models","This paper presents our contributions towards advancing the state of
Vietnamese language understanding and generation through the development and
dissemination of open datasets and pre-trained models for Vietnamese
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).",Nguyen Quang Duc
2024-06-18T09:53:37Z,http://arxiv.org/abs/2406.12449v1,"Retrieval-Augmented Generation for Generative Artificial Intelligence in
  Medicine","Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care.",Rui Yang
2024-06-21T07:52:01Z,http://arxiv.org/abs/2406.14938v1,Towards Retrieval Augmented Generation over Large Video Libraries,"Video content creators need efficient tools to repurpose content, a task that
often requires complex manual or automated searches. Crafting a new video from
large video libraries remains a challenge. In this paper we introduce the task
of Video Library Question Answering (VLQA) through an interoperable
architecture that applies Retrieval Augmented Generation (RAG) to video
libraries. We propose a system that uses large language models (LLMs) to
generate search queries, retrieving relevant video moments indexed by speech
and visual metadata. An answer generation module then integrates user queries
with this metadata to produce responses with specific video timestamps. This
approach shows promise in multimedia content retrieval, and AI-assisted video
content creation.",Yannis Tevissen
2024-07-09T09:46:23Z,http://arxiv.org/abs/2407.06718v1,"A Simple Architecture for Enterprise Large Language Model Applications
  based on Role based security and Clearance Levels using Retrieval-Augmented
  Generation or Mixture of Experts","This study proposes a simple architecture for Enterprise application for
Large Language Models (LLMs) for role based security and NATO clearance levels.
Our proposal aims to address the limitations of current LLMs in handling
security and information access. The proposed architecture could be used while
utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of
experts models (MoE). It could be used only with RAG, or only with MoE or with
both of them. Using roles and security clearance level of the user, documents
in RAG and experts in MoE are filtered. This way information leakage is
prevented.",Atilla √ñzg√ºr
2024-07-12T16:18:00Z,http://arxiv.org/abs/2407.09394v1,"PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with
  User-Centric Agents","Large Language Models (LLMs) struggle with generating reliable outputs due to
outdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG)
models address this by enhancing LLMs with external knowledge, but often fail
to personalize the retrieval process. This paper introduces PersonaRAG, a novel
framework incorporating user-centric agents to adapt retrieval and generation
based on real-time user data and interactions. Evaluated across various
question answering datasets, PersonaRAG demonstrates superiority over baseline
models, providing tailored answers to user needs. The results suggest promising
directions for user-adapted information retrieval systems.",Saber Zerhoudi
2024-10-02T23:14:29Z,http://arxiv.org/abs/2410.03771v1,"SeeSay: An Assistive Device for the Visually Impaired Using Retrieval
  Augmented Generation","In this paper, we present SeeSay, an assistive device designed for
individuals with visual impairments. This system leverages large language
models (LLMs) for speech recognition and visual querying. It effectively
identifies, records, and responds to the user's environment by providing audio
guidance using retrieval-augmented generation (RAG). Our experiments
demonstrate the system's capability to recognize its surroundings and respond
to queries with audio feedback in diverse settings. We hope that the SeeSay
system will facilitate users' comprehension and recollection of their
surroundings, thereby enhancing their environmental perception, improving
navigational capabilities, and boosting overall independence.",Melody Yu
2024-10-28T09:55:52Z,http://arxiv.org/abs/2410.20878v1,"AutoRAG: Automated Framework for optimization of Retrieval Augmented
  Generation Pipeline","Using LLMs (Large Language Models) in conjunction with external documents has
made RAG (Retrieval-Augmented Generation) an essential technology. Numerous
techniques and modules for RAG are being researched, but their performance can
vary across different datasets. Finding RAG modules that perform well on
specific datasets is challenging. In this paper, we propose the AutoRAG
framework, which automatically identifies suitable RAG modules for a given
dataset. AutoRAG explores and approximates the optimal combination of RAG
modules for the dataset. Additionally, we share the results of optimizing a
dataset using AutoRAG. All experimental results and data are publicly available
and can be accessed through our GitHub repository
https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",Dongkyu Kim
2024-12-16T21:09:28Z,http://arxiv.org/abs/2412.12358v1,"BioRAGent: A Retrieval-Augmented Generation System for Showcasing
  Generative Query Expansion and Domain-Specific Search for Scientific Q&A","We present BioRAGent, an interactive web-based retrieval-augmented generation
(RAG) system for biomedical question answering. The system uses large language
models (LLMs) for query expansion, snippet extraction, and answer generation
while maintaining transparency through citation links to the source documents
and displaying generated queries for further editing. Building on our
successful participation in the BioASQ 2024 challenge, we demonstrate how
few-shot learning with LLMs can be effectively applied for a professional
search setting. The system supports both direct short paragraph style responses
and responses with inline citations. Our demo is available online, and the
source code is publicly accessible through GitHub.",Samy Ateia
2024-11-27T18:27:07Z,http://arxiv.org/abs/2411.18583v1,"Automated Literature Review Using NLP Techniques and LLM-Based
  Retrieval-Augmented Generation","This research presents and compares multiple approaches to automate the
generation of literature reviews using several Natural Language Processing
(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language
Model (LLM). The ever-increasing number of research articles provides a huge
challenge for manual literature review. It has resulted in an increased demand
for automation. Developing a system capable of automatically generating the
literature reviews from only the PDF files as input is the primary objective of
this research work. The effectiveness of several Natural Language Processing
(NLP) strategies, such as the frequency-based method (spaCy), the transformer
model (Simple T5), and retrieval-augmented generation (RAG) with Large Language
Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR
dataset is chosen for this research experiment and three distinct techniques
are utilized to implement three different systems for auto-generating the
literature reviews. The ROUGE scores are used for the evaluation of all three
systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo
achieved the highest ROUGE-1 score, 0.364. The transformer model comes in
second place and spaCy is at the last position. Finally, a graphical user
interface is created for the best system based on the large language model.",Nurshat Fateh Ali
2023-09-26T19:23:54Z,http://arxiv.org/abs/2309.15217v1,RAGAS: Automated Evaluation of Retrieval Augmented Generation,"We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With RAGAs, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.",Shahul Es
2023-11-07T18:03:23Z,http://arxiv.org/abs/2311.04177v1,"Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for
  Retrieval Augmented Generation","Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,
(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of
performing amazing tasks typically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not improve over time; they neither
acquire new knowledge nor learn from their successes or failures. Some
approaches to improving the intelligence of LLMs include fine-tuning models
based on problem-solving performance (Zelikman et al., 2022), and building
bigger and more sophisticated models (Bubeck et al., 2023). However, these
methods have the drawback of requiring substantial data and computational
resources to retrain existing models. In this paper, we explore the use of
Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to
improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale
Memory for Retrieval Augmented Generation), a system that learns from its
successes without incurring high training costs. We demonstrate that the
storage and subsequent retrieval of reasoning chains have a positive influence
on performance in grade-school math problems.",Eric Melz
2024-02-13T12:40:39Z,http://arxiv.org/abs/2402.08416v1,Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning,"Large Language Models~(LLMs) have gained immense popularity and are being
increasingly applied in various domains. Consequently, ensuring the security of
these models is of paramount importance. Jailbreak attacks, which manipulate
LLMs to generate malicious content, are recognized as a significant
vulnerability. While existing research has predominantly focused on direct
jailbreak attacks on LLMs, there has been limited exploration of indirect
methods. The integration of various plugins into LLMs, notably Retrieval
Augmented Generation~(RAG), which enables LLMs to incorporate external
knowledge bases into their response generation such as GPTs, introduces new
avenues for indirect jailbreak attacks.
  To fill this gap, we investigate indirect jailbreak attacks on LLMs,
particularly GPTs, introducing a novel attack vector named Retrieval Augmented
Generation Poisoning. This method, Pandora, exploits the synergy between LLMs
and RAG through prompt manipulation to generate unexpected responses. Pandora
uses maliciously crafted content to influence the RAG process, effectively
initiating jailbreak attacks. Our preliminary tests show that Pandora
successfully conducts jailbreak attacks in four different scenarios, achieving
higher success rates than direct attacks, with 64.3\% for GPT-3.5 and 34.8\%
for GPT-4.",Gelei Deng
2024-03-21T07:47:57Z,http://arxiv.org/abs/2403.14197v1,"Context Quality Matters in Training Fusion-in-Decoder for Extractive
  Open-Domain Question Answering","Retrieval-augmented generation models augment knowledge encoded in a language
model by providing additional relevant external knowledge (context) during
generation. Although it has been shown that the quantity and quality of context
impact the performance of retrieval-augmented generation models during
inference, limited research explores how these characteristics affect model
training. This paper explores how context quantity and quality during model
training affect the performance of Fusion-in-Decoder (FiD), the
state-of-the-art retrieval-augmented generation model, in extractive
open-domain question answering tasks. Experimental results suggest that FiD
models overfit to context quality during training and show suboptimal
performance when evaluated on different context quality. Through the
experimental results, we also reveal FiD models trained with different context
quality have different cross-attention distribution patterns. Specifically, as
context quality during training increases, FiD models tend to attend more
uniformly to each passage in context. Finally, based on these observations, we
propose a method to mitigate overfitting to specific context quality by
introducing bias to the cross-attention distribution, which we demonstrate to
be effective in improving the performance of FiD models on different context
quality.",Kosuke Akimoto
2024-03-31T08:58:54Z,http://arxiv.org/abs/2404.00610v1,RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,"Large Language Models (LLMs) exhibit remarkable capabilities but are prone to
generating inaccurate or hallucinatory responses. This limitation stems from
their reliance on vast pretraining datasets, making them susceptible to errors
in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation
(RAG) addresses this by incorporating external, relevant documents into the
response generation process, thus leveraging non-parametric knowledge alongside
LLMs' in-context learning abilities. However, existing RAG implementations
primarily focus on initial input for context retrieval, overlooking the nuances
of ambiguous or complex queries that necessitate further clarification or
decomposition for accurate responses. To this end, we propose learning to
Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper,
endeavoring to enhance the model by equipping it with capabilities for explicit
rewriting, decomposition, and disambiguation. Our experimental results indicate
that our method, when applied to a 7B Llama2 model, surpasses the previous
state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA
datasets, and also demonstrates enhanced performance in handling complex,
multi-hop QA datasets. Our code is available at
https://github.com/chanchimin/RQ-RAG.",Chi-Min Chan
2024-05-26T10:11:40Z,http://arxiv.org/abs/2405.16506v2,GRAG: Graph Retrieval-Augmented Generation,"Naive Retrieval-Augmented Generation (RAG) focuses on individual documents
during retrieval and, as a result, falls short in handling networked documents
which are very popular in many applications such as citation graphs, social
media, and knowledge graphs. To overcome this limitation, we introduce Graph
Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges
in retrieving textual subgraphs and integrating the joint textual and
topological information into Large Language Models (LLMs) to enhance its
generation. To enable efficient textual subgraph retrieval, we propose a novel
divide-and-conquer strategy that retrieves the optimal subgraph structure in
linear time. To achieve graph context-aware generation, incorporate textual
graphs into LLMs through two complementary views-the text view and the graph
view-enabling LLMs to more effectively comprehend and utilize the graph
context. Extensive experiments on graph reasoning benchmarks demonstrate that
in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach
significantly outperforms current state-of-the-art RAG methods.",Yuntong Hu
2024-05-28T12:18:50Z,http://arxiv.org/abs/2405.18111v3,"ATM: Adversarial Tuning Multi-agent System Makes a Robust
  Retrieval-Augmented Generator","Large language models (LLMs) are proven to benefit a lot from
retrieval-augmented generation (RAG) in alleviating hallucinations confronted
with knowledge-intensive questions. RAG adopts information retrieval techniques
to inject external knowledge from semantic-relevant documents as input
contexts. However, since today's Internet is flooded with numerous noisy and
fabricating content, it is inevitable that RAG systems are vulnerable to these
noises and prone to respond incorrectly. To this end, we propose to optimize
the retrieval-augmented Generator with an Adversarial Tuning Multi-agent system
(ATM). The ATM steers the Generator to have a robust perspective of useful
documents for question answering with the help of an auxiliary Attacker agent
through adversarially tuning the agents for several iterations. After rounds of
multi-agent iterative tuning, the Generator can eventually better discriminate
useful documents amongst fabrications. The experimental results verify the
effectiveness of ATM and we also observe that the Generator can achieve better
performance compared to the state-of-the-art baselines.",Junda Zhu
2024-06-03T17:31:06Z,http://arxiv.org/abs/2406.01549v2,"An Information Bottleneck Perspective for Effective Noise Filtering on
  Retrieval-Augmented Generation","Retrieval-augmented generation integrates the capabilities of large language
models with relevant information retrieved from an extensive corpus, yet
encounters challenges when confronted with real-world noisy data. One recent
solution is to train a filter module to find relevant content but only achieve
suboptimal noise compression. In this paper, we propose to introduce the
information bottleneck theory into retrieval-augmented generation. Our approach
involves the filtration of noise by simultaneously maximizing the mutual
information between compression and ground output, while minimizing the mutual
information between compression and retrieved passage. In addition, we derive
the formula of information bottleneck to facilitate its application in novel
comprehensive evaluations, the selection of supervised fine-tuning data, and
the construction of reinforcement learning rewards. Experimental results
demonstrate that our approach achieves significant improvements across various
question answering datasets, not only in terms of the correctness of answer
generation but also in the conciseness with $2.5\%$ compression rate.",Kun Zhu
2024-06-19T06:19:48Z,http://arxiv.org/abs/2406.13249v2,"R^2AG: Incorporating Retrieval Information into Retrieval Augmented
  Generation","Retrieval augmented generation (RAG) has been applied in many scenarios to
augment large language models (LLMs) with external documents provided by
retrievers. However, a semantic gap exists between LLMs and retrievers due to
differences in their training objectives and architectures. This misalignment
forces LLMs to passively accept the documents provided by the retrievers,
leading to incomprehension in the generation process, where the LLMs are
burdened with the task of distinguishing these documents using their inherent
knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill
this gap by incorporating Retrieval information into Retrieval Augmented
Generation. Specifically, R$^2$AG utilizes the nuanced features from the
retrievers and employs a R$^2$-Former to capture retrieval information. Then, a
retrieval-aware prompting strategy is designed to integrate retrieval
information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios
where LLMs and retrievers are frozen. Extensive experiments across five
datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our
analysis reveals that retrieval information serves as an anchor to aid LLMs in
the generation process, thereby filling the semantic gap.",Fuda Ye
2024-06-19T16:42:57Z,http://arxiv.org/abs/2406.13692v2,"Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented
  Generation","Retrieval-augmented language models (RALMs) have shown strong performance and
wide applicability in knowledge-intensive tasks. However, there are significant
trustworthiness concerns as RALMs are prone to generating unfaithful outputs,
including baseless information or contradictions with the retrieved context.
This paper proposes SynCheck, a lightweight monitor that leverages fine-grained
decoding dynamics including sequence likelihood, uncertainty quantification,
context influence, and semantic alignment to synchronously detect unfaithful
sentences. By integrating efficiently measurable and complementary signals,
SynCheck enables accurate and immediate feedback and intervention, achieving
0.85 AUROC in detecting faithfulness errors across six long-form
retrieval-augmented generation tasks, improving prior best method by 4%.
Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding
algorithm guided by beam search for long-form retrieval-augmented generation.
Empirical results demonstrate that FOD outperforms traditional strategies such
as abstention, reranking, or contrastive decoding significantly in terms of
faithfulness, achieving over 10% improvement across six datasets.",Di Wu
2024-07-06T02:22:25Z,http://arxiv.org/abs/2407.04925v1,RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations,"Massive Open Online Courses (MOOCs) have significantly enhanced educational
accessibility by offering a wide variety of courses and breaking down
traditional barriers related to geography, finance, and time. However, students
often face difficulties navigating the vast selection of courses, especially
when exploring new fields of study. Driven by this challenge, researchers have
been exploring course recommender systems to offer tailored guidance that
aligns with individual learning preferences and career aspirations. These
systems face particular challenges in effectively addressing the ``cold start''
problem for new users. Recent advancements in recommender systems suggest
integrating large language models (LLMs) into the recommendation process to
enhance personalized recommendations and address the ``cold start'' problem.
Motivated by these advancements, our study introduces RAMO (Retrieval-Augmented
Generation for MOOCs), a system specifically designed to overcome the ``cold
start'' challenges of traditional course recommender systems. The RAMO system
leverages the capabilities of LLMs, along with Retrieval-Augmented Generation
(RAG)-facilitated contextual understanding, to provide course recommendations
through a conversational interface, aiming to enhance the e-learning
experience.",Jiarui Rao
2024-07-25T13:47:01Z,http://arxiv.org/abs/2407.18044v1,"The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented
  Generation","Digital health chatbots powered by Large Language Models (LLMs) have the
potential to significantly improve personal health management for chronic
conditions by providing accessible and on-demand health coaching and
question-answering. However, these chatbots risk providing unverified and
inaccurate information because LLMs generate responses based on patterns
learned from diverse internet data. Retrieval Augmented Generation (RAG) can
help mitigate hallucinations and inaccuracies in LLM responses by grounding it
on reliable content. However, efficiently and accurately retrieving most
relevant set of content for real-time user questions remains a challenge. In
this work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a
novel approach that pre-computes a database of potential queries from a content
base using LLMs. For an incoming patient question, QB-RAG efficiently matches
it against this pre-generated query database using vector search, improving
alignment between user questions and the content. We establish a theoretical
foundation for QB-RAG and provide a comparative analysis of existing retrieval
enhancement techniques for RAG systems. Finally, our empirical evaluation
demonstrates that QB-RAG significantly improves the accuracy of healthcare
question answering, paving the way for robust and trustworthy LLM applications
in digital health.",Eric Yang
2024-08-16T05:15:12Z,http://arxiv.org/abs/2408.08535v1,"CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for
  Advanced Retrieval-Augmented Generation in Fact-Checking","Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.",Rong-Ching Chang
2024-09-24T07:24:01Z,http://arxiv.org/abs/2409.15815v1,"AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For
  Asthma Patient Support","Asthma rates have risen globally, driven by environmental and lifestyle
factors. Access to immediate medical care is limited, particularly in
developing countries, necessitating automated support systems. Large Language
Models like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have
advanced natural language processing in general and question answering in
particular, however, they are prone to producing factually incorrect responses
(i.e. hallucinations). Retrieval-augmented generation systems, integrating
curated documents, can improve large language models' performance and reduce
the incidence of hallucination. We introduce AsthmaBot, a multi-lingual,
multi-modal retrieval-augmented generation system for asthma support.
Evaluation of an asthma-related frequently asked questions dataset shows
AsthmaBot's efficacy. AsthmaBot has an added interactive and intuitive
interface that integrates different data modalities (text, images, videos) to
make it accessible to the larger public. AsthmaBot is available online via
\url{asthmabot.datanets.org}.",Adil Bahaj
2024-10-15T06:39:35Z,http://arxiv.org/abs/2410.11321v1,Self-adaptive Multimodal Retrieval-Augmented Generation,"Traditional Retrieval-Augmented Generation (RAG) methods are limited by their
reliance on a fixed number of retrieved documents, often resulting in
incomplete or noisy information that undermines task performance. Although
recent adaptive approaches alleviated these problems, their application in
intricate and real-world multimodal tasks remains limited. To address these, we
propose a new approach called Self-adaptive Multimodal Retrieval-Augmented
Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG
not only dynamically filters relevant documents based on the input query,
including image captions when needed, but also verifies the quality of both the
retrieved documents and the output. Extensive experimental results show that
SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy
and response generation. By further ablation experiments and effectiveness
analysis, SAM-RAG maintains high recall quality while improving overall task
performance in multimodal RAG task. Our codes are available at
https://github.com/SAM-RAG/SAM_RAG.",Wenjia Zhai
2024-10-17T06:57:29Z,http://arxiv.org/abs/2410.13272v1,"FRAG: Toward Federated Vector Database Management for Collaborative and
  Secure Retrieval-Augmented Generation","This paper introduces \textit{Federated Retrieval-Augmented Generation
(FRAG)}, a novel database management paradigm tailored for the growing needs of
retrieval-augmented generation (RAG) systems, which are increasingly powered by
large-language models (LLMs). FRAG enables mutually-distrusted parties to
collaboratively perform Approximate $k$-Nearest Neighbor (ANN) searches on
encrypted query vectors and encrypted data stored in distributed vector
databases, all while ensuring that no party can gain any knowledge about the
queries or data of others. Achieving this paradigm presents two key challenges:
(i) ensuring strong security guarantees, such as Indistinguishability under
Chosen-Plaintext Attack (IND-CPA), under practical assumptions (e.g., we avoid
overly optimistic assumptions like non-collusion among parties); and (ii)
maintaining performance overheads comparable to traditional, non-federated RAG
systems. To address these challenges, FRAG employs a single-key homomorphic
encryption protocol that simplifies key management across mutually-distrusted
parties. Additionally, FRAG introduces a \textit{multiplicative caching}
technique to efficiently encrypt floating-point numbers, significantly
improving computational performance in large-scale federated environments. We
provide a rigorous security proof using standard cryptographic reductions and
demonstrate the practical scalability and efficiency of FRAG through extensive
experiments on both benchmark and real-world datasets.",Dongfang Zhao
2024-11-01T08:02:09Z,http://arxiv.org/abs/2411.00437v1,"E2E-AFG: An End-to-End Model with Adaptive Filtering for
  Retrieval-Augmented Generation","Retrieval-augmented generation methods often neglect the quality of content
retrieved from external knowledge bases, resulting in irrelevant information or
potential misinformation that negatively affects the generation results of
large language models. In this paper, we propose an end-to-end model with
adaptive filtering for retrieval-augmented generation (E2E-AFG), which
integrates answer existence judgment and text generation into a single
end-to-end framework. This enables the model to focus more effectively on
relevant content while reducing the influence of irrelevant information and
generating accurate answers. We evaluate E2E-AFG on six representative
knowledge-intensive language datasets, and the results show that it
consistently outperforms baseline models across all tasks, demonstrating the
effectiveness and robustness of the proposed approach.",Yun Jiang
2024-11-14T06:19:18Z,http://arxiv.org/abs/2411.09213v1,"Comprehensive and Practical Evaluation of Retrieval-Augmented Generation
  Systems for Medical Question Answering","Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.",Nghia Trung Ngo
2024-11-25T06:48:38Z,http://arxiv.org/abs/2411.16133v1,Context Awareness Gate For Retrieval Augmented Generation,"Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach
to mitigate the limitations of large language models (LLMs) in answering
domain-specific questions. Previous research has predominantly focused on
improving the accuracy and quality of retrieved data chunks to enhance the
overall performance of the generation pipeline. However, despite ongoing
advancements, the critical issue of retrieving irrelevant information -- which
can impair the ability of the model to utilize its internal knowledge
effectively -- has received minimal attention. In this work, we investigate the
impact of retrieving irrelevant information in open-domain question answering,
highlighting its significant detrimental effect on the quality of LLM outputs.
To address this challenge, we propose the Context Awareness Gate (CAG)
architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt
based on whether the user query necessitates external context retrieval.
Additionally, we introduce the Vector Candidates method, a core mathematical
component of CAG that is statistical, LLM-independent, and highly scalable. We
further examine the distributions of relationships between contexts and
questions, presenting a statistical analysis of these distributions. This
analysis can be leveraged to enhance the context retrieval process in Retrieval
Augmented Generation (RAG) systems.",Mohammad Hassan Heydari
2024-12-19T02:17:35Z,http://arxiv.org/abs/2412.14457v1,VISA: Retrieval Augmented Generation with Visual Source Attribution,"Generation with source attribution is important for enhancing the
verifiability of retrieval-augmented generation (RAG) systems. However,
existing approaches in RAG primarily link generated content to document-level
references, making it challenging for users to locate evidence among multiple
content-rich retrieved documents. To address this challenge, we propose
Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel
approach that combines answer generation with visual source attribution.
Leveraging large vision-language models (VLMs), VISA identifies the evidence
and highlights the exact regions that support the generated answers with
bounding boxes in the retrieved document screenshots. To evaluate its
effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia
webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the
medical domain. Experimental results demonstrate the effectiveness of VISA for
visual source attribution on documents' original look, as well as highlighting
the challenges for improvement. Code, data, and model checkpoints will be
released.",Xueguang Ma
2024-12-17T15:40:08Z,http://arxiv.org/abs/2412.15272v1,"SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven
  Retrieval-Augmented Generation","Recent advancements in large language models (LLMs) have shown impressive
versatility across various tasks. To eliminate its hallucinations,
retrieval-augmented generation (RAG) has emerged as a powerful approach,
leveraging external knowledge sources like knowledge graphs (KGs). In this
paper, we study the task of KG-driven RAG and propose a novel Similar Graph
Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively
addresses the challenge of aligning query texts and KG structures through a
two-stage process: (1) query-to-pattern, which uses an LLM to transform queries
into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the
alignment between the pattern and candidate subgraphs using a graph semantic
distance (GSD) metric. We also develop an optimized retrieval algorithm that
efficiently identifies the top-$k$ subgraphs within 1-second latency on a
10-million-scale KG. Extensive experiments show that SimGRAG outperforms
state-of-the-art KG-driven RAG methods in both question answering and fact
verification, offering superior plug-and-play usability and scalability.",Yuzheng Cai
2023-05-03T21:40:54Z,http://arxiv.org/abs/2305.02437v3,Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory,"With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation~(we define this as
primal problem). The traditional approach for memory retrieval involves
selecting memory that exhibits the highest similarity to the input. However,
this method is constrained by the quality of the fixed corpus from which memory
is retrieved. In this paper, by exploring the duality of the primal problem:
better generation also prompts better memory, we propose a novel framework,
selfmem, which addresses this limitation by iteratively employing a
retrieval-augmented generator to create an unbounded memory pool and using a
memory selector to choose one output as memory for the subsequent generation
round. This enables the model to leverage its own output, referred to as
self-memory, for improved generation. We evaluate the effectiveness of selfmem
on three distinct text generation tasks: neural machine translation,
abstractive text summarization, and dialogue generation, under two generation
paradigms: fine-tuned small model and few-shot LLM. Our approach achieves
state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),
and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in
enhancing retrieval-augmented generation models. Furthermore, we conduct
thorough analyses of each component in the selfmem framework to identify
bottlenecks and provide insights for future research.",Xin Cheng
2023-05-11T17:13:40Z,http://arxiv.org/abs/2305.06983v2,Active Retrieval Augmented Generation,"Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing retrieval
augmented LMs employ a retrieve-and-generate setup that only retrieves
information once based on the input. This is limiting, however, in more general
scenarios involving generation of long texts, where continually gathering
information throughout generation is essential. In this work, we provide a
generalized view of active retrieval augmented generation, methods that
actively decide when and what to retrieve across the course of the generation.
We propose Forward-Looking Active REtrieval augmented generation (FLARE), a
generic method which iteratively uses a prediction of the upcoming sentence to
anticipate future content, which is then utilized as a query to retrieve
relevant documents to regenerate the sentence if it contains low-confidence
tokens. We test FLARE along with baselines comprehensively over 4 long-form
knowledge-intensive generation tasks/datasets. FLARE achieves superior or
competitive performance on all tasks, demonstrating the effectiveness of our
method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",Zhengbao Jiang
2023-07-12T04:44:31Z,http://arxiv.org/abs/2307.05915v2,"Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval
  Augmented Generation Models for Open Book Question-Answering","We propose a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation (RAG) model to the target domain using
supervised fine-tuning and reinforcement learning with synthetic feedback in a
few-shot setting. This, we hypothesize, will yield an aligned, uncertainty
calibrated model that is competitive with GPT-4 based in-context retrieval
augmented generation in generating relevant answers at lower serving costs. The
framework's synthetic generation pipeline will generate synthetic training data
comprising <passage, question, answer> tuples using an open-source LLM and a
novel consistency filtering scheme. The pipeline will be designed to generate
both abstractive and extractive questions that span the entire corpus. The
framework proposes to fine-tune a smaller RAG model comprising a dense
retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In
parallel, the framework will train a Reward model to score domain grounded
answers higher than hallucinated answers using an a priori relevance ordering
of synthetically assembled samples. In the next phase, the framework will align
the RAG model with the target domain using reinforcement learning (Proximal
Policy Optimization). This step may improve the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework will calibrate the model's uncertainty for extractive
question-answers.",C. S. Krishna
2024-01-29T04:36:39Z,http://arxiv.org/abs/2401.15884v3,Corrective Retrieval Augmented Generation,"Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.",Shi-Qi Yan
2024-02-20T17:44:06Z,http://arxiv.org/abs/2402.13178v2,Benchmarking Retrieval-Augmented Generation for Medicine,"While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the ""lost-in-the-middle"" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.",Guangzhi Xiong
2024-03-15T07:45:37Z,http://arxiv.org/abs/2403.10081v3,"DRAGIN: Dynamic Retrieval Augmented Generation based on the Information
  Needs of Large Language Models","Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",Weihang Su
2024-05-22T16:15:17Z,http://arxiv.org/abs/2405.13792v2,"xRAG: Extreme Context Compression for Retrieval-augmented Generation
  with One Token","This paper introduces xRAG, an innovative context compression method tailored
for retrieval-augmented generation. xRAG reinterprets document embeddings in
dense retrieval--traditionally used solely for retrieval--as features from the
retrieval modality. By employing a modality fusion methodology, xRAG seamlessly
integrates these embeddings into the language model representation space,
effectively eliminating the need for their textual counterparts and achieving
an extreme compression rate. In xRAG, the only trainable component is the
modality bridge, while both the retriever and the language model remain frozen.
This design choice allows for the reuse of offline-constructed document
embeddings and preserves the plug-and-play nature of retrieval augmentation.
Experimental results demonstrate that xRAG achieves an average improvement of
over 10% across six knowledge-intensive tasks, adaptable to various language
model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts
configuration. xRAG not only significantly outperforms previous context
compression methods but also matches the performance of uncompressed models on
several datasets, while reducing overall FLOPs by a factor of 3.53. Our work
pioneers new directions in retrieval-augmented generation from the perspective
of multimodality fusion, and we hope it lays the foundation for future
efficient and scalable retrieval-augmented systems",Xin Cheng
2024-06-11T15:15:33Z,http://arxiv.org/abs/2406.07348v3,"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented
  Generation for Question-Answering","Retrieval-Augmented Generation (RAG) has recently demonstrated the
performance of Large Language Models (LLMs) in the knowledge-intensive tasks
such as Question-Answering (QA). RAG expands the query context by incorporating
external knowledge bases to enhance the response accuracy. However, it would be
inefficient to access LLMs multiple times for each query and unreliable to
retrieve all the relevant documents by a single query. We have found that even
though there is low relevance between some critical documents and query, it is
possible to retrieve the remaining documents by combining parts of the
documents with the query. To mine the relevance, a two-stage retrieval
framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is
proposed to improve document retrieval recall and the accuracy of answers while
maintaining efficiency. Additionally, a compact classifier is applied to two
different selection strategies to determine the contribution of the retrieved
documents to answering the query and retrieve the relatively relevant
documents. Meanwhile, DR-RAG call the LLMs only once, which significantly
improves the efficiency of the experiment. The experimental results on
multi-hop QA datasets show that DR-RAG can significantly improve the accuracy
of the answers and achieve new progress in QA systems.",Zijian Hei
2024-07-24T12:27:33Z,http://arxiv.org/abs/2407.21055v1,"Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework
  for Medical Applications","Large Language Models (LLMs) have exhibited remarkable proficiency in natural
language understanding, prompting extensive exploration of their potential
applications across diverse domains. In the medical domain, open-source LLMs
have demonstrated moderate efficacy following domain-specific fine-tuning;
however, they remain substantially inferior to proprietary models such as GPT-4
and GPT-3.5. These open-source models encounter limitations in the
comprehensiveness of domain-specific knowledge and exhibit a propensity for
'hallucinations' during text generation. To mitigate these issues, researchers
have implemented the Retrieval-Augmented Generation (RAG) approach, which
augments LLMs with background information from external knowledge bases while
preserving the model's internal parameters. However, document noise can
adversely affect performance, and the application of RAG in the medical field
remains in its nascent stages. This study presents the Bailicai framework: a
novel integration of retrieval-augmented generation with large language models
optimized for the medical domain. The Bailicai framework augments the
performance of LLMs in medicine through the implementation of four sub-modules.
Experimental results demonstrate that the Bailicai approach surpasses existing
medical domain LLMs across multiple medical benchmarks and exceeds the
performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates
the prevalent issue of hallucinations in medical applications of LLMs and
ameliorates the noise-related challenges associated with traditional RAG
techniques when processing irrelevant or pseudo-relevant documents.",Cui Long
2024-09-06T13:06:29Z,http://arxiv.org/abs/2409.13707v1,"Retrieval Augmented Generation-Based Incident Resolution Recommendation
  System for IT Support","Clients wishing to implement generative AI in the domain of IT Support and
AIOps face two critical issues: domain coverage and model size constraints due
to model choice limitations. Clients might choose to not use larger proprietary
models such as GPT-4 due to cost and privacy concerns and so are limited to
smaller models with potentially less domain coverage that do not generalize to
the client's domain. Retrieval augmented generation is a common solution that
addresses both of these issues: a retrieval system first retrieves the
necessary domain knowledge which a smaller generative model leverages as
context for generation. We present a system developed for a client in the IT
Support domain for support case solution recommendation that combines retrieval
augmented generation (RAG) for answer generation with an encoder-only model for
classification and a generative large language model for query generation. We
cover architecture details, data collection and annotation, development journey
and preliminary validations, expected final deployment process and evaluation
plans, and finally lessons learned.",Paulina Toro Isaza
2024-10-15T14:51:45Z,http://arxiv.org/abs/2410.22353v1,"RuleRAG: Rule-guided retrieval-augmented generation with language models
  for question answering","Retrieval-augmented generation (RAG) framework has shown promising potential
in knowledge-intensive question answering (QA) by retrieving external corpus
and generating based on augmented context. However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-Guided
Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic
rules as demonstrations for in-context learning (RuleRAG-ICL) to guide
retrievers to retrieve logically related documents in the directions of rules
and uniformly guide generators to generate answers attributed by the guidance
of the same set of rules. Moreover, the combination of queries and rules can be
further used as supervised fine-tuning data to update retrievers and generators
(RuleRAG-FT) to achieve better rule-based instruction following capability,
leading to retrieve more supportive results and generate more acceptable
answers. To emphasize the attribution of rules, we construct five rule-aware QA
benchmarks, including three temporal and two static scenarios, and equip
RuleRAG with several kinds of retrievers and generators. Experiments
demonstrate that training-free RuleRAG-ICL effectively improves the retrieval
quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in
exact match scores over standard RAG on average across the five benchmarks, and
further fine-tuned RuleRAG-FT consistently yields more significant performance
enhancement. Extensive analyses indicate that RuleRAG scales well with
increasing numbers of retrieved documents and exhibits generalization ability
for untrained rules.",Zhongwu Chen
2024-11-01T17:11:16Z,http://arxiv.org/abs/2411.00744v1,"CORAG: A Cost-Constrained Retrieval Optimization System for
  Retrieval-Augmented Generation","Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.",Ziting Wang
2024-11-25T16:10:05Z,http://arxiv.org/abs/2411.16523v1,"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology
  Report Generation","In the current paradigm of image captioning, deep learning models are trained
to generate text from image embeddings of latent features. We challenge the
assumption that these latent features ought to be high-dimensional vectors
which require model fine tuning to handle. Here we propose Label Boosted
Retrieval Augmented Generation (LaB-RAG), a text-based approach to image
captioning that leverages image descriptors in the form of categorical labels
to boost standard retrieval augmented generation (RAG) with pretrained large
language models (LLMs). We study our method in the context of radiology report
generation (RRG), where the task is to generate a clinician's report detailing
their observations from a set of radiological images, such as X-rays. We argue
that simple linear classifiers over extracted image embeddings can effectively
transform X-rays into text-space as radiology-specific labels. In combination
with standard RAG, we show that these derived text labels can be used with
general-domain LLMs to generate radiology reports. Without ever training our
generative language model or image feature encoder models, and without ever
directly ""showing"" the LLM an X-ray, we demonstrate that LaB-RAG achieves
better results across natural language and radiology language metrics compared
with other retrieval-based RRG methods, while attaining competitive results
compared to other fine-tuned vision-language RRG models. We further present
results of our experiments with various components of LaB-RAG to better
understand our method. Finally, we critique the use of a popular RRG metric,
arguing it is possible to artificially inflate its results without true
data-leakage.",Steven Song
2024-12-08T07:18:19Z,http://arxiv.org/abs/2412.05838v1,"A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation
  Across Diverse Data","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
incorporating external, domain-specific data into the generative process. While
LLMs are highly capable, they often rely on static, pre-trained datasets,
limiting their ability to integrate dynamic or private data. Traditional RAG
systems typically use a single-agent architecture to handle query generation,
data retrieval, and response synthesis. However, this approach becomes
inefficient when dealing with diverse data sources, such as relational
databases, document stores, and graph databases, often leading to performance
bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system
to address these limitations. Specialized agents, each optimized for a specific
data source, handle query generation for relational, NoSQL, and document-based
systems. These agents collaborate within a modular framework, with query
execution delegated to an environment designed for compatibility across various
database types. This distributed approach enhances query efficiency, reduces
token overhead, and improves response accuracy by ensuring that each agent
focuses on its specialized task. The proposed system is scalable and adaptable,
making it ideal for generative AI workflows that require integration with
diverse, dynamic, or private data sources. By leveraging specialized agents and
a modular execution environment, the system provides an efficient and robust
solution for handling complex, heterogeneous data environments in generative AI
applications.",Aniruddha Salve
2024-12-17T18:42:21Z,http://arxiv.org/abs/2412.13163v2,C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System,"Organizations seeking to utilize Large Language Models (LLMs) for knowledge
querying and analysis often encounter challenges in maintaining an LLM
fine-tuned on targeted, up-to-date information that keeps answers relevant and
grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible
solution for organizations looking to overcome the challenges of maintaining
proprietary models and to help reduce LLM hallucinations in their query
responses. However, RAG comes with its own issues regarding scaling data
pipelines across tiered-access and disparate data sources. In many scenarios,
it is necessary to query beyond a single data silo to provide richer and more
relevant context for an LLM. Analyzing data sources within and across
organizational trust boundaries is often limited by complex data-sharing
policies that prohibit centralized data storage, therefore, inhibit the fast
and effective setup and scaling of RAG solutions. In this paper, we introduce
Confidential Computing (CC) techniques as a solution for secure Federated
Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG
system (C-FedRAG) enables secure connection and scaling of a RAG workflows
across a decentralized network of data providers by ensuring context
confidentiality. We also demonstrate how to implement a C-FedRAG system using
the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and
MIRAGE benchmarking dataset.",Parker Addison
2024-03-31T12:01:34Z,http://arxiv.org/abs/2404.00657v1,Observations on Building RAG Systems for Technical Documents,"Retrieval augmented generation (RAG) for technical documents creates
challenges as embeddings do not often capture domain information. We review
prior art for important factors affecting RAG and perform experiments to
highlight best practices and potential challenges to build RAG systems for
technical documents.",Sumit Soman
2021-02-09T04:50:35Z,http://arxiv.org/abs/2102.04643v1,"Efficient Retrieval Augmented Generation from Unstructured Knowledge for
  Task-Oriented Dialog","This paper summarizes our work on the first track of the ninth Dialog System
Technology Challenge (DSTC 9), ""Beyond Domain APIs: Task-oriented
Conversational Modeling with Unstructured Knowledge Access"". The goal of the
task is to generate responses to user turns in a task-oriented dialog that
require knowledge from unstructured documents. The task is divided into three
subtasks: detection, selection and generation. In order to be compute
efficient, we formulate the selection problem in terms of hierarchical
classification steps. We achieve our best results with this model.
Alternatively, we employ siamese sequence embedding models, referred to as
Dense Knowledge Retrieval, to retrieve relevant documents. This method further
reduces the computation time by a factor of more than 100x at the cost of
degradation in R@1 of 5-6% compared to the first model. Then for either
approach, we use Retrieval Augmented Generation to generate responses based on
multiple selected snippets and we show how the method can be used to fine-tune
trained embeddings.",David Thulke
2022-11-14T02:00:32Z,http://arxiv.org/abs/2211.07067v1,"Retrieval-Augmented Generative Question Answering for Event Argument
  Extraction","Event argument extraction has long been studied as a sequential prediction
problem with extractive-based methods, tackling each argument in isolation.
Although recent work proposes generation-based methods to capture
cross-argument dependency, they require generating and post-processing a
complicated target sequence (template). Motivated by these observations and
recent pretrained language models' capabilities of learning from
demonstrations. We propose a retrieval-augmented generative QA model (R-GQA)
for event argument extraction. It retrieves the most similar QA pair and
augments it as prompt to the current example's context, then decodes the
arguments as answers. Our approach outperforms substantially prior methods
across various settings (i.e. fully supervised, domain transfer, and fewshot
learning). Finally, we propose a clustering-based sampling strategy (JointEnc)
and conduct a thorough analysis of how different strategies influence the
few-shot learning performance. The implementations are available at https://
github.com/xinyadu/RGQA",Xinya Du
2023-09-27T21:26:03Z,http://arxiv.org/abs/2309.16035v3,"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering","Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks such as medical question answering
(QA). In addition, LLMs tend to function as ""black-boxes"", making it
challenging to modify their behavior. To address the problem, our work employs
a transparent process of retrieval augmented generation (RAG), aiming to
improve LLM responses without the need for fine-tuning or retraining.
Specifically, we propose a comprehensive retrieval strategy to extract medical
facts from an external knowledge base, and then inject them into the LLM's
query prompt. Focusing on medical QA, we evaluate the impact of different
retrieval models and the number of facts on LLM performance using the
MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited
an accuracy improvement from 44.46% to 48.54%. This work underscores the
potential of RAG to enhance LLM performance, offering a practical approach to
mitigate the challenges posed by black-box LLMs.",Yucheng Shi
2023-11-16T00:39:39Z,http://arxiv.org/abs/2311.09476v2,"ARES: An Automated Evaluation Framework for Retrieval-Augmented
  Generation Systems","Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. By creating its own synthetic training data, ARES finetunes
lightweight LM judges to assess the quality of individual RAG components. To
mitigate potential prediction errors, ARES utilizes a small set of
human-annotated datapoints for prediction-powered inference (PPI). Across eight
different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES
accurately evaluates RAG systems while using only a few hundred human
annotations during evaluation. Furthermore, ARES judges remain effective across
domain shifts, proving accurate even after changing the type of queries and/or
documents used in the evaluated RAG systems. We make our code and datasets
publicly available on Github.",Jon Saad-Falcon
2023-11-29T15:02:46Z,http://arxiv.org/abs/2311.17696v4,"How to Build an AI Tutor that Can Adapt to Any Course and Provide
  Accurate Answers Using Large Language Model and Retrieval-Augmented
  Generation","This paper proposes a low-code solution to build an AI tutor that leverages
advanced AI techniques to provide accurate and contextually relevant responses
in a personalized learning environment. The OpenAI Assistants API allows AI
Tutor to easily embed, store, retrieve, and manage files and chat history,
enabling a low-code solution. Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG) technology generate sophisticated answers
based on course-specific materials. The application efficiently organizes and
retrieves relevant information through vector embedding and similarity-based
retrieval algorithms. The AI Tutor prototype demonstrates its ability to
generate relevant, accurate answers with source citations. It represents a
significant advancement in technology-enhanced tutoring systems, democratizing
access to high-quality, customized educational support in higher education.",Chenxi Dong
2023-12-09T23:33:16Z,http://arxiv.org/abs/2312.05708v1,Context Tuning for Retrieval Augmented Generation,"Large language models (LLMs) have the remarkable ability to solve new tasks
with just a few examples, but they need access to the right tools. Retrieval
Augmented Generation (RAG) addresses this problem by retrieving a list of
relevant tools for a given task. However, RAG's tool retrieval step requires
all the required information to be explicitly present in the query. This is a
limitation, as semantic search, the widely adopted tool retrieval method, can
fail when the query is incomplete or lacks context. To address this limitation,
we propose Context Tuning for RAG, which employs a smart context retrieval
system to fetch relevant information that improves both tool retrieval and plan
generation. Our lightweight context retrieval model uses numerical,
categorical, and habitual usage signals to retrieve and rank context items. Our
empirical results demonstrate that context tuning significantly enhances
semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for
context retrieval and tool retrieval tasks respectively, and resulting in an
11.6% increase in LLM-based planner accuracy. Additionally, we show that our
proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART
outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at
plan generation, even after tool retrieval, reduces hallucination.",Raviteja Anantha
2023-12-08T18:50:20Z,http://arxiv.org/abs/2312.07559v2,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,"Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA's matches expert human researchers on LitQA.",Jakub L√°la
2023-12-19T23:11:06Z,http://arxiv.org/abs/2312.13303v2,"RealGen: Retrieval Augmented Generation for Controllable Traffic
  Scenarios","Simulation plays a crucial role in the development of autonomous vehicles
(AVs) due to the potential risks associated with real-world testing. Although
significant progress has been made in the visual aspects of simulators,
generating complex behavior among agents remains a formidable challenge. It is
not only imperative to ensure realism in the scenarios generated but also
essential to incorporate preferences and conditions to facilitate controllable
generation for AV training and evaluation. Traditional methods, mainly relying
on memorizing the distribution of training datasets, often fall short in
generating unseen scenarios. Inspired by the success of retrieval augmented
generation in large language models, we present RealGen, a novel
retrieval-based in-context learning framework for traffic scenario generation.
RealGen synthesizes new scenarios by combining behaviors from multiple
retrieved examples in a gradient-free way, which may originate from templates
or tagged scenarios. This in-context learning framework endows versatile
generative capabilities, including the ability to edit scenarios, compose
various behaviors, and produce critical scenarios. Evaluations show that
RealGen offers considerable flexibility and controllability, marking a new
direction in the field of controllable traffic scenario generation. Check our
project website for more information: https://realgen.github.io.",Wenhao Ding
2024-01-03T00:09:34Z,http://arxiv.org/abs/2401.01469v1,"Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation","Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.",Walid Saba
2024-01-03T17:01:44Z,http://arxiv.org/abs/2401.01835v1,"Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework
  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)","Addressing the complexity of comprehensive information retrieval, this study
introduces an innovative, iterative retrieval-augmented generation system. Our
approach uniquely integrates a vector-space driven re-ranking mechanism with
concurrent brainstorming to expedite the retrieval of highly relevant
documents, thereby streamlining the generation of potential queries. This sets
the stage for our novel hybrid process, which synergistically combines
hypothesis formulation with satisfying decision-making strategy to determine
content adequacy, leveraging a chain of thought-based prompting technique. This
unified hypothesize-satisfied phase intelligently distills information to
ascertain whether user queries have been satisfactorily addressed. Upon
reaching this criterion, the system refines its output into a concise
representation, maximizing conceptual density with minimal verbosity. The
iterative nature of the workflow enhances process efficiency and accuracy.
Crucially, the concurrency within the brainstorming phase significantly
accelerates recursive operations, facilitating rapid convergence to solution
satisfaction. Compared to conventional methods, our system demonstrates a
marked improvement in computational time and cost-effectiveness. This research
advances the state-of-the-art in intelligent retrieval systems, setting a new
benchmark for resource-efficient information extraction and abstraction in
knowledge-intensive applications.",Arash Shahmansoori
2024-01-11T12:04:11Z,http://arxiv.org/abs/2401.05856v1,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",Scott Barnett
2024-01-20T14:59:43Z,http://arxiv.org/abs/2401.11246v1,"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented
  Generation in Niche Domains, Exemplified by Korean Medicine","We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.",Bongsu Kang
2024-01-23T09:54:36Z,http://arxiv.org/abs/2401.12599v1,"Revolutionizing Retrieval-Augmented Generation with Enhanced PDF
  Structure Recognition","With the rapid development of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) has become a predominant method in the
field of professional knowledge-based question answering. Presently, major
foundation model companies have opened up Embedding and Chat API interfaces,
and frameworks like LangChain have already integrated the RAG process. It
appears that the key models and steps in RAG have been resolved, leading to the
question: are professional knowledge QA systems now approaching perfection?
This article discovers that current primary methods depend on the premise of
accessing high-quality text corpora. However, since professional documents are
mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts
the effectiveness of professional knowledge-based QA. We conducted an empirical
RAG experiment across hundreds of questions from the corresponding real-world
professional documents. The results show that, ChatDOC, a RAG system equipped
with a panoptic and pinpoint PDF parser, retrieves more accurate and complete
segments, and thus better answers. Empirical experiments show that ChatDOC is
superior to baseline on nearly 47% of questions, ties for 38% of cases, and
falls short on only 15% of cases. It shows that we may revolutionize RAG with
enhanced PDF structure recognition.",Demiao Lin
2024-01-26T08:23:29Z,http://arxiv.org/abs/2402.01717v1,"From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical
  Regulatory Compliance Process","Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.",Jaewoong Kim
2024-01-31T22:06:07Z,http://arxiv.org/abs/2402.03367v2,RAG-Fusion: a New Take on Retrieval-Augmented Generation,"Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.",Zackary Rackauckas
2024-02-05T11:58:56Z,http://arxiv.org/abs/2402.05128v2,"Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation","Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.",Hessa Abdulrahman Alawwad
2024-02-05T22:35:42Z,http://arxiv.org/abs/2402.05131v3,Financial Report Chunking for Effective Retrieval Augmented Generation,"Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question & Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.",Antonio Jimeno Yepes
2024-02-18T15:41:31Z,http://arxiv.org/abs/2402.11626v1,Metacognitive Retrieval-Augmented Large Language Models,"Retrieval-augmented generation have become central in natural language
processing due to their efficacy in generating factual content. While
traditional methods employ single-time retrieval, more recent approaches have
shifted towards multi-time retrieval for multi-hop reasoning tasks. However,
these strategies are bound by predefined reasoning steps, potentially leading
to inaccuracies in response generation. This paper introduces MetaRAG, an
approach that combines the retrieval-augmented generation process with
metacognition. Drawing from cognitive psychology, metacognition allows an
entity to self-reflect and critically evaluate its cognitive processes. By
integrating this, MetaRAG enables the model to monitor, evaluate, and plan its
response strategies, enhancing its introspective reasoning abilities. Through a
three-step metacognitive regulation pipeline, the model can identify
inadequacies in initial cognitive responses and fixes them. Empirical
evaluations show that MetaRAG significantly outperforms existing methods.",Yujia Zhou
2024-02-19T02:48:44Z,http://arxiv.org/abs/2402.11794v1,"Unveiling the Magic: Investigating Attention Distillation in
  Retrieval-augmented Generation","Retrieval-augmented generation framework can address the limitations of large
language models by enabling real-time knowledge updates for more accurate
answers. An efficient way in the training phase of retrieval-augmented models
is attention distillation, which uses attention scores as a supervision signal
instead of manually annotated query-document pairs. Despite its growing
popularity, the detailed mechanisms behind the success of attention
distillation remain unexplored, particularly the specific patterns it leverages
to benefit training. In this paper, we address this gap by conducting a
comprehensive review of attention distillation workflow and identifying key
factors influencing the learning quality of retrieval-augmented language
models. We further propose indicators for optimizing models' training methods
and avoiding ineffective training.",Zizhong Li
2024-02-19T07:06:52Z,http://arxiv.org/abs/2402.11891v1,"FeB4RAG: Evaluating Federated Search in the Context of Retrieval
  Augmented Generation","Federated search systems aggregate results from multiple search engines,
selecting appropriate sources to enhance result quality and align with user
intent. With the increasing uptake of Retrieval-Augmented Generation (RAG)
pipelines, federated search can play a pivotal role in sourcing relevant
information across heterogeneous data sources to generate informed responses.
However, existing datasets, such as those developed in the past TREC FedWeb
tracks, predate the RAG paradigm shift and lack representation of modern
information retrieval challenges. To bridge this gap, we present FeB4RAG, a
novel dataset specifically designed for federated search within RAG frameworks.
This dataset, derived from 16 sub-collections of the widely used \beir
benchmarking collection, includes 790 information requests (akin to
conversational queries) tailored for chatbot applications, along with top
results returned by each resource and associated LLM-derived relevance
judgements. Additionally, to support the need for this collection, we
demonstrate the impact on response generation of a high quality federated
search system for RAG compared to a naive approach to federated search. We do
so by comparing answers generated through the RAG pipeline through a
qualitative side-by-side comparison. Our collection fosters and supports the
development and evaluation of new federated search methods, especially in the
context of RAG pipelines.",Shuai Wang
2024-02-26T09:59:04Z,http://arxiv.org/abs/2402.16457v2,"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for
  Short-form Open-Domain Question Answering","Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine
the necessity of retrieval for queries instead of retrieving indiscriminately
to enhance the efficiency and relevance of the sourced information. However,
previous works largely overlook the evaluation of ARAG approaches, leading to
their effectiveness being understudied. This work presents a benchmark,
RetrievalQA, comprising 1,271 short-form questions covering new world and
long-tail knowledge. The knowledge necessary to answer the questions is absent
from LLMs; therefore, external information must be retrieved to answer
correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG
methods. We observe that calibration-based methods heavily rely on threshold
tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable
retrieval decisions. Based on our findings, we propose Time-Aware Adaptive
Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the
necessity of retrieval without calibration or additional training. The dataset
and code will be available at https://github.com/hyintell/RetrievalQA",Zihan Zhang
2024-02-23T18:35:15Z,http://arxiv.org/abs/2402.16893v1,"The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)","Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.",Shenglai Zeng
2024-02-26T12:56:17Z,http://arxiv.org/abs/2403.00820v1,"Retrieval Augmented Generation Systems: Automatic Dataset Creation,
  Evaluation and Boolean Agent Setup","Retrieval Augmented Generation (RAG) systems have seen huge popularity in
augmenting Large-Language Model (LLM) outputs with domain specific and time
sensitive data. Very recently a shift is happening from simple RAG setups that
query a vector database for additional information with every user input to
more sophisticated forms of RAG. However, different concrete approaches compete
on mostly anecdotal evidence at the moment. In this paper we present a rigorous
dataset creation and evaluation workflow to quantitatively compare different
RAG strategies. We use a dataset created this way for the development and
evaluation of a boolean agent RAG setup: A system in which a LLM can decide
whether to query a vector database or not, thus saving tokens on questions that
can be answered with internal knowledge. We publish our code and generated
dataset online.",Tristan Kenneweg
2024-03-07T06:38:41Z,http://arxiv.org/abs/2403.04256v1,Federated Recommendation via Hybrid Retrieval Augmented Generation,"Federated Recommendation (FR) emerges as a novel paradigm that enables
privacy-preserving recommendations. However, traditional FR systems usually
represent users/items with discrete identities (IDs), suffering from
performance degradation due to the data sparsity and heterogeneity in FR. On
the other hand, Large Language Models (LLMs) as recommenders have proven
effective across various recommendation scenarios. Yet, LLM-based recommenders
encounter challenges such as low inference efficiency and potential
hallucination, compromising their performance in real-world scenarios. To this
end, we propose GPT-FedRec, a federated recommendation framework leveraging
ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.
GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval
process, mining ID-based user patterns and text-based item features. Next, the
retrieved results are converted into text prompts and fed into GPT for
re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims
to extract generalized features from data and exploit pretrained knowledge
within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the
RAG approach also prevents LLM hallucination, improving the recommendation
performance for real-world users. Experimental results on diverse benchmark
datasets demonstrate the superior performance of GPT-FedRec against
state-of-the-art baseline methods.",Huimin Zeng
2024-03-08T21:09:20Z,http://arxiv.org/abs/2403.05676v1,"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System
  Co-design","Retrieval-augmented generation (RAG) can enhance the generation quality of
large language models (LLMs) by incorporating external token databases.
However, retrievals from large databases can constitute a substantial portion
of the overall generation time, particularly when retrievals are periodically
performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design
approach to reduce generation latency and enhance generation quality. PipeRAG
integrates (1) pipeline parallelism to enable concurrent retrieval and
generation processes, (2) flexible retrieval intervals to maximize the
efficiency of pipeline parallelism, and (3) a performance model to
automatically balance retrieval quality and latency based on the generation
states and underlying hardware. Our evaluation shows that, by combining the
three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in
end-to-end generation latency while improving generation quality. These
promising results showcase the effectiveness of co-designing algorithms with
underlying systems, paving the way for the adoption of PipeRAG in future RAG
systems.",Wenqi Jiang
2024-03-13T18:47:00Z,http://arxiv.org/abs/2403.08904v1,"Detecting Hallucination and Coverage Errors in Retrieval Augmented
  Generation for Controversial Topics","We explore a strategy to handle controversial topics in LLM-based chatbots
based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the
absence of a single true answer and surface multiple perspectives. We frame
this as retrieval augmented generation, where perspectives are retrieved from a
knowledge base and the LLM is tasked with generating a fluent and faithful
response from the given perspectives. As a starting point, we use a
deterministic retrieval system and then focus on common LLM failure modes that
arise during this approach to text generation, namely hallucination and
coverage errors. We propose and evaluate three methods to detect such errors
based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our
results demonstrate that LLM-based classifiers, even when trained only on
synthetic errors, achieve high error detection performance, with ROC AUC scores
of 95.3% for hallucination and 90.5% for coverage error detection on
unambiguous error cases. We show that when no training data is available, our
other methods still yield good results on hallucination (84.0%) and coverage
error (85.2%) detection.",Tyler A. Chang
2024-03-14T02:26:31Z,http://arxiv.org/abs/2403.09040v2,"RAGGED: Towards Informed Design of Retrieval Augmented Generation
  Systems","Retrieval-augmented generation (RAG) can significantly improve the
performance of language models (LMs) by providing additional context for tasks
such as document-based question answering (DBQA). However, the effectiveness of
RAG is highly dependent on its configuration. To systematically find the
optimal configuration, we introduce RAGGED, a framework for analyzing RAG
configurations across various DBQA tasks. Using the framework, we discover
distinct LM behaviors in response to varying context quantities, context
qualities, and retrievers. For instance, while some models are robust to noisy
contexts, monotonically performing better with more contexts, others are more
noise-sensitive and can effectively use only a few contexts before declining in
performance. This framework also provides a deeper analysis of these
differences by evaluating the LMs' sensitivity to signal and noise under
specific context quality conditions. Using RAGGED, researchers and
practitioners can derive actionable insights about how to optimally configure
their RAG systems for their specific question-answering tasks.",Jennifer Hsia
2024-02-25T11:22:19Z,http://arxiv.org/abs/2403.12077v1,"Evaluating Robustness of Generative Search Engine on Adversarial Factual
  Questions","Generative search engines have the potential to transform how people seek
information online, but generated responses from existing large language models
(LLMs)-backed generative search engines may not always be accurate.
Nonetheless, retrieval-augmented generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable part of a claim. To this end, we propose evaluating the
robustness of generative search engines in the realistic and high-risk setting,
where adversaries have only black-box system access and seek to deceive the
model into returning incorrect responses. Through a comprehensive human
evaluation of various generative search engines, such as Bing Chat,
PerplexityAI, and YouChat across diverse queries, we demonstrate the
effectiveness of adversarial factual questions in inducing incorrect responses.
Moreover, retrieval-augmented generation exhibits a higher susceptibility to
factual errors compared to LLMs without retrieval. These findings highlight the
potential security risks of these systems and emphasize the need for rigorous
evaluation before deployment.",Xuming Hu
2024-03-18T15:19:17Z,http://arxiv.org/abs/2403.15450v1,Loops On Retrieval Augmented Generation (LoRAG),"This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new
framework designed to enhance the quality of retrieval-augmented text
generation through the incorporation of an iterative loop mechanism. The
architecture integrates a generative model, a retrieval mechanism, and a
dynamic loop module, allowing for iterative refinement of the generated text
through interactions with relevant information retrieved from the input
context. Experimental evaluations on benchmark datasets demonstrate that LoRAG
surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score,
and perplexity, showcasing its effectiveness in achieving both coherence and
relevance in generated text. The qualitative assessment further illustrates
LoRAG's capability to produce contextually rich and coherent outputs. This
research contributes valuable insights into the potential of iterative loops in
mitigating challenges in text generation, positioning LoRAG as a promising
advancement in the field.",Ayush Thakur
2024-03-24T21:02:35Z,http://arxiv.org/abs/2403.16295v1,"LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation","With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.",Ashish Chouhan
2024-03-27T08:42:31Z,http://arxiv.org/abs/2403.18350v2,"Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language","The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).",Ali Mahboub
2024-03-28T17:07:02Z,http://arxiv.org/abs/2403.19584v1,"Img2Loc: Revisiting Image Geolocalization using Multi-modality
  Foundation Models and Image-based Retrieval-Augmented Generation","Geolocating precise locations from images presents a challenging problem in
computer vision and information retrieval.Traditional methods typically employ
either classification, which dividing the Earth surface into grid cells and
classifying images accordingly, or retrieval, which identifying locations by
matching images with a database of image-location pairs. However,
classification-based approaches are limited by the cell size and cannot yield
precise predictions, while retrieval-based systems usually suffer from poor
search quality and inadequate coverage of the global landscape at varied scale
and aggregation levels. To overcome these drawbacks, we present Img2Loc, a
novel system that redefines image geolocalization as a text generation task.
This is achieved using cutting-edge large multi-modality models like GPT4V or
LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based
representations to generate an image-based coordinate query database. It then
uniquely combines query results with images itself, forming elaborate prompts
customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and
YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art
models but does so without any model training.",Zhongliang Zhou
2024-04-04T15:21:22Z,http://arxiv.org/abs/2404.03514v2,"Embedding-Informed Adaptive Retrieval-Augmented Generation of Large
  Language Models","Retrieval-augmented large language models (LLMs) have been remarkably
competent in various NLP tasks. However, it was observed by previous works that
retrieval is not always helpful, especially when the LLM is already
knowledgeable on the query to answer. Motivated by this, Adaptive
Retrieval-Augmented Generation (ARAG) studies retrieving only when the
knowledge asked by the query is absent in the LLM. Previous works of ARAG
either require accessing the pre-training corpus or prompting with additional
model inferences. Aiming to avoid such drawbacks, we propose to determine
whether the model is knowledgeable on a query via inspecting the
(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such
embeddings capture rich information on the model's intrinsic knowledge base,
which enables an efficient way of judging the necessity to retrieve from an
external corpus. Extensive experiments demonstrate our ARAG approach's superior
performance across various benchmarks.",Chengkai Huang
2024-04-04T21:47:43Z,http://arxiv.org/abs/2404.04302v1,"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs
  for Legal Question Answering","Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)
output by providing prior knowledge as context to input. This is beneficial for
knowledge-intensive and expert reliant tasks, including legal
question-answering, which require evidence to validate generated text outputs.
We highlight that Case-Based Reasoning (CBR) presents key opportunities to
structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,
where CBR cycle's initial retrieval stage, its indexing vocabulary, and
similarity knowledge containers are used to enhance LLM queries with
contextually relevant cases. This integration augments the original LLM query,
providing a richer prompt. We present an evaluation of CBR-RAG, and examine
different representations (i.e. general and domain-specific embeddings) and
methods of comparison (i.e. inter, intra and hybrid similarity) on the task of
legal question-answering. Our results indicate that the context provided by
CBR's case reuse enforces similarity between relevant components of the
questions and the evidence base leading to significant improvements in the
quality of generated answers.",Nirmalie Wiratunga
2024-04-10T11:03:17Z,http://arxiv.org/abs/2404.06910v2,"Superposition Prompting: Improving and Accelerating Retrieval-Augmented
  Generation","Despite the successes of large language models (LLMs), they exhibit
significant drawbacks, particularly when processing long contexts. Their
inference cost scales quadratically with respect to sequence length, making it
expensive for deployment in some real-world text processing applications, such
as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the
""distraction phenomenon"", where irrelevant context in the prompt degrades
output quality. To address these drawbacks, we propose a novel RAG prompting
methodology, *superposition prompting*, which can be directly applied to
pre-trained transformer-based LLMs *without the need for fine-tuning*. At a
high level, superposition prompting allows the LLM to process input documents
in parallel *prompt paths*, discarding paths once they are deemed irrelevant.
We demonstrate the capability of our method to simultaneously enhance time
efficiency across a variety of question-answering benchmarks using multiple
pre-trained LLMs. Furthermore, our technique significantly improves accuracy
when the retrieved context is large relative the context the model was trained
on. For example, our approach facilitates a 93x reduction in compute time while
*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the
MPT-7B instruction-tuned model over naive RAG.",Thomas Merth
2024-03-22T17:13:46Z,http://arxiv.org/abs/2404.07220v2,"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy
  with Semantic Search and Hybrid Query-Based Retrievers","Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a
private knowledge base of documents with Large Language Models (LLM) to build
Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes
increasingly challenging as the corpus of documents scales up, with Retrievers
playing an outsized role in the overall RAG accuracy by extracting the most
relevant document from the corpus to provide context to the LLM. In this paper,
we propose the 'Blended RAG' method of leveraging semantic search techniques,
such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid
query strategies. Our study achieves better retrieval results and sets new
benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID
datasets. We further extend such a 'Blended Retriever' to the RAG system to
demonstrate far superior results on Generative Q\&A datasets like SQUAD, even
surpassing fine-tuning performance.",Kunal Sawarkar
2024-04-12T01:42:09Z,http://arxiv.org/abs/2404.08189v1,"Reducing hallucination in structured outputs via Retrieval-Augmented
  Generation","A common and fundamental limitation of Generative AI (GenAI) is its
propensity to hallucinate. While large language models (LLM) have taken the
world by storm, without eliminating or at least reducing hallucinations,
real-world GenAI systems may face challenges in user adoption. In the process
of deploying an enterprise application that produces workflows based on natural
language requirements, we devised a system leveraging Retrieval Augmented
Generation (RAG) to greatly improve the quality of the structured output that
represents such workflows. Thanks to our implementation of RAG, our proposed
system significantly reduces hallucinations in the output and improves the
generalization of our LLM in out-of-domain settings. In addition, we show that
using a small, well-trained retriever encoder can reduce the size of the
accompanying LLM, thereby making deployments of LLM-based systems less
resource-intensive.",Patrice B√©chard
2024-04-19T00:48:30Z,http://arxiv.org/abs/2404.12560v1,"Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for
  Text-to-SQL","The current state-of-the-art (SOTA) for automated text-to-SQL still falls
well short of expert human performance as measured by execution accuracy (EX)
on the BIRD-SQL benchmark. The most accurate methods are also slow and
expensive. To advance the SOTA for text-to-SQL while reducing cost and
improving speed, we explore the combination of low-cost fine tuning, novel
methods for diverse retrieval-augmented generation (RAG) and new input and
output formats that help large language models (LLMs) achieve higher EX. We
introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record
for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher
performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI,
but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the
next-best model using OpenAI, which instead uses the more expensive GPT-4.
Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by
over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push
EX higher.",Dayton G. Thorpe
2024-04-19T13:27:38Z,http://arxiv.org/abs/2404.12879v1,"Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented
  Generation","While Retrieval-Augmented Generation (RAG) plays a crucial role in the
application of Large Language Models (LLMs), existing retrieval methods in
knowledge-dense domains like law and medicine still suffer from a lack of
multi-perspective views, which are essential for improving interpretability and
reliability. Previous research on multi-view retrieval often focused solely on
different semantic forms of queries, neglecting the expression of specific
domain knowledge perspectives. This paper introduces a novel multi-view RAG
framework, MVRAG, tailored for knowledge-dense domains that utilizes
intention-aware query rewriting from multiple domain viewpoints to enhance
retrieval precision, thereby improving the effectiveness of the final
inference. Experiments conducted on legal and medical case retrieval
demonstrate significant improvements in recall and precision rates with our
framework. Our multi-perspective retrieval approach unleashes the potential of
multi-view information enhancing RAG tasks, accelerating the further
application of LLMs in knowledge-intensive fields.",Guanhua Chen
2024-04-21T21:22:28Z,http://arxiv.org/abs/2404.13781v1,Evaluating Retrieval Quality in Retrieval-Augmented Generation,"Evaluating retrieval-augmented generation (RAG) presents challenges,
particularly for retrieval models within these systems. Traditional end-to-end
evaluation methods are computationally expensive. Furthermore, evaluation of
the retrieval model's performance based on query-document relevance labels
shows a small correlation with the RAG system's downstream performance. We
propose a novel evaluation approach, eRAG, where each document in the retrieval
list is individually utilized by the large language model within the RAG
system. The output generated for each document is then evaluated based on the
downstream task ground truth labels. In this manner, the downstream performance
for each document serves as its relevance label. We employ various downstream
task metrics to obtain document-level annotations and aggregate them using
set-based or ranking metrics. Extensive experiments on a wide range of datasets
demonstrate that eRAG achieves a higher correlation with downstream RAG
performance compared to baseline methods, with improvements in Kendall's $\tau$
correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant
computational advantages, improving runtime and consuming up to 50 times less
GPU memory than end-to-end evaluation.",Alireza Salemi
2024-05-05T05:42:33Z,http://arxiv.org/abs/2405.02816v1,"Stochastic RAG: End-to-End Retrieval-Augmented Generation through
  Expected Utility Maximization","This paper introduces Stochastic RAG--a novel approach for end-to-end
optimization of retrieval-augmented generation (RAG) models that relaxes the
simplifying assumptions of marginalization and document independence, made in
most prior work. Stochastic RAG casts the retrieval process in RAG as a
stochastic sampling without replacement process. Through this formulation, we
employ straight-through Gumbel-top-k that provides a differentiable
approximation for sampling without replacement and enables effective end-to-end
optimization for RAG. We conduct extensive experiments on seven diverse
datasets on a wide range of tasks, from open-domain question answering to fact
verification to slot-filling for relation extraction and to dialogue systems.
By applying this optimization method to a recent and effective RAG model, we
advance state-of-the-art results on six out of seven datasets.",Hamed Zamani
2024-05-13T02:33:25Z,http://arxiv.org/abs/2405.07437v2,Evaluation of Retrieval-Augmented Generation: A Survey,"Retrieval-Augmented Generation (RAG) has recently gained traction in natural
language processing. Numerous studies and real-world applications are
leveraging its ability to enhance generative models through external
information retrieval. Evaluating these RAG systems, however, poses unique
challenges due to their hybrid structure and reliance on dynamic knowledge
sources. To better understand these challenges, we conduct A Unified Evaluation
Process of RAG (Auepora) and aim to provide a comprehensive overview of the
evaluation and benchmarks of RAG systems. Specifically, we examine and compare
several quantifiable metrics of the Retrieval and Generation components, such
as relevance, accuracy, and faithfulness, within the current RAG benchmarks,
encompassing the possible output and ground truth pairs. We then analyze the
various datasets and metrics, discuss the limitations of current benchmarks,
and suggest potential directions to advance the field of RAG benchmarks.",Hao Yu
2024-05-13T17:44:05Z,http://arxiv.org/abs/2405.07963v2,"PyZoBot: A Platform for Conversational Information Extraction and
  Synthesis from Curated Zotero Reference Libraries through Advanced
  Retrieval-Augmented Generation","The exponential growth of scientific literature has resulted in information
overload, challenging researchers to effectively synthesize relevant
publications. This paper explores the integration of traditional reference
management software with advanced computational techniques, including Large
Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an
AI-driven platform developed in Python, incorporating Zoteros reference
management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge
extraction and synthesis from extensive human-curated scientific literature
databases. It demonstrates proficiency in handling complex natural language
queries, integrating data from multiple sources, and meticulously presenting
references to uphold research integrity and facilitate further exploration. By
leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot
offers an effective solution to manage information overload and keep pace with
rapid scientific advancements. The development of such AI-enhanced tools
promises significant improvements in research efficiency and effectiveness
across various disciplines.",Suad Alshammari
2024-05-13T19:05:42Z,http://arxiv.org/abs/2405.08120v1,"From Questions to Insightful Answers: Building an Informed Chatbot for
  University Resources","This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot
system built using Retrieval Augmented Generation (RAG) pipelines to enhance
the user experience and access to information within academic settings.The
objective of BARKPLUG V.2 is to provide information to users about various
campus resources, including academic departments, programs, campus facilities,
and student resources at a university setting in an interactive fashion. Our
system leverages university data as an external data corpus and ingests it into
our RAG pipelines for domain-specific question-answering tasks. We evaluate the
effectiveness of our system in generating accurate and pertinent responses for
Mississippi State University, as a case study, using quantitative measures,
employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS).
Furthermore, we evaluate the usability of this system via subjective
satisfaction surveys using the System Usability Scale (SUS). Our system
demonstrates impressive quantitative performance, with a mean RAGAS score of
0.96, and experience, as validated by usability assessments.",Subash Neupane
2024-05-21T07:35:21Z,http://arxiv.org/abs/2405.13084v2,"The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented
  Generation (FutureDial-RAG)","Recently, increasing research interests have focused on retrieval augmented
generation (RAG) to mitigate hallucination for large language models (LLMs).
Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which
aims at promoting the study of RAG for dialog systems. The challenge builds
upon the MobileCS2 dataset, a real-life customer service datasets with nearly
3000 high-quality dialogs containing annotations for knowledge base query and
corresponding results. Over the dataset, we define two tasks, track 1 for
knowledge retrieval and track 2 for response generation, which are core
research questions in dialog systems with RAG. We build baseline systems for
the two tracks and design metrics to measure whether the systems can perform
accurate retrieval and generate informative and coherent response. The baseline
results show that it is very challenging to perform well on the two tasks,
which encourages the participating teams and the community to study how to make
better use of RAG for real-life dialog systems.",Yucheng Cai
2024-05-21T20:03:40Z,http://arxiv.org/abs/2405.13179v4,"RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented
  Generation and Readability Control for Layman Summarization of Biomedical
  Texts","This paper introduces the RAG-RLRC-LaySum framework, designed to make complex
biomedical research understandable to laymen through advanced Natural Language
Processing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,
enhanced by a reranking method, utilizes multiple knowledge sources to ensure
the precision and pertinence of lay summaries. Additionally, our Reinforcement
Learning for Readability Control (RLRC) strategy improves readability, making
scientific content comprehensible to non-specialists. Evaluations using the
publicly accessible PLOS and eLife datasets show that our methods surpass Plain
Gemini model, demonstrating a 20% increase in readability scores, a 15%
improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual
accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific
knowledge, enhancing public engagement with biomedical discoveries.",Yuelyu Ji
2024-05-22T12:12:40Z,http://arxiv.org/abs/2405.13576v1,"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation
  Research","With the advent of Large Language Models (LLMs), the potential of Retrieval
Augmented Generation (RAG) techniques have garnered considerable research
attention. Numerous novel algorithms and models have been introduced to enhance
various aspects of RAG systems. However, the absence of a standardized
framework for implementation, coupled with the inherently intricate RAG
process, makes it challenging and time-consuming for researchers to compare and
evaluate these approaches in a consistent environment. Existing RAG toolkits
like LangChain and LlamaIndex, while available, are often heavy and unwieldy,
failing to meet the personalized needs of researchers. In response to this
challenge, we propose FlashRAG, an efficient and modular open-source toolkit
designed to assist researchers in reproducing existing RAG methods and in
developing their own RAG algorithms within a unified framework. Our toolkit
implements 12 advanced RAG methods and has gathered and organized 32 benchmark
datasets. Our toolkit has various features, including customizable modular
framework, rich collection of pre-implemented RAG works, comprehensive
datasets, efficient auxiliary pre-processing scripts, and extensive and
standard evaluation metrics. Our toolkit and resources are available at
https://github.com/RUC-NLPIR/FlashRAG.",Jiajie Jin
2024-05-24T11:05:45Z,http://arxiv.org/abs/2405.15436v1,"Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented
  Knowledge Graphs and Vector Database for Accreditation Reporting Assistance","In higher education, accreditation is a quality assurance process, where an
institution demonstrates a commitment to delivering high quality programs and
services to their students. For business schools nationally and internationally
the Association to Advance Collegiate Schools of Business (AACSB) accreditation
is the gold standard. For a business school to receive and subsequently
maintain accreditation, the school must undertake a rigorous, time consuming
reporting and peer review process, to demonstrate alignment with the AACSB
Standards. For this project we create a hybrid context retrieval augmented
generation pipeline that can assist in the documentation alignment and
reporting process necessary for accreditation. We implement both a vector
database and knowledge graph, as knowledge stores containing both institutional
data and AACSB Standard data. The output of the pipeline can be used by
institution stakeholders to build their accreditation report, dually grounded
by the context from the knowledge stores. To develop our knowledge graphs we
utilized both a manual construction process as well as an LLM Augmented
Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework
and observed optimal performance on answer relevancy and answer correctness
metrics.",Candace Edwards
2024-05-26T04:03:13Z,http://arxiv.org/abs/2405.16420v1,"M-RAG: Reinforcing Large Language Model Performance through
  Retrieval-Augmented Generation with Multiple Partitions","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant memories from an external database. However, existing RAG
methods typically organize all memories in a whole database, potentially
limiting focus on crucial memories and introducing noise. In this paper, we
introduce a multiple partition paradigm for RAG (called M-RAG), where each
database partition serves as a basic unit for RAG execution. Based on this
paradigm, we propose a novel framework that leverages LLMs with Multi-Agent
Reinforcement Learning to optimize different language generation tasks
explicitly. Through comprehensive experiments conducted on seven datasets,
spanning three language generation tasks and involving three distinct language
model architectures, we confirm that M-RAG consistently outperforms various
baseline methods, achieving improvements of 11%, 8%, and 12% for text
summarization, machine translation, and dialogue generation, respectively.",Zheng Wang
2024-05-27T23:39:17Z,http://arxiv.org/abs/2405.17706v1,"Video Enriched Retrieval Augmented Generation Using Aligned Video
  Captions","In this work, we propose the use of ""aligned visual captions"" as a mechanism
for integrating information contained within videos into retrieval augmented
generation (RAG) based chat assistant systems. These captions are able to
describe the visual and audio content of videos in a large corpus while having
the advantage of being in a textual format that is both easy to reason about &
incorporate into large language model (LLM) prompts, but also typically require
less multimedia content to be inserted into the multimodal LLM context window,
where typical configurations can aggressively fill up the context window by
sampling video frames from the source video. Furthermore, visual captions can
be adapted to specific use cases by prompting the original foundational model /
captioner for particular visual details or fine tuning. In hopes of helping
advancing progress in this area, we curate a dataset and describe automatic
evaluation procedures on common RAG tasks.",Kevin Dela Rosa
2024-05-29T20:56:52Z,http://arxiv.org/abs/2405.19519v1,"Two-layer retrieval augmented generation framework for low-resource
  medical question-answering: proof of concept using Reddit data","Retrieval augmented generation (RAG) provides the capability to constrain
generative model outputs, and mitigate the possibility of hallucination, by
providing relevant in-context text. The number of tokens a generative large
language model (LLM) can incorporate as context is finite, thus limiting the
volume of knowledge from which to generate an answer. We propose a two-layer
RAG framework for query-focused answer generation and evaluate a
proof-of-concept for this framework in the context of query-focused summary
generation from social media forums, focusing on emerging drug-related
information. The evaluations demonstrate the effectiveness of the two-layer
framework in resource constrained settings to enable researchers in obtaining
near real-time data from users.",Sudeshna Das
2024-06-01T14:45:03Z,http://arxiv.org/abs/2406.00456v1,"Mix-of-Granularity: Optimize the Chunking Granularity for
  Retrieval-Augmented Generation","Integrating information from different reference data sources is a major
challenge for Retrieval-Augmented Generation (RAG) systems because each
knowledge source adopts a unique data structure and follows different
conventions. Retrieving from multiple knowledge sources with one fixed strategy
usually leads to under-exploitation of information. To mitigate this drawback,
inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that
dynamically determines the optimal granularity of a knowledge database based on
input queries using a router. The router is efficiently trained with a newly
proposed loss function employing soft labels. We further extend MoG to
Mix-of-Granularity-Graph (MoGG), where reference documents are pre-processed
into graphs, enabling the retrieval of relevant information from distantly
situated chunks. Extensive experiments demonstrate that both MoG and MoGG
effectively predict optimal granularity levels, significantly enhancing the
performance of the RAG system in downstream tasks. The code of both MoG and
MoGG will be made public.",Zijie Zhong
2024-06-04T12:43:23Z,http://arxiv.org/abs/2406.02266v1,"Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning
  Compressor","Despite the prevalence of retrieval-augmented language models (RALMs), the
seamless integration of these models with retrieval mechanisms to enhance
performance in document-based tasks remains challenging. While some
post-retrieval processing Retrieval-Augmented Generation (RAG) methods have
achieved success, most still lack the ability to distinguish pertinent from
extraneous information, leading to potential inconsistencies and reduced
precision in the generated output, which subsequently affects the truthfulness
of the language model's responses. To address these limitations, this work
proposes a novel two-stage consistency learning approach for retrieved
information compression in retrieval-augmented language models to enhance
performance. By incorporating consistency learning, the aim is to generate
summaries that maintain coherence and alignment with the intended semantic
representations of a teacher model while improving faithfulness to the original
retrieved documents. The proposed method is empirically validated across
multiple datasets, demonstrating notable enhancements in precision and
efficiency for question-answering tasks. It outperforms existing baselines and
showcases the synergistic effects of combining contrastive and consistency
learning paradigms within the retrieval-augmented generation framework.",Chuankai Xu
2024-06-06T03:17:44Z,http://arxiv.org/abs/2406.03714v1,"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis
  with Context-Aware Contrastive Language-Audio Pretraining","Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker
using only a short speech prompt. They leverage a strong in-context ability to
mimic the speech prompts, including speaker style, prosody, and emotion.
Therefore, the selection of a speech prompt greatly influences the generated
speech, akin to the importance of a prompt in large language models (LLMs).
However, current prompt-based TTS models choose the speech prompt manually or
simply at random. Hence, in this paper, we adapt retrieval augmented generation
(RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we
additionally consider contextual information during the retrieval process and
present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model
to extract context-aware, style-related features. The objective and subjective
evaluations demonstrate that our proposed RAG method outperforms baselines, and
our CA-CLAP achieves better results than text-only retrieval methods.",Jinlong Xue
2024-05-31T23:30:52Z,http://arxiv.org/abs/2406.04369v1,RAG Does Not Work for Enterprises,"Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of
large language model outputs by incorporating knowledge retrieval. However,
implementing RAG in enterprises poses challenges around data security,
accuracy, scalability, and integration. This paper explores the unique
requirements for enterprise RAG, surveys current approaches and limitations,
and discusses potential advances in semantic search, hybrid queries, and
optimized retrieval. It proposes an evaluation framework to validate enterprise
RAG solutions, including quantitative testing, qualitative analysis, ablation
studies, and industry case studies. This framework aims to help demonstrate the
ability of purpose-built RAG architectures to deliver accuracy and relevance
improvements with enterprise-grade security, compliance and integration. The
paper concludes with implications for enterprise deployments, limitations, and
future research directions. Close collaboration between researchers and
industry partners may accelerate progress in developing and deploying
retrieval-augmented generation technology.",Tilmann Bruckhaus
2024-06-09T14:11:19Z,http://arxiv.org/abs/2406.05794v3,"RE-RAG: Improving Open-Domain QA Performance and Interpretability with
  Relevance Estimator in Retrieval-Augmented Generation","The Retrieval Augmented Generation (RAG) framework utilizes a combination of
parametric knowledge and external knowledge to demonstrate state-of-the-art
performance on open-domain question answering tasks. However, the RAG framework
suffers from performance degradation when the query is accompanied by
irrelevant contexts. In this work, we propose the RE-RAG framework, which
introduces a relevance estimator (RE) that not only provides relative relevance
between contexts as previous rerankers did, but also provides confidence, which
can be used to classify whether given context is useful for answering the given
question. We propose a weakly supervised method for training the RE simply
utilizing question-answer data without any labels for correct contexts. We show
that RE trained with a small generator (sLM) can not only improve the sLM
fine-tuned together with RE but also improve previously unreferenced large
language models (LLMs). Furthermore, we investigate new decoding strategies
that utilize the proposed confidence measured by RE such as choosing to let the
user know that it is ""unanswerable"" to answer the question given the retrieved
contexts or choosing to rely on LLM's parametric knowledge rather than
unrelated contexts.",Kiseung Kim
2024-06-09T17:55:55Z,http://arxiv.org/abs/2406.05870v2,"Machine Against the RAG: Jamming Retrieval-Augmented Generation with
  Blocker Documents","Retrieval-augmented generation (RAG) systems respond to queries by retrieving
relevant documents from a knowledge database, then generating an answer by
applying an LLM to the retrieved documents. We demonstrate that RAG systems
that operate on databases with untrusted content are vulnerable to a new class
of denial-of-service attacks we call jamming. An adversary can add a single
``blocker'' document to the database that will be retrieved in response to a
specific query and result in the RAG system not answering this query -
ostensibly because it lacks the information or because the answer is unsafe.
  We describe and measure the efficacy of several methods for generating
blocker documents, including a new method based on black-box optimization. This
method (1) does not rely on instruction injection, (2) does not require the
adversary to know the embedding or LLM used by the target RAG system, and (3)
does not use an auxiliary LLM to generate blocker documents.
  We evaluate jamming attacks on several LLMs and embeddings and demonstrate
that the existing safety metrics for LLMs do not capture their vulnerability to
jamming. We then discuss defenses against blocker documents.",Avital Shafran
2024-06-11T08:35:23Z,http://arxiv.org/abs/2406.07053v1,"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation
  and LLMs","Large Language Models (LLMs) have immense potential to transform the
telecommunications industry. They could help professionals understand complex
standards, generate code, and accelerate development. However, traditional LLMs
struggle with the precision and source verification essential for telecom work.
To address this, specialized LLM-based solutions tailored to telecommunication
standards are needed. Retrieval-augmented generation (RAG) offers a way to
create precise, fact-based answers. This paper proposes TelecomRAG, a framework
for a Telecommunication Standards Assistant that provides accurate, detailed,
and verifiable responses. Our implementation, using a knowledge base built from
3GPP Release 16 and Release 18 specification documents, demonstrates how this
assistant surpasses generic LLMs, offering superior accuracy, technical depth,
and verifiability, and thus significant value to the telecommunications field.",Girma M. Yilma
2024-06-12T01:19:36Z,http://arxiv.org/abs/2406.07796v2,"Battling Botpoop using GenAI for Higher Education: A Study of a
  Retrieval Augmented Generation Chatbots Impact on Learning","Generative artificial intelligence (GenAI) and large language models (LLMs)
have simultaneously opened new avenues for enhancing human learning and
increased the prevalence of poor-quality information in student response -
termed Botpoop. This study introduces Professor Leodar, a custom-built,
Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to
enhance educational while reducing Botpoop. Deployed at Nanyang Technological
University, Singapore, Professor Leodar offers a glimpse into the future of
AI-assisted learning, offering personalized guidance, 24/7 availability, and
contextually relevant information. Through a mixed-methods approach, we examine
the impact of Professor Leodar on learning, engagement, and exam preparedness,
with 97.1% of participants reporting positive experiences. These findings help
define possible roles of AI in education and highlight the potential of custom
GenAI chatbots. Our combination of chatbot development, in-class deployment and
outcomes study offers a benchmark for GenAI educational tools and is a stepping
stone for redefining the interplay between AI and human learning.",Maung Thway
2024-06-12T22:05:51Z,http://arxiv.org/abs/2406.09459v1,Ad Auctions for LLMs via Retrieval Augmented Generation,"In the field of computational advertising, the integration of ads into the
outputs of large language models (LLMs) presents an opportunity to support
these services without compromising content integrity. This paper introduces
novel auction mechanisms for ad allocation and pricing within the textual
outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a
segment auction where an ad is probabilistically retrieved for each discourse
segment (paragraph, section, or entire output) according to its bid and
relevance, following the RAG framework, and priced according to competing bids.
We show that our auction maximizes logarithmic social welfare, a new notion of
welfare that balances allocation efficiency and fairness, and we characterize
the associated incentive-compatible pricing rule. These results are extended to
multi-ad allocation per segment. An empirical evaluation validates the
feasibility and effectiveness of our approach over several ad auction
scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more
flexibility to allocate ads.",MohammadTaghi Hajiaghayi
2024-06-10T08:23:52Z,http://arxiv.org/abs/2406.10251v3,"The Impact of Quantization on Retrieval-Augmented Generation: An
  Analysis of Small LLMs","Post-training quantization reduces the computational demand of Large Language
Models (LLMs) but can weaken some of their capabilities. Since LLM abilities
emerge with scale, smaller LLMs are more sensitive to quantization. In this
paper, we explore how quantization affects smaller LLMs' ability to perform
retrieval-augmented generation (RAG), specifically in longer contexts. We chose
personalization for evaluation because it is a challenging domain to perform
using RAG as it requires long-context reasoning over multiple documents. We
compare the original FP16 and the quantized INT4 performance of multiple 7B and
8B LLMs on two tasks while progressively increasing the number of retrieved
documents to test how quantized models fare against longer contexts. To better
understand the effect of retrieval, we evaluate three retrieval models in our
experiments. Our findings reveal that if a 7B LLM performs the task well,
quantization does not impair its performance and long-context reasoning
capabilities. We conclude that it is possible to utilize RAG with quantized
smaller LLMs.",Mert Yazan
2024-06-17T12:23:32Z,http://arxiv.org/abs/2406.11460v1,"TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) offers an effective approach for
addressing question answering (QA) tasks. However, the imperfections of the
retrievers in RAG models often result in the retrieval of irrelevant
information, which could introduce noises and degrade the performance,
especially when handling multi-hop questions that require multiple steps of
reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose
TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series
of logically connected knowledge triples, to identify and integrate supporting
evidence from the retrieved documents for answering questions. Specifically,
TRACE employs a KG Generator to create a knowledge graph (KG) from the
retrieved documents, and then uses an Autoregressive Reasoning Chain
Constructor to build reasoning chains. Experimental results on three multi-hop
QA datasets show that TRACE achieves an average performance improvement of up
to 14.03% compared to using all the retrieved documents. Moreover, the results
indicate that using reasoning chains as context, rather than the entire
documents, is often sufficient to correctly answer questions.",Jinyuan Fang
2024-06-17T17:59:35Z,http://arxiv.org/abs/2406.11830v1,Language Modeling with Editable External Knowledge,"When the world changes, so does the text that humans write about it. How do
we build language models that can be easily updated to reflect these changes?
One popular approach is retrieval-augmented generation, in which new documents
are inserted into a knowledge base and retrieved during prediction for
downstream tasks. Most prior work on these systems have focused on improving
behavior during prediction through better retrieval or reasoning. This paper
introduces ERASE, which instead improves model behavior when new documents are
acquired, by incrementally deleting or rewriting other entries in the knowledge
base each time a document is added. In two new benchmark datasets evaluating
models' ability to answer questions about a stream of news articles or
conversations, ERASE improves accuracy relative to conventional
retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)
absolute. Code and data are available at https://github.com/belindal/ERASE",Belinda Z. Li
2024-06-18T09:25:35Z,http://arxiv.org/abs/2406.12430v1,"PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large
  Language Models as Decision Makers","In this paper, we conduct a study to utilize LLMs as a solution for decision
making that requires complex data analysis. We define Decision QA as the task
of answering the best decision, $d_{best}$, for a decision-making question $Q$,
business rules $R$ and a database $D$. Since there is no benchmark that can
examine Decision QA, we propose Decision QA benchmark, DQA. It has two
scenarios, Locating and Building, constructed from two video games (Europa
Universalis IV and Victoria 3) that have almost the same goal as Decision QA.
To address Decision QA effectively, we also propose a new RAG technique called
the iterative plan-then-retrieval augmented generation (PlanRAG). Our
PlanRAG-based LM generates the plan for decision making as the first step, and
the retriever generates the queries for data analysis as the second step. The
proposed method outperforms the state-of-the-art iterative RAG method by 15.8%
in the Locating scenario and by 7.4% in the Building scenario, respectively. We
release our code and benchmark at https://github.com/myeon9h/PlanRAG.",Myeonghwa Lee
2024-06-18T20:51:34Z,http://arxiv.org/abs/2406.13050v1,Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation,"Despite their impressive capabilities, large language models (LLMs) often
face challenges such as temporal misalignment and generating hallucinatory
content. Enhancing LLMs with retrieval mechanisms to fetch relevant information
from external sources offers a promising solution. Inspired by the proverb
""Think twice before you act,"" we propose a dual-angle evaluated
retrieval-augmented generation framework \textit{Think-then-Act}. Unlike
previous approaches that indiscriminately rewrite queries or perform retrieval
regardless of necessity, or generate temporary responses before deciding on
additional retrieval, which increases model generation costs, our framework
employs a two-phase process: (i) assessing the input query for clarity and
completeness to determine if rewriting is necessary; and (ii) evaluating the
model's capability to answer the query and deciding if additional retrieval is
needed. Experimental results on five datasets show that the
\textit{Think-then-Act} framework significantly improves performance. Our
framework demonstrates notable improvements in accuracy and efficiency compared
to existing baselines and performs well in both English and non-English
contexts. Ablation studies validate the optimal model confidence threshold,
highlighting the resource optimization benefits of our approach.",Yige Shen
2024-06-20T22:53:09Z,http://arxiv.org/abs/2406.14773v1,"Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)
  via Pure Synthetic Data","Retrieval-augmented generation (RAG) enhances the outputs of language models
by integrating relevant information retrieved from external knowledge sources.
However, when the retrieval process involves private data, RAG systems may face
severe privacy risks, potentially leading to the leakage of sensitive
information. To address this issue, we propose using synthetic data as a
privacy-preserving alternative for the retrieval data. We propose SAGE, a novel
two-stage synthetic data generation paradigm. In the stage-1, we employ an
attribute-based extraction and generation approach to preserve key contextual
information from the original data. In the stage-2, we further enhance the
privacy properties of the synthetic data through an agent-based iterative
refinement process. Extensive experiments demonstrate that using our synthetic
data as the retrieval context achieves comparable performance to using the
original data while substantially reducing privacy risks. Our work takes the
first step towards investigating the possibility of generating high-utility and
privacy-preserving synthetic data for RAG, opening up new opportunities for the
safe application of RAG systems in various domains.",Shenglai Zeng
2024-06-21T06:26:38Z,http://arxiv.org/abs/2406.14891v2,"Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop
  Question Answering","Multi-Hop Question Answering (MHQA) tasks present a significant challenge for
large language models (LLMs) due to the intensive knowledge required. Current
solutions, like Retrieval-Augmented Generation, typically retrieve potential
documents from an external corpus to read an answer. However, the performance
of this retrieve-then-read paradigm is constrained by the retriever and the
inevitable noise in the retrieved documents. To mitigate these challenges, we
introduce a novel generate-then-ground (GenGround) framework, synergizing the
parametric knowledge of LLMs and external documents to solve a multi-hop
question. GenGround empowers LLMs to alternate two phases until the final
answer is derived: (1) formulate a simpler, single-hop question and directly
generate the answer; (2) ground the question-answer pair in retrieved
documents, amending any wrong predictions in the answer. We also propose an
instructional grounding distillation method to generalize our method into
smaller models. Extensive experiments conducted on four datasets illustrate the
superiority of our method.",Zhengliang Shi
2024-06-21T14:29:39Z,http://arxiv.org/abs/2406.15187v2,"UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world
  Document Analysis","The use of Retrieval-Augmented Generation (RAG) has improved Large Language
Models (LLMs) in collaborating with external data, yet significant challenges
exist in real-world scenarios. In areas such as academic literature and finance
question answering, data are often found in raw text and tables in HTML or PDF
formats, which can be lengthy and highly unstructured. In this paper, we
introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that
involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We
revisit popular LLM- and RAG-based solutions for document analysis and evaluate
the design choices and answer qualities across multiple document domains and
diverse query types. Our evaluation yields interesting findings and highlights
the importance of data parsing and retrieval. We hope our benchmark can shed
light and better serve real-world document analysis applications. The benchmark
suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",Yulong Hui
2024-06-23T17:18:19Z,http://arxiv.org/abs/2406.16167v1,"FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy
  in Large Language Models","We present a novel extension to Retrieval Augmented Generation with the goal
of mitigating factual inaccuracies in the output of large language models.
Specifically, our method draws on the cognitive linguistic theory of frame
semantics for the indexing and retrieval of factual information relevant to
helping large language models answer queries. We conduct experiments to
demonstrate the effectiveness of this method both in terms of retrieval
effectiveness and in terms of the relevance of the frames and frame relations
automatically generated. Our results show that this novel mechanism of Frame
Semantic-based retrieval, designed to improve Retrieval Augmented Generation
(FS-RAG), is effective and offers potential for providing data-driven insights
into frame semantics theory. We provide open access to our program code and
prompts.",Harish Tayyar Madabushi
2024-06-26T04:49:41Z,http://arxiv.org/abs/2406.18064v3,"Evaluating Quality of Answers for Retrieval-Augmented Generation: A
  Strong LLM Is All You Need","We present a comprehensive study of answer quality evaluation in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
""thumbs-up"" or ""thumbs-down"" gesture commonly used in chat applications. This
approach suits factual business contexts where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.",Yang Wang
2024-06-26T07:02:49Z,http://arxiv.org/abs/2406.18114v2,"Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode
  and Effects Analysis","Failure mode and effects analysis (FMEA) is a critical tool for mitigating
potential failures, particular during ramp-up phases of new products. However,
its effectiveness is often limited by the missing reasoning capabilities of the
FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for fine-tuning on custom datasets for
reasoning within FMEA contexts. However, LLMs face challenges in tasks that
require factual knowledge, a gap that retrieval-augmented generation (RAG)
approaches aim to fill. RAG retrieves information from a non-parametric data
store and uses a language model to generate responses. Building on this idea,
we propose to advance the non-parametric data store with a knowledge graph
(KG). By enhancing the RAG framework with a KG, our objective is to leverage
analytical and semantic question-answering capabilities on FMEA data. This
paper contributes by presenting a new ontology for FMEA observations, an
algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced
RAG framework. Our approach is validated through a human study and we measure
the performance of the context retrieval recall and precision.",Lukas Bahr
2024-06-27T15:18:21Z,http://arxiv.org/abs/2406.19251v1,"AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for
  Retrieval-Augmented Generation","Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.",Jia Fu
2024-06-26T05:36:23Z,http://arxiv.org/abs/2406.19417v1,"""Glue pizza and eat rocks"" -- Exploiting Vulnerabilities in
  Retrieval-Augmented Generative Models","Retrieval-Augmented Generative (RAG) models enhance Large Language Models
(LLMs) by integrating external knowledge bases, improving their performance in
applications like fact-checking and information searching. In this paper, we
demonstrate a security threat where adversaries can exploit the openness of
these knowledge bases by injecting deceptive content into the retrieval
database, intentionally changing the model's behavior. This threat is critical
as it mirrors real-world usage scenarios where RAG systems interact with
publicly accessible knowledge bases, such as web scrapings and user-contributed
data pools. To be more realistic, we target a realistic setting where the
adversary has no knowledge of users' queries, knowledge base data, and the LLM
parameters. We demonstrate that it is possible to exploit the model
successfully through crafted content uploads with access to the retriever. Our
findings emphasize an urgent need for security measures in the design and
deployment of RAG systems to prevent potential manipulation and ensure the
integrity of machine-generated content.",Zhen Tan
2024-06-27T19:20:09Z,http://arxiv.org/abs/2406.19493v2,"Development and Evaluation of a Retrieval-Augmented Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems","Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.",Anubhab Majumder
2024-07-01T09:09:27Z,http://arxiv.org/abs/2407.01102v1,BERGEN: A Benchmarking Library for Retrieval-Augmented Generation,"Retrieval-Augmented Generation allows to enhance Large Language Models with
external knowledge. In response to the recent popularity of generative LLMs,
many RAG approaches have been proposed, which involve an intricate number of
different configurations such as evaluation datasets, collections, metrics,
retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in
comparing approaches and understanding the impact of each component in the
pipeline. In this work, we study best practices that lay the groundwork for a
systematic evaluation of RAG and present BERGEN, an end-to-end library for
reproducible research standardizing RAG experiments. In an extensive study
focusing on QA, we benchmark different state-of-the-art retrievers, rerankers,
and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our
open-source library BERGEN is available under
\url{https://github.com/naver/bergen}.",David Rau
2024-07-01T10:26:19Z,http://arxiv.org/abs/2407.01158v1,"Learning to Explore and Select for Coverage-Conditioned
  Retrieval-Augmented Generation","Interactions with billion-scale large language models typically yield
long-form responses due to their extensive parametric capacities, along with
retrieval-augmented features. While detailed responses provide insightful
viewpoint of a specific subject, they frequently generate redundant and less
engaging content that does not meet user interests. In this work, we focus on
the role of query outlining (i.e., selected sequence of queries) in scenarios
that users request a specific range of information, namely coverage-conditioned
($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets
of information-seeking queries decomposed with various perspectives on certain
topics. By utilizing QTree, we train QPlanner, a 7B language model generating
customized query outlines that follow coverage-conditioned queries. We analyze
the effectiveness of generated outlines through automatic and human evaluation,
targeting on retrieval-augmented generation (RAG). Moreover, the experimental
results demonstrate that QPlanner with alignment training can further provide
outlines satisfying diverse user interests. Our resources are available at
https://github.com/youngerous/qtree.",Takyoung Kim
2024-07-01T12:06:34Z,http://arxiv.org/abs/2407.01219v1,Searching for Best Practices in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) techniques have proven to be effective
in integrating up-to-date information, mitigating hallucinations, and enhancing
response quality, particularly in specialized domains. While many RAG
approaches have been proposed to enhance large language models through
query-dependent retrievals, these approaches still suffer from their complex
implementation and prolonged response times. Typically, a RAG workflow involves
multiple processing steps, each of which can be executed in various ways. Here,
we investigate existing RAG approaches and their potential combinations to
identify optimal RAG practices. Through extensive experiments, we suggest
several strategies for deploying RAG that balance both performance and
efficiency. Moreover, we demonstrate that multimodal retrieval techniques can
significantly enhance question-answering capabilities about visual inputs and
accelerate the generation of multimodal content using a ""retrieval as
generation"" strategy.",Xiaohua Wang
2024-07-01T16:56:50Z,http://arxiv.org/abs/2407.01463v1,Retrieval-augmented generation in multilingual settings,"Retrieval-augmented generation (RAG) has recently emerged as a promising
solution for incorporating up-to-date or domain-specific knowledge into large
language models (LLMs) and improving LLM factuality, but is predominantly
studied in English-only settings. In this work, we consider RAG in the
multilingual setting (mRAG), i.e. with user queries and the datastore in 13
languages, and investigate which components and with which adjustments are
needed to build a well-performing mRAG pipeline, that can be used as a strong
baseline in future works. Our findings highlight that despite the availability
of high-quality off-the-shelf multilingual retrievers and generators,
task-specific prompt engineering is needed to enable generation in user
languages. Moreover, current evaluation metrics need adjustments for
multilingual setting, to account for variations in spelling named entities. The
main limitations to be addressed in future works include frequent
code-switching in non-Latin alphabet languages, occasional fluency errors,
wrong reading of the provided documents, or irrelevant retrieval. We release
the code for the resulting mRAG baseline pipeline at
https://github.com/naver/bergen.",Nadezhda Chirkova
2024-07-02T17:59:17Z,http://arxiv.org/abs/2407.02485v1,"RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in
  LLMs","Large language models (LLMs) typically utilize the top-k contexts from a
retriever in retrieval-augmented generation (RAG). In this work, we propose a
novel instruction fine-tuning framework RankRAG, which instruction-tunes a
single LLM for the dual purpose of context ranking and answer generation in
RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding
a small fraction of ranking data into the training blend, and outperform
existing expert ranking models, including the same LLM exclusively fine-tuned
on a large amount of ranking data. For generation, we compare our model with
many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and
ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG
benchmarks. Specifically, our Llama3-RankRAG significantly outperforms
Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In
addition, it also performs comparably to GPT-4 on five RAG benchmarks in the
biomedical domain without instruction fine-tuning on biomedical data,
demonstrating its superb capability for generalization to new domains.",Yue Yu
2024-07-03T15:55:14Z,http://arxiv.org/abs/2407.03227v2,"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and
  Schema Pruning","We focus on Text-to-SQL semantic parsing from the perspective of
retrieval-augmented generation. Motivated by challenges related to the size of
commercial database schemata and the deployability of business intelligence
solutions, we propose $\text{ASTReS}$ that dynamically retrieves input database
information and uses abstract syntax trees to select few-shot examples for
in-context learning.
  Furthermore, we investigate the extent to which an in-parallel semantic
parser can be leveraged for generating approximated versions of the expected
SQL queries, to support our retrieval. We take this approach to the extreme--we
adapt a model consisting of less than $500$M parameters, to act as an extremely
efficient approximator, enhancing it with the ability to process schemata in a
parallelised manner. We apply $\text{ASTReS}$ to monolingual and cross-lingual
benchmarks for semantic parsing, showing improvements over state-of-the-art
baselines. Comprehensive experiments highlight the contribution of modules
involved in this retrieval-augmented generation setting, revealing interesting
directions for future work.",Zhili Shen
2024-07-04T06:26:51Z,http://arxiv.org/abs/2407.07913v1,"CaseGPT: a case reasoning framework based on language models and
  retrieval-augmented generation","This paper presents CaseGPT, an innovative approach that combines Large
Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to
enhance case-based reasoning in the healthcare and legal sectors. The system
addresses the challenges of traditional database queries by enabling fuzzy
searches based on imprecise descriptions, thereby improving data searchability
and usability. CaseGPT not only retrieves relevant case data but also generates
insightful suggestions and recommendations based on patterns discerned from
existing case data. This functionality proves especially valuable for tasks
such as medical diagnostics, legal precedent research, and case strategy
formulation. The paper includes an in-depth discussion of the system's
methodology, its performance in both medical and legal domains, and its
potential for future applications. Our experiments demonstrate that CaseGPT
significantly outperforms traditional keyword-based and simple LLM-based
systems in terms of precision, recall, and efficiency.",Rui Yang
2024-07-11T08:24:16Z,http://arxiv.org/abs/2407.08275v1,"Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval
  Augmented Generation Systems","The choice of embedding model is a crucial step in the design of Retrieval
Augmented Generation (RAG) systems. Given the sheer volume of available
options, identifying clusters of similar models streamlines this model
selection process. Relying solely on benchmark performance scores only allows
for a weak assessment of model similarity. Thus, in this study, we evaluate the
similarity of embedding models within the context of RAG systems. Our
assessment is two-fold: We use Centered Kernel Alignment to compare embeddings
on a pair-wise level. Additionally, as it is especially pertinent to RAG
systems, we evaluate the similarity of retrieval results between these models
using Jaccard and rank similarity. We compare different families of embedding
models, including proprietary ones, across five datasets from the popular
Benchmark Information Retrieval (BEIR). Through our experiments we identify
clusters of models corresponding to model families, but interestingly, also
some inter-family clusters. Furthermore, our analysis of top-k retrieval
similarity reveals high-variance at low k values. We also identify possible
open-source alternatives to proprietary models, with Mistral exhibiting the
highest similarity to OpenAI models.",Laura Caspari
2024-07-16T23:50:07Z,http://arxiv.org/abs/2407.12216v2,"Mindful-RAG: A Study of Points of Failure in Retrieval Augmented
  Generation","Large Language Models (LLMs) are proficient at generating coherent and
contextually relevant text but face challenges when addressing
knowledge-intensive queries in domain-specific and factual question-answering
tasks. Retrieval-augmented generation (RAG) systems mitigate this by
incorporating external knowledge sources, such as structured knowledge graphs
(KGs). However, LLMs often struggle to produce accurate answers despite access
to KG-extracted information containing necessary facts. Our study investigates
this dilemma by analyzing error patterns in existing KG-based RAG methods and
identifying eight critical failure points. We observed that these errors
predominantly occur due to insufficient focus on discerning the question's
intent and adequately gathering relevant context from the knowledge graph
facts. Drawing on this analysis, we propose the Mindful-RAG approach, a
framework designed for intent-based and contextually aligned knowledge
retrieval. This method explicitly targets the identified failures and offers
improvements in the correctness and relevance of responses provided by LLMs,
representing a significant step forward from existing methods.",Garima Agrawal
2024-07-18T06:06:53Z,http://arxiv.org/abs/2407.13193v2,Retrieval-Augmented Generation for Natural Language Processing: A Survey,"Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG training, including RAG with/without datastore update. Then, we
introduce the application of RAG in representative natural language processing
tasks and industrial scenarios. Finally, this paper discusses the future
directions and challenges of RAG for promoting its development.",Shangyu Wu
2024-07-22T03:44:27Z,http://arxiv.org/abs/2407.15353v2,"Customized Retrieval Augmented Generation and Benchmarking for EDA Tool
  Documentation QA","Retrieval augmented generation (RAG) enhances the accuracy and reliability of
generative AI models by sourcing factual information from external databases,
which is extensively employed in document-grounded question-answering (QA)
tasks. Off-the-shelf RAG flows are well pretrained on general-purpose
documents, yet they encounter significant challenges when being applied to
knowledge-intensive vertical domains, such as electronic design automation
(EDA). This paper addresses such issue by proposing a customized RAG framework
along with three domain-specific techniques for EDA tool documentation QA,
including a contrastive learning scheme for text embedding model fine-tuning, a
reranker distilled from proprietary LLM, and a generative LLM fine-tuned with
high-quality domain corpus. Furthermore, we have developed and released a
documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced
RTL-to-GDSII design platform. Experimental results demonstrate that our
proposed RAG flow and techniques have achieved superior performance on ORD-QA
as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA
benchmark and the training dataset for our customized RAG flow are open-source
at https://github.com/lesliepy99/RAG-EDA.",Yuan Pu
2024-07-22T11:55:14Z,http://arxiv.org/abs/2407.15569v2,"An Empirical Study of Retrieval Augmented Generation with
  Chain-of-Thought","Since the launch of ChatGPT at the end of 2022, generative dialogue models
represented by ChatGPT have quickly become essential tools in daily life. As
user expectations increase, enhancing the capability of generative dialogue
models to solve complex problems has become a focal point of current research.
This paper delves into the effectiveness of the RAFT (Retrieval Augmented
Fine-Tuning) method in improving the performance of Generative dialogue models.
RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and
retrieval augmented generation (RAG), which significantly enhanced the model's
information extraction and logical reasoning abilities. We evaluated the RAFT
method across multiple datasets and analysed its performance in various
reasoning tasks, including long-form QA and short-form QA tasks, tasks in both
Chinese and English, and supportive and comparison reasoning tasks. Notably, it
addresses the gaps in previous research regarding long-form QA tasks and
Chinese datasets. Moreover, we also evaluate the benefit of the
chain-of-thought (CoT) in the RAFT method. This work offers valuable insights
for studies focused on enhancing the performance of generative dialogue models.",Yuetong Zhao
2024-07-22T15:53:27Z,http://arxiv.org/abs/2407.15748v1,"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval
  Augmented Generation","In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the
first specialised AI chatbot for cybersecurity. MoRSE aims to provide
comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG
(Retrieval Augmented Generation) systems designed to retrieve and organize
information from multidimensional cybersecurity contexts. MoRSE differs from
traditional RAGs by using parallel retrievers that work together to retrieve
semantically related information in different formats and structures. Unlike
traditional Large Language Models (LLMs) that rely on Parametric Knowledge
Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases
in response to user queries. Subsequently, MoRSE uses this information to
generate accurate answers. In addition, MoRSE benefits from real-time updates
to its knowledge bases, enabling continuous knowledge enrichment without
retraining. We have evaluated the effectiveness of MoRSE against other
state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific
questions. The experimental evaluation has shown that the improvement in terms
of relevance and correctness of the answer is more than 10\% compared to known
solutions such as GPT-4 and Mixtral 7x8.",Marco Simoni
2024-07-23T20:51:52Z,http://arxiv.org/abs/2407.16833v2,"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach","Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.",Zhuowan Li
2024-07-26T20:42:40Z,http://arxiv.org/abs/2407.19075v1,"ESAC (EQ-SANS Assisting Chatbot): Application of Large Language Models
  and Retrieval-Augmented Generation for Enhanced User Experience at EQ-SANS","Neutron scattering experiments have played vital roles in exploring materials
properties in the past decades. While user interfaces have been improved over
time, neutron scattering experiments still require specific knowledge or
training by an expert due to the complexity of such advanced instrumentation
and the limited number of experiments each person may perform each year. This
paper introduces an innovative chatbot application that leverages Large
Language Models(LLM) and Retrieval-Augmented Generation (RAG) technologies to
significantly enhance the user experience at the EQ-SANS, a small-angle neutron
scattering instrument at the Spallation Neutron Source of Oak Ridge National
Laboratory. Through a user-centric design approach, the EQ-SANS Assisting
Chatbot (ESAC) serves as an interactive reference for users, thereby
facilitating the use of the instrument by visiting scientists. By bridging the
gap between the users of EQ-SANS and the control systems required to perform
their experiments, the ESAC sets a new standard for interactive learning and
support for the scientific community using large-scale scientific facilities.",Changwoo Do
2024-07-31T16:04:03Z,http://arxiv.org/abs/2407.21712v1,Adaptive Retrieval-Augmented Generation for Conversational Systems,"Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.",Xi Wang
2024-08-02T08:03:38Z,http://arxiv.org/abs/2408.01084v2,"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for
  Handling Noisy Contexts","When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge the gap between
external knowledge and the LLMs' parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLMs
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.",Youna Kim
2024-08-05T15:16:24Z,http://arxiv.org/abs/2408.02545v1,"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation","Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.",Daniel Fleischer
2024-08-09T12:26:57Z,http://arxiv.org/abs/2408.05026v1,"Retrieval-augmented code completion for local projects using large
  language models","The use of large language models (LLMs) is becoming increasingly widespread
among software developers. However, privacy and computational requirements are
problematic with commercial solutions and the use of LLMs. In this work, we
focus on using LLMs with around 160 million parameters that are suitable for
local execution and augmentation with retrieval from local projects. We train
two models based on the transformer architecture, the generative model GPT-2
and the retrieval-adapted RETRO model, on open-source Python files, and
empirically evaluate and compare them, confirming the benefits of vector
embedding based retrieval. Further, we improve our models' performance with
In-context retrieval-augmented generation, which retrieves code snippets based
on the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented
generation on larger models and conclude that, despite its simplicity, the
approach is more suitable than using the RETRO architecture. We highlight the
key role of proper tokenization in achieving the full potential of LLMs in code
completion.",Marko Hostnik
2024-08-14T10:03:28Z,http://arxiv.org/abs/2408.07425v1,Exploring Retrieval Augmented Generation in Arabic,"Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful
technique in natural language processing, combining the strengths of
retrieval-based and generation-based models to enhance text generation tasks.
However, the application of RAG in Arabic, a language with unique
characteristics and resource constraints, remains underexplored. This paper
presents a comprehensive case study on the implementation and evaluation of RAG
for Arabic text. The work focuses on exploring various semantic embedding
models in the retrieval stage and several LLMs in the generation stage, in
order to investigate what works and what doesn't in the context of Arabic. The
work also touches upon the issue of variations between document dialect and
query dialect in the retrieval stage. Results show that existing semantic
embedding models and LLMs can be effectively employed to build Arabic RAG
pipelines.",Samhaa R. El-Beltagy
2024-08-15T10:20:54Z,http://arxiv.org/abs/2408.08067v2,"RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented
  Generation","Despite Retrieval-Augmented Generation (RAG) showing promising capability in
leveraging external knowledge, a comprehensive evaluation of RAG systems is
still challenging due to the modular nature of RAG, evaluation of long-form
responses and reliability of measurements. In this paper, we propose a
fine-grained evaluation framework, RAGChecker, that incorporates a suite of
diagnostic metrics for both the retrieval and generation modules. Meta
evaluation verifies that RAGChecker has significantly better correlations with
human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8
RAG systems and conduct an in-depth analysis of their performance, revealing
insightful patterns and trade-offs in the design choices of RAG architectures.
The metrics of RAGChecker can guide researchers and practitioners in developing
more effective RAG systems. This work has been open sourced at
https://github.com/amazon-science/RAGChecker.",Dongyu Ru
2024-08-21T17:43:11Z,http://arxiv.org/abs/2408.11800v2,"WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy
  Domain","In the rapidly evolving landscape of Natural Language Processing (NLP) and
text generation, the emergence of Retrieval Augmented Generation (RAG) presents
a promising avenue for improving the quality and reliability of generated text
by leveraging information retrieved from user specified database. Benchmarking
is essential to evaluate and compare the performance of the different RAG
configurations in terms of retriever and generator, providing insights into
their effectiveness, scalability, and suitability for the specific domain and
applications. In this paper, we present a comprehensive framework to generate a
domain relevant RAG benchmark. Our framework is based on automatic
question-answer generation with Human (domain experts)-AI Large Language Model
(LLM) teaming. As a case study, we demonstrate the framework by introducing
WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises
of multiple scientific documents/reports related to environmental impact of
wind energy projects. Our framework systematically evaluates RAG performance
using diverse metrics and multiple question types with varying complexity
level. We also demonstrate the performance of different models on our
benchmark.",Rounak Meyur
2024-08-20T09:29:31Z,http://arxiv.org/abs/2408.11875v1,"Hierarchical Retrieval-Augmented Generation Model with Rethink for
  Multi-hop Question Answering","Multi-hop Question Answering (QA) necessitates complex reasoning by
integrating multiple pieces of information to resolve intricate questions.
However, existing QA systems encounter challenges such as outdated information,
context window length limitations, and an accuracy-quantity trade-off. To
address these issues, we propose a novel framework, the Hierarchical
Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising
Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We
introduce a new hierarchical retrieval strategy that incorporates both sparse
retrieval at the document level and dense retrieval at the chunk level,
effectively integrating their strengths. Additionally, we propose a
single-candidate retrieval method to mitigate the limitations of
multi-candidate retrieval. We also construct two new corpora, Indexed
Wikicorpus and Profile Wikicorpus, to address the issues of outdated and
insufficient knowledge.
  Our experimental results on four datasets demonstrate that HiRAG outperforms
state-of-the-art models across most metrics, and our Indexed Wikicorpus is
effective. The code for HiRAG is available at
https://github.com/2282588541a/HiRAG",Xiaoming Zhang
2024-08-18T11:47:55Z,http://arxiv.org/abs/2408.14484v1,Agentic Retrieval-Augmented Generation for Time Series Analysis,"Time series modeling is crucial for many applications, however, it faces
challenges such as complex spatio-temporal dependencies and distribution shifts
in learning from historical context to predict task-specific outcomes. To
address these challenges, we propose a novel approach using an agentic
Retrieval-Augmented Generation (RAG) framework for time series analysis. The
framework leverages a hierarchical, multi-agent architecture where the master
agent orchestrates specialized sub-agents and delegates the end-user request to
the relevant sub-agent. The sub-agents utilize smaller, pre-trained language
models (SLMs) customized for specific time series tasks through fine-tuning
using instruction tuning and direct preference optimization, and retrieve
relevant prompts from a shared repository of prompt pools containing distilled
knowledge about historical patterns and trends to improve predictions on new
data. Our proposed modular, multi-agent RAG approach offers flexibility and
achieves state-of-the-art performance across major time series tasks by
tackling complex challenges more effectively than task-specific customized
methods across benchmark datasets.",Chidaksh Ravuru
2024-08-28T04:44:43Z,http://arxiv.org/abs/2408.15533v2,"LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via
  Layer-wise Relevance Propagation","Retrieval-Augmented Generation (RAG) has become a primary technique for
mitigating hallucinations in large language models (LLMs). However, incomplete
knowledge extraction and insufficient understanding can still mislead LLMs to
produce irrelevant or even contradictory responses, which means hallucinations
persist in RAG. In this paper, we propose LRP4RAG, a method based on the
Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations
in RAG. Specifically, we first utilize LRP to compute the relevance between the
input and output of the RAG generator. We then apply further extraction and
resampling to the relevance matrix. The processed relevance data are input into
multiple classifiers to determine whether the output contains hallucinations.
To the best of our knowledge, this is the first time that LRP has been used for
detecting RAG hallucinations, and extensive experiments demonstrate that
LRP4RAG outperforms existing baselines.",Haichuan Hu
2024-09-03T07:17:41Z,http://arxiv.org/abs/2409.01666v1,In Defense of RAG in the Era of Long-Context Language Models,"Overcoming the limited context limitations in early-generation LLMs,
retrieval-augmented generation (RAG) has been a reliable solution for
context-based answer generation in the past. Recently, the emergence of
long-context LLMs allows the models to incorporate much longer text sequences,
making RAG less attractive. Recent studies show that long-context LLMs
significantly outperform RAG in long-context applications. Unlike the existing
works favoring the long-context LLM over RAG, we argue that the extremely long
context in LLMs suffers from a diminished focus on relevant information and
leads to potential degradation in answer quality. This paper revisits the RAG
in long-context answer generation. We propose an order-preserve
retrieval-augmented generation (OP-RAG) mechanism, which significantly improves
the performance of RAG for long-context question-answer applications. With
OP-RAG, as the number of retrieved chunks increases, the answer quality
initially rises, and then declines, forming an inverted U-shaped curve. There
exist sweet points where OP-RAG could achieve higher answer quality with much
less tokens than long-context LLM taking the whole context as input. Extensive
experiments on public benchmark demonstrate the superiority of our OP-RAG.",Tan Yu
2024-09-03T13:05:38Z,http://arxiv.org/abs/2409.01864v1,"The Role of Large Language Models in Musicology: Are We Ready to Trust
  the Machines?","In this work, we explore the use and reliability of Large Language Models
(LLMs) in musicology. From a discussion with experts and students, we assess
the current acceptance and concerns regarding this, nowadays ubiquitous,
technology. We aim to go one step further, proposing a semi-automatic method to
create an initial benchmark using retrieval-augmented generation models and
multiple-choice question generation, validated by human experts. Our evaluation
on 400 human-validated questions shows that current vanilla LLMs are less
reliable than retrieval augmented generation from music dictionaries. This
paper suggests that the potential of LLMs in musicology requires musicology
driven research that can specialized LLMs by including accurate and reliable
domain knowledge.",Pedro Ramoneda
2024-09-05T01:58:29Z,http://arxiv.org/abs/2409.03171v2,"MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented
  Generation Question Answering","In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.",Mitchell DeHaven
2024-09-13T07:28:47Z,http://arxiv.org/abs/2409.08597v1,"LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented
  Generation","Recent advancements in integrating speech information into large language
models (LLMs) have significantly improved automatic speech recognition (ASR)
accuracy. However, existing methods often constrained by the capabilities of
the speech encoders under varied acoustic conditions, such as accents. To
address this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)
paradigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech
datastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy
via LLM in-context learning (ICL) capabilities. Experiments on Mandarin and
various Chinese dialect datasets demonstrate significant improvements in ASR
accuracy compared to existing methods, validating the effectiveness of our
approach, especially in handling accent variations.",Shaojun Li
2024-08-29T16:11:20Z,http://arxiv.org/abs/2409.09046v1,"HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications","While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.",Rishi Kalra
2024-09-18T17:03:30Z,http://arxiv.org/abs/2409.12140v2,MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion,"We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos are available at: https://motion-rag.github.io/",Sai Shashank Kalakonda
2024-09-19T05:14:55Z,http://arxiv.org/abs/2409.12468v2,"Familiarity-Aware Evidence Compression for Retrieval-Augmented
  Generation","Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.",Dongwon Jung
2024-09-19T16:23:42Z,http://arxiv.org/abs/2409.12880v1,"Enhancing E-commerce Product Title Translation with Retrieval-Augmented
  Generation and Large Language Models","E-commerce stores enable multilingual product discovery which require
accurate product title translation. Multilingual large language models (LLMs)
have shown promising capacity to perform machine translation tasks, and it can
also enhance and translate product titles cross-lingually in one step. However,
product title translation often requires more than just language conversion
because titles are short, lack context, and contain specialized terminology.
This study proposes a retrieval-augmented generation (RAG) approach that
leverages existing bilingual product information in e-commerce by retrieving
similar bilingual examples and incorporating them as few-shot prompts to
enhance LLM-based product title translation. Experiment results show that our
proposed RAG approach improve product title translation quality with chrF score
gains of up to 15.3% for language pairs where the LLM has limited proficiency.",Bryan Zhang
2024-09-19T23:38:59Z,http://arxiv.org/abs/2409.13122v2,"RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal
  Reinforcement and Retrieval-Augmented Generation","In real-world software engineering tasks, solving a problem often requires
understanding and modifying multiple functions, classes, and files across a
large codebase. Therefore, on the repository level, it is crucial to extract
the relevant information to achieve accurate code completion effectively.
Existing code completion tools have achieved some success, but they struggle to
optimize the retrieval and generation process dynamically. In this paper, we
propose RepoGenReflex, a generic, dynamic, effective framework to address this
challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with
Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal
results for repository-level code completion. RepoGenReflex uses Reflector to
give directional feedback to the next loop. RepoGenReflex chooses the optimal
results stored in the Experience cache based on the RAG-VRL loop. To validate
the framework's generalization ability, we propose a new benchmark RepoGenEval,
which consists of the latest, high-quality real-world repositories in line
completion scenarios. Our experiments demonstrate that RepoGenReflex achieves
significant improvements after optimizing the Reflector component, resulting in
enhanced accuracy and relevance of code completions. Additionally,
RepoGenReflex consistently demonstrates superior performance and effectiveness
across standard code completion tasks, highlighting the robustness and
adaptability of our framework.",Jicheng Wang
2024-09-20T10:36:49Z,http://arxiv.org/abs/2409.13385v2,"Contextual Compression in Retrieval-Augmented Generation for Large
  Language Models: A Survey","Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.",Sourav Verma
2024-09-21T17:41:46Z,http://arxiv.org/abs/2409.14206v1,"AI Assistants for Spaceflight Procedures: Combining Generative
  Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge
  Graphs With Augmented Reality Cues","This paper describes the capabilities and potential of the intelligent
personal assistant (IPA) CORE (Checklist Organizer for Research and
Exploration), designed to support astronauts during procedures onboard the
International Space Station (ISS), the Lunar Gateway station, and beyond. We
reflect on the importance of a reliable and flexible assistant capable of
offline operation and highlight the usefulness of audiovisual interaction using
augmented reality elements to intuitively display checklist information. We
argue that current approaches to the design of IPAs in space operations fall
short of meeting these criteria. Therefore, we propose CORE as an assistant
that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for
a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements
to ensure an intuitive understanding of procedure steps, reliability, offline
availability, and flexibility in terms of response style and procedure updates.",Oliver Bensch
2024-09-24T03:25:36Z,http://arxiv.org/abs/2409.15699v1,"Lighter And Better: Towards Flexible Context Adaptation For Retrieval
  Augmented Generation","The existing Retrieval-Augmented Generation (RAG) systems face significant
challenges in terms of cost and effectiveness. On one hand, they need to encode
the lengthy retrieved contexts before responding to the input tasks, which
imposes substantial computational overhead. On the other hand, directly using
generic Large Language Models (LLMs) often leads to sub-optimal answers, while
task-specific fine-tuning may compromise the LLMs' general capabilities. To
address these challenges, we introduce a novel approach called FlexRAG
(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts
are compressed into compact embeddings before being encoded by the LLMs.
Simultaneously, these compressed embeddings are optimized to enhance downstream
RAG performance. A key feature of FlexRAG is its flexibility, which enables
effective support for diverse compression ratios and selective preservation of
important contexts. Thanks to these technical designs, FlexRAG achieves
superior generation quality while significantly reducing running costs.
Comprehensive experiments on various question-answering datasets validate our
approach as a cost-effective and flexible solution for RAG systems.",Zheng Liu
2024-09-24T05:39:53Z,http://arxiv.org/abs/2409.15763v2,"IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through
  Semantic Comprehension in Retrieval-Augmented Generation Scenarios","In Retrieval-Augmented Generation (RAG) tasks using Large Language Models
(LLMs), the quality of retrieved information is critical to the final output.
This paper introduces the IRSC benchmark for evaluating the performance of
embedding models in multilingual RAG tasks. The benchmark encompasses five
retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,
keyword retrieval, and summary retrieval. Our research addresses the current
lack of comprehensive testing and effective comparison methods for embedding
models in RAG scenarios. We introduced new metrics: the Similarity of Semantic
Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),
and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our
contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and
3) insights into the cross-lingual limitations of embedding models. The IRSC
benchmark aims to enhance the understanding and development of accurate
retrieval systems in RAG tasks. All code and datasets are available at:
https://github.com/Jasaxion/IRSC_Benchmark",Hai Lin
2024-09-24T14:52:14Z,http://arxiv.org/abs/2409.16146v2,"Controlling Risk of Retrieval-augmented Generation: A Counterfactual
  Prompting Framework","Retrieval-augmented generation (RAG) has emerged as a popular solution to
mitigate the hallucination issues of large language models. However, existing
studies on RAG seldom address the issue of predictive uncertainty, i.e., how
likely it is that a RAG model's prediction is incorrect, resulting in
uncontrollable risks in real-world applications. In this work, we emphasize the
importance of risk control, ensuring that RAG models proactively refuse to
answer questions with low confidence. Our research identifies two critical
latent factors affecting RAG's confidence in its predictions: the quality of
the retrieved results and the manner in which these results are utilized. To
guide RAG models in assessing their own confidence based on these two latent
factors, we develop a counterfactual prompting framework that induces the
models to alter these factors and analyzes the effect on their answers. We also
introduce a benchmarking procedure to collect answers with the option to
abstain, facilitating a series of experiments. For evaluation, we introduce
several risk-related metrics and the experimental results demonstrate the
effectiveness of our approach. Our code and benchmark dataset are available at
https://github.com/ict-bigdatalab/RC-RAG.",Lu Chen
2024-09-26T16:12:33Z,http://arxiv.org/abs/2409.18003v1,"Enhancing Tourism Recommender Systems for Sustainable City Trips Using
  Retrieval-Augmented Generation","Tourism Recommender Systems (TRS) have traditionally focused on providing
personalized travel suggestions, often prioritizing user preferences without
considering broader sustainability goals. Integrating sustainability into TRS
has become essential with the increasing need to balance environmental impact,
local community interests, and visitor satisfaction. This paper proposes a
novel approach to enhancing TRS for sustainable city trips using Large Language
Models (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We
enhance the traditional RAG system by incorporating a sustainability metric
based on a city's popularity and seasonal demand during the prompt augmentation
phase. This modification, called Sustainability Augmented Reranking (SAR),
ensures the system's recommendations align with sustainability goals.
Evaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and
Mistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently
matches or outperforms the baseline (without SAR) across most metrics,
highlighting the benefits of incorporating sustainability into TRS.",Ashmi Banerjee
2024-09-28T16:22:53Z,http://arxiv.org/abs/2409.19401v1,"Crafting Personalized Agents through Retrieval-Augmented Generation on
  Editable Memory Graphs","In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.",Zheng Wang
2024-09-29T22:04:26Z,http://arxiv.org/abs/2409.19804v1,"Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in
  Retrieval-Augmented Generation Systems","RAG (Retrieval-Augmented Generation) have recently gained significant
attention for their enhanced ability to integrate external knowledge sources in
open-domain question answering (QA) tasks. However, it remains unclear how
these models address fairness concerns, particularly with respect to sensitive
attributes such as gender, geographic location, and other demographic factors.
First, as language models evolve to prioritize utility, like improving exact
match accuracy, fairness may have been largely overlooked. Second, RAG methods
are complex pipelines, making it hard to identify and address biases, as each
component is optimized for different goals. In this paper, we aim to
empirically evaluate fairness in several RAG methods. We propose a fairness
evaluation framework tailored to RAG methods, using scenario-based questions
and analyzing disparities across demographic attributes. The experimental
results indicate that, despite recent advances in utility-driven optimization,
fairness issues persist in both the retrieval and generation stages,
highlighting the need for more targeted fairness interventions within RAG
pipelines. We will release our dataset and code upon acceptance of the paper.",Xuyang Wu
2024-09-30T08:26:53Z,http://arxiv.org/abs/2409.20075v1,"BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the
  E-commerce Domain","Retrieval Augmented Generation (RAG) system is important in domains such as
e-commerce, which has many long-tail entities and frequently updated
information. Most existing works adopt separate modules for retrieval and
generation, which may be suboptimal since the retrieval task and the generation
task cannot benefit from each other to improve performance. We propose a novel
Backbone Shared RAG framework (BSharedRAG). It first uses a domain-specific
corpus to continually pre-train a base model as a domain-specific backbone
model and then trains two plug-and-play Low-Rank Adaptation (LoRA) modules
based on the shared backbone to minimize retrieval and generation losses
respectively. Experimental results indicate that our proposed BSharedRAG
outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in
retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation.
Our codes, models, and dataset are available at https://bsharedrag.github.io.",Kaisi Guan
2024-09-12T23:29:33Z,http://arxiv.org/abs/2410.00004v1,"Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy
  Similarity Searches and Domain Shift Generalization","The retrieval augmented generation (RAG) system such as Retro has been shown
to improve language modeling capabilities and reduce toxicity and
hallucinations by retrieving from a database of non-parametric memory
containing trillions of entries. We introduce Retro-li that shows retrieval can
also help using a small-scale database, but it demands more accurate and better
neighbors when searching in a smaller hence sparser non-parametric memory. This
can be met by using a proper semantic similarity search. We further propose
adding a regularization to the non-parametric memory for the first time: it
significantly reduces perplexity when the neighbor search operations are noisy
during inference, and it improves generalization when a domain shift occurs. We
also show that Retro-li's non-parametric memory can potentially be implemented
on analog in-memory computing hardware, exhibiting O(1) search time while
causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our
code is available at:
https://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",Gentiana Rashiti
2024-10-01T16:48:13Z,http://arxiv.org/abs/2410.00857v1,"Quantifying reliance on external information over parametric knowledge
  during Retrieval Augmented Generation (RAG) using mechanistic analysis","Retrieval Augmented Generation (RAG) is a widely used approach for leveraging
external context in several natural language applications such as question
answering and information retrieval. Yet, the exact nature in which a Language
Model (LM) leverages this non-parametric memory or retrieved context isn't
clearly understood. This paper mechanistically examines the RAG pipeline to
highlight that LMs demonstrate a ""shortcut'' effect and have a strong bias
towards utilizing the retrieved context to answer questions, while relying
minimally on model priors. We propose (a) Causal Mediation Analysis; for
proving that parametric memory is minimally utilized when answering a question
and (b) Attention Contributions and Knockouts for showing the last token
residual stream do not get enriched from the subject token in the question, but
gets enriched from tokens of RAG-context. We find this pronounced ""shortcut''
behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi)",Reshmi Ghosh
2024-10-03T17:39:38Z,http://arxiv.org/abs/2410.02719v1,"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling
  for Retrieval-Augmented Generation","We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.",Zixuan Li
2024-10-05T17:11:37Z,http://arxiv.org/abs/2410.04231v1,"Metadata-based Data Exploration with Retrieval-Augmented Generation for
  Large Language Models","Developing the capacity to effectively search for requisite datasets is an
urgent requirement to assist data users in identifying relevant datasets
considering the very limited available metadata. For this challenge, the
utilization of third-party data is emerging as a valuable source for
improvement. Our research introduces a new architecture for data exploration
which employs a form of Retrieval-Augmented Generation (RAG) to enhance
metadata-based data discovery. The system integrates large language models
(LLMs) with external vector databases to identify semantic relationships among
diverse types of datasets. The proposed framework offers a new method for
evaluating semantic similarity among heterogeneous data sources and for
improving data exploration. Our study includes experimental results on four
critical tasks: 1) recommending similar datasets, 2) suggesting combinable
datasets, 3) estimating tags, and 4) predicting variables. Our results
demonstrate that RAG can enhance the selection of relevant datasets,
particularly from different categories, when compared to conventional metadata
approaches. However, performance varied across tasks and models, which confirms
the significance of selecting appropriate techniques based on specific use
cases. The findings suggest that this approach holds promise for addressing
challenges in data exploration and discovery, although further refinement is
necessary for estimation tasks.",Teruaki Hayashi
2024-10-08T08:00:12Z,http://arxiv.org/abs/2410.05779v2,LightRAG: Simple and Fast Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG.",Zirui Guo
2024-10-08T08:34:54Z,http://arxiv.org/abs/2410.05801v1,"Retrieving, Rethinking and Revising: The Chain-of-Verification Can
  Improve Retrieval Augmented Generation","Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language
Models (LLMs) by incorporating extensive knowledge retrieved from external
sources. However, such approach encounters some challenges: Firstly, the
original queries may not be suitable for precise retrieval, resulting in
erroneous contextual knowledge; Secondly, the language model can easily
generate inconsistent answer with external references due to their knowledge
boundary limitation. To address these issues, we propose the
chain-of-verification (CoV-RAG) to enhance the external retrieval correctness
and internal generation consistency. Specifically, we integrate the
verification module into the RAG, engaging in scoring, judgment, and rewriting.
To correct external retrieval errors, CoV-RAG retrieves new knowledge using a
revised query. To correct internal generation errors, we unify QA and
verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our
comprehensive experiments across various LLMs demonstrate the effectiveness and
adaptability compared with other strong baselines. Especially, our CoV-RAG can
significantly surpass the state-of-the-art baselines using different LLM
backbones.",Bolei He
2024-10-10T03:51:58Z,http://arxiv.org/abs/2410.07589v1,"No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in
  LLMs, Even for Vigilant Users","Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness
and cost-efficiency in mitigating hallucinations and enhancing the
domain-specific generation capabilities of large language models (LLMs).
However, is this effectiveness and cost-efficiency truly a free lunch? In this
study, we comprehensively investigate the fairness costs associated with RAG by
proposing a practical three-level threat model from the perspective of user
awareness of fairness. Specifically, varying levels of user fairness awareness
result in different degrees of fairness censorship on the external dataset. We
examine the fairness implications of RAG using uncensored, partially censored,
and fully censored datasets. Our experiments demonstrate that fairness
alignment can be easily undermined through RAG without the need for fine-tuning
or retraining. Even with fully censored and supposedly unbiased external
datasets, RAG can lead to biased outputs. Our findings underscore the
limitations of current alignment methods in the context of RAG-based LLMs and
highlight the urgent need for new strategies to ensure fairness. We propose
potential mitigations and call for further research to develop robust fairness
safeguards in RAG-based LLMs.",Mengxuan Hu
2024-10-10T03:52:54Z,http://arxiv.org/abs/2410.07590v1,"TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed
  KV Caches for Chunked Text","Current Retrieval-Augmented Generation (RAG) systems concatenate and process
numerous retrieved document chunks for prefill which requires a large volume of
computation, therefore leading to significant latency in time-to-first-token
(TTFT). To reduce the computation overhead as well as TTFT, we introduce
TurboRAG, a novel RAG system that redesigns the inference paradigm of the
current RAG system by first pre-computing and storing the key-value (KV) caches
of documents offline, and then directly retrieving the saved KV cache for
prefill. Hence, online computation of KV caches is eliminated during inference.
In addition, we provide a number of insights into the mask matrix and
positional embedding mechanisms, plus fine-tune a pretrained language model to
maintain model accuracy of TurboRAG. Our approach is applicable to most
existing large language models and their applications without any requirement
in modification of models and inference systems. Experimental results across a
suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x
compared to the conventional RAG systems (on an average of 8.6x), but reserving
comparable performance to the standard RAG systems.",Songshuo Lu
2024-10-12T19:24:18Z,http://arxiv.org/abs/2410.09623v1,"Quebec Automobile Insurance Question-Answering With Retrieval-Augmented
  Generation","Large Language Models (LLMs) perform outstandingly in various downstream
tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has
been shown to improve performance for legal question answering (Nuruzzaman and
Hussain, 2020; Louis et al., 2024). However, there are limited applications in
insurance questions-answering, a specific type of legal document. This paper
introduces two corpora: the Quebec Automobile Insurance Expertise Reference
Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance
Questions. Our study leverages both corpora to automatically and manually
assess a GPT4-o, a state-of-the-art LLM, to answer Quebec automobile insurance
questions. Our results demonstrate that, on average, using our expertise
reference corpus generates better responses on both automatic and manual
evaluation metrics. However, they also highlight that LLM QA is unreliable
enough for mass utilization in critical areas. Indeed, our results show that
between 5% to 13% of answered questions include a false statement that could
lead to customer misunderstanding.",David Beauchemin
2024-10-15T06:26:24Z,http://arxiv.org/abs/2410.11315v1,"SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented
  Generation","Recent studies in Retrieval-Augmented Generation (RAG) have investigated
extracting evidence from retrieved passages to reduce computational costs and
enhance the final RAG performance, yet it remains challenging. Existing methods
heavily rely on heuristic-based augmentation, encountering several issues: (1)
Poor generalization due to hand-crafted context filtering; (2) Semantics
deficiency due to rule-based context chunking; (3) Skewed length due to
sentence-wise filter learning. To address these issues, we propose a
model-based evidence extraction learning framework, SEER, optimizing a vanilla
model as an evidence extractor with desired properties through self-aligned
learning. Extensive experiments show that our method largely improves the final
RAG performance, enhances the faithfulness, helpfulness, and conciseness of the
extracted evidence, and reduces the evidence length by 9.25 times. The code
will be available at https://github.com/HITsz-TMG/SEER.",Xinping Zhao
2024-10-15T09:02:09Z,http://arxiv.org/abs/2410.11414v1,"ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via
  Mechanistic Interpretability","Retrieval-Augmented Generation (RAG) models are designed to incorporate
external knowledge, reducing hallucinations caused by insufficient parametric
(internal) knowledge. However, even with accurate and relevant retrieved
content, RAG models can still produce hallucinations by generating outputs that
conflict with the retrieved information. Detecting such hallucinations requires
disentangling how Large Language Models (LLMs) utilize external and parametric
knowledge. Current detection methods often focus on one of these mechanisms or
without decoupling their intertwined effects, making accurate detection
difficult. In this paper, we investigate the internal mechanisms behind
hallucinations in RAG scenarios. We discover hallucinations occur when the
Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual
stream, while Copying Heads fail to effectively retain or integrate external
knowledge from retrieved content. Based on these findings, we propose ReDeEP, a
novel method that detects hallucinations by decoupling LLM's utilization of
external context and parametric knowledge. Our experiments show that ReDeEP
significantly improves RAG hallucination detection accuracy. Additionally, we
introduce AARF, which mitigates hallucinations by modulating the contributions
of Knowledge FFNs and Copying Heads.",Zhongxiang Sun
2024-10-01T03:54:45Z,http://arxiv.org/abs/2410.12812v1,"Optimizing and Evaluating Enterprise Retrieval-Augmented Generation
  (RAG): A Content Design Perspective","Retrieval-augmented generation (RAG) is a popular technique for using large
language models (LLMs) to build customer-support, question-answering solutions.
In this paper, we share our team's practical experience building and
maintaining enterprise-scale RAG solutions that answer users' questions about
our software based on product documentation. Our experience has not always
matched the most common patterns in the RAG literature. This paper focuses on
solution strategies that are modular and model-agnostic. For example, our
experience over the past few years - using different search methods and LLMs,
and many knowledge base collections - has been that simple changes to the way
we create knowledge base content can have a huge impact on our RAG solutions'
success. In this paper, we also discuss how we monitor and evaluate results.
Common RAG benchmark evaluation techniques have not been useful for evaluating
responses to novel user questions, so we have found a flexible, ""human in the
lead"" approach is required.",Sarah Packowski
2024-10-03T22:29:47Z,http://arxiv.org/abs/2410.12837v1,"A Comprehensive Survey of Retrieval-Augmented Generation (RAG):
  Evolution, Current Landscape and Future Directions","This paper presents a comprehensive study of Retrieval-Augmented Generation
(RAG), tracing its evolution from foundational concepts to the current state of
the art. RAG combines retrieval mechanisms with generative language models to
enhance the accuracy of outputs, addressing key limitations of LLMs. The study
explores the basic architecture of RAG, focusing on how retrieval and
generation are integrated to handle knowledge-intensive tasks. A detailed
review of the significant technological advancements in RAG is provided,
including key innovations in retrieval-augmented language models and
applications across various domains such as question-answering, summarization,
and knowledge-based tasks. Recent research breakthroughs are discussed,
highlighting novel methods for improving retrieval efficiency. Furthermore, the
paper examines ongoing challenges such as scalability, bias, and ethical
concerns in deployment. Future research directions are proposed, focusing on
improving the robustness of RAG models, expanding the scope of application of
RAG models, and addressing societal implications. This survey aims to serve as
a foundational resource for researchers and practitioners in understanding the
potential of RAG and its trajectory in natural language processing.",Shailja Gupta
2024-10-17T03:38:54Z,http://arxiv.org/abs/2410.13192v2,"Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented
  Generation with Large Language Models","The integration of documents generated by LLMs themselves (Self-Docs)
alongside retrieved documents has emerged as a promising strategy for
retrieval-augmented generation systems. However, previous research primarily
focuses on optimizing the use of Self-Docs, with their inherent properties
remaining underexplored. To bridge this gap, we first investigate the overall
effectiveness of Self-Docs, identifying key factors that shape their
contribution to RAG performance (RQ1). Building on these insights, we develop a
taxonomy grounded in Systemic Functional Linguistics to compare the influence
of various Self-Docs categories (RQ2) and explore strategies for combining them
with external sources (RQ3). Our findings reveal which types of Self-Docs are
most beneficial and offer practical guidelines for leveraging them to achieve
significant improvements in knowledge-intensive question answering tasks.",Jiatao Li
2024-10-17T06:30:55Z,http://arxiv.org/abs/2410.13258v1,"A Systematic Investigation of Knowledge Retrieval and Selection for
  Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has emerged as a powerful method for
enhancing natural language generation by integrating external knowledge into a
model's output. While prior work has demonstrated the importance of improving
knowledge retrieval for boosting generation quality, the role of knowledge
selection remains less clear. In this paper, we perform a comprehensive
analysis of how knowledge retrieval and selection influence downstream
generation performance in RAG systems. By simulating different retrieval and
selection conditions through a controlled mixture of gold and distractor
knowledge, we assess the impact of these factors on generation outcomes. Our
findings indicate that the downstream generator model's capability, as well as
the complexity of the task and dataset, significantly influence the impact of
knowledge retrieval and selection on the overall RAG system performance. In
typical scenarios, improving the knowledge recall score is key to enhancing
generation outcomes, with the knowledge selector providing a limited additional
benefit when a strong generator model is used on clear, well-defined tasks. For
weaker generator models or more ambiguous tasks and datasets, the knowledge F1
score becomes a critical factor, and the knowledge selector plays a more
prominent role in improving overall performance.",Xiangci Li
2024-10-17T07:46:49Z,http://arxiv.org/abs/2410.13293v2,"SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation","Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations. Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM). Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a ""reasoning score"" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more
structured problem-solving process potentially providing educational benefits
for students.",Prakhar Dixit
2024-10-18T04:17:49Z,http://arxiv.org/abs/2410.14167v1,"Optimizing Retrieval-Augmented Generation with Elasticsearch for
  Enhanced Question-Answering Systems","This study aims to improve the accuracy and quality of large-scale language
models (LLMs) in answering questions by integrating Elasticsearch into the
Retrieval Augmented Generation (RAG) framework. The experiment uses the
Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and
compares the performance of different retrieval methods, including traditional
methods based on keyword matching or semantic similarity calculation, BM25-RAG
and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that
ES-RAG not only has obvious advantages in retrieval efficiency but also
performs well in key indicators such as accuracy, which is 0.51 percentage
points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search
capabilities and rich configuration options enable the entire
question-answering system to better handle complex queries and provide more
flexible and efficient responses based on the diverse needs of users. Future
research directions can further explore how to optimize the interaction
mechanism between Elasticsearch and LLM, such as introducing higher-level
semantic understanding and context-awareness capabilities, to achieve a more
intelligent and humanized question-answering experience.",Jiajing Chen
2024-10-18T14:02:34Z,http://arxiv.org/abs/2410.14479v1,"Backdoored Retrievers for Prompt Injection Attacks on Retrieval
  Augmented Generation of Large Language Models","Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating coherent text but remain limited by the static nature of their
training data. Retrieval Augmented Generation (RAG) addresses this issue by
combining LLMs with up-to-date information retrieval, but also expand the
attack surface of the system. This paper investigates prompt injection attacks
on RAG, focusing on malicious objectives beyond misinformation, such as
inserting harmful links, promoting unauthorized services, and initiating
denial-of-service behaviors. We build upon existing corpus poisoning techniques
and propose a novel backdoor attack aimed at the fine-tuning process of the
dense retriever component. Our experiments reveal that corpus poisoning can
achieve significant attack success rates through the injection of a small
number of compromised documents into the retriever corpus. In contrast,
backdoor attacks demonstrate even higher success rates but necessitate a more
complex setup, as the victim must fine-tune the retriever using the attacker
poisoned dataset.",Cody Clop
2024-10-21T12:21:49Z,http://arxiv.org/abs/2410.15944v1,"Developing Retrieval Augmented Generation (RAG) based LLM Systems from
  PDFs: An Experience Report","This paper presents an experience report on the development of Retrieval
Augmented Generation (RAG) systems using PDF documents as the primary data
source. The RAG architecture combines generative capabilities of Large Language
Models (LLMs) with the precision of information retrieval. This approach has
the potential to redefine how we interact with and augment both structured and
unstructured knowledge in generative models to enhance transparency, accuracy,
and contextuality of responses. The paper details the end-to-end pipeline, from
data collection, preprocessing, to retrieval indexing and response generation,
highlighting technical challenges and practical solutions. We aim to offer
insights to researchers and practitioners developing similar systems using two
distinct approaches: OpenAI's Assistant API with GPT Series and Llama's
open-source models. The practical implications of this research lie in
enhancing the reliability of generative AI systems in various sectors where
domain-specific knowledge and real-time information retrieval is important. The
Python code used in this work is also available at:
https://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.",Ayman Asad Khan
2024-10-23T15:24:16Z,http://arxiv.org/abs/2410.17952v1,"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large
  Language Models to Specialized Domains","Retrieval-augmented generation (RAG) enhances the question-answering (QA)
abilities of large language models (LLMs) by integrating external knowledge.
However, adapting general-purpose RAG systems to specialized fields such as
science and medicine poses unique challenges due to distribution shifts and
limited access to domain-specific data. To tackle this, we propose SimRAG, a
self-training approach that equips the LLM with joint capabilities of question
answering and question generation for domain adaptation. Our method first
fine-tunes the LLM on instruction-following, question-answering, and
search-related data. Then, it prompts the same LLM to generate diverse
domain-relevant questions from unlabeled corpora, with an additional filtering
strategy to retain high-quality synthetic examples. By leveraging these
synthetic examples, the LLM can improve their performance on domain-specific
RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three
domains, demonstrate that SimRAG outperforms baselines by 1.2\%--8.6\%.",Ran Xu
2024-10-24T00:49:46Z,http://arxiv.org/abs/2410.18344v1,"Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned
  and Retrieval-Augmented Generation Models","This paper introduces a novel approach to enhancing closed-domain Question
Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley
National Laboratory (LBL) Science Information Technology (ScienceIT) domain.
Utilizing a rich dataset derived from the ScienceIT documentation, our study
embarks on a detailed comparison of two fine-tuned large language models and
five retrieval-augmented generation (RAG) models. Through data processing
techniques, we transform the documentation into structured
context-question-answer triples, leveraging the latest Large Language Models
(AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for
data-driven insights. Additionally, we introduce the Aggregated Knowledge Model
(AKM), which synthesizes responses from the seven models mentioned above using
K-means clustering to select the most representative answers. The evaluation of
these models across multiple metrics offers a comprehensive look into their
effectiveness and suitability for the LBL ScienceIT environment. The results
demonstrate the potential benefits of integrating fine-tuning and
retrieval-augmented strategies, highlighting significant performance
improvements achieved with the AKM. The insights gained from this study can be
applied to develop specialized QA systems tailored to specific domains.",Fengchen Liu
2024-10-28T05:35:04Z,http://arxiv.org/abs/2410.20753v1,Plan$\times$RAG: Planning-guided Retrieval Augmented Generation,"We introduce Planning-guided Retrieval Augmented Generation
(Plan$\times$RAG), a novel framework that augments the
\emph{retrieve-then-reason} paradigm of existing RAG frameworks to
\emph{plan-then-retrieve}. Plan$\times$RAG formulates a reasoning plan as a
directed acyclic graph (DAG), decomposing queries into interrelated atomic
sub-queries. Answer generation follows the DAG structure, allowing significant
gains in efficiency through parallelized retrieval and generation. While
state-of-the-art RAG solutions require extensive data generation and
fine-tuning of language models (LMs), Plan$\times$RAG incorporates frozen LMs
as plug-and-play experts to generate high-quality answers. Compared to existing
RAG solutions, Plan$\times$RAG demonstrates significant improvements in
reducing hallucinations and bolstering attribution due to its structured
sub-query decomposition. Overall, Plan$\times$RAG offers a new perspective on
integrating external knowledge in LMs while ensuring attribution by design,
contributing towards more reliable LM-based systems.",Prakhar Verma
2024-10-28T08:32:09Z,http://arxiv.org/abs/2410.20833v1,"LLMs are Biased Evaluators But Not Biased for Retrieval Augmented
  Generation","Recent studies have demonstrated that large language models (LLMs) exhibit
significant biases in evaluation tasks, particularly in preferentially rating
and favoring self-generated content. However, the extent to which this bias
manifests in fact-oriented tasks, especially within retrieval-augmented
generation (RAG) frameworks-where keyword extraction and factual accuracy take
precedence over stylistic elements-remains unclear. Our study addresses this
knowledge gap by simulating two critical phases of the RAG framework. In the
first phase, we access the suitability of human-authored versus model-generated
passages, emulating the pointwise reranking process. The second phase involves
conducting pairwise reading comprehension tests to simulate the generation
process. Contrary to previous findings indicating a self-preference in rating
tasks, our results reveal no significant self-preference effect in RAG
frameworks. Instead, we observe that factual accuracy significantly influences
LLMs' output, even in the absence of prior knowledge. Our research contributes
to the ongoing discourse on LLM biases and their implications for RAG-based
system, offering insights that may inform the development of more robust and
unbiased LLM systems.",Yen-Shan Chen
2024-10-30T12:09:29Z,http://arxiv.org/abs/2410.22954v1,Retrieval-Augmented Generation with Estimation of Source Reliability,"Retrieval-augmented generation (RAG) addresses key limitations of large
language models (LLMs), such as hallucinations and outdated knowledge, by
incorporating external databases. These databases typically consult multiple
sources to encompass up-to-date and various information. However, standard RAG
methods often overlook the heterogeneous source reliability in the multi-source
database and retrieve documents solely based on relevance, making them prone to
propagating misinformation. To address this, we propose Reliability-Aware RAG
(RA-RAG) which estimates the reliability of multiple sources and incorporates
this information into both retrieval and aggregation processes. Specifically,
it iteratively estimates source reliability and true answers for a set of
queries with no labelling. Then, it selectively retrieves relevant documents
from a few of reliable sources and aggregates them using weighted majority
voting, where the selective retrieval ensures scalability while not
compromising the performance. We also introduce a benchmark designed to reflect
real-world scenarios with heterogeneous source reliability and demonstrate the
effectiveness of RA-RAG compared to a set of baselines.",Jeongyeon Hwang
2024-10-30T13:29:36Z,http://arxiv.org/abs/2410.23000v2,"Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented
  Generation with Key Point Recall","Retrieval-augmented generation (RAG) is a promising approach to address the
limitations of fixed knowledge in large language models (LLMs). However,
current benchmarks for evaluating RAG systems suffer from two key deficiencies:
(1) they fail to adequately measure LLMs' capability in handling long-context
retrieval due to a lack of datasets that reflect the characteristics of
retrieved documents, and (2) they lack a comprehensive evaluation method for
assessing LLMs' ability to generate long-form responses that effectively
exploits retrieved information. To address these shortcomings, we introduce the
Long$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG
comprises 280 questions spanning 10 domains and across 8 question categories,
each associated with 5 retrieved documents with an average length of 2,444
words. KPR evaluates the extent to which LLMs incorporate key points extracted
from the retrieved documents into their generated responses, providing a more
nuanced assessment of their ability to exploit retrieved information.",Zehan Qi
2024-10-30T15:06:32Z,http://arxiv.org/abs/2410.23090v1,"CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation
  Generation","Retrieval-Augmented Generation (RAG) has become a powerful paradigm for
enhancing large language models (LLMs) through external knowledge retrieval.
Despite its widespread attention, existing academic research predominantly
focuses on single-turn RAG, leaving a significant gap in addressing the
complexities of multi-turn conversations found in real-world applications. To
bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess
RAG systems in realistic multi-turn conversational settings. CORAL includes
diverse information-seeking conversations automatically derived from Wikipedia
and tackles key challenges such as open-domain coverage, knowledge intensity,
free-form responses, and topic shifts. It supports three core tasks of
conversational RAG: passage retrieval, response generation, and citation
labeling. We propose a unified framework to standardize various conversational
RAG methods and conduct a comprehensive evaluation of these methods on CORAL,
demonstrating substantial opportunities for improving existing approaches.",Yiruo Cheng
2024-10-31T13:05:39Z,http://arxiv.org/abs/2410.23902v1,"Responsible Retrieval Augmented Generation for Climate Decision Making
  from Documents","Climate decision making is constrained by the complexity and inaccessibility
of key information within lengthy, technical, and multi-lingual documents.
Generative AI technologies offer a promising route for improving the
accessibility of information contained within these documents, but suffer from
limitations. These include (1) a tendency to hallucinate or mis-represent
information, (2) difficulty in steering or guaranteeing properties of generated
output, and (3) reduced performance in specific technical domains. To address
these challenges, we introduce a novel evaluation framework with
domain-specific dimensions tailored for climate-related documents. We then
apply this framework to evaluate Retrieval-Augmented Generation (RAG)
approaches and assess retrieval- and generation-quality within a prototype tool
that answers questions about individual climate law and policy documents. In
addition, we publish a human-annotated dataset and scalable automated
evaluation tools, with the aim of facilitating broader adoption and robust
assessment of these systems in the climate domain. Our findings highlight the
key components of responsible deployment of RAG to enhance decision-making,
while also providing insights into user experience (UX) considerations for
safely deploying such systems to build trust with users in high-risk domains.",Matyas Juhasz
2024-11-01T15:50:58Z,http://arxiv.org/abs/2411.00689v1,"Towards Multi-Source Retrieval-Augmented Generation via Synergizing
  Reasoning and Preference-Driven Retrieval","Retrieval-Augmented Generation (RAG) has emerged as a reliable external
knowledge augmentation technique to mitigate hallucination issues and
parameterized knowledge limitations in Large Language Models (LLMs). Existing
Adaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval
sources due to their inability to select the right source at the right time. To
address this, we propose a multi-source ARAG framework, termed MSPR, which
synergizes reasoning and preference-driven retrieval to adaptive decide ""when
and what to retrieve"" and ""which retrieval source to use"". To better adapt to
retrieval sources of differing characteristics, we also employ retrieval action
adjustment and answer feedback strategy. They enable our framework to fully
explore the high-quality primary source while supplementing it with secondary
sources at the right time. Extensive and multi-dimensional experiments
conducted on three datasets demonstrate the superiority and effectiveness of
MSPR.",Qingfei Zhao
2024-11-04T02:30:05Z,http://arxiv.org/abs/2411.01751v1,RAGViz: Diagnose and Visualize Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",Tevin Wang
2024-11-04T21:12:08Z,http://arxiv.org/abs/2411.02617v1,"TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support for Network","The telecommunications industry's rapid evolution demands intelligent systems
capable of managing complex networks and adapting to emerging technologies.
While large language models (LLMs) show promise in addressing these challenges,
their deployment in telecom environments faces significant constraints due to
edge device limitations and inconsistent documentation. To bridge this gap, we
present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)
system built on the Phi-2 small language model (SLM). To improve context
retrieval, TeleOracle employs a two-stage retriever that incorporates semantic
chunking and hybrid keyword and semantic search. Additionally, we expand the
context window during inference to enhance the model's performance on
open-ended queries. We also employ low-rank adaption for efficient fine-tuning.
A thorough analysis of the model's performance indicates that our RAG framework
is effective in aligning Phi-2 to the telecom domain in a downstream question
and answer (QnA) task, achieving a 30% improvement in accuracy over the base
Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our
model not only performs on par with the much larger LLMs but also achieves a
higher faithfulness score, indicating higher adherence to the retrieved
context.",Nouf Alabbasi
2024-11-05T06:11:17Z,http://arxiv.org/abs/2411.02832v2,PersianRAG: A Retrieval-Augmented Generation System for Persian Language,"Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.",Hossein Hosseini
2024-11-06T15:32:28Z,http://arxiv.org/abs/2411.05844v1,"LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration","GraphRAG addresses significant challenges in Retrieval-Augmented Generation
(RAG) by leveraging graphs with embedded knowledge to enhance the reasoning
capabilities of Large Language Models (LLMs). Despite its promising potential,
the GraphRAG community currently lacks a unified framework for fine-grained
decomposition of the graph-based knowledge retrieval process. Furthermore,
there is no systematic categorization or evaluation of existing solutions
within the retrieval process. In this paper, we present LEGO-GraphRAG, a
modular framework that decomposes the retrieval process of GraphRAG into three
interconnected modules: subgraph-extraction, path-filtering, and
path-refinement. We systematically summarize and classify the algorithms and
neural network (NN) models relevant to each module, providing a clearer
understanding of the design space for GraphRAG instances. Additionally, we
identify key design factors, such as Graph Coupling and Computational Cost,
that influence the effectiveness of GraphRAG implementations. Through extensive
empirical studies, we construct high-quality GraphRAG instances using a
representative selection of solutions and analyze their impact on retrieval and
reasoning performance. Our findings offer critical insights into optimizing
GraphRAG instance design, ultimately contributing to the advancement of more
accurate and contextually relevant LLM applications.",Yukun Cao
2024-11-09T17:38:01Z,http://arxiv.org/abs/2411.06237v2,"Leveraging Retrieval-Augmented Generation for Persian University
  Knowledge Retrieval","This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.",Arshia Hemmat
2024-11-11T09:03:52Z,http://arxiv.org/abs/2411.06805v1,"AssistRAG: Boosting the Potential of Large Language Models with an
  Intelligent Information Assistant","The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as ""hallucination"". Initial retrieval-augmented
generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.",Yujia Zhou
2024-11-13T08:43:37Z,http://arxiv.org/abs/2411.08438v1,"Towards Optimizing a Retrieval Augmented Generation using Large Language
  Model on Academic Data","Given the growing trend of many organizations integrating Retrieval Augmented
Generation (RAG) into their operations, we assess RAG on domain-specific data
and test state-of-the-art models across various optimization techniques. We
incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble
Retriever, and In-Context-Learning, to enhance the functionality and
performance in the academic domain. We focus on data retrieval, specifically
targeting various study programs at a large technical university. We
additionally introduce a novel evaluation approach, the RAG Confusion Matrix
designed to assess the effectiveness of various configurations within the RAG
framework. By exploring the integration of both open-source (e.g., Llama2,
Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer
valuable insights into the application and optimization of RAG frameworks in
domain-specific contexts. Our experiments show a significant performance
increase when including multi-query in the retrieval phase.",Anum Afzal
2024-10-28T06:41:05Z,http://arxiv.org/abs/2411.08891v1,Calibrated Decision-Making through LLM-Assisted Retrieval,"Recently, large language models (LLMs) have been increasingly used to support
various decision-making tasks, assisting humans in making informed decisions.
However, when LLMs confidently provide incorrect information, it can lead
humans to make suboptimal decisions. To prevent LLMs from generating incorrect
information on topics they are unsure of and to improve the accuracy of
generated content, prior works have proposed Retrieval Augmented Generation
(RAG), where external documents are referenced to generate responses. However,
traditional RAG methods focus only on retrieving documents most relevant to the
input query, without specifically aiming to ensure that the human user's
decisions are well-calibrated. To address this limitation, we propose a novel
retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),
which ensures that decisions informed by the retrieved documents are
well-calibrated. Then we empirically validate that CalibRAG improves
calibration performance as well as accuracy, compared to other baselines across
various datasets.",Chaeyun Jang
2024-11-07T22:11:51Z,http://arxiv.org/abs/2411.11895v1,Deploying Large Language Models With Retrieval Augmented Generation,"Knowing that the generative capabilities of large language models (LLM) are
sometimes hampered by tendencies to hallucinate or create non-factual
responses, researchers have increasingly focused on methods to ground generated
outputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a
key approach for integrating knowledge from data sources outside of the LLM's
training set, including proprietary and up-to-date information. While many
research papers explore various RAG strategies, their true efficacy is tested
in real-world applications with actual data. The journey from conceiving an
idea to actualizing it in the real world is a lengthy process. We present
insights from the development and field-testing of a pilot project that
integrates LLMs with RAG for information retrieval. Additionally, we examine
the impacts on the information value chain, encompassing people, processes, and
technology. Our aim is to identify the opportunities and challenges of
implementing this emerging technology, particularly within the context of
behavioral research in the information systems (IS) field. The contributions of
this work include the development of best practices and recommendations for
adopting this promising technology while ensuring compliance with industry
regulations through a proposed AI governance model.",Sonal Prabhune
2024-11-20T20:10:43Z,http://arxiv.org/abs/2411.13691v1,"Retrieval-Augmented Generation for Domain-Specific Question Answering: A
  Case Study on Pittsburgh and CMU","We designed a Retrieval-Augmented Generation (RAG) system to provide large
language models with relevant documents for answering domain-specific questions
about Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800
subpages using a greedy scraping strategy and employed a hybrid annotation
process, combining manual and Mistral-generated question-answer pairs,
achieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework
integrates BM25 and FAISS retrievers, enhanced with a reranker for improved
document retrieval accuracy. Experimental results show that the RAG system
significantly outperforms a non-RAG baseline, particularly in time-sensitive
and complex queries, with an F1 score improvement from 5.45% to 42.21% and
recall of 56.18%. This study demonstrates the potential of RAG systems in
enhancing answer precision and relevance, while identifying areas for further
optimization in document retrieval and model training.",Haojia Sun
2024-11-21T01:00:25Z,http://arxiv.org/abs/2411.13773v1,FastRAG: Retrieval Augmented Generation for Semi-structured Data,"Efficiently processing and interpreting network data is critical for the
operation of increasingly complex networks. Recent advances in Large Language
Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved
data processing in network management. However, existing RAG methods like
VectorRAG and GraphRAG struggle with the complexity and implicit nature of
semi-structured technical data, leading to inefficiencies in time, cost, and
retrieval. This paper introduces FastRAG, a novel RAG approach designed for
semi-structured data. FastRAG employs schema learning and script learning to
extract and structure data without needing to submit entire data sources to an
LLM. It integrates text search with knowledge graph (KG) querying to improve
accuracy in retrieving context-rich information. Evaluation results demonstrate
that FastRAG provides accurate question answering, while improving up to 90% in
time and 85% in cost compared to GraphRAG.",Amar Abane
2024-11-21T20:39:13Z,http://arxiv.org/abs/2411.14572v1,"Towards Knowledge Checking in Retrieval-augmented Generation: A
  Representation Perspective","Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing
the performance of Large Language Models (LLMs). However, these systems face
challenges in effectively integrating external knowledge with the LLM's
internal knowledge, often leading to issues with misleading or unhelpful
information. This work aims to provide a systematic study on knowledge checking
in RAG systems. We conduct a comprehensive analysis of LLM representation
behaviors and demonstrate the significance of using representations in
knowledge checking. Motivated by the findings, we further develop
representation-based classifiers for knowledge filtering. We show substantial
improvements in RAG performance, even when dealing with noisy knowledge
databases. Our study provides new insights into leveraging LLM representations
for enhancing the reliability and effectiveness of RAG systems.",Shenglai Zeng
2024-11-23T09:56:21Z,http://arxiv.org/abs/2411.16732v1,"Multi-Reranker: Maximizing performance of retrieval-augmented generation
  in the FinanceRAG challenge","As Large Language Models (LLMs) increasingly address domain-specific
problems, their application in the financial sector has expanded rapidly. Tasks
that are both highly valuable and time-consuming, such as analyzing financial
statements, disclosures, and related documents, are now being effectively
tackled using LLMs. This paper details the development of a high-performance,
finance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF
'24 FinanceRAG competition. We optimized performance through ablation studies
on query expansion and corpus refinement during the pre-retrieval phase. To
enhance retrieval accuracy, we employed multiple reranker models. Notably, we
introduced an efficient method for managing long context sizes during the
generation phase, significantly improving response quality without sacrificing
performance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our
key contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced
retrieval algorithm, and (3) a novel approach for long-context management. This
work demonstrates the potential of LLMs in effectively processing and analyzing
complex financial data to generate accurate and valuable insights. The source
code and further details are available at https://github.com/cv-lee/FinanceRAG.",Joohyun Lee
2024-12-02T14:55:02Z,http://arxiv.org/abs/2412.01572v3,"MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation
  through Question Complexity","Retrieval Augmented Generation (RAG) has proven to be highly effective in
boosting the generative performance of language model in knowledge-intensive
tasks. However, existing RAG framework either indiscriminately perform
retrieval or rely on rigid single-class classifiers to select retrieval
methods, leading to inefficiencies and suboptimal performance across queries of
varying complexity. To address these challenges, we propose a reinforcement
learning-based framework that dynamically selects the most suitable retrieval
strategy based on query complexity. % our solution Our approach leverages a
multi-armed bandit algorithm, which treats each retrieval method as a distinct
``arm'' and adapts the selection process by balancing exploration and
exploitation. Additionally, we introduce a dynamic reward function that
balances accuracy and efficiency, penalizing methods that require more
retrieval steps, even if they lead to a correct result. Our method achieves new
state of the art results on multiple single-hop and multi-hop datasets while
reducing retrieval costs. Our code are available at
https://github.com/FUTUREEEEEE/MBA .",Xiaqiang Tang
2024-12-03T16:52:06Z,http://arxiv.org/abs/2412.02563v1,Semantic Tokens in Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) architectures have recently garnered
significant attention for their ability to improve truth grounding and
coherence in natural language processing tasks. However, the reliability of RAG
systems in producing accurate answers diminishes as the volume of data they
access increases. Even with smaller datasets, these systems occasionally fail
to address simple queries. This issue arises from their dependence on
state-of-the-art large language models (LLMs), which can introduce uncertainty
into the system's outputs. In this work, I propose a novel Comparative RAG
system that introduces an evaluator module to bridge the gap between
probabilistic RAG systems and deterministically verifiable responses. The
evaluator compares external recommendations with the retrieved document chunks,
adding a decision-making layer that enhances the system's reliability. This
approach ensures that the chunks retrieved are both semantically relevant and
logically consistent with deterministic insights, thereby improving the
accuracy and overall efficiency of RAG systems. This framework paves the way
for more reliable and scalable question-answering applications in domains
requiring high precision and verifiability.",Joel Suro
2024-12-05T07:23:14Z,http://arxiv.org/abs/2412.03933v1,"Exploring AI Text Generation, Retrieval-Augmented Generation, and
  Detection Technologies: a Comprehensive Overview","The rapid development of Artificial Intelligence (AI) has led to the creation
of powerful text generation models, such as large language models (LLMs), which
are widely used for diverse applications. However, concerns surrounding
AI-generated content, including issues of originality, bias, misinformation,
and accountability, have become increasingly prominent. This paper offers a
comprehensive overview of AI text generators (AITGs), focusing on their
evolution, capabilities, and ethical implications. This paper also introduces
Retrieval-Augmented Generation (RAG), a recent approach that improves the
contextual relevance and accuracy of text generation by integrating dynamic
information retrieval. RAG addresses key limitations of traditional models,
including their reliance on static knowledge and potential inaccuracies in
handling real-world data. Additionally, the paper reviews detection tools that
help differentiate AI-generated text from human-written content and discusses
the ethical challenges these technologies pose. The paper explores future
directions for improving detection accuracy, supporting ethical AI development,
and increasing accessibility. The paper contributes to a more responsible and
reliable use of AI in content creation through these discussions.",Fnu Neha
2024-12-06T01:20:16Z,http://arxiv.org/abs/2412.04697v1,"Privacy-Preserving Retrieval Augmented Generation with Differential
  Privacy","With the recent remarkable advancement of large language models (LLMs), there
has been a growing interest in utilizing them in the domains with highly
sensitive data that lies outside their training data. For this purpose,
retrieval augmented generation (RAG) is particularly effective -- it assists
LLMs by directly providing relevant information from the external knowledge
sources. However, without extra privacy safeguards, RAG outputs risk leaking
sensitive information from the external data source. In this work, we explore
RAG under differential privacy (DP), a formal guarantee of data privacy. The
main challenge with differentially private RAG is how to generate long accurate
answers within a moderate privacy budget. We address this by proposing an
algorithm that smartly spends privacy budget only for the tokens that require
the sensitive information and uses the non-private LLM for other tokens. Our
extensive empirical evaluations reveal that our algorithm outperforms the
non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$
across different models and datasets.",Tatsuki Koga
2024-12-11T16:32:41Z,http://arxiv.org/abs/2412.08519v1,"Bridging Relevance and Reasoning: Rationale Distillation in
  Retrieval-Augmented Generation","The reranker and generator are two critical components in the
Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking
relevant documents and generating responses. However, due to differences in
pre-training data and objectives, there is an inevitable gap between the
documents ranked as relevant by the reranker and those required by the
generator to support answering the query. To address this gap, we propose
RADIO, a novel and practical preference alignment framework with RAtionale
DIstillatiOn. Specifically, We first propose a rationale extraction method that
leverages the reasoning capabilities of Large Language Models (LLMs) to extract
the rationales necessary for answering the query. Subsequently, a
rationale-based alignment process is designed to rerank the documents based on
the extracted rationales, and fine-tune the reranker to align the preferences.
We conduct extensive experiments on two tasks across three datasets to
demonstrate the effectiveness of our approach compared to baseline methods. Our
code is released online to ease reproduction.",Pengyue Jia
2024-12-12T06:38:40Z,http://arxiv.org/abs/2412.08985v1,"Assessing the Robustness of Retrieval-Augmented Generation Systems in
  K-12 Educational Question Answering with Knowledge Discrepancies","Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable
potential as question answering systems in the K-12 Education domain, where
knowledge is typically queried within the restricted scope of authoritative
textbooks. However, the discrepancy between textbooks and the parametric
knowledge in Large Language Models (LLMs) could undermine the effectiveness of
RAG systems. To systematically investigate the robustness of RAG systems under
such knowledge discrepancies, we present EduKDQA, a question answering dataset
that simulates knowledge discrepancies in real applications by applying
hypothetical knowledge updates in answers and source documents. EduKDQA
includes 3,005 questions covering five subjects, under a comprehensive question
typology from the perspective of context utilization and knowledge integration.
We conducted extensive experiments on retrieval and question answering
performance. We find that most RAG systems suffer from a substantial
performance drop in question answering with knowledge discrepancies, while
questions that require integration of contextual knowledge and parametric
knowledge pose a challenge to LLMs.",Tianshi Zheng
2024-12-13T14:11:26Z,http://arxiv.org/abs/2412.10151v1,"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval
  Augmented Generation","We propose the VLR-Bench, a visual question answering (VQA) benchmark for
evaluating vision language models (VLMs) based on retrieval augmented
generation (RAG). Unlike existing evaluation datasets for external
knowledge-based VQA, the proposed VLR-Bench includes five input passages. This
allows testing of the ability to determine which passage is useful for
answering a given query, a capability lacking in previous research. In this
context, we constructed a dataset of 32,000 automatically generated
instruction-following examples, which we denote as VLR-IF. This dataset is
specifically designed to enhance the RAG capabilities of VLMs by enabling them
to learn how to generate appropriate answers based on input passages. We
evaluated the validity of the proposed benchmark and training data and verified
its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3
model. The proposed VLR-Bench and VLR-IF datasets are publicly available
online.",Hyeonseok Lim
2024-12-14T06:24:55Z,http://arxiv.org/abs/2412.10704v1,"VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal
  Retrieval-Augmented Generation","Understanding information from a collection of multiple documents,
particularly those with visually rich elements, is important for
document-grounded question answering. This paper introduces VisDoMBench, the
first comprehensive benchmark designed to evaluate QA systems in multi-document
settings with rich multimodal content, including tables, charts, and
presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval
Augmented Generation (RAG) approach that simultaneously utilizes visual and
textual RAG, combining robust visual retrieval capabilities with sophisticated
linguistic reasoning. VisDoMRAG employs a multi-step reasoning process
encompassing evidence curation and chain-of-thought reasoning for concurrent
textual and visual RAG pipelines. A key novelty of VisDoMRAG is its
consistency-constrained modality fusion mechanism, which aligns the reasoning
processes across modalities at inference time to produce a coherent final
answer. This leads to enhanced accuracy in scenarios where critical information
is distributed across modalities and improved answer verifiability through
implicit context attribution. Through extensive experiments involving
open-source and proprietary large language models, we benchmark
state-of-the-art document QA methods on VisDoMBench. Extensive results show
that VisDoMRAG outperforms unimodal and long-context LLM baselines for
end-to-end multimodal document QA by 12-20%.",Manan Suri
2024-12-16T15:12:53Z,http://arxiv.org/abs/2412.11854v1,"Towards Understanding Systems Trade-offs in Retrieval-Augmented
  Generation Model Inference","The rapid increase in the number of parameters in large language models
(LLMs) has significantly increased the cost involved in fine-tuning and
retraining LLMs, a necessity for keeping models up to date and improving
accuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to
improving the capabilities and accuracy of LLMs without the necessity of
retraining. Although RAG eliminates the need for continuous retraining to
update model data, it incurs a trade-off in the form of slower model inference
times. Resultingly, the use of RAG in enhancing the accuracy and capabilities
of LLMs often involves diverse performance implications and trade-offs based on
its design. In an effort to begin tackling and mitigating the performance
penalties associated with RAG from a systems perspective, this paper introduces
a detailed taxonomy and characterization of the different elements within the
RAG ecosystem for LLMs that explore trade-offs within latency, throughput, and
memory. Our study reveals underlying inefficiencies in RAG for systems
deployment, that can result in TTFT latencies that are twice as long and
unoptimized datastores that consume terabytes of storage.",Michael Shen
2024-12-19T07:01:25Z,http://arxiv.org/abs/2412.14581v1,"CORD: Balancing COnsistency and Rank Distillation for Robust
  Retrieval-Augmented Generation","With the adoption of retrieval-augmented generation (RAG), large language
models (LLMs) are expected to ground their generation to the retrieved
contexts. Yet, this is hindered by position bias of LLMs, failing to evenly
attend to all contexts. Previous work has addressed this by synthesizing
contexts with perturbed positions of gold segment, creating a
position-diversified train set. We extend this intuition to propose consistency
regularization with augmentation and distillation. First, we augment each
training instance with its position perturbation to encourage consistent
predictions, regardless of ordering. We also distill behaviors of this pair,
although it can be counterproductive in certain RAG scenarios where the given
order from the retriever is crucial for generation quality. We thus propose
CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples
noise-controlled perturbations from an interpolation space, ensuring both
consistency and respect for the rank prior. Empirical results show this balance
enables CORD to outperform consistently in diverse RAG benchmarks.",Youngwon Lee
2024-12-19T14:37:11Z,http://arxiv.org/abs/2412.14905v1,"Dehallucinating Parallel Context Extension for Retrieval-Augmented
  Generation","Large language models (LLMs) are susceptible to generating hallucinated
information, despite the integration of retrieval-augmented generation (RAG).
Parallel context extension (PCE) is a line of research attempting to
effectively integrating parallel (unordered) contexts, while it still suffers
from hallucinations when adapted to RAG scenarios. In this paper, we propose
DePaC (Dehallucinating Parallel Context Extension), which alleviates the
hallucination problem with context-aware negative training and
information-calibrated aggregation. DePaC is designed to alleviate two types of
in-context hallucination: fact fabrication (i.e., LLMs present claims that are
not supported by the contexts) and fact omission (i.e., LLMs fail to present
claims that can be supported by the contexts). Specifically, (1) for fact
fabrication, we apply the context-aware negative training that fine-tunes the
LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to
answer when contexts are not related to questions; (2) for fact omission, we
propose the information-calibrated aggregation which prioritizes context
windows with higher information increment from their contexts. The experimental
results on nine RAG tasks demonstrate that DePaC significantly alleviates the
two types of hallucination and consistently achieves better performances on
these tasks.",Zexiong Ma
2024-12-21T14:27:38Z,http://arxiv.org/abs/2412.16643v1,"TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented
  Generation","Although the rise of large language models (LLMs) has introduced new
opportunities for time series forecasting, existing LLM-based solutions require
excessive training and exhibit limited transferability. In view of these
challenges, we propose TimeRAG, a framework that incorporates
Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which
constructs a time series knowledge base from historical sequences, retrieves
reference sequences from the knowledge base that exhibit similar patterns to
the query sequence measured by Dynamic Time Warping (DTW), and combines these
reference sequences and the prediction query as a textual prompt to the time
series forecasting LLM. Experiments on datasets from various domains show that
the integration of RAG improved the prediction accuracy of the original model
by 2.97% on average.",Silin Yang
2024-12-21T16:59:00Z,http://arxiv.org/abs/2412.16701v1,"AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed
  articles","Recent advancements in generative AI have flourished the development of
highly adept Large Language Models (LLMs) that integrate diverse data types to
empower decision-making. Among these, Multimodal Retrieval-Augmented Generation
(RAG) applications are promising for their capability to combine the strengths
of information retrieval and generative models, enhancing their utility across
various domains, including biomedical research. This paper introduces
AlzheimerRAG, a Multimodal RAG pipeline tool for biomedical research use cases,
primarily focusing on Alzheimer's disease from PubMed articles. Our pipeline
incorporates multimodal fusion techniques to integrate textual and visual data
processing by efficiently indexing and accessing vast amounts of biomedical
literature. Preliminary experimental results against benchmarks, such as BioASQ
and PubMedQA, have returned improved results in information retrieval and
synthesis of domain-specific information. We also demonstrate a case study with
our RAG pipeline across different Alzheimer's clinical scenarios. We infer that
AlzheimerRAG can generate responses with accuracy non-inferior to humans and
with low rates of hallucination. Overall, a reduction in cognitive task load is
observed, which allows researchers to gain multimodal insights, improving
understanding and treatment of Alzheimer's disease.",Aritra Kumar Lahiri
2024-12-21T17:31:52Z,http://arxiv.org/abs/2412.16708v1,"Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under
  Adversarial Poisoning Attacks","Retrieval-Augmented Generation (RAG) systems have emerged as a promising
solution to mitigate LLM hallucinations and enhance their performance in
knowledge-intensive domains. However, these systems are vulnerable to
adversarial poisoning attacks, where malicious passages injected into retrieval
databases can mislead the model into generating factually incorrect outputs. In
this paper, we investigate both the retrieval and the generation components of
RAG systems to understand how to enhance their robustness against such attacks.
From the retrieval perspective, we analyze why and how the adversarial contexts
are retrieved and assess how the quality of the retrieved passages impacts
downstream generation. From a generation perspective, we evaluate whether LLMs'
advanced critical thinking and internal knowledge capabilities can be leveraged
to mitigate the impact of adversarial contexts, i.e., using skeptical prompting
as a self-defense mechanism. Our experiments and findings provide actionable
insights into designing safer and more resilient retrieval-augmented
frameworks, paving the way for their reliable deployment in real-world
applications.",Jinyan Su
2024-12-24T13:45:22Z,http://arxiv.org/abs/2412.18431v1,GeAR: Graph-enhanced Agent for Retrieval-augmented Generation,"Retrieval-augmented generation systems rely on effective document retrieval
capabilities. By design, conventional sparse or dense retrievers face
challenges in multi-hop retrieval scenarios. In this paper, we present GeAR,
which advances RAG performance through two key innovations: (i) graph
expansion, which enhances any conventional base retriever, such as BM25, and
(ii) an agent framework that incorporates graph expansion. Our evaluation
demonstrates GeAR's superior retrieval performance on three multi-hop question
answering datasets. Additionally, our system achieves state-of-the-art results
with improvements exceeding 10% on the challenging MuSiQue dataset, while
requiring fewer tokens and iterations compared to other multi-step retrieval
systems.",Zhili Shen
2023-12-18T03:19:31Z,http://arxiv.org/abs/2312.10904v2,"Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)","Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.",Sabrina Toro
2024-09-23T21:42:47Z,http://arxiv.org/abs/2409.15566v1,GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation,"The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.",Brendan Hogan Rappazzo
2022-02-02T16:18:41Z,http://arxiv.org/abs/2202.01110v2,A Survey on Retrieval-Augmented Text Generation,"Recently, retrieval-augmented text generation attracted increasing attention
of the computational linguistics community. Compared with conventional
generation models, retrieval-augmented text generation has remarkable
advantages and particularly has achieved state-of-the-art performance in many
NLP tasks. This paper aims to conduct a survey about retrieval-augmented text
generation. It firstly highlights the generic paradigm of retrieval-augmented
generation, and then it reviews notable approaches according to different tasks
including dialogue response generation, machine translation, and other
generation tasks. Finally, it points out some important directions on top of
recent methods to facilitate future research.",Huayang Li
2021-06-22T03:17:59Z,http://arxiv.org/abs/2106.11517v1,"Fine-tune the Entire RAG Architecture (including DPR retriever) for
  Question-Answering","In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.",Shamane Siriwardhana
2023-11-17T08:21:56Z,http://arxiv.org/abs/2311.10384v2,Retrieval Augmented Generation of Symbolic Music with LLMs,"We explore the use of large language models (LLMs) for music generation using
a retrieval system to select relevant examples. We find promising initial
results for music generation in a dialogue with the user, especially
considering the ease with which such a system can be implemented. The code is
available online.",Nicolas Jonason
2024-03-18T02:01:58Z,http://arxiv.org/abs/2403.11413v1,"Dynamic Contexts for Generating Suggestion Questions in RAG Based
  Conversational Systems","When interacting with Retrieval-Augmented Generation (RAG)-based
conversational agents, the users must carefully craft their queries to be
understood correctly. Yet, understanding the system's capabilities can be
challenging for the users, leading to ambiguous questions that necessitate
further clarification. This work aims to bridge the gap by developing a
suggestion question generator. To generate suggestion questions, our approach
involves utilizing dynamic context, which includes both dynamic few-shot
examples and dynamically retrieved contexts. Through experiments, we show that
the dynamic contexts approach can generate better suggestion questions as
compared to other prompting approaches.",Anuja Tayal
2024-05-13T09:17:19Z,http://arxiv.org/abs/2405.13008v1,Control Token with Dense Passage Retrieval,"This study addresses the hallucination problem in large language models
(LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a
technique that involves embedding relevant information in the prompt to obtain
accurate answers. However, RAG also faced inherent issues in retrieving correct
information. To address this, we employed the Dense Passage Retrieval(DPR)
(Karpukhin et al., 2020) model for fetching domain-specific documents related
to user queries. Despite this, the DPR model still lacked accuracy in document
retrieval. We enhanced the DPR model by incorporating control tokens, achieving
significantly superior performance over the standard DPR model, with a 13%
improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",Juhwan Lee
2024-05-23T19:23:40Z,http://arxiv.org/abs/2405.15007v1,RE-Adapt: Reverse Engineered Adaptation of Large Language Models,"We introduce RE-Adapt, an approach to fine-tuning large language models on
new domains without degrading any pre-existing instruction-tuning. We reverse
engineer an adapter which isolates what an instruction-tuned model has learned
beyond its corresponding pretrained base model. Importantly, this requires no
additional data or training. We can then fine-tune the base model on a new
domain and readapt it to instruction following with the reverse engineered
adapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other
methods of fine-tuning, across multiple popular LLMs and datasets, even when
the models are used in conjunction with retrieval-augmented generation.",William Fleshman
2024-06-10T09:29:08Z,http://arxiv.org/abs/2406.06124v1,"Enhancing Long-Term Memory using Hierarchical Aggregate Tree for
  Retrieval Augmented Generation","Large language models have limited context capacity, hindering reasoning over
long conversations. We propose the Hierarchical Aggregate Tree memory structure
to recursively aggregate relevant dialogue context through conditional tree
traversals. HAT encapsulates information from children nodes, enabling broad
coverage with depth control. We formulate finding best context as optimal tree
traversal. Experiments show HAT improves dialog coherence and summary quality
over baseline contexts, demonstrating the techniques effectiveness for multi
turn reasoning without exponential parameter growth. This memory augmentation
enables more consistent, grounded longform conversations from LLMs",Aadharsh Aadhithya A
2024-07-17T05:50:32Z,http://arxiv.org/abs/2407.12325v1,Optimizing Query Generation for Enhanced Document Retrieval in RAG,"Large Language Models (LLMs) excel in various language tasks but they often
generate incorrect information, a phenomenon known as ""hallucinations"".
Retrieval-Augmented Generation (RAG) aims to mitigate this by using document
retrieval for accurate responses. However, RAG still faces hallucinations due
to vague queries. This study aims to improve RAG by optimizing query generation
with a query-document alignment score, refining queries using LLMs for better
precision and efficiency of document retrieval. Experiments have shown that our
approach improves document retrieval, resulting in an average accuracy gain of
1.6%.",Hamin Koo
2024-07-19T12:28:22Z,http://arxiv.org/abs/2407.14246v2,Unipa-GPT: Large Language Models for university-oriented QA in Italian,"This paper illustrates the architecture and training of Unipa-GPT, a chatbot
relying on a Large Language Model, developed for assisting students in choosing
a bachelor/master degree course at the University of Palermo. Unipa-GPT relies
on gpt-3.5-turbo, it was presented in the context of the European Researchers'
Night (SHARPER night). In our experiments we adopted both the Retrieval
Augmented Generation (RAG) approach and fine-tuning to develop the system. The
whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned
systems are compared, and a brief discussion on their performance is reported.
Further comparison with other Large Language Models and the experimental
results during the SHARPER night are illustrated.",Irene Siragusa
2024-07-23T15:17:11Z,http://arxiv.org/abs/2407.16565v1,"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases
  Generation with Small Language Models","Recent surge in the accessibility of large language models (LLMs) to the
general population can lead to untrackable use of such models for
medical-related recommendations. Language generation via LLMs models has two
key problems: firstly, they are prone to hallucination and therefore, for any
medical purpose they require scientific and factual grounding; secondly, LLMs
pose tremendous challenge to computational resources due to their gigantic
model size. In this work, we introduce pRAGe, a pipeline for Retrieval
Augmented Generation and evaluation of medical paraphrases generation using
Small Language Models (SLM). We study the effectiveness of SLMs and the impact
of external knowledge base for medical paraphrase generation in French.",Ioana Buhnila
2024-08-05T20:21:54Z,http://arxiv.org/abs/2408.02811v1,Development of REGAI: Rubric Enabled Generative Artificial Intelligence,"This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology.",Zach Johnson
2024-08-19T22:01:45Z,http://arxiv.org/abs/2408.10435v1,Enhanced document retrieval with topic embeddings,"Document retrieval systems have experienced a revitalized interest with the
advent of retrieval-augmented generation (RAG). RAG architecture offers a lower
hallucination rate than LLM-only applications. However, the accuracy of the
retrieval mechanism is known to be a bottleneck in the efficiency of these
applications. A particular case of subpar retrieval performance is observed in
situations where multiple documents from several different but related topics
are in the corpus. We have devised a new vectorization method that takes into
account the topic information of the document. The paper introduces this new
method for text vectorization and evaluates it in the context of RAG.
Furthermore, we discuss the challenge of evaluating RAG systems, which pertains
to the case at hand.",Kavsar Huseynova
2024-08-27T16:09:56Z,http://arxiv.org/abs/2408.15171v1,"Measuring text summarization factuality using atomic facts entailment
  metrics in the context of retrieval augmented generation","The use of large language models (LLMs) has significantly increased since the
introduction of ChatGPT in 2022, demonstrating their value across various
applications. However, a major challenge for enterprise and commercial adoption
of LLMs is their tendency to generate inaccurate information, a phenomenon
known as ""hallucination."" This project proposes a method for estimating the
factuality of a summary generated by LLMs when compared to a source text. Our
approach utilizes Naive Bayes classification to assess the accuracy of the
content produced.",N. E. Kriman
2024-10-03T19:05:47Z,http://arxiv.org/abs/2410.02914v1,Streamlining Conformal Information Retrieval via Score Refinement,"Information retrieval (IR) methods, like retrieval augmented generation, are
fundamental to modern applications but often lack statistical guarantees.
Conformal prediction addresses this by retrieving sets guaranteed to include
relevant information, yet existing approaches produce large-sized sets,
incurring high computational costs and slow response times. In this work, we
introduce a score refinement method that applies a simple monotone
transformation to retrieval scores, leading to significantly smaller conformal
sets while maintaining their statistical guarantees. Experiments on various
BEIR benchmarks validate the effectiveness of our approach in producing compact
sets containing relevant information.",Yotam Intrator
2024-10-08T14:09:12Z,http://arxiv.org/abs/2410.06062v3,"LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge Graphs","We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.",Vincent Emonet
2024-10-04T16:46:07Z,http://arxiv.org/abs/2410.09077v1,"A Large Language Model-based Framework for Semi-Structured Tender
  Document Retrieval-Augmented Generation","The drafting of documents in the procurement field has progressively become
more complex and diverse, driven by the need to meet legal requirements, adapt
to technological advancements, and address stakeholder demands. While large
language models (LLMs) show potential in document generation, most LLMs lack
specialized knowledge in procurement. To address this gap, we use
retrieval-augmented techniques to achieve professional document generation,
ensuring accuracy and relevance in procurement documentation.",Yilong Zhao
2024-10-15T14:42:18Z,http://arxiv.org/abs/2410.11655v1,Retrieval Augmented Spelling Correction for E-Commerce Applications,"The rapid introduction of new brand names into everyday language poses a
unique challenge for e-commerce spelling correction services, which must
distinguish genuine misspellings from novel brand names that use unconventional
spelling. We seek to address this challenge via Retrieval Augmented Generation
(RAG). On this approach, product names are retrieved from a catalog and
incorporated into the context used by a large language model (LLM) that has
been fine-tuned to do contextual spelling correction. Through quantitative
evaluation and qualitative error analyses, we find improvements in spelling
correction utilizing the RAG framework beyond a stand-alone LLM. We also
demonstrate the value of additional finetuning of the LLM to incorporate
retrieved context.",Xuan Guo
2024-10-16T21:53:48Z,http://arxiv.org/abs/2410.13070v1,Is Semantic Chunking Worth the Computational Cost?,"Recent advances in Retrieval-Augmented Generation (RAG) systems have
popularized semantic chunking, which aims to improve retrieval performance by
dividing documents into semantically coherent segments. Despite its growing
adoption, the actual benefits over simpler fixed-size chunking, where documents
are split into consecutive, fixed-size segments, remain unclear. This study
systematically evaluates the effectiveness of semantic chunking using three
common retrieval-related tasks: document retrieval, evidence retrieval, and
retrieval-based answer generation. The results show that the computational
costs associated with semantic chunking are not justified by consistent
performance gains. These findings challenge the previous assumptions about
semantic chunking and highlight the need for more efficient chunking strategies
in RAG systems.",Renyi Qu
2024-10-01T20:29:14Z,http://arxiv.org/abs/2410.13865v1,Classifying Peace in Global Media Using RAG and Intergroup Reciprocity,"This paper presents a novel approach to identifying insights of peace in
global media using a Retrieval Augmented Generation (RAG) model and concepts of
Positive and Negative Intergroup Reciprocity (PIR/NIR). By refining the
definitions of PIR and NIR, we offer a more accurate and meaningful analysis of
intergroup relations as represented in media articles. Our methodology provides
insights into the dynamics that contribute to or detract from peace at a
national level.",K. Lian
2024-10-27T00:49:52Z,http://arxiv.org/abs/2410.20301v1,WindTunnel -- A Framework for Community Aware Sampling of Large Corpora,"Conducting comprehensive information retrieval experiments, such as in search
or retrieval augmented generation, often comes with high computational costs.
This is because evaluating a retrieval algorithm requires indexing the entire
corpus, which is significantly larger than the set of (query, result) pairs
under evaluation. This issue is especially pronounced in big data and neural
retrieval, where indexing becomes increasingly time-consuming and complex. In
this paper, we present WindTunnel, a novel framework developed at Yext to
generate representative samples of large corpora, enabling efficient end-to-end
information retrieval experiments. By preserving the community structure of the
dataset, WindTunnel overcomes limitations in current sampling methods,
providing more accurate evaluations.",Michael Iannelli
2024-11-08T19:44:12Z,http://arxiv.org/abs/2411.05934v1,"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for
  Bengali Mathematical Olympiad Problem Solving","We present an innovative approach for solving mathematical problems in
Bengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our
method uses advanced deep learning models, notably the Qwen 2.5 series, with
improvements made through prompt engineering, model quantization, and Tool
Integrated Reasoning (TIR) to handle complex calculations. Initially, we
explored various model architectures, including fine-tuned Mistral and
quantized Qwen models, refining them with translation techniques,
Retrieval-Augmented Generation (RAG), and custom dataset curation. Manual
hyperparameter tuning optimized parameters like temperature and top-p to
enhance model adaptability and accuracy. Removal of RAG and parameter
adjustments further improved robustness. Our approach highlights the potential
of advanced NLP techniques in solving Bengali mathematical problems.",Saad Tahmid
2024-11-11T18:58:46Z,http://arxiv.org/abs/2411.07238v1,OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model,"OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.",Sumeth Yuenyong
2024-11-16T03:06:39Z,http://arxiv.org/abs/2411.12759v1,"A Novel Approach to Eliminating Hallucinations in Large Language
  Model-Assisted Causal Discovery","The increasing use of large language models (LLMs) in causal discovery as a
substitute for human domain experts highlights the need for optimal model
selection. This paper presents the first hallucination survey of popular LLMs
for causal discovery. We show that hallucinations exist when using LLMs in
causal discovery so the choice of LLM is important. We propose using Retrieval
Augmented Generation (RAG) to reduce hallucinations when quality data is
available. Additionally, we introduce a novel method employing multiple LLMs
with an arbiter in a debate to audit edges in causal graphs, achieving a
comparable reduction in hallucinations to RAG.",Grace Sng
2024-12-16T20:33:33Z,http://arxiv.org/abs/2412.15262v1,Advanced ingestion process powered by LLM parsing for RAG system,"Retrieval Augmented Generation (RAG) systems struggle with processing
multimodal documents of varying structural complexity. This paper introduces a
novel multi-strategy parsing approach using LLM-powered OCR to extract content
from diverse document types, including presentations and high text density
files both scanned or not. The methodology employs a node-based extraction
technique that creates relationships between different information types and
generates context-aware metadata. By implementing a Multimodal Assembler Agent
and a flexible embedding strategy, the system enhances document comprehension
and retrieval capabilities. Experimental evaluations across multiple knowledge
bases demonstrate the approach's effectiveness, showing improvements in answer
relevancy and information faithfulness.",Arnau Perez
2024-12-21T16:31:41Z,http://arxiv.org/abs/2412.16689v1,Formal Language Knowledge Corpus for Retrieval Augmented Generation,"The integration of retrieval-augmented techniques with LLMs has shown promise
in improving performance across various domains. However, their utility in
tasks requiring advanced reasoning, such as generating and evaluating
mathematical statements and proofs, remains underexplored. This study explores
the use of Lean, a programming language for writing mathematical proofs, to
populate the knowledge corpus used by RAG systems. We hope for this to lay the
foundation to exploring different methods of using RAGs to improve the
performance of LLMs in advanced logical reasoning tasks.",Majd Zayyad
2021-08-31T15:51:27Z,http://arxiv.org/abs/2108.13934v2,Robust Retrieval Augmented Generation for Zero-shot Slot Filling,"Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.",Michael Glass
2022-10-06T01:21:25Z,http://arxiv.org/abs/2210.02627v1,"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)
  Models for Open Domain Question Answering","Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain
Question Answering (ODQA). RAG has only been trained and explored with a
Wikipedia-based external knowledge base and is not optimized for use in other
specialized domains such as healthcare and news. In this paper, we evaluate the
impact of joint training of the retriever and generator components of RAG for
the task of domain adaptation in ODQA. We propose \textit{RAG-end2end}, an
extension to RAG, that can adapt to a domain-specific knowledge base by
updating all components of the external knowledge base during training. In
addition, we introduce an auxiliary training signal to inject more
domain-specific knowledge. This auxiliary signal forces \textit{RAG-end2end} to
reconstruct a given sentence by accessing the relevant information from the
external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does
joint training of the retriever and generator for the end QA task and domain
adaptation. We evaluate our approach with datasets from three domains:
COVID-19, News, and Conversations, and achieve significant performance
improvements compared to the original RAG model. Our work has been open-sourced
through the Huggingface Transformers library, attesting to our work's
credibility and technical consistency.",Shamane Siriwardhana
2023-07-13T17:25:28Z,http://arxiv.org/abs/2307.06985v10,Retrieval Augmented Generation using Engineering Design Knowledge,"Aiming to support Retrieval Augmented Generation (RAG) in the design process,
we present a method to identify explicit, engineering design facts - {head
entity :: relationship :: tail entity} from patented artefact descriptions.
Given a sentence with a pair of entities (based on noun phrases) marked in a
unique manner, our method extracts the relationship that is explicitly
communicated in the sentence. For this task, we create a dataset of 375,084
examples and fine-tune language models for relation identification (token
classification) and elicitation (sequence-to-sequence). The token
classification approach achieves up to 99.7 % accuracy. Upon applying the
method to a domain of 4,870 fan system patents, we populate a knowledge base of
over 2.93 million facts. Using this knowledge base, we demonstrate how Large
Language Models (LLMs) are guided by explicit facts to synthesise knowledge and
generate technical and cohesive responses when sought out for knowledge
retrieval tasks in the design process.",L. Siddharth
2023-10-04T22:09:28Z,http://arxiv.org/abs/2310.03184v2,"Retrieval-augmented Generation to Improve Math Question-Answering:
  Trade-offs Between Groundedness and Human Preference","For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.",Zachary Levonian
2023-10-17T18:18:32Z,http://arxiv.org/abs/2310.11511v1,"Self-RAG: Learning to Retrieve, Generate, and Critique through
  Self-Reflection","Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.",Akari Asai
2023-11-10T07:13:06Z,http://arxiv.org/abs/2311.05903v2,"Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-Prompting for Non-Specialist LLM Users","Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.",Jennifer Dodgson
2023-12-11T15:45:27Z,http://arxiv.org/abs/2312.06457v1,"Large Language Models with Retrieval-Augmented Generation for Zero-Shot
  Disease Phenotyping","Identifying disease phenotypes from electronic health records (EHRs) is
critical for numerous secondary uses. Manually encoding physician knowledge
into rules is particularly challenging for rare diseases due to inadequate EHR
coding, necessitating review of clinical notes. Large language models (LLMs)
offer promise in text understanding but may not efficiently handle real-world
clinical documentation. We propose a zero-shot LLM-based method enriched by
retrieval-augmented generation and MapReduce, which pre-identifies
disease-related text snippets to be used in parallel as queries for the LLM to
establish diagnosis. We show that this method as applied to pulmonary
hypertension (PH), a rare disease characterized by elevated arterial pressures
in the lungs, significantly outperforms physician logic rules ($F_1$ score of
0.62 vs. 0.75). This method has the potential to enhance rare disease cohort
identification, expanding the scope of robust clinical research and care gap
identification.",Will E. Thompson
2023-12-14T14:26:57Z,http://arxiv.org/abs/2312.08976v2,Dynamic Retrieval-Augmented Generation,"Current state-of-the-art large language models are effective in generating
high-quality text and encapsulating a broad spectrum of world knowledge. These
models, however, often hallucinate and lack locally relevant factual data.
Retrieval-augmented approaches were introduced to overcome these problems and
provide more accurate responses. Typically, the retrieved information is simply
appended to the main request, restricting the context window size of the model.
We propose a novel approach for the Dynamic Retrieval-Augmented Generation
(DRAG), based on the entity-augmented generation, which injects compressed
embeddings of the retrieved entities into the generative model. The proposed
pipeline was developed for code-generation tasks, yet can be transferred to
some domains of natural language processing. To train the model, we collect and
publish a new project-level code generation dataset. We use it for the
evaluation along with publicly available datasets. Our approach achieves
several targets: (1) lifting the length limitations of the context window,
saving on the prompt size; (2) allowing huge expansion of the number of
retrieval entities available for the context; (3) alleviating the problem of
misspelling or failing to find relevant entity names. This allows the model to
beat all baselines (except GPT-3.5) with a strong margin.",Anton Shapkin
2023-12-16T14:47:03Z,http://arxiv.org/abs/2312.10466v1,"RIGHT: Retrieval-augmented Generation for Mainstream Hashtag
  Recommendation","Automatic mainstream hashtag recommendation aims to accurately provide users
with concise and popular topical hashtags before publication. Generally,
mainstream hashtag recommendation faces challenges in the comprehensive
difficulty of newly posted tweets in response to new topics, and the accurate
identification of mainstream hashtags beyond semantic correctness. However,
previous retrieval-based methods based on a fixed predefined mainstream hashtag
list excel in producing mainstream hashtags, but fail to understand the
constant flow of up-to-date information. Conversely, generation-based methods
demonstrate a superior ability to comprehend newly posted tweets, but their
capacity is constrained to identifying mainstream hashtags without additional
features. Inspired by the recent success of the retrieval-augmented technique,
in this work, we attempt to adopt this framework to combine the advantages of
both approaches. Meantime, with the help of the generator component, we could
rethink how to further improve the quality of the retriever component at a low
cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag
Recommender (RIGHT), which consists of three components: 1) a retriever seeks
relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances
mainstream identification by introducing global signals; and 3) a generator
incorporates input tweets and selected hashtags to directly generate the
desired hashtags. The experimental results show that our method achieves
significant improvements over state-of-the-art baselines. Moreover, RIGHT can
be easily integrated into large language models, improving the performance of
ChatGPT by more than 10%.",Run-Ze Fan
2023-12-18T07:47:33Z,http://arxiv.org/abs/2312.10997v5,Retrieval-Augmented Generation for Large Language Models: A Survey,"Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.",Yunfan Gao
2023-12-18T17:18:04Z,http://arxiv.org/abs/2312.11361v3,"""Knowing When You Don't Know"": A Multilingual Relevance Assessment
  Dataset for Robust Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.",Nandan Thakur
2023-12-30T16:56:24Z,http://arxiv.org/abs/2401.00280v3,"Advancing TTP Analysis: Harnessing the Power of Large Language Models
  with Retrieval Augmented Generation","Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use
to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK
framework can be challenging for cybersecurity practitioners due to presumed
expertise and complex dependencies. Meanwhile, advancements with Large Language
Models (LLMs) have led to recent surge in studies exploring its uses in
cybersecurity operations. It is, however, unclear how LLMs can be used in an
efficient and proper way to provide accurate responses for critical domains
such as cybersecurity. This leads us to investigate how to better use two types
of LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only
(e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended
purposes (i.e., tactics) of a cyberattack procedure. This work studies and
compares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs.
Retrieval Augmented Generation (RAG) for decoder-only LLMs (without
fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with
relevant contexts for each cyberattack procedure. Our studies show decoder-only
LLMs with RAG achieves better performance than encoder-only models with SFT,
particularly when directly relevant context is extracted by RAG. The
decoder-only results could suffer low `Precision' while achieving high
`Recall'. Our findings further highlight a counter-intuitive observation that
more generic prompts tend to yield better predictions of cyberattack tactics
than those that are more specifically tailored.",Reza Fayyazi
2024-01-21T03:46:00Z,http://arxiv.org/abs/2401.11391v1,"Interactive AI with Retrieval-Augmented Generation for Next Generation
  Networking","With the advance of artificial intelligence (AI), the emergence of Google
Gemini and OpenAI Q* marks the direction towards artificial general
intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has
been introduced, which can interactively understand and respond not only to
human user input but also to dynamic system and network conditions. In this
article, we explore an integration and enhancement of IAI in networking. We
first comprehensively review recent developments and future perspectives of AI
and then introduce the technology and components of IAI. We then explore the
integration of IAI into the next-generation networks, focusing on how implicit
and explicit interactions can enhance network functionality, improve user
experience, and promote efficient network management. Subsequently, we propose
an IAI-enabled network management and optimization framework, which consists of
environment, perception, action, and brain units. We also design the pluggable
large language model (LLM) module and retrieval augmented generation (RAG)
module to build the knowledge base and contextual memory for decision-making in
the brain unit. We demonstrate the effectiveness of the framework through case
studies. Finally, we discuss potential research directions for IAI-based
networks.",Ruichen Zhang
2024-01-27T11:41:48Z,http://arxiv.org/abs/2401.15391v1,"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
  Queries","Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.",Yixuan Tang
2024-02-05T06:52:53Z,http://arxiv.org/abs/2402.02764v1,"List-aware Reranking-Truncation Joint Model for Search and
  Retrieval-augmented Generation","The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.",Shicheng Xu
2024-02-11T12:25:41Z,http://arxiv.org/abs/2402.07179v3,"Prompt Perturbation in Retrieval-Augmented Generation based Large
  Language Models","The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.",Zhibo Hu
