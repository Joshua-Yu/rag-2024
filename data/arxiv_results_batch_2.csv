Published Date,Link,Title,Summary,First Author
2024-02-12T13:13:04Z,http://arxiv.org/abs/2402.07630v3,"G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering","Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}",Xiaoxin He
2024-02-04T20:42:30Z,http://arxiv.org/abs/2402.14594v1,"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.",Zifei FeiFei Han
2024-02-27T19:08:05Z,http://arxiv.org/abs/2402.17840v3,"Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems","Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. We also study multiple
effects of RAG setup on the extractability of data, indicating that following
unexpected instructions to regurgitate data can be an outcome of failure in
effectively utilizing contexts for modern LMs, and further show that such
vulnerability can be greatly mitigated by position bias elimination strategies.
Extending our study to production RAG models GPTs, we design an attack that can
cause datastore leakage with a 100% success rate on 25 randomly selected
customized GPTs with at most 2 queries, and we extract text data verbatim at a
rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words
by prompting the GPTs with only 100 queries generated by themselves.",Zhenting Qi
2024-02-28T08:24:38Z,http://arxiv.org/abs/2402.18150v2,"Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.",Shicheng Xu
2024-03-03T08:07:55Z,http://arxiv.org/abs/2403.01432v5,"Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge","Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting
strong performance across diverse tasks and domains. However, it has been
observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LMs on
low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning
(FT) over synthetic data. This paper explores and evaluates the impact of RAG
and FT on customizing LMs in handling low-frequency entities on question
answering tasks. We conduct extensive experiments on twelve LMs of varying size
and type and different fine tuning, data augmentation, and retrieval models.
Our findings indicate that while FT boosts the performance across entities of
varying popularity, RAG surpasses FT by a large margin particularly for least
popular factual knowledge. Additionally, the success of both RAG and FT
approaches is amplified by improving retrieval and data augmentation
techniques. Fine tuning, while beneficial for small LMs, requires extensive
resources. To address this issue, we propose the new Stimulus RAG approach that
surpasses the effectiveness of fine tuning based approaches, thereby
eliminating the need for the costly data augmentation and fine tuning step for
enriching LMs with less popular factual knowledge. The code is available at
\url{https://github.com/informagi/RAGvsFT}.",Heydar Soudani
2024-03-12T21:06:31Z,http://arxiv.org/abs/2403.09727v1,"Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems","The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.",Robert Lakatos
2024-03-27T18:09:55Z,http://arxiv.org/abs/2403.18920v1,CPR: Retrieval Augmented Generation for Copyright Protection,"Retrieval Augmented Generation (RAG) is emerging as a flexible and robust
technique to adapt models to private users data without training, to handle
credit attribution, and to allow efficient machine unlearning at scale.
However, RAG techniques for image generation may lead to parts of the retrieved
samples being copied in the model's output. To reduce risks of leaking private
information contained in the retrieved set, we introduce Copy-Protected
generation with Retrieval (CPR), a new method for RAG with strong copyright
protection guarantees in a mixed-private setting for diffusion models.CPR
allows to condition the output of diffusion models on a set of retrieved
images, while also guaranteeing that unique identifiable information about
those example is not exposed in the generated outputs. In particular, it does
so by sampling from a mixture of public (safe) distribution and private (user)
distribution by merging their diffusion scores at inference. We prove that CPR
satisfies Near Access Freeness (NAF) which bounds the amount of information an
attacker may be able to extract from the generated images. We provide two
algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously
proposed rejection-sampling-based NAF methods, our methods enable efficient
copyright-protected sampling with a single run of backward diffusion. We show
that our method can be applied to any pre-trained conditional diffusion model,
such as Stable Diffusion or unCLIP. In particular, we empirically show that
applying CPR on top of unCLIP improves quality and text-to-image alignment of
the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit
attribution, copy-right protection, and deterministic, constant time,
unlearning.",Aditya Golatkar
2024-04-18T18:32:30Z,http://arxiv.org/abs/2404.12457v2,RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has shown significant improvements in
various natural language processing tasks by integrating the strengths of large
language models (LLMs) and external knowledge databases. However, RAG
introduces long sequence generation and leads to high computation and memory
costs. We propose RAGCache, a novel multilevel dynamic caching system tailored
for RAG. Our analysis benchmarks current RAG systems, pinpointing the
performance bottleneck (i.e., long sequence due to knowledge injection) and
optimization opportunities (i.e., caching knowledge's intermediate states).
Based on these insights, we design RAGCache, which organizes the intermediate
states of retrieved knowledge in a knowledge tree and caches them in the GPU
and host memory hierarchy. RAGCache proposes a replacement policy that is aware
of LLM inference characteristics and RAG retrieval patterns. It also
dynamically overlaps the retrieval and inference steps to minimize the
end-to-end latency. We implement RAGCache and evaluate it on vLLM, a
state-of-the-art LLM inference system and Faiss, a state-of-the-art vector
database. The experimental results show that RAGCache reduces the time to first
token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to
vLLM integrated with Faiss.",Chao Jin
2024-04-20T14:42:43Z,http://arxiv.org/abs/2404.13397v1,Retrieval-Augmented Generation-based Relation Extraction,"Information Extraction (IE) is a transformative process that converts
unstructured text data into a structured format by employing entity and
relation extraction (RE) methodologies. The identification of the relation
between a pair of entities plays a crucial role within this framework. Despite
the existence of various techniques for relation extraction, their efficacy
heavily relies on access to labeled data and substantial computational
resources. In addressing these challenges, Large Language Models (LLMs) emerge
as promising solutions; however, they might return hallucinating responses due
to their own training data. To overcome these limitations, Retrieved-Augmented
Generation-based Relation Extraction (RAG4RE) in this work is proposed,
offering a pathway to enhance the performance of relation extraction tasks.
  This work evaluated the effectiveness of our RAG4RE approach utilizing
different LLMs. Through the utilization of established benchmarks, such as
TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to
comprehensively evaluate the efficacy of our RAG4RE approach. In particularly,
we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our
investigation. The results of our study demonstrate that our RAG4RE approach
surpasses performance of traditional RE approaches based solely on LLMs,
particularly evident in the TACRED dataset and its variations. Furthermore, our
approach exhibits remarkable performance compared to previous RE methodologies
across both TACRED and TACREV datasets, underscoring its efficacy and potential
for advancing RE tasks in natural language processing.",Sefika Efeoglu
2024-04-22T09:56:59Z,http://arxiv.org/abs/2404.14043v1,"LLMs Know What They Need: Leveraging a Missing Information Guided
  Framework to Empower Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating
outdated knowledge or hallucination by supplying LLMs with updated and relevant
knowledge. However, there are still several difficulties for RAG in
understanding complex multi-hop query and retrieving relevant documents, which
require LLMs to perform reasoning and retrieve step by step. Inspired by
human's reasoning process in which they gradually search for the required
information, it is natural to ask whether the LLMs could notice the missing
information in each reasoning step. In this work, we first experimentally
verified the ability of LLMs to extract information as well as to know the
missing. Based on the above discovery, we propose a Missing Information Guided
Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the
identification of missing information to generate a targeted query that steers
the subsequent knowledge retrieval. Besides, we design a sentence-level
re-ranking filtering approach to filter the irrelevant content out from
document, along with the information extraction capability of LLMs to extract
useful information from cleaned-up documents, which in turn to bolster the
overall efficacy of RAG. Extensive experiments conducted on multiple public
datasets reveal the superiority of the proposed MIGRES method, and analytical
experiments demonstrate the effectiveness of our proposed modules.",Keheng Wang
2024-04-26T23:05:20Z,http://arxiv.org/abs/2404.17723v2,"Retrieval-Augmented Generation with Knowledge Graphs for Customer
  Service Question Answering","In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.",Zhentao Xu
2024-05-07T04:04:53Z,http://arxiv.org/abs/2405.03989v2,"A Method for Parsing and Vectorization of Semi-structured Data used in
  Retrieval Augmented Generation","This paper presents a novel method for parsing and vectorizing
semi-structured data to enhance the functionality of Retrieval-Augmented
Generation (RAG) within Large Language Models (LLMs). We developed a
comprehensive pipeline for converting various data formats into .docx, enabling
efficient parsing and structured data extraction. The core of our methodology
involves the construction of a vector database using Pinecone, which integrates
seamlessly with LLMs to provide accurate, context-specific responses,
particularly in environmental management and wastewater treatment operations.
Through rigorous testing with both English and Chinese texts in diverse
document formats, our results demonstrate a marked improvement in the precision
and reliability of LLMs outputs. The RAG-enhanced models displayed enhanced
ability to generate contextually rich and technically accurate responses,
underscoring the potential of vector knowledge bases in significantly boosting
the performance of LLMs in specialized domains. This research not only
illustrates the effectiveness of our method but also highlights its potential
to revolutionize data processing and analysis in environmental sciences,
setting a precedent for future advancements in AI-driven applications. Our code
is available at https://github.com/linancn/TianGong-AI-Unstructure.git.",Hang Yang
2024-05-07T22:31:50Z,http://arxiv.org/abs/2405.04700v1,"Robust Implementation of Retrieval-Augmented Generation on Edge-based
  Computing-in-Memory Architectures","Large Language Models (LLMs) deployed on edge devices learn through
fine-tuning and updating a certain portion of their parameters. Although such
learning methods can be optimized to reduce resource utilization, the overall
required resources remain a heavy burden on edge devices. Instead,
Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method,
can improve the quality of the LLM-generated content without updating model
parameters. However, the RAG-based LLM may involve repetitive searches on the
profile data in every user-LLM interaction. This search can lead to significant
latency along with the accumulation of user data. Conventional efforts to
decrease latency result in restricting the size of saved user data, thus
reducing the scalability of RAG as user data continuously grows. It remains an
open question: how to free RAG from the constraints of latency and scalability
on edge devices? In this paper, we propose a novel framework to accelerate RAG
via Computing-in-Memory (CiM) architectures. It accelerates matrix
multiplications by performing in-situ computation inside the memory while
avoiding the expensive data transfer between the computing unit and memory. Our
framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive
learning-based training method and noise-aware training, can enable RAG to
efficiently search profile data with CiM. To the best of our knowledge, this is
the first work utilizing CiM to accelerate RAG.",Ruiyang Qin
2024-05-05T18:32:06Z,http://arxiv.org/abs/2405.06681v1,"Leveraging Lecture Content for Improved Feedback: Explorations with
  GPT-4 and Retrieval Augmented Generation","This paper presents the use of Retrieval Augmented Generation (RAG) to
improve the feedback generated by Large Language Models for programming tasks.
For this purpose, corresponding lecture recordings were transcribed and made
available to the Large Language Model GPT-4 as external knowledge source
together with timestamps as metainformation by using RAG. The purpose of this
is to prevent hallucinations and to enforce the use of the technical terms and
phrases from the lecture. In an exercise platform developed to solve
programming problems for an introductory programming lecture, students can
request feedback on their solutions generated by GPT-4. For this task GPT-4
receives the students' code solution, the compiler output, the result of unit
tests and the relevant passages from the lecture notes available through the
use of RAG as additional context. The feedback generated by GPT-4 should guide
students to solve problems independently and link to the lecture content, using
the time stamps of the transcript as meta-information. In this way, the
corresponding lecture videos can be viewed immediately at the corresponding
positions. For the evaluation, students worked with the tool in a workshop and
decided for each feedback whether it should be extended by RAG or not. First
results based on a questionnaire and the collected usage data show that the use
of RAG can improve feedback generation and is preferred by students in some
situations. Due to the slower speed of feedback generation, the benefits are
situation dependent.",Sven Jacobs
2024-05-13T07:56:15Z,http://arxiv.org/abs/2405.07530v1,Prompt-based Code Completion via Multi-Retrieval Augmented Generation,"Automated code completion, aiming at generating subsequent tokens from
unfinished code, has been significantly benefited from recent progress in
pre-trained Large Language Models (LLMs). However, these models often suffer
from coherence issues and hallucinations when dealing with complex code logic
or extrapolating beyond their training data. Existing Retrieval Augmented
Generation (RAG) techniques partially address these issues by retrieving
relevant code with a separate encoding model where the retrieved snippet serves
as contextual reference for code completion. However, their retrieval scope is
subject to a singular perspective defined by the encoding model, which largely
overlooks the complexity and diversity inherent in code semantics. To address
this limitation, we propose ProCC, a code completion framework leveraging
prompt engineering and the contextual multi-armed bandits algorithm to flexibly
incorporate and adapt to multiple perspectives of code. ProCC first employs a
prompt-based multi-retriever system which crafts prompt templates to elicit LLM
knowledge to understand code semantics with multiple retrieval perspectives.
Then, it adopts the adaptive retrieval selection algorithm to incorporate code
similarity into the decision-making process to determine the most suitable
retrieval perspective for the LLM to complete the code. Experimental results
demonstrate that ProCC outperforms state-of-the-art code completion technique
by 8.6% on our collected open-source benchmark suite and 10.1% on the
private-domain benchmark suite collected from a billion-user e-commerce company
in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in
a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned
model.",Hanzhuo Tan
2024-05-15T12:41:20Z,http://arxiv.org/abs/2405.13021v1,"IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning
  Inner Monologues","Although the Retrieval-Augmented Generation (RAG) paradigms can use external
knowledge to enhance and ground the outputs of Large Language Models (LLMs) to
mitigate generative hallucinations and static knowledge base problems, they
still suffer from limited flexibility in adopting Information Retrieval (IR)
systems with varying capabilities, constrained interpretability during the
multi-round retrieval process, and a lack of end-to-end optimization. To
address these challenges, we propose a novel LLM-centric approach, IM-RAG, that
integrates IR systems with LLMs to support multi-round RAG through learning
Inner Monologues (IM, i.e., the human inner voice that narrates one's
thoughts). During the IM process, the LLM serves as the core reasoning model
(i.e., Reasoner) to either propose queries to collect more information via the
Retriever or to provide a final answer based on the conversational context. We
also introduce a Refiner that improves the outputs from the Retriever,
effectively bridging the gap between the Reasoner and IR modules with varying
capabilities and fostering multi-round communications. The entire IM process is
optimized via Reinforcement Learning (RL) where a Progress Tracker is
incorporated to provide mid-step rewards, and the answer prediction is further
separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive
experiments with the HotPotQA dataset, a popular benchmark for retrieval-based,
multi-step question-answering. The results show that our approach achieves
state-of-the-art (SOTA) performance while providing high flexibility in
integrating IR modules as well as strong interpretability exhibited in the
learned inner monologues.",Diji Yang
2024-05-22T07:21:32Z,http://arxiv.org/abs/2405.13401v4,"TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in
  Large Language Models","Large language models (LLMs) have raised concerns about potential security
threats despite performing significantly in Natural Language Processing (NLP).
Backdoor attacks initially verified that LLM is doing substantial harm at all
stages, but the cost and robustness have been criticized. Attacking LLMs is
inherently risky in security review, while prohibitively expensive. Besides,
the continuous iteration of LLMs will degrade the robustness of backdoors. In
this paper, we propose TrojanRAG, which employs a joint backdoor attack in the
Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack
scenarios. Specifically, the adversary constructs elaborate target contexts and
trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized
by contrastive learning, thus constraining the triggering conditions to a
parameter subspace to improve the matching. To improve the recall of the RAG
for the target contexts, we introduce a knowledge graph to construct structured
data to achieve hard matching at a fine-grained level. Moreover, we normalize
the backdoor scenarios in LLMs to analyze the real harm caused by backdoors
from both attackers' and users' perspectives and further verify whether the
context is a favorable tool for jailbreaking models. Extensive experimental
results on truthfulness, language understanding, and harmfulness show that
TrojanRAG exhibits versatility threats while maintaining retrieval capabilities
on normal queries.",Pengzhou Cheng
2024-05-27T08:26:45Z,http://arxiv.org/abs/2405.16933v1,"Empowering Large Language Models to Set up a Knowledge Retrieval Indexer
  via Self-Learning","Retrieval-Augmented Generation (RAG) offers a cost-effective approach to
injecting real-time knowledge into large language models (LLMs). Nevertheless,
constructing and validating high-quality knowledge repositories require
considerable effort. We propose a pre-retrieval framework named Pseudo-Graph
Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students
by providing them with abundant raw reading materials and encouraging them to
engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through
common topics or complementary facts to form a pseudo-graph database. During
the retrieval phase, PG-RAG mimics the human behavior in flipping through
notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates
highly corroborated fact paths to provide a structured and refined sub-graph
assisting LLMs. We validated PG-RAG on three specialized question-answering
datasets. In single-document tasks, PG-RAG significantly outperformed the
current best baseline, KGP-LLaMA, across all key evaluation metrics, with an
average overall performance improvement of 11.6%. Specifically, its BLEU score
increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In
multi-document scenarios, the average metrics of PG-RAG were at least 2.35%
higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed
stable improvements of around 7.55% and 12.75%, respectively. Our code:
https://github.com/IAAR-Shanghai/PGRAG.",Xun Liang
2024-05-27T19:02:18Z,http://arxiv.org/abs/2405.17602v1,Augmenting Textual Generation via Topology Aware Retrieval,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.",Yu Wang
2024-05-29T03:17:16Z,http://arxiv.org/abs/2405.18727v2,CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control,"Retrieval-augmented generation (RAG) has emerged as a promising solution for
mitigating hallucinations of large language models (LLMs) with retrieved
external knowledge. Adaptive RAG enhances this approach by enabling dynamic
retrieval during generation, activating retrieval only when the query exceeds
LLM's internal knowledge. Existing methods primarily focus on detecting LLM's
confidence via statistical uncertainty. Instead, we present the first attempts
to solve adaptive RAG from a representation perspective and develop an inherent
control-based framework, termed \name. Specifically, we extract the features
that represent the honesty and confidence directions of LLM and adopt them to
control LLM behavior and guide retrieval timing decisions. We also design a
simple yet effective query formulation strategy to support adaptive retrieval.
Experiments show that \name is superior to existing adaptive RAG methods on a
diverse set of tasks, the honesty steering can effectively make LLMs more
honest and confidence monitoring is a promising indicator of retrieval
trigger.Our code is available at \url{https://github.com/HSLiu-Initial/CtrlA}.",Huanshuo Liu
2024-05-30T19:46:36Z,http://arxiv.org/abs/2405.20446v2,"Is My Data in Your Retrieval Database? Membership Inference Attacks
  Against Retrieval Augmented Generation","Retrieval Augmented Generation (RAG) systems have shown great promise in
natural language processing. However, their reliance on data stored in a
retrieval database, which may contain proprietary or sensitive information,
introduces new privacy concerns. Specifically, an attacker may be able to infer
whether a certain text passage appears in the retrieval database by observing
the outputs of the RAG system, an attack known as a Membership Inference Attack
(MIA). Despite the significance of this threat, MIAs against RAG systems have
yet remained under-explored. This study addresses this gap by introducing an
efficient and easy-to-use method for conducting MIA against RAG systems. We
demonstrate the effectiveness of our attack using two benchmark datasets and
multiple generative models, showing that the membership of a document in the
retrieval database can be efficiently determined through the creation of an
appropriate prompt in both black-box and gray-box settings. Moreover, we
introduce an initial defense strategy based on adding instructions to the RAG
template, which shows high effectiveness for some datasets and models. Our
findings highlight the importance of implementing security countermeasures in
deployed RAG systems and developing more advanced defenses to protect the
privacy and security of retrieval databases.",Maya Anderson
2024-05-24T16:36:47Z,http://arxiv.org/abs/2406.00029v1,Clustered Retrieved Augmented Generation (CRAG),"Providing external knowledge to Large Language Models (LLMs) is a key point
for using these models in real-world applications for several reasons, such as
incorporating up-to-date content in a real-time manner, providing access to
domain-specific knowledge, and contributing to hallucination prevention. The
vector database-based Retrieval Augmented Generation (RAG) approach has been
widely adopted to this end. Thus, any part of external knowledge can be
retrieved and provided to some LLM as the input context. Despite RAG approach's
success, it still might be unfeasible for some applications, because the
context retrieved can demand a longer context window than the size supported by
LLM. Even when the context retrieved fits into the context window size, the
number of tokens might be expressive and, consequently, impact costs and
processing time, becoming impractical for most applications. To address these,
we propose CRAG, a novel approach able to effectively reduce the number of
prompting tokens without degrading the quality of the response generated
compared to a solution using RAG. Through our experiments, we show that CRAG
can reduce the number of tokens by at least 46\%, achieving more than 90\% in
some cases, compared to RAG. Moreover, the number of tokens with CRAG does not
increase considerably when the number of reviews analyzed is higher, unlike
RAG, where the number of tokens is almost 9x higher when there are 75 reviews
compared to 4 reviews.",Simon Akesson
2024-06-03T02:56:14Z,http://arxiv.org/abs/2406.00944v2,A Theory for Token-Level Harmonization in Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.",Shicheng Xu
2024-06-09T05:33:51Z,http://arxiv.org/abs/2406.05654v2,"DomainRAG: A Chinese Benchmark for Evaluating Domain-specific
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) offers a promising solution to address
various limitations of Large Language Models (LLMs), such as hallucination and
difficulties in keeping up with real-time updates. This approach is
particularly critical in expert and domain-specific applications where LLMs
struggle to cover expert knowledge. Therefore, evaluating RAG models in such
scenarios is crucial, yet current studies often rely on general knowledge
sources like Wikipedia to assess the models' abilities in solving common-sense
problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific
context, college enrollment. We identified six required abilities for RAG
models, including the ability in conversational RAG, analyzing structural
information, faithfulness to external knowledge, denoising, solving
time-sensitive problems, and understanding multi-document interactions. Each
ability has an associated dataset with shared corpora to evaluate the RAG
models' performance. We evaluated popular LLMs such as Llama, Baichuan,
ChatGLM, and GPT models. Experimental results indicate that existing
closed-book LLMs struggle with domain-specific questions, highlighting the need
for RAG models to solve expert problems. Moreover, there is room for RAG models
to improve their abilities in comprehending conversational history, analyzing
structural information, denoising, processing multi-document interactions, and
faithfulness in expert knowledge. We expect future studies could solve these
problems better.",Shuting Wang
2024-06-17T06:48:31Z,http://arxiv.org/abs/2406.11258v2,"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented
  Generation","Large Language Models (LLMs) have shown great potential in the biomedical
domain with the advancement of retrieval-augmented generation (RAG). However,
existing retrieval-augmented approaches face challenges in addressing diverse
queries and documents, particularly for medical knowledge queries, resulting in
sub-optimal performance. To address these limitations, we propose a novel
plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search
(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree
search, SeRTS boosts the zero-shot performance of retrieving high-quality and
informative results for RAG. We further enhance retrieval performance by
fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the
trajectories collected by SeRTS as feedback. Controlled experiments using the
BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method
significantly improves the performance of the BM25 retriever and surpasses the
strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than
self-reflection. Our proposed method effectively adapts LLMs to document
retrieval tasks, enhancing their ability to retrieve highly relevant documents
for RAG in the context of medical knowledge queries. This work presents a
significant step forward in leveraging LLMs for accurate and comprehensive
biomedical question answering.",Minda Hu
2024-06-18T12:09:02Z,http://arxiv.org/abs/2406.12534v4,Unified Active Retrieval for Retrieval Augmented Generation,"In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and
applying it to every instruction is sub-optimal. Therefore, determining whether
to retrieve is crucial for RAG, which is usually referred to as Active
Retrieval. However, existing active retrieval methods face two challenges: 1.
They usually rely on a single criterion, which struggles with handling various
types of instructions. 2. They depend on specialized and highly differentiated
procedures, and thus combining them makes the RAG system more complicated and
leads to higher response latency. To address these challenges, we propose
Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts
them into plug-and-play classification tasks, which achieves multifaceted
retrieval timing judgements with negligible extra inference cost. We further
introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to
process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR
significantly outperforms existing work on the retrieval timing judgement and
the performance of downstream tasks, which shows the effectiveness of UAR and
its helpfulness to downstream tasks.",Qinyuan Cheng
2024-06-18T12:52:51Z,http://arxiv.org/abs/2406.12566v3,"RichRAG: Crafting Rich Responses for Multi-faceted Queries in
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) effectively addresses issues of static
knowledge and hallucination in large language models. Existing studies mostly
focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with
diverse sub-intents, for which they desire rich and long-form answers covering
multiple relevant aspects. To tackle this important yet underexplored problem,
we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect
explorer to identify potential sub-aspects of input questions, a multi-faceted
retriever to build a candidate pool of diverse external documents related to
these sub-aspects, and a generative list-wise ranker, which is a key module to
provide the top-k most valuable documents for the final generator. These ranked
documents sufficiently cover various query aspects and are aware of the
generator's preferences, hence incentivizing it to produce rich and
comprehensive responses for users. The training of our ranker involves a
supervised fine-tuning stage to ensure the basic coverage of documents, and a
reinforcement learning stage to align downstream LLM's preferences to the
ranking of documents. Experimental results on two publicly available datasets
prove that our framework effectively and efficiently provides comprehensive and
satisfying responses to users.",Shuting Wang
2024-06-19T09:14:41Z,http://arxiv.org/abs/2406.13372v2,"Thread: A Logic-Based Data Organization Paradigm for How-To Question
  Answering with Retrieval Augmented Generation","Recent advances in retrieval-augmented generation have significantly improved
the performance of question-answering systems, particularly on factoid '5Ws'
questions. However, these systems still face substantial challenges when
addressing '1H' questions, specifically how-to questions, which are integral to
decision-making processes and require dynamic, step-by-step answers. The key
limitation lies in the prevalent data organization paradigm, chunk, which
divides documents into fixed-size segments, and disrupts the logical coherence
and connections within the context. To overcome this, in this paper, we propose
Thread, a novel data organization paradigm aimed at enabling current systems to
handle how-to questions more effectively. Specifically, we introduce a new
knowledge granularity, termed 'logic unit', where documents are transformed
into more structured and loosely interconnected logic units with large language
models. Extensive experiments conducted across both open-domain and industrial
settings demonstrate that Thread outperforms existing paradigms significantly,
improving the success rate of handling how-to questions by 21% to 33%.
Moreover, Thread exhibits high adaptability in processing various document
formats, drastically reducing the candidate quantity in the knowledge base and
minimizing the required information to one-fourth compared with chunk,
optimizing both efficiency and effectiveness.",Kaikai An
2024-06-19T15:25:29Z,http://arxiv.org/abs/2406.13629v2,"InstructRAG: Instructing Retrieval-Augmented Generation via
  Self-Synthesized Rationales","Retrieval-augmented generation (RAG) has shown promising potential to enhance
the accuracy and factuality of language models (LMs). However, imperfect
retrievers or noisy corpora can introduce misleading or even erroneous
information to the retrieved contents, posing a significant challenge to the
generation quality. Existing RAG methods typically address this challenge by
directly predicting final answers despite potentially noisy inputs, resulting
in an implicit denoising process that is difficult to interpret and verify. On
the other hand, the acquisition of explicit denoising supervision is often
costly, involving significant human efforts. In this work, we propose
InstructRAG, where LMs explicitly learn the denoising process through
self-synthesized rationales -- First, we instruct the LM to explain how the
ground-truth answer is derived from retrieved documents. Then, these rationales
can be used either as demonstrations for in-context learning of explicit
denoising or as supervised fine-tuning data to train the model. Compared to
standard RAG approaches, InstructRAG requires no additional supervision, allows
for easier verification of the predicted answers, and effectively improves
generation accuracy. Experiments show InstructRAG consistently outperforms
existing RAG methods in both training-free and trainable scenarios, achieving a
relative improvement of 8.3% over the best baseline method on average across
five knowledge-intensive benchmarks. Extensive analysis indicates that
InstructRAG scales well with increased numbers of retrieved documents and
consistently exhibits robust denoising ability even in out-of-domain datasets,
demonstrating strong generalizability.",Zhepei Wei
2024-06-19T16:10:26Z,http://arxiv.org/abs/2406.13663v4,"Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation","Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.",Jirui Qi
2024-06-19T19:06:36Z,http://arxiv.org/abs/2406.13779v1,"FoRAG: Factuality-optimized Retrieval Augmented Generation for
  Web-enhanced Long-form Question Answering","Retrieval Augmented Generation (RAG) has become prevalent in
question-answering (QA) tasks due to its ability of utilizing search engine to
enhance the quality of long-form question-answering (LFQA). Despite the
emergence of various open source methods and web-enhanced commercial systems
such as Bing Chat, two critical problems remain unsolved, i.e., the lack of
factuality and clear logic in the generated long-form answers. In this paper,
we remedy these issues via a systematic study on answer generation in
web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced
generator to achieve clear logic in the generation of multifaceted answers and
construct two datasets accordingly. Then we propose a factuality optimization
method based on a carefully designed doubly fine-grained RLHF framework, which
contains automatic evaluation and reward modeling in different levels of
granularity. Our generic framework comprises conventional fine-grained RLHF
methods as special cases. Extensive experiments verify the superiority of our
proposed \textit{Factuality-optimized RAG (FoRAG)} method on both English and
Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat,
the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly
used metrics (i.e., coherence, helpfulness, and factuality), while the number
of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets
and models are made publicly available for better reproducibility:
https://huggingface.co/forag.",Tianchi Cai
2024-06-20T10:04:09Z,http://arxiv.org/abs/2406.14162v3,"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation","Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.",Jingwei Ni
2024-06-20T23:20:34Z,http://arxiv.org/abs/2406.14783v2,Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework,"Challenges in the automated evaluation of Retrieval-Augmented Generation
(RAG) Question-Answering (QA) systems include hallucination problems in
domain-specific knowledge and the lack of gold standard benchmarks for company
internal tasks. This results in difficulties in evaluating RAG variations, like
RAG-Fusion (RAGF), in the context of a product QA task at Infineon
Technologies. To solve these problems, we propose a comprehensive evaluation
framework, which leverages Large Language Models (LLMs) to generate large
datasets of synthetic queries based on real user queries and in-domain
documents, uses LLM-as-a-judge to rate retrieved documents and answers,
evaluates the quality of answers, and ranks different variants of
Retrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based
competition. LLM-as-a-judge rating of a random sample of synthetic queries
shows a moderate, positive correlation with domain expert scoring in relevance,
accuracy, completeness, and precision. While RAGF outperformed RAG in Elo
score, a significance analysis against expert annotations also shows that RAGF
significantly outperforms RAG in completeness, but underperforms in precision.
In addition, Infineon's RAGF assistant demonstrated slightly higher performance
in document relevance based on MRR@5 scores. We find that RAGElo positively
aligns with the preferences of human annotators, though due caution is still
required. Finally, RAGF's approach leads to more complete answers based on
expert annotations and better answers overall based on RAGElo's evaluation
criteria.",Zackary Rackauckas
2024-06-24T17:37:52Z,http://arxiv.org/abs/2406.16828v1,"Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024
  Retrieval-Augmented Generation Track","Did you try out the new Bing Search? Or maybe you fiddled around with Google
AI~Overviews? These might sound familiar because the modern-day search stack
has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language
models (LLMs) to provide a well-informed, attributed, concise summary in
contrast to the traditional search paradigm that relies on displaying a ranked
list of documents. Therefore, given these recent advancements, it is crucial to
have an arena to build, test, visualize, and systematically evaluate RAG-based
search systems. With this in mind, we propose the TREC 2024 RAG Track to foster
innovation in evaluating RAG systems. In our work, we lay out the steps we've
made towards making this track a reality -- we describe the details of our
reusable framework, Ragnar\""ok, explain the curation of the new MS MARCO V2.1
collection choice, release the development topics for the track, and
standardize the I/O definitions which assist the end user. Next, using
Ragnar\""ok, we identify and provide key industrial baselines such as OpenAI's
GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface
for an interactive arena allowing benchmarking pairwise RAG systems by
crowdsourcing. We open-source our Ragnar\""ok framework and baselines to achieve
a unified standard for future RAG systems.",Ronak Pradeep
2024-06-26T07:21:02Z,http://arxiv.org/abs/2406.18122v1,Poisoned LangChain: Jailbreak LLMs by LangChain,"With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.",Ziqiu Wang
2024-06-26T18:26:53Z,http://arxiv.org/abs/2406.18676v2,"Understand What LLM Needs: Dual Preference Alignment for
  Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) has demonstrated effectiveness in
mitigating the hallucination problem of large language models (LLMs). However,
the difficulty of aligning the retriever with the diverse LLMs' knowledge
preferences inevitably poses an inevitable challenge in developing a reliable
RAG system. To address this issue, we propose DPA-RAG, a universal framework
designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction
pipline and incorporate five novel query augmentation strategies to alleviate
preference data scarcity. Based on preference data, DPA-RAG accomplishes both
external and internal preference alignment: 1) It jointly integrate pair-wise,
point-wise, and contrastive preference alignment abilities into the reranker,
achieving external preference alignment among RAG components. 2) It further
introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),
enabling LLMs to implicitly capture knowledge aligned with their reasoning
preferences, achieving LLMs' internal alignment. Experimental results across
four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all
baselines and seamlessly integrates both black-box and open-sourced LLM
readers. Further qualitative analysis and discussions also provide empirical
guidance for achieving reliable RAG systems. Our code is publicly available at
https://github.com/dongguanting/DPA-RAG.",Guanting Dong
2024-06-27T14:58:38Z,http://arxiv.org/abs/2406.19234v2,"Generating Is Believing: Membership Inference Attacks against
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that
mitigates issues such as hallucinations and knowledge staleness in Large
Language Models (LLMs) by retrieving relevant knowledge from an external
database to assist in content generation. Existing research has demonstrated
potential privacy risks associated with the LLMs of RAG. However, the privacy
risks posed by the integration of an external database, which often contains
sensitive data such as medical records or personal identities, have remained
largely unexplored. In this paper, we aim to bridge this gap by focusing on
membership privacy of RAG's external database, with the aim of determining
whether a given sample is part of the RAG's database. Our basic idea is that if
a sample is in the external database, it will exhibit a high degree of semantic
similarity to the text generated by the RAG system. We present S$^2$MIA, a
\underline{M}embership \underline{I}nference \underline{A}ttack that utilizes
the \underline{S}emantic \underline{S}imilarity between a given sample and the
content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate
the potential to breach the membership privacy of the RAG database. Extensive
experiment results demonstrate that S$^2$MIA can achieve a strong inference
performance compared with five existing MIAs, and is able to escape from the
protection of three representative defenses.",Yuying Li
2024-07-01T08:35:04Z,http://arxiv.org/abs/2407.01080v2,"Face4RAG: Factual Consistency Evaluation for Retrieval Augmented
  Generation in Chinese","The prevailing issue of factual inconsistency errors in conventional
Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency
Evaluation (FCE). Despite the various FCE methods proposed earlier, these
methods are evaluated on datasets generated by specific Large Language Models
(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE
methods perform on other LLMs with different error distributions or even unseen
error types, as these methods may fail to detect the error types generated by
other LLMs. To fill this gap, in this paper, we propose the first comprehensive
FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our
benchmark consists of a synthetic dataset built upon a carefully designed
typology for factuality inconsistency error and a real-world dataset
constructed from six commonly used LLMs, enabling evaluation of FCE methods on
specific error types or real-world error distributions. On the proposed
benchmark, we discover the failure of existing FCE methods to detect the
logical fallacy, which refers to a mismatch of logic structures between the
answer and the retrieved reference. To fix this issue, we further propose a new
method called \emph{L-Face4RAG} with two novel designs of logic-preserving
answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG
substantially outperforms previous methods for factual inconsistency detection
on a wide range of tasks, notably beyond the RAG task from which it is
originally motivated. Both the benchmark and our proposed method are publicly
available.\footnote{\url{https://huggingface.co/datasets/yq27/Face4RAG}\label{link_face4rag}}",Yunqi Xu
2024-07-04T04:30:04Z,http://arxiv.org/abs/2407.03627v5,"DSLR: Document Refinement with Sentence-Level Re-ranking and
  Reconstruction to Enhance Retrieval-Augmented Generation","Recent advancements in Large Language Models (LLMs) have significantly
improved their performance across various Natural Language Processing (NLP)
tasks. However, LLMs still struggle with generating non-factual responses due
to limitations in their parametric memory. Retrieval-Augmented Generation (RAG)
systems address this issue by incorporating external knowledge with a retrieval
module. Despite their successes, however, current RAG systems face challenges
with retrieval failures and the limited ability of LLMs to filter out
irrelevant information. Therefore, in this work, we propose DSLR (Document
Refinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised
framework that decomposes retrieved documents into sentences, filters out
irrelevant sentences, and reconstructs them again into coherent passages. We
experimentally validate DSLR on multiple open-domain QA datasets and the
results demonstrate that DSLR significantly enhances the RAG performance over
conventional fixed-size passage. Furthermore, our DSLR enhances performance in
specific, yet realistic scenarios without the need for additional training,
providing an effective and efficient solution for refining retrieved documents
in RAG systems.",Taeho Hwang
2024-07-11T06:50:19Z,http://arxiv.org/abs/2407.08223v1,"Speculative RAG: Enhancing Retrieval Augmented Generation through
  Drafting","Retrieval augmented generation (RAG) combines the generative abilities of
large language models (LLMs) with external knowledge sources to provide more
accurate and up-to-date responses. Recent RAG advancements focus on improving
retrieval outcomes through iterative LLM refinement or self-critique
capabilities acquired through additional instruction tuning of LLMs. In this
work, we introduce Speculative RAG - a framework that leverages a larger
generalist LM to efficiently verify multiple RAG drafts produced in parallel by
a smaller, distilled specialist LM. Each draft is generated from a distinct
subset of retrieved documents, offering diverse perspectives on the evidence
while reducing input token counts per draft. This approach enhances
comprehension of each subset and mitigates potential position bias over long
context. Our method accelerates RAG by delegating drafting to the smaller
specialist LM, with the larger generalist LM performing a single verification
pass over the drafts. Extensive experiments demonstrate that Speculative RAG
achieves state-of-the-art performance with reduced latency on TriviaQA,
MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy
by up to 12.97% while reducing latency by 51% compared to conventional RAG
systems on PubHealth.",Zilong Wang
2024-06-25T20:23:15Z,http://arxiv.org/abs/2407.11005v1,"RAGBench: Explainable Benchmark for Retrieval-Augmented Generation
  Systems","Retrieval-Augmented Generation (RAG) has become a standard architectural
pattern for incorporating domain-specific knowledge into user-facing chat
applications powered by Large Language Models (LLMs). RAG systems are
characterized by (1) a document retriever that queries a domain-specific corpus
for context information relevant to an input query, and (2) an LLM that
generates a response based on the provided query and context. However,
comprehensive evaluation of RAG systems remains a challenge due to the lack of
unified evaluation criteria and annotated datasets. In response, we introduce
RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k
examples. It covers five unique industry-specific domains and various RAG task
types. RAGBench examples are sourced from industry corpora such as user
manuals, making it particularly relevant for industry applications. Further, we
formalize the TRACe evaluation framework: a set of explainable and actionable
RAG evaluation metrics applicable across all RAG domains. We release the
labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.
RAGBench explainable labels facilitate holistic evaluation of RAG systems,
enabling actionable feedback for continuous improvement of production
applications. Thorough extensive benchmarking, we find that LLM-based RAG
evaluation methods struggle to compete with a finetuned RoBERTa model on the
RAG evaluation task. We identify areas where existing approaches fall short and
propose the adoption of RAGBench with TRACe towards advancing the state of RAG
evaluation systems.",Robert Friel
2024-07-16T18:09:21Z,http://arxiv.org/abs/2407.12101v1,Better RAG using Relevant Information Gain,"A common way to extend the memory of large language models (LLMs) is by
retrieval augmented generation (RAG), which inserts text retrieved from a
larger memory into an LLM's context window. However, the context window is
typically limited to several thousand tokens, which limits the number of
retrieved passages that can inform a model's response. For this reason, it's
important to avoid occupying context window space with redundant information by
ensuring a degree of diversity among retrieved passages. At the same time, the
information should also be relevant to the current task. Most prior methods
that encourage diversity among retrieved results, such as Maximal Marginal
Relevance (MMR), do so by incorporating an objective that explicitly trades off
diversity and relevance. We propose a novel simple optimization metric based on
relevant information gain, a probabilistic measure of the total information
relevant to a query for a set of retrieved results. By optimizing this metric,
diversity organically emerges from our system. When used as a drop-in
replacement for the retrieval component of a RAG system, this method yields
state-of-the-art performance on question answering tasks from the Retrieval
Augmented Generation Benchmark (RGB), outperforming existing metrics that
directly optimize for relevance and diversity.",Marc Pickett
2024-07-17T07:44:18Z,http://arxiv.org/abs/2407.12888v1,"Explainable Biomedical Hypothesis Generation via Retrieval Augmented
  Generation enabled Large Language Models","The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.",Alexander R. Pelletier
2024-07-18T17:55:55Z,http://arxiv.org/abs/2407.13757v1,"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation
  of Large Language Models","Retrieval-Augmented Generation (RAG) is applied to solve hallucination
problems and real-time constraints of large language models, but it also
induces vulnerabilities against retrieval corruption attacks. Existing research
mainly explores the unreliability of RAG in white-box and closed-domain QA
tasks. In this paper, we aim to reveal the vulnerabilities of
Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks
for opinion manipulation. We explore the impact of such attacks on user
cognition and decision-making, providing new insight to enhance the reliability
and security of RAG models. We manipulate the ranking results of the retrieval
model in RAG with instruction and use these results as data to train a
surrogate model. By employing adversarial retrieval attack methods to the
surrogate model, black-box transfer attacks on RAG are further realized.
Experiments conducted on opinion datasets across multiple topics show that the
proposed attack strategy can significantly alter the opinion polarity of the
content generated by RAG. This demonstrates the model's vulnerability and, more
importantly, reveals the potential negative impact on user cognition and
decision-making, making it easier to mislead users into accepting incorrect or
biased information.",Zhuo Chen
2024-07-20T10:46:42Z,http://arxiv.org/abs/2407.14838v1,"Retrieval Augmented Generation Integrated Large Language Models in Smart
  Contract Vulnerability Detection","The rapid growth of Decentralized Finance (DeFi) has been accompanied by
substantial financial losses due to smart contract vulnerabilities,
underscoring the critical need for effective security auditing. With attacks
becoming more frequent, the necessity and demand for auditing services has
escalated. This especially creates a financial burden for independent
developers and small businesses, who often have limited available funding for
these services. Our study builds upon existing frameworks by integrating
Retrieval-Augmented Generation (RAG) with large language models (LLMs),
specifically employing GPT-4-1106 for its 128k token context window. We
construct a vector store of 830 known vulnerable contracts, leveraging Pinecone
for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and
LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a
binary answer for vulnerability detection. We first test 52 smart contracts 40
times each against a provided vulnerability type, verifying the replicability
and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7%
success rate in guided detection of vulnerabilities. Second, we challenge the
model under a ""blind"" audit setup, without the vulnerability type provided in
the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates
the general vulnerability detection capabilities without hinted context
assistance. Under these conditions, a 60.71% success rate was observed. While
the results are promising, we still emphasize the need for human auditing at
this time. We provide this study as a proof of concept for a cost-effective
smart contract auditing process, moving towards democratic access to security.",Jeffy Yu
2024-06-18T14:23:54Z,http://arxiv.org/abs/2407.16896v1,"Free to play: UN Trade and Development's experience with developing its
  own open-source Retrieval Augmented Generation Large Language Model
  application","Generative artificial intelligence (AI), and in particular Large Language
Models (LLMs), have exploded in popularity and attention since the release to
the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in
November of 2022. Due to the power of these general purpose models and their
ability to communicate in natural language, they can be useful in a range of
domains, including the work of official statistics and international
organizations. However, with such a novel and seemingly complex technology, it
can feel as if generative AI is something that happens to an organization,
something that can be talked about but not understood, that can be commented on
but not contributed to. Additionally, the costs of adoption and operation of
proprietary solutions can be both uncertain and high, a barrier for often
cost-constrained international organizations. In the face of these challenges,
United Nations Trade and Development (UNCTAD), through its Global Crisis
Response Group (GCRG), has explored and developed its own open-source Retrieval
Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more
useful for the organization's domain and work. Developing in-house solutions
comes with pros and cons, with pros including cost, flexibility, and fostering
institutional knowledge. Cons include time and skill investments and gaps and
application polish and power. The three libraries developed to produce the app,
nlp_pipeline for document processing and statistical analysis, local_rag_llm
for running a local RAG LLM, and streamlit_rag for the user interface, are
publicly available on PyPI and GitHub with Dockerfiles. A fourth library,
local_llm_finetune, is also available for fine-tuning existing LLMs which can
then be used in the application.",Daniel Hopp
2024-07-29T00:41:48Z,http://arxiv.org/abs/2407.19619v1,"Enhancing Code Translation in Language Models with Few-Shot Learning via
  Retrieval-Augmented Generation","The advent of large language models (LLMs) has significantly advanced the
field of code translation, enabling automated translation between programming
languages. However, these models often struggle with complex translation tasks
due to inadequate contextual understanding. This paper introduces a novel
approach that enhances code translation through Few-Shot Learning, augmented
with retrieval-based techniques. By leveraging a repository of existing code
translations, we dynamically retrieve the most relevant examples to guide the
model in translating new code segments. Our method, based on
Retrieval-Augmented Generation (RAG), substantially improves translation
quality by providing contextual examples from which the model can learn in
real-time. We selected RAG over traditional fine-tuning methods due to its
ability to utilize existing codebases or a locally stored corpus of code, which
allows for dynamic adaptation to diverse translation tasks without extensive
retraining. Extensive experiments on diverse datasets with open LLM models such
as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code
Instruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5
Turbo and GPT-4o, demonstrate our approach's superiority over traditional
zero-shot methods, especially in translating between Fortran and CPP. We also
explored varying numbers of shots i.e. examples provided during inference,
specifically 1, 2, and 3 shots and different embedding models for RAG,
including Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and
effectiveness of our approach.",Manish Bhattarai
2024-07-31T08:43:17Z,http://arxiv.org/abs/2407.21439v2,"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training","Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities. However, a significant drawback of MLLMs is their reliance on
static training data, leading to outdated information and limited contextual
awareness. This static nature hampers their ability to provide accurate and
up-to-date responses, particularly in dynamic or rapidly evolving contexts.
Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)
offers a promising solution, the system would inevitably encounter the
multi-granularity noisy correspondence (MNC) problem, which hinders accurate
retrieval and generation. In this work, we propose RagVL, a novel framework
with knowledge-enhanced reranking and noise-injected training, to address these
limitations. We instruction-tune the MLLM with a simple yet effective
instruction template to induce its ranking ability and serve it as a reranker
to precisely filter the top-k retrieved images. For generation, we inject
visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments on the subsets of two datasets
that require retrieving and reasoning over images to answer a given query
verify the effectiveness of our method. Code and models are available at
https://github.com/IDEA-FinAI/RagVL.",Zhanpeng Chen
2024-07-31T09:16:33Z,http://arxiv.org/abs/2407.21459v1,"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government
  Financial Data and Regulations to Enhance Decision Making","Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.",Gilang Fajar Febrian
2024-08-05T22:34:28Z,http://arxiv.org/abs/2408.02854v3,"Wiping out the limitations of Large Language Models -- A Taxonomy for
  Retrieval Augmented Generation","Current research on RAGs is distributed across various disciplines, and since
the technology is evolving very quickly, its unit of analysis is mostly on
technological innovations, rather than applications in business contexts. Thus,
in this research, we aim to create a taxonomy to conceptualize a comprehensive
overview of the constituting characteristics that define RAG applications,
facilitating the adoption of this technology in the IS community. To the best
of our knowledge, no RAG application taxonomies have been developed so far. We
describe our methodology for developing the taxonomy, which includes the
criteria for selecting papers, an explanation of our rationale for employing a
Large Language Model (LLM)-supported approach to extract and identify initial
characteristics, and a concise overview of our systematic process for
conceptualizing the taxonomy. Our systematic taxonomy development process
includes four iterative phases designed to refine and enhance our understanding
and presentation of RAG's core dimensions. We have developed a total of five
meta-dimensions and sixteen dimensions to comprehensively capture the concept
of Retrieval-Augmented Generation (RAG) applications. When discussing our
findings, we also detail the specific research areas and pose key research
questions to guide future information system researchers as they explore the
emerging topics of RAG systems.",Mahei Manhai Li
2024-08-08T03:11:12Z,http://arxiv.org/abs/2408.04187v2,"Medical Graph RAG: Towards Safe Medical Large Language Model via Graph
  Retrieval-Augmented Generation","We introduce a novel graph-based Retrieval-Augmented Generation (RAG)
framework specifically designed for the medical domain, called
\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)
capabilities for generating evidence-based medical responses, thereby improving
safety and reliability when handling private medical data. Graph-based RAG
(GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong
potential for gaining holistic insights from long-form documents. However, its
standard implementation is overly complex for general use and lacks the ability
to generate evidence-based responses, limiting its effectiveness in the medical
field. To extend the capabilities of GraphRAG to the medical domain, we propose
unique Triple Graph Construction and U-Retrieval techniques over it. In our
graph construction, we create a triple-linked structure that connects user
documents to credible medical sources and controlled vocabularies. In the
retrieval process, we propose U-Retrieval which combines Top-down Precise
Retrieval with Bottom-up Response Refinement to balance global context
awareness with precise indexing. These effort enable both source information
retrieval and comprehensive response generation. Our approach is validated on 9
medical Q\&A benchmarks, 2 health fact-checking benchmarks, and one collected
dataset testing long-form generation. The results show that MedGraphRAG
consistently outperforms state-of-the-art models across all benchmarks, while
also ensuring that responses include credible source documentation and
definitions. Our code is released at:
https://github.com/MedicineToken/Medical-Graph-RAG.",Junde Wu
2024-08-09T09:07:48Z,http://arxiv.org/abs/2408.04948v1,"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction","Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain",Bhaskarjit Sarmah
2024-08-14T15:19:16Z,http://arxiv.org/abs/2408.07611v2,"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs","Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.",Weijian Xie
2024-08-12T08:54:32Z,http://arxiv.org/abs/2408.08901v1,Bayesian inference to improve quality of Retrieval Augmented Generation,"Retrieval Augmented Generation or RAG is the most popular pattern for modern
Large Language Model or LLM applications. RAG involves taking a user query and
finding relevant paragraphs of context in a large corpus typically captured in
a vector database. Once the first level of search happens over a vector
database, the top n chunks of relevant text are included directly in the
context and sent as prompt to the LLM. Problem with this approach is that
quality of text chunks depends on effectiveness of search. There is no strong
post processing after search to determine if the chunk does hold enough
information to include in prompt. Also many times there may be chunks that have
conflicting information on the same subject and the model has no prior
experience which chunk to prioritize to make a decision. Often times, this
leads to the model providing a statement that there are conflicting statements,
and it cannot produce an answer. In this research we propose a Bayesian
approach to verify the quality of text chunks from the search results. Bayes
theorem tries to relate conditional probabilities of the hypothesis with
evidence and prior probabilities. We propose that, finding likelihood of text
chunks to give a quality answer and using prior probability of quality of text
chunks can help us improve overall quality of the responses from RAG systems.
We can use the LLM itself to get a likelihood of relevance of a context
paragraph. For prior probability of the text chunk, we use the page number in
the documents parsed. Assumption is that that paragraphs in earlier pages have
a better probability of being findings and more relevant to generalizing an
answer.",Dattaraj Rao
2024-08-15T12:20:24Z,http://arxiv.org/abs/2408.08921v2,Graph Retrieval-Augmented Generation: A Survey,"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.",Boci Peng
2024-08-19T06:05:24Z,http://arxiv.org/abs/2408.09713v2,"Carbon Footprint Accounting Driven by Large Language Models and
  Retrieval-augmented Generation","Carbon footprint accounting is crucial for quantifying greenhouse gas
emissions and achieving carbon neutrality.The dynamic nature of processes,
accounting rules, carbon-related policies, and energy supply structures
necessitates real-time updates of CFA. Traditional life cycle assessment
methods rely heavily on human expertise, making near-real-time updates
challenging. This paper introduces a novel approach integrating large language
models (LLMs) with retrieval-augmented generation technology to enhance the
real-time, professional, and economical aspects of carbon footprint information
retrieval and analysis. By leveraging LLMs' logical and language understanding
abilities and RAG's efficient retrieval capabilities, the proposed method
LLMs-RAG-CFA can retrieve more relevant professional information to assist
LLMs, enhancing the model's generative abilities. This method offers broad
professional coverage, efficient real-time carbon footprint information
acquisition and accounting, and cost-effective automation without frequent
LLMs' parameter updates. Experimental results across five industries(primary
aluminum, lithium battery, photovoltaic, new energy vehicles, and
transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional
methods and other LLMs, achieving higher information retrieval rates and
significantly lower information deviations and carbon footprint accounting
deviations. The economically viable design utilizes RAG technology to balance
real-time updates with cost-effectiveness, providing an efficient, reliable,
and cost-saving solution for real-time carbon emission management, thereby
enhancing environmental sustainability practices.",Haijin Wang
2024-08-19T18:30:18Z,http://arxiv.org/abs/2408.10343v1,"LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the
  Legal Domain","Retrieval-Augmented Generation (RAG) systems are showing promising potential,
and are becoming increasingly relevant in AI-powered legal applications.
Existing benchmarks, such as LegalBench, assess the generative capabilities of
Large Language Models (LLMs) in the legal domain, but there is a critical gap
in evaluating the retrieval component of RAG systems. To address this, we
introduce LegalBench-RAG, the first benchmark specifically designed to evaluate
the retrieval step of RAG pipelines within the legal space. LegalBench-RAG
emphasizes precise retrieval by focusing on extracting minimal, highly relevant
text segments from legal documents. These highly relevant snippets are
preferred over retrieving document IDs, or large sequences of imprecise chunks,
both of which can exceed context window limitations. Long context windows cost
more to process, induce higher latency, and lead LLMs to forget or hallucinate
information. Additionally, precise results allow LLMs to generate citations for
the end user. The LegalBench-RAG benchmark is constructed by retracing the
context used in LegalBench queries back to their original locations within the
legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus
of over 79M characters, entirely human-annotated by legal experts. We also
introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and
experimentation. By providing a dedicated benchmark for legal retrieval,
LegalBench-RAG serves as a critical tool for companies and researchers focused
on enhancing the accuracy and performance of RAG systems in the legal domain.
The LegalBench-RAG dataset is publicly available at
https://github.com/zeroentropy-cc/legalbenchrag.",Nicholas Pipitone
2024-08-21T07:20:48Z,http://arxiv.org/abs/2408.11381v2,"RAGLAB: A Modular and Research-Oriented Unified Framework for
  Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate human-level capabilities in
dialogue, reasoning, and knowledge retention. However, even the most advanced
LLMs face challenges such as hallucinations and real-time updating of their
knowledge. Current research addresses this bottleneck by equipping LLMs with
external knowledge, a technique known as Retrieval Augmented Generation (RAG).
However, two key issues constrained the development of RAG. First, there is a
growing lack of comprehensive and fair comparisons between novel RAG
algorithms. Second, open-source tools such as LlamaIndex and LangChain employ
high-level abstractions, which results in a lack of transparency and limits the
ability to develop novel algorithms and evaluation metrics. To close this gap,
we introduce RAGLAB, a modular and research-oriented open-source library.
RAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem
for investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair
comparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers
can efficiently compare the performance of various algorithms and develop novel
algorithms.",Xuanwang Zhang
2024-08-21T17:00:05Z,http://arxiv.org/abs/2408.11775v1,"Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support: For 3GPP Standards","Recent studies show that large language models (LLMs) struggle with technical
standards in telecommunications. We propose a fine-tuned retrieval-augmented
generation (RAG) system based on the Phi-2 small language model (SLM) to serve
as an oracle for communication networks. Our developed system leverages
forward-looking semantic chunking to adaptively determine parsing breakpoints
based on embedding similarity, enabling effective processing of diverse
document formats. To handle the challenge of multiple similar contexts in
technical standards, we employ a re-ranking algorithm to prioritize the most
relevant retrieved chunks. Recognizing the limitations of Phi-2's small context
window, we implement a recent technique, namely SelfExtend, to expand the
context window during inference, which not only boosts the performance but also
can accommodate a wider range of user queries and design requirements from
customers to specialized technicians. For fine-tuning, we utilize the low-rank
adaptation (LoRA) technique to enhance computational efficiency during training
and enable effective fine-tuning on small datasets. Our comprehensive
experiments demonstrate substantial improvements over existing
question-answering approaches in the telecom domain, achieving performance that
exceeds larger language models such as GPT-4 (which is about 880 times larger
in size). This work presents a novel approach to leveraging SLMs for
communication networks, offering a balance of efficiency and performance. This
work can serve as a foundation towards agentic language models for networks.",Omar Erak
2024-08-21T17:25:45Z,http://arxiv.org/abs/2408.11793v2,"Leveraging Chemistry Foundation Models to Facilitate Structure Focused
  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and
  Materials Design","Molecular property prediction and generative design via deep learning models
has been the subject of intense research given its potential to accelerate
development of new, high-performance materials. More recently, these workflows
have been significantly augmented with the advent of large language models
(LLMs) and systems of autonomous agents capable of utilizing pre-trained models
to make predictions in the context of more complex research tasks. While
effective, there is still room for substantial improvement within agentic
systems on the retrieval of salient information for material design tasks.
Within this context, alternative uses of predictive deep learning models, such
as leveraging their latent representations to facilitate cross-modal retrieval
augmented generation within agentic systems for task-specific materials design,
has remained unexplored. Herein, we demonstrate that large, pre-trained
chemistry foundation models can serve as a basis for enabling
structure-focused, semantic chemistry information retrieval for both
small-molecules, complex polymeric materials, and reactions. Additionally, we
show the use of chemistry foundation models in conjunction with multi-modal
models such as OpenCLIP facilitate unprecedented queries and information
retrieval across multiple characterization data domains. Finally, we
demonstrate the integration of these models within multi-agent systems to
facilitate structure and topological-based natural language queries and
information retrieval for different research tasks.",Nathaniel H. Park
2024-08-18T11:52:24Z,http://arxiv.org/abs/2408.13273v1,"Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach
  for Temporal Knowledge Graph Forecasting","Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.",Geethan Sannidhi
2024-08-26T09:23:35Z,http://arxiv.org/abs/2408.14523v1,Retrieval Augmented Generation for Dynamic Graph Modeling,"Dynamic graph modeling is crucial for analyzing evolving patterns in various
applications. Existing approaches often integrate graph neural networks with
temporal modules or redefine dynamic graph modeling as a generative sequence
task. However, these methods typically rely on isolated historical contexts of
the target nodes from a narrow perspective, neglecting occurrences of similar
patterns or relevant cases associated with other nodes. In this work, we
introduce the Retrieval-Augmented Generation for Dynamic Graph Modeling
(RAG4DyG) framework, which leverages guidance from contextually and temporally
analogous examples to broaden the perspective of each node. This approach
presents two critical challenges: (1) How to identify and retrieve high-quality
demonstrations that are contextually and temporally analogous to dynamic graph
samples? (2) How can these demonstrations be effectively integrated to improve
dynamic graph modeling? To address these challenges, we propose RAG4DyG, which
enriches the understanding of historical contexts by retrieving and learning
from contextually and temporally pertinent demonstrations. Specifically, we
employ a time- and context-aware contrastive learning module to identify and
retrieve relevant cases for each query sequence. Moreover, we design a graph
fusion strategy to integrate the retrieved cases, thereby augmenting the
inherent historical contexts for improved prediction. Extensive experiments on
real-world datasets across different domains demonstrate the effectiveness of
RAG4DyG for dynamic graph modeling.",Yuxia Wu
2024-09-09T13:20:31Z,http://arxiv.org/abs/2409.05591v2,"MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge
  Discovery","Retrieval-Augmented Generation (RAG) leverages retrieval tools to access
external databases, thereby enhancing the generation quality of large language
models (LLMs) through optimized context. However, the existing retrieval
methods are constrained inherently, as they can only perform relevance matching
between explicitly stated queries and well-formed knowledge, but unable to
handle tasks involving ambiguous information needs or unstructured knowledge.
Consequently, existing RAG systems are primarily effective for straightforward
question-answering tasks. In this work, we propose MemoRAG, a novel
retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG
adopts a dual-system architecture. On the one hand, it employs a light but
long-range LLM to form the global memory of database. Once a task is presented,
it generates draft answers, cluing the retrieval tools to locate useful
information within the database. On the other hand, it leverages an expensive
but expressive LLM, which generates the ultimate answer based on the retrieved
information. Building on this general framework, we further optimize MemoRAG's
performance by enhancing its cluing mechanism and memorization capacity. In our
experiment, MemoRAG achieves superior performance across a variety of
evaluation tasks, including both complex ones where conventional RAG fails and
straightforward ones where RAG is commonly applied.",Hongjin Qian
2024-09-16T09:06:44Z,http://arxiv.org/abs/2409.10102v1,Trustworthiness in Retrieval-Augmented Generation Systems: A Survey,"Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.",Yujia Zhou
2024-09-17T13:44:42Z,http://arxiv.org/abs/2409.11190v2,"SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer","We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.",Anmol Gautam
2024-09-17T15:29:34Z,http://arxiv.org/abs/2409.11279v1,"P-RAG: Progressive Retrieval Augmented Generation For Planning on
  Embodied Everyday Task","Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.",Weiye Xu
2024-09-17T23:10:04Z,http://arxiv.org/abs/2409.11598v2,"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented
  Generation","Many language models now enhance their responses with retrieval capabilities,
leading to the widespread adoption of retrieval-augmented generation (RAG)
systems. However, despite retrieval being a core component of RAG, much of the
research in this area overlooks the extensive body of work on fair ranking,
neglecting the importance of considering all stakeholders involved. This paper
presents the first systematic evaluation of RAG systems integrated with fair
rankings. We focus specifically on measuring the fair exposure of each relevant
item across the rankings utilized by RAG systems (i.e., item-side fairness),
aiming to promote equitable growth for relevant item providers. To gain a deep
understanding of the relationship between item-fairness, ranking quality, and
generation quality in the context of RAG, we analyze nine different RAG systems
that incorporate fair rankings across seven distinct datasets. Our findings
indicate that RAG systems with fair rankings can maintain a high level of
generation quality and, in many cases, even outperform traditional RAG systems,
despite the general trend of a tradeoff between ensuring fairness and
maintaining system-effectiveness. We believe our insights lay the groundwork
for responsible and equitable RAG systems and open new avenues for future
research. We publicly release our codebase and dataset at
https://github.com/kimdanny/Fair-RAG.",To Eun Kim
2024-09-19T17:52:07Z,http://arxiv.org/abs/2409.12941v2,"Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented
  Generation","Large Language Models (LLMs) have demonstrated significant performance
improvements across various cognitive tasks. An emerging application is using
LLMs to enhance retrieval-augmented generation (RAG) capabilities. These
systems require LLMs to understand user queries, retrieve relevant information,
and synthesize coherent and accurate responses. Given the increasing real-world
deployment of such systems, comprehensive evaluation becomes crucial. To this
end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),
a high-quality evaluation dataset designed to test LLMs' ability to provide
factual responses, assess retrieval capabilities, and evaluate the reasoning
required to generate final answers. While previous work has provided datasets
and benchmarks to evaluate these abilities in isolation, FRAMES offers a
unified framework that provides a clearer picture of LLM performance in
end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions
that require the integration of information from multiple sources. We present
baseline results demonstrating that even state-of-the-art LLMs struggle with
this task, achieving 0.40 accuracy with no retrieval. The accuracy is
significantly improved with our proposed multi-step retrieval pipeline,
achieving an accuracy of 0.66 (>50% improvement). We hope our work will help
bridge evaluation gaps and assist in developing more robust and capable RAG
systems.",Satyapriya Krishna
2024-09-03T03:31:37Z,http://arxiv.org/abs/2409.13694v2,"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study","Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach for mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of externeal knowledge source. However, in
real-world applications, most situations involve diverse knowledge from various
sources, yet this area has been less explored. The main dilemma is the lack of
a suitable dataset containing multiple knowledge sources and pre-exploration of
the associated issues. To address these challenges, we standardize a benchmark
dataset that combines structured and unstructured knowledge across diverse and
complementary domains. Based on this dataset, we further develop a
plug-and-play RAG framework, PruningRAG, whose main characteristic is to employ
multi-granularity pruning strategies for optimizing the integration of relevant
information and minimizing misleading context. Building upon the standardized
dataset and PruningRAG, we also report a series of experimental results, as
well as insightful findings. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of
advancing future research in the RAG community.",Shuo Yu
2024-09-23T14:51:22Z,http://arxiv.org/abs/2409.15076v1,"Enhancing Scientific Reproducibility Through Automated BioCompute Object
  Creation Using Retrieval-Augmented Generation from Publications","The exponential growth in computational power and accessibility has
transformed the complexity and scale of bioinformatics research, necessitating
standardized documentation for transparency, reproducibility, and regulatory
compliance. The IEEE BioCompute Object (BCO) standard addresses this need but
faces adoption challenges due to the overhead of creating compliant
documentation, especially for legacy research. This paper presents a novel
approach to automate the creation of BCOs from scientific papers using
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We
describe the development of the BCO assistant tool that leverages RAG to
extract relevant information from source papers and associated code
repositories, addressing key challenges such as LLM hallucination and
long-context understanding. The implementation incorporates optimized retrieval
processes, including a two-pass retrieval with re-ranking, and employs
carefully engineered prompts for each BCO domain. We discuss the tool's
architecture, extensibility, and evaluation methods, including automated and
manual assessment approaches. The BCO assistant demonstrates the potential to
significantly reduce the time and effort required for retroactive documentation
of bioinformatics research while maintaining compliance with the standard. This
approach opens avenues for AI-assisted scientific documentation and knowledge
extraction from publications thereby enhancing scientific reproducibility. The
BCO assistant tool and documentation is available at
https://biocompute-objects.github.io/bco-rag/.",Sean Kim
2024-09-23T17:56:08Z,http://arxiv.org/abs/2409.15260v1,"Generative AI Is Not Ready for Clinical Use in Patient Education for
  Lower Back Pain Patients, Even With Retrieval-Augmented Generation","Low back pain (LBP) is a leading cause of disability globally. Following the
onset of LBP and subsequent treatment, adequate patient education is crucial
for improving functionality and long-term outcomes. Despite advancements in
patient education strategies, significant gaps persist in delivering
personalized, evidence-based information to patients with LBP. Recent
advancements in large language models (LLMs) and generative artificial
intelligence (GenAI) have demonstrated the potential to enhance patient
education. However, their application and efficacy in delivering educational
content to patients with LBP remain underexplored and warrant further
investigation. In this study, we introduce a novel approach utilizing LLMs with
Retrieval-Augmented Generation (RAG) and few-shot learning to generate tailored
educational materials for patients with LBP. Physical therapists manually
evaluated our model responses for redundancy, accuracy, and completeness using
a Likert scale. In addition, the readability of the generated education
materials is assessed using the Flesch Reading Ease score. The findings
demonstrate that RAG-based LLMs outperform traditional LLMs, providing more
accurate, complete, and readable patient education materials with less
redundancy. Having said that, our analysis reveals that the generated materials
are not yet ready for use in clinical practice. This study underscores the
potential of AI-driven models utilizing RAG to improve patient education for
LBP; however, significant challenges remain in ensuring the clinical relevance
and granularity of content generated by these models.",Yi-Fei Zhao
2024-09-12T02:43:40Z,http://arxiv.org/abs/2409.17275v1,"On the Vulnerability of Applying Retrieval-Augmented Generation within
  Knowledge-Intensive Application Domains","Retrieval-Augmented Generation (RAG) has been empirically shown to enhance
the performance of large language models (LLMs) in knowledge-intensive domains
such as healthcare, finance, and legal contexts. Given a query, RAG retrieves
relevant documents from a corpus and integrates them into the LLMs' generation
process. In this study, we investigate the adversarial robustness of RAG,
focusing specifically on examining the retrieval system. First, across 225
different setup combinations of corpus, retriever, query, and targeted
information, we show that retrieval systems are vulnerable to universal
poisoning attacks in medical Q\&A. In such attacks, adversaries generate
poisoned documents containing a broad spectrum of targeted information, such as
personally identifiable information. When these poisoned documents are inserted
into a corpus, they can be accurately retrieved by any users, as long as
attacker-specified queries are used. To understand this vulnerability, we
discovered that the deviation from the query's embedding to that of the
poisoned document tends to follow a pattern in which the high similarity
between the poisoned document and the query is retained, thereby enabling
precise retrieval. Based on these findings, we develop a new detection-based
defense to ensure the safe use of RAG. Through extensive experiments spanning
various Q\&A domains, we observed that our proposed method consistently
achieves excellent detection rates in nearly all cases.",Xun Xian
2024-09-29T15:40:54Z,http://arxiv.org/abs/2409.19745v2,"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Generation with Zero Inference Overhead","Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.",Tao Tan
2024-10-01T04:20:14Z,http://arxiv.org/abs/2410.00387v1,"Boosting the Capabilities of Compact Models in Low-Data Contexts with
  Large Language Models and Retrieval-Augmented Generation","The data and compute requirements of current language modeling technology
pose challenges for the processing and analysis of low-resource languages.
Declarative linguistic knowledge has the potential to partially bridge this
data scarcity gap by providing models with useful inductive bias in the form of
language-specific rules. In this paper, we propose a retrieval augmented
generation (RAG) framework backed by a large language model (LLM) to correct
the output of a smaller model for the linguistic task of morphological
glossing. We leverage linguistic information to make up for the lack of data
and trainable parameters, while allowing for inputs from written descriptive
grammars interpreted and distilled through an LLM.
  The results demonstrate that significant leaps in performance and efficiency
are possible with the right combination of: a) linguistic inputs in the form of
grammars, b) the interpretive power of LLMs, and c) the trainability of smaller
token classification networks. We show that a compact, RAG-supported model is
highly effective in data-scarce settings, achieving a new state-of-the-art for
this task and our target languages. Our work also offers documentary linguists
a more reliable and more usable tool for morphological glossing by providing
well-reasoned explanations and confidence scores for each output.",Bhargav Shandilya
2024-10-02T01:59:07Z,http://arxiv.org/abs/2410.01171v1,"BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented
  Generation","Large language models excel at creative generation but continue to struggle
with the issues of hallucination and bias. While retrieval-augmented generation
(RAG) provides a framework for grounding LLMs' responses in accurate and
up-to-date information, it still raises the question of bias: which sources
should be selected for inclusion in the context? And how should their
importance be weighted? In this paper, we study the challenge of cross-lingual
RAG and present a dataset to investigate the robustness of existing systems at
answering queries about geopolitical disputes, which exist at the intersection
of linguistic, cultural, and political boundaries. Our dataset is sourced from
Wikipedia pages containing information relevant to the given queries and we
investigate the impact of including additional context, as well as the
composition of this context in terms of language and source, on an LLM's
response. Our results show that existing RAG systems continue to be challenged
by cross-lingual use cases and suffer from a lack of consistency when they are
provided with competing information in multiple languages. We present case
studies to illustrate these issues and outline steps for future research to
address these challenges. We make our dataset and code publicly available at
https://github.com/manestay/bordIRlines.",Bryan Li
2024-10-04T14:21:27Z,http://arxiv.org/abs/2410.03461v1,"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding
  Verification in Retrieval Augmented Generation","While retrieval augmented generation (RAG) has been shown to enhance
factuality of large language model (LLM) outputs, LLMs still suffer from
hallucination, generating incorrect or irrelevant information. One common
detection strategy involves prompting the LLM again to assess whether its
response is grounded in the retrieved evidence, but this approach is costly.
Alternatively, lightweight natural language inference (NLI) models for
efficient grounding verification can be used at inference time. While existing
pre-trained NLI models offer potential solutions, their performance remains
subpar compared to larger models on realistic RAG inputs. RAG inputs are more
complex than most datasets used for training NLI models and have
characteristics specific to the underlying knowledge base, requiring adaptation
of the NLI models to a specific target domain. Additionally, the lack of
labeled instances in the target domain makes supervised domain adaptation,
e.g., through fine-tuning, infeasible. To address these challenges, we
introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework
enables unsupervised domain adaptation through synthetic data generation.
Unlike previous methods that rely on handcrafted filtering and augmentation
strategies, Auto-GDA employs an iterative process to continuously improve the
quality of generated samples using weak labels from less efficient teacher
models and discrete optimization to select the most promising augmented
samples. Experimental results demonstrate the effectiveness of our approach,
with models fine-tuned on synthetic data using Auto-GDA often surpassing the
performance of the teacher model and reaching the performance level of LLMs at
10 % of their computational cost.",Tobias Leemann
2024-10-06T03:42:15Z,http://arxiv.org/abs/2410.04343v1,Inference Scaling for Long-Context Retrieval Augmented Generation,"The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring strategies beyond simply increasing the quantity of
knowledge. We focus on two inference scaling strategies: in-context learning
and iterative prompting. These strategies provide additional flexibility to
scale test-time computation (e.g., by increasing retrieved documents or
generation steps), thereby enhancing LLMs' ability to effectively acquire and
utilize contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",Zhenrui Yue
2024-10-10T19:14:55Z,http://arxiv.org/abs/2410.08320v1,"Do You Know What You Are Talking About? Characterizing Query-Knowledge
  Relevance For Reliable Retrieval Augmented Generation","Language models (LMs) are known to suffer from hallucinations and
misinformation. Retrieval augmented generation (RAG) that retrieves verifiable
information from an external knowledge corpus to complement the parametric
knowledge in LMs provides a tangible solution to these problems. However, the
generation quality of RAG is highly dependent on the relevance between a user's
query and the retrieved documents. Inaccurate responses may be generated when
the query is outside of the scope of knowledge represented in the external
knowledge corpus or if the information in the corpus is out-of-date. In this
work, we establish a statistical framework that assesses how well a query can
be answered by an RAG system by capturing the relevance of knowledge. We
introduce an online testing procedure that employs goodness-of-fit (GoF) tests
to inspect the relevance of each user query to detect out-of-knowledge queries
with low knowledge relevance. Additionally, we develop an offline testing
framework that examines a collection of user queries, aiming to detect
significant shifts in the query distribution which indicates the knowledge
corpus is no longer sufficiently capable of supporting the interests of the
users. We demonstrate the capabilities of these strategies through a systematic
evaluation on eight question-answering (QA) datasets, the results of which
indicate that the new testing framework is an efficient solution to enhance the
reliability of existing RAG systems.",Zhuohang Li
2024-10-11T14:03:29Z,http://arxiv.org/abs/2410.08821v1,"Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors
and hallucinated outputs generated by Large Language Models (LLMs) in
open-domain question-answering tasks (OpenQA) via introducing external
knowledge. For complex QA, however, existing RAG methods use LLMs to actively
predict retrieval timing and directly use the retrieved information for
generation, regardless of whether the retrieval timing accurately reflects the
actual information needs, or sufficiently considers prior retrieved knowledge,
which may result in insufficient information gathering and interaction,
yielding low-quality answers. To address these, we propose a generic RAG
approach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA
tasks, which includes the iterative information collector, adaptive memory
reviewer, and task-oriented generator, while following a new
Retriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an
overarching view of knowledge growth, iteratively gathering new information in
the form of notes and updating them into the existing optimal knowledge
structure, enhancing high-quality knowledge interactions. In addition, we
employ an adaptive, note-based stop-exploration strategy to decide ""what to
retrieve and when to stop"" to encourage sufficient knowledge exploration. We
conduct extensive experiments on five complex QA datasets, and the results
demonstrate the superiority and effectiveness of our method and its components.
The code and data are at https://github.com/thunlp/Adaptive-Note.",Ruobing Wang
2024-10-08T05:13:27Z,http://arxiv.org/abs/2410.09090v1,"Automating Bibliometric Analysis with Sentence Transformers and
  Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and
  Contextual Search for Customized Literature Characterization for High-Impact
  Urban Research","Bibliometric analysis is essential for understanding research trends, scope,
and impact in urban science, especially in high-impact journals, such Nature
Portfolios. However, traditional methods, relying on keyword searches and basic
NLP techniques, often fail to uncover valuable insights not explicitly stated
in article titles or keywords. These approaches are unable to perform semantic
searches and contextual understanding, limiting their effectiveness in
classifying topics and characterizing studies. In this paper, we address these
limitations by leveraging Generative AI models, specifically transformers and
Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric
analysis. We developed a technical workflow that integrates a vector database,
Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and
Large Language Models (LLMs) to enable contextual search, topic ranking, and
characterization of research using customized prompt templates. A pilot study
analyzing 223 urban science-related articles published in Nature Communications
over the past decade highlights the effectiveness of our approach in generating
insightful summary statistics on the quality, scope, and characteristics of
papers in high-impact journals. This study introduces a new paradigm for
enhancing bibliometric analysis and knowledge retrieval in urban research,
positioning an AI agent as a powerful tool for advancing research evaluation
and understanding.",Haowen Xu
2024-10-12T16:30:51Z,http://arxiv.org/abs/2410.09584v1,"Toward General Instruction-Following Alignment for Retrieval-Augmented
  Generation","Following natural instructions is crucial for the effective application of
Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in
Large Language Models (LLMs), research on assessing and improving
instruction-following (IF) alignment within the RAG domain remains limited. To
address this issue, we propose VIF-RAG, the first automated, scalable, and
verifiable synthetic pipeline for instruction-following alignment in RAG
systems. We start by manually crafting a minimal set of atomic instructions
(<100) and developing combination rules to synthesize and verify complex
instructions for a seed set. We then use supervised models for instruction
rewriting while simultaneously generating code to automate the verification of
instruction quality via a Python executor. Finally, we integrate these
instructions with extensive RAG and general data samples, scaling up to a
high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further
bridge the gap in instruction-following auto-evaluation for RAG systems, we
introduce FollowRAG Benchmark, which includes approximately 3K test samples,
covering 22 categories of general instruction constraints and four
knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG
can seamlessly integrate with different RAG benchmarks. Using FollowRAG and
eight widely-used IF and foundational abilities benchmarks for LLMs, we
demonstrate that VIF-RAG markedly enhances LLM performance across a broad range
of general instruction constraints while effectively leveraging its
capabilities in RAG scenarios. Further analysis offers practical insights for
achieving IF alignment in RAG systems. Our code and datasets are released at
https://FollowRAG.github.io.",Guanting Dong
2024-10-13T02:34:47Z,http://arxiv.org/abs/2410.09699v1,"Honest AI: Fine-Tuning ""Small"" Language Models to Say ""I Don't Know"",
  and Reducing Hallucination in RAG","Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune ""small"" language models to say ""I don't know"" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.",Xinxi Chen
2024-10-14T09:17:43Z,http://arxiv.org/abs/2410.10315v2,"EasyRAG: Efficient Retrieval-Augmented Generation Framework for
  Automated Network Operations","This paper presents EasyRAG, a simple, lightweight, and efficient
retrieval-augmented generation framework for automated network operations. Our
framework has three advantages. The first is accurate question answering. We
designed a straightforward RAG scheme based on (1) a specific data processing
workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker
for reranking (4) LLM answer generation and optimization. This approach
achieved first place in the GLM4 track in the preliminary round and second
place in the GLM4 track in the semifinals. The second is simple deployment. Our
method primarily consists of BM25 retrieval and BGE-reranker reranking,
requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy,
and highly scalable; we provide a flexible code library with various search and
generation strategies, facilitating custom process implementation. The last one
is efficient inference. We designed an efficient inference acceleration scheme
for the entire coarse ranking, reranking, and generation process that
significantly reduces the inference latency of RAG while maintaining a good
level of accuracy; each acceleration scheme can be plug-and-play into any
component of the RAG process, consistently enhancing the efficiency of the RAG
system. Our code and data are released at
\url{https://github.com/BUAADreamer/EasyRAG}.",Zhangchi Feng
2024-10-14T15:04:18Z,http://arxiv.org/abs/2410.10594v1,"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
  Documents","Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 25--39\%
end-to-end performance gain over traditional text-based RAG pipeline. Further
analysis reveals that VisRAG is effective in utilizing training data and
demonstrates strong generalization capability, positioning it as a promising
solution for RAG on multi-modality documents. Our code and data are available
at https://github.com/openbmb/visrag .",Shi Yu
2024-10-08T12:42:42Z,http://arxiv.org/abs/2410.10869v1,"Application of NotebookLM, a Large Language Model with
  Retrieval-Augmented Generation, for Lung Cancer Staging","Purpose: In radiology, large language models (LLMs), including ChatGPT, have
recently gained attention, and their utility is being rapidly evaluated.
However, concerns have emerged regarding their reliability in clinical
applications due to limitations such as hallucinations and insufficient
referencing. To address these issues, we focus on the latest technology,
retrieval-augmented generation (RAG), which enables LLMs to reference reliable
external knowledge (REK). Specifically, this study examines the utility and
reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for
staging lung cancer.
  Materials and methods: We summarized the current lung cancer staging
guideline in Japan and provided this as REK to NotebookLM. We then tasked
NotebookLM with staging 100 fictional lung cancer cases based on CT findings
and evaluated its accuracy. For comparison, we performed the same task using a
gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.
  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer
staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the
REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in
searching reference locations within the REK.
  Conclusion: NotebookLM successfully performed lung cancer staging by
utilizing the REK, demonstrating superior performance compared to GPT-4o.
Additionally, it provided highly accurate reference locations within the REK,
allowing radiologists to efficiently evaluate the reliability of NotebookLM's
responses and detect possible hallucinations. Overall, this study highlights
the potential of NotebookLM, a RAG-LLM, in image diagnosis.",Ryota Tozuka
2024-10-14T18:34:29Z,http://arxiv.org/abs/2410.11001v1,"Graph of Records: Boosting Retrieval Augmented Generation for
  Long-context Summarization with Graphs","Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose \textit{graph of
records} (\textbf{GoR}), which leverages historical responses generated by LLMs
to enhance RAG for long-context global summarization. Inspired by the
\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR further features a \textit{graph neural network} and an elaborately
designed \textit{BERTScore}-based objective for self-supervised model training,
enabling seamless supervision signal backpropagation between reference
summaries and node embeddings. We comprehensively compare GoR with 12 baselines
across four long-context summarization datasets, and the results indicate that
our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\%
improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP
dataset). Extensive experiments further demonstrate the effectiveness of GoR.
Code is available at https://github.com/ulab-uiuc/GoR",Haozhen Zhang
2024-10-16T05:20:32Z,http://arxiv.org/abs/2410.12248v1,"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for
  Retrieval-Augmented Generation with Enhanced Data Diversity","Retrieval-Augmented Generation (RAG) aims to enhance large language models
(LLMs) to generate more accurate and reliable answers with the help of the
retrieved context from external knowledge sources, thereby reducing the
incidence of hallucinations. Despite the advancements, evaluating these systems
remains a crucial research area due to the following issues: (1) Limited data
diversity: The insufficient diversity of knowledge sources and query types
constrains the applicability of RAG systems; (2) Obscure problems location:
Existing evaluation methods have difficulty in locating the stage of the RAG
pipeline where problems occur; (3) Unstable retrieval evaluation: These methods
often fail to effectively assess retrieval performance, particularly when the
chunking strategy changes. To tackle these challenges, we propose a
Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough
evaluation across the entire RAG pipeline, including chunking, retrieval,
reranking, and generation. To effectively evaluate the first three phases, we
introduce multi-granularity keywords, including coarse-grained and fine-grained
keywords, to assess the retrieved context instead of relying on the annotation
of golden chunks. Moreover, we release a holistic benchmark dataset tailored
for diverse data scenarios covering a wide range of document formats and query
types. We demonstrate the utility of the CoFE-RAG framework by conducting
experiments to evaluate each stage of RAG systems. Our evaluation method
provides unique insights into the effectiveness of RAG systems in handling
diverse data scenarios, offering a more nuanced understanding of their
capabilities and limitations.",Jintao Liu
2024-10-17T12:53:29Z,http://arxiv.org/abs/2410.13509v1,"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable
  Data Rewards","Retrieval-Augmented Generation (RAG) has proven its effectiveness in
mitigating hallucinations in Large Language Models (LLMs) by retrieving
knowledge from external resources. To adapt LLMs for RAG pipelines, current
approaches use instruction tuning to optimize LLMs, improving their ability to
utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses
on equipping LLMs to handle diverse RAG tasks using different instructions.
However, it trains RAG modules to overfit training signals and overlooks the
varying data preferences among agents within the RAG system. In this paper, we
propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG
systems by aligning data preferences between different RAG modules. DDR works
by collecting the rewards to optimize each agent with a rollout method. This
method prompts agents to sample some potential responses as perturbations,
evaluates the impact of these perturbations on the whole RAG system, and
subsequently optimizes the agent to produce outputs that improve the
performance of the RAG system. Our experiments on various knowledge-intensive
tasks demonstrate that DDR significantly outperforms the SFT method,
particularly for LLMs with smaller-scale parameters that depend more on the
retrieved knowledge. Additionally, DDR exhibits a stronger capability to align
the data preference between RAG modules. The DDR method makes generation module
more effective in extracting key information from documents and mitigating
conflicts between parametric memory and external knowledge. All codes are
available at https://github.com/OpenMatch/RAG-DDR.",Xinze Li
2024-10-17T16:18:49Z,http://arxiv.org/abs/2410.13716v1,"MIRAGE-Bench: Automatic Multilingual Benchmark Arena for
  Retrieval-Augmented Generation Systems","Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different
heuristic-based metrics for evaluation, but these require human preferences as
ground truth for reference. In contrast, arena-based benchmarks, where two
models compete each other, require an expensive Large Language Model (LLM) as a
judge for a reliable evaluation. We present an easy and efficient technique to
get the best of both worlds. The idea is to train a learning to rank model as a
""surrogate"" judge using RAG-based evaluation heuristics as input, to produce a
synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a
standardized arena-based multilingual RAG benchmark for 18 diverse languages on
Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and
extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG
extensively coupling both heuristic features and LLM as a judge evaluator. In
our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high
correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned
using heuristic features with pairwise evaluations and between GPT-4o as a
teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We
observe proprietary and large open-source LLMs currently dominate in
multilingual RAG. MIRAGE-Bench is available at:
https://github.com/vectara/mirage-bench.",Nandan Thakur
2024-10-18T22:07:36Z,http://arxiv.org/abs/2410.14881v2,"Class-RAG: Real-Time Content Moderation with Retrieval Augmented
  Generation","Robust content moderation classifiers are essential for the safety of
Generative AI systems. In this task, differences between safe and unsafe inputs
are often extremely subtle, making it difficult for classifiers (and indeed,
even humans) to properly distinguish violating vs. benign samples without
context or explanation. Scaling risk discovery and mitigation through
continuous model fine-tuning is also slow, challenging and costly, preventing
developers from being able to respond quickly and effectively to emergent
harms. We propose a Classification approach employing Retrieval-Augmented
Generation (Class-RAG). Class-RAG extends the capability of its base LLM
through access to a retrieval library which can be dynamically updated to
enable semantic hotfixing for immediate, flexible risk mitigation. Compared to
model fine-tuning, Class-RAG demonstrates flexibility and transparency in
decision-making, outperforms on classification and is more robust against
adversarial attack, as evidenced by empirical studies. Our findings also
suggest that Class-RAG performance scales with retrieval library size,
indicating that increasing the library size is a viable and low-cost approach
to improve content moderation.",Jianfa Chen
2024-10-20T03:51:01Z,http://arxiv.org/abs/2410.15267v1,"When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep
  Secret or Forget Knowledge?","The deployment of large language models (LLMs) like ChatGPT and Gemini has
shown their powerful natural language generation capabilities. However, these
models can inadvertently learn and retain sensitive information and harmful
content during training, raising significant ethical and legal concerns. To
address these issues, machine unlearning has been introduced as a potential
solution. While existing unlearning methods take into account the specific
characteristics of LLMs, they often suffer from high computational demands,
limited applicability, or the risk of catastrophic forgetting. To address these
limitations, we propose a lightweight unlearning framework based on
Retrieval-Augmented Generation (RAG) technology. By modifying the external
knowledge base of RAG, we simulate the effects of forgetting without directly
interacting with the unlearned LLM. We approach the construction of unlearned
knowledge as a constrained optimization problem, deriving two key components
that underpin the effectiveness of RAG-based unlearning. This RAG-based
approach is particularly effective for closed-source LLMs, where existing
unlearning methods often fail. We evaluate our framework through extensive
experiments on both open-source and closed-source models, including ChatGPT,
Gemini, Llama-2-7b-chat-hf, and PaLM 2. The results demonstrate that our
approach meets five key unlearning criteria: effectiveness, universality,
harmlessness, simplicity, and robustness. Meanwhile, this approach can extend
to multimodal large language models and LLM-based agents.",Shang Wang
2024-10-21T01:36:08Z,http://arxiv.org/abs/2410.15572v1,"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka
  Chatbots: Design Insights and User Perceptions","In an era where cultural preservation is increasingly intertwined with
technological innovation, this study introduces a groundbreaking approach to
promoting and safeguarding the rich heritage of Taiwanese Hakka culture through
the development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.
Traditional large language models (LLMs), while powerful, often fall short in
delivering accurate and contextually rich responses, particularly in culturally
specific domains. By integrating external databases with generative AI models,
RAG technology bridges this gap, empowering chatbots to not only provide
precise answers but also resonate deeply with the cultural nuances that are
crucial for authentic interactions. This study delves into the intricate
process of augmenting the chatbot's knowledge base with targeted cultural data,
specifically curated to reflect the unique aspects of Hakka traditions,
language, and practices. Through dynamic information retrieval, the
RAG-enhanced chatbot becomes a versatile tool capable of handling complex
inquiries that demand an in-depth understanding of Hakka cultural context. This
is particularly significant in an age where digital platforms often dilute
cultural identities, making the role of culturally aware AI systems more
critical than ever. System usability studies conducted as part of our research
reveal a marked improvement in both user satisfaction and engagement,
highlighting the chatbot's effectiveness in fostering a deeper connection with
Hakka culture. The feedback underscores the potential of RAG technology to not
only enhance user experience but also to serve as a vital instrument in the
broader mission of ethnic mainstreaming and cultural celebration.",Chen-Chi Chang
2024-10-23T11:32:46Z,http://arxiv.org/abs/2410.17783v1,"Leveraging the Domain Adaptation of Retrieval Augmented Generation
  Models for Question Answering and Reducing Hallucination","While ongoing advancements in Large Language Models have demonstrated
remarkable success across various NLP tasks, Retrieval Augmented Generation
Model stands out to be highly effective on downstream applications like
Question Answering. Recently, RAG-end2end model further optimized the
architecture and achieved notable performance improvements on domain
adaptation. However, the effectiveness of these RAG-based architectures remains
relatively unexplored when fine-tuned on specialized domains such as customer
service for building a reliable conversational AI system. Furthermore, a
critical challenge persists in reducing the occurrence of hallucinations while
maintaining high domain-specific accuracy. In this paper, we investigated the
performance of diverse RAG and RAG-like architectures through domain adaptation
and evaluated their ability to generate accurate and relevant response grounded
in the contextual knowledge base. To facilitate the evaluation of the models,
we constructed a novel dataset HotelConvQA, sourced from wide range of
hotel-related conversations and fine-tuned all the models on our domain
specific dataset. We also addressed a critical research gap on determining the
impact of domain adaptation on reducing hallucinations across different RAG
architectures, an aspect that was not properly measured in prior work. Our
evaluation shows positive results in all metrics by employing domain
adaptation, demonstrating strong performance on QA tasks and providing insights
into their efficacy in reducing hallucinations. Our findings clearly indicate
that domain adaptation not only enhances the models' performance on QA tasks
but also significantly reduces hallucination across all evaluated RAG
architectures.",Salman Rakin
2024-10-23T17:24:58Z,http://arxiv.org/abs/2410.18050v2,"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for
  Long-Context Question Answering","Long-Context Question Answering (LCQA), a challenging task, aims to reason
over long-context documents to yield accurate answers to questions. Existing
long-context Large Language Models (LLMs) for LCQA often struggle with the
""lost in the middle"" issue. Retrieval-Augmented Generation (RAG) mitigates this
issue by providing external factual evidence. However, its chunking strategy
disrupts the global long-context information, and its low-quality retrieval in
long contexts hinders LLMs from identifying effective factual details due to
substantial noise. To this end, we propose LongRAG, a general,
dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance
RAG's understanding of complex long-context knowledge (i.e., global information
and factual details). We design LongRAG as a plug-and-play paradigm,
facilitating adaptation to various domains and LLMs. Extensive experiments on
three multi-hop datasets demonstrate that LongRAG significantly outperforms
long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG
(up by 17.25%). Furthermore, we conduct quantitative ablation studies and
multi-dimensional analyses, highlighting the effectiveness of the system's
components and fine-tuning strategies. Data and code are available at
https://github.com/QingFei1/LongRAG.",Qingfei Zhao
2024-10-26T10:43:39Z,http://arxiv.org/abs/2410.20142v1,"Mask-based Membership Inference Attacks for Retrieval-Augmented
  Generation","Retrieval-Augmented Generation (RAG) has been an effective approach to
mitigate hallucinations in large language models (LLMs) by incorporating
up-to-date and domain-specific knowledge. Recently, there has been a trend of
storing up-to-date or copyrighted data in RAG knowledge databases instead of
using it for LLM training. This practice has raised concerns about Membership
Inference Attacks (MIAs), which aim to detect if a specific target document is
stored in the RAG system's knowledge database so as to protect the rights of
data producers. While research has focused on enhancing the trustworthiness of
RAG systems, existing MIAs for RAG systems remain largely insufficient.
Previous work either relies solely on the RAG system's judgment or is easily
influenced by other documents or the LLM's internal knowledge, which is
unreliable and lacks explainability. To address these limitations, we propose a
Mask-Based Membership Inference Attacks (MBA) framework. Our framework first
employs a masking algorithm that effectively masks a certain number of words in
the target document. The masked text is then used to prompt the RAG system, and
the RAG system is required to predict the mask values. If the target document
appears in the knowledge database, the masked text will retrieve the complete
target document as context, allowing for accurate mask prediction. Finally, we
adopt a simple yet effective threshold-based method to infer the membership of
target document by analyzing the accuracy of mask prediction. Our mask-based
approach is more document-specific, making the RAG system's generation less
susceptible to distractions from other documents or the LLM's internal
knowledge. Extensive experiments demonstrate the effectiveness of our approach
compared to existing baseline models.",Mingrui Liu
2024-10-27T21:12:12Z,http://arxiv.org/abs/2410.20598v2,"R^3AG: First Workshop on Refined and Reliable Retrieval Augmented
  Generation","Retrieval-augmented generation (RAG) has gained wide attention as the key
component to improve generative models with external knowledge augmentation
from information retrieval. It has shown great prominence in enhancing the
functionality and performance of large language model (LLM)-based applications.
However, with the comprehensive application of RAG, more and more problems and
limitations have been identified, thus urgently requiring further fundamental
exploration to improve current RAG frameworks. This workshop aims to explore in
depth how to conduct refined and reliable RAG for downstream AI tasks.
  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024
to call for participants to re-examine and formulate the basic principles and
practical implementation of refined and reliable RAG. The workshop serves as a
platform for both academia and industry researchers to conduct discussions,
share insights, and foster research to build the next generation of RAG
systems. Participants will engage in discussions and presentations focusing on
fundamental challenges, cutting-edge research, and potential pathways to
improve RAG. At the end of the workshop, we aim to have a clearer understanding
of how to improve the reliability and applicability of RAG with more robust
information retrieval and language generation.",Zihan Wang
2024-10-28T04:39:32Z,http://arxiv.org/abs/2410.20724v2,"Simple is Effective: The Roles of Graphs and Large Language Models in
  Knowledge-Graph-Based Retrieval-Augmented Generation","Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.",Mufei Li
2024-10-29T11:53:19Z,http://arxiv.org/abs/2410.21970v1,"Not All Languages are Equal: Insights into Multilingual
  Retrieval-Augmented Generation","RALMs (Retrieval-Augmented Language Models) broaden their knowledge scope by
incorporating external textual resources. However, the multilingual nature of
global knowledge necessitates RALMs to handle diverse languages, a topic that
has received limited research focus. In this work, we propose
\textit{Futurepedia}, a carefully crafted benchmark containing parallel texts
across eight representative languages. We evaluate six multilingual RALMs using
our benchmark to explore the challenges of multilingual RALMs. Experimental
results reveal linguistic inequalities: 1) high-resource languages stand out in
Monolingual Knowledge Extraction; 2) Indo-European languages lead RALMs to
provide answers directly from documents, alleviating the challenge of
expressing answers across languages; 3) English benefits from RALMs' selection
bias and speaks louder in multilingual knowledge selection. Based on these
findings, we offer advice for improving multilingual Retrieval Augmented
Generation. For monolingual knowledge extraction, careful attention must be
paid to cascading errors from translating low-resource languages into
high-resource ones. In cross-lingual knowledge transfer, encouraging RALMs to
provide answers within documents in different languages can improve transfer
performance. For multilingual knowledge selection, incorporating more
non-English documents and repositioning English documents can help mitigate
RALMs' selection bias. Through comprehensive experiments, we underscore the
complexities inherent in multilingual RALMs and offer valuable insights for
future research.",Suhang Wu
2024-11-01T01:40:23Z,http://arxiv.org/abs/2411.00300v1,"Rationale-Guided Retrieval Augmented Generation for Medical Question
  Answering","Large language models (LLM) hold significant potential for applications in
biomedicine, but they struggle with hallucinations and outdated knowledge.
While retrieval-augmented generation (RAG) is generally employed to address
these issues, it also has its own set of challenges: (1) LLMs are vulnerable to
irrelevant or incorrect context, (2) medical queries are often not
well-targeted for helpful information, and (3) retrievers are prone to bias
toward the specific source corpus they were trained on. In this study, we
present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the
reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key
innovations: a small filtering model trained on perplexity-based labels of
rationales, which selectively augments informative snippets of documents while
filtering out distractors; LLM-generated rationales as queries to improve the
utility of retrieved snippets; a structure designed to retrieve snippets evenly
from a comprehensive set of four biomedical corpora, effectively mitigating
retriever bias. Our experiments demonstrate that RAG$^2$ improves the
state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\%, and
it outperforms the previous best medical RAG model by up to 5.6\% across three
medical question-answering benchmarks. Our code is available at
https://github.com/dmis-lab/RAG2.",Jiwoong Sohn
2024-11-03T22:27:40Z,http://arxiv.org/abs/2411.01705v1,Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.",Yuefeng Peng
2024-11-05T09:27:21Z,http://arxiv.org/abs/2411.02937v3,"Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA
  Dataset and Self-adaptive Planning Agent","Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the ""hallucination"" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of ""dynamic""
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.",Yangning Li
2024-11-06T14:42:39Z,http://arxiv.org/abs/2411.03957v1,"Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in
  Retrieval-Augmented Generation","Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.",Yuhang Liu
2024-11-07T19:50:28Z,http://arxiv.org/abs/2411.05141v1,"Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with
  Retrieval-Augmented Generation","Current leading Text-To-Audio (TTA) generation models suffer from degraded
performance on zero-shot and few-shot settings. It is often challenging to
generate high-quality audio for audio events that are unseen or uncommon in the
training set. Inspired by the success of Retrieval-Augmented Generation (RAG)
in Large Language Model (LLM)-based knowledge-intensive tasks, we extend the
TTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,
a novel retrieval-augmented TTA approach based on Audiobox, a conditional
flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution
which generates audio conditioned on text, we augmented the conditioning input
with retrieved audio samples that provide additional acoustic information to
generate the target audio. Our retrieval method does not require the external
database to have labeled audio, offering more practical use cases. To evaluate
our proposed method, we curated test sets in zero-shot and few-shot settings.
Our empirical results show that the proposed model can effectively leverage the
retrieved audio samples and significantly improve zero-shot and few-shot TTA
performance, with large margins on multiple evaluation metrics, while
maintaining the ability to generate semantically aligned audio for the
in-domain setting. In addition, we investigate the effect of different
retrieval methods and data sources.",Mu Yang
2024-11-09T02:13:14Z,http://arxiv.org/abs/2411.06037v2,Sufficient Context: A New Lens on Retrieval Augmented Generation Systems,"Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a way to classify instances that have enough information to answer
the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the
context is sufficient, but often output incorrect answers instead of abstaining
when the context is not. On the other hand, open-source LLMs (Llama, Mistral,
Gemma) hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2-10% for Gemini, GPT,
and Gemma.",Hailey Joren
2024-11-12T13:14:09Z,http://arxiv.org/abs/2411.07773v1,Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.",Tianyu Liu
2024-11-21T13:18:03Z,http://arxiv.org/abs/2411.14110v1,"RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented
  Generation Applications with Agent-based Attacks","While large language models (LLMs) have achieved notable success in
generative tasks, they still face limitations, such as lacking up-to-date
knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)
enhances LLM performance by integrating external knowledge bases, providing
additional context which significantly improves accuracy and knowledge
coverage. However, building these external knowledge bases often requires
substantial resources and may involve sensitive information. In this paper, we
propose an agent-based automated privacy attack called RAG-Thief, which can
extract a scalable amount of private data from the private database used in RAG
applications. We conduct a systematic study on the privacy risks associated
with RAG applications, revealing that the vulnerability of LLMs makes the
private knowledge bases suffer significant privacy risks. Unlike previous
manual attacks which rely on traditional prompt injection techniques, RAG-Thief
starts with an initial adversarial query and learns from model responses,
progressively generating new queries to extract as many chunks from the
knowledge base as possible. Experimental results show that our RAG-Thief can
extract over 70% information from the private knowledge bases within customized
RAG applications deployed on local machines and real-world platforms, including
OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy
vulnerabilities in current RAG applications and underscore the pressing need
for stronger safeguards.",Changyue Jiang
2024-11-29T03:01:05Z,http://arxiv.org/abs/2411.19443v1,"Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language
  Models","Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.",Tian Yu
2024-12-03T17:23:47Z,http://arxiv.org/abs/2412.02592v1,"OCR Hinders RAG: Evaluating the Cascading Impact of OCR on
  Retrieval-Augmented Generation","Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge to reduce hallucinations and incorporate
up-to-date information without retraining. As an essential part of RAG,
external knowledge bases are commonly built by extracting structured data from
unstructured PDF documents using Optical Character Recognition (OCR). However,
given the imperfect prediction of OCR and the inherent non-uniform
representation of structured data, knowledge bases inevitably contain various
OCR noises. In this paper, we introduce OHRBench, the first benchmark for
understanding the cascading impact of OCR on RAG systems. OHRBench includes 350
carefully selected unstructured PDF documents from six real-world RAG
application domains, along with Q&As derived from multimodal elements in
documents, challenging existing OCR solutions used for RAG To better understand
OCR's impact on RAG systems, we identify two primary types of OCR noise:
Semantic Noise and Formatting Noise and apply perturbation to generate a set of
structured data with varying degrees of each OCR noise. Using OHRBench, we
first conduct a comprehensive evaluation of current OCR solutions and reveal
that none is competent for constructing high-quality knowledge bases for RAG
systems. We then systematically evaluate the impact of these two noise types
and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the
potential of employing Vision-Language Models (VLMs) without OCR in RAG
systems. Code: https://github.com/opendatalab/OHR-Bench",Junyuan Zhang
2024-12-06T16:22:32Z,http://arxiv.org/abs/2412.05159v1,"Enhancing Cross-Language Code Translation via Task-Specific Embedding
  Alignment in Retrieval-Augmented Generation","We introduce a novel method to enhance cross-language code translation from
Fortran to C++ by integrating task-specific embedding alignment into a
Retrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval
approaches that utilize generic embeddings agnostic to the downstream task, our
strategy aligns the retrieval model directly with the objective of maximizing
translation quality, as quantified by the CodeBLEU metric. This alignment
ensures that the embeddings are semantically and syntactically meaningful for
the specific code translation task. Our methodology involves constructing a
dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and
generating their corresponding C++ translations using the LLaMA 3.1-8B language
model. We compute pairwise CodeBLEU scores between the generated translations
and ground truth examples to capture fine-grained similarities. These scores
serve as supervision signals in a contrastive learning framework, where we
optimize the embedding model to retrieve Fortran-C++ pairs that are most
beneficial for improving the language model's translation performance. By
integrating these CodeBLEU-optimized embeddings into the RAG framework, our
approach significantly enhances both retrieval accuracy and code generation
quality over methods employing generic embeddings. On the HPC Fortran2C++
dataset, our method elevates the average CodeBLEU score from 0.64 to 0.73,
achieving a 14% relative improvement. On the Numerical Recipes dataset, we
observe an increase from 0.52 to 0.60, marking a 15% relative improvement.
Importantly, these gains are realized without any fine-tuning of the language
model, underscoring the efficiency and practicality of our approach.",Manish Bhattarai
2024-12-10T04:55:57Z,http://arxiv.org/abs/2412.07189v1,"When Graph Meets Retrieval Augmented Generation for Wireless Networks: A
  Tutorial and Case Study","The rapid development of next-generation networking technologies underscores
their transformative role in revolutionizing modern communication systems,
enabling faster, more reliable, and highly interconnected solutions. However,
such development has also brought challenges to network optimizations. Thanks
to the emergence of Large Language Models (LLMs) in recent years, tools
including Retrieval Augmented Generation (RAG) have been developed and applied
in various fields including networking, and have shown their effectiveness.
Taking one step further, the integration of knowledge graphs into RAG
frameworks further enhanced the performance of RAG in networking applications
such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing
more contextually relevant responses through more accurate retrieval of related
network information. This paper introduces the RAG framework that integrates
knowledge graphs in its database and explores such framework's application in
networking. We begin by exploring RAG's applications in networking and the
limitations of conventional RAG and present the advantages that knowledge
graphs' structured knowledge representation brings to the retrieval and
generation processes. Next, we propose a detailed GraphRAG-based framework for
networking, including a step-by-step tutorial on its construction. Our
evaluation through a case study on channel gain prediction demonstrates
GraphRAG's enhanced capability in generating accurate, contextually rich
responses, surpassing traditional RAG models. Finally, we discuss key future
directions for applying knowledge-graphs-empowered RAG frameworks in
networking, including robust updates, mitigation of hallucination, and enhanced
security measures for networking applications.",Yang Xiong
2024-12-10T15:56:03Z,http://arxiv.org/abs/2412.07618v2,"Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced
  Retrieval-Augmented Generation on Knowledge Graphs","Despite the superior performance of Large language models on many NLP tasks,
they still face significant limitations in memorizing extensive world
knowledge. Recent studies have demonstrated that leveraging the
Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs
that encapsulate extensive factual data in a structured format, robustly
enhances the reasoning capabilities of LLMs. However, deploying such systems in
real-world scenarios presents challenges: the continuous evolution of
non-stationary environments may lead to performance degradation and user
satisfaction requires a careful balance of performance and responsiveness. To
address these challenges, we introduce a Multi-objective Multi-Armed Bandit
enhanced RAG framework, supported by multiple retrieval methods with diverse
capabilities under rich and evolving retrieval contexts in practice. Within
this framework, each retrieval method is treated as a distinct ``arm''. The
system utilizes real-time user feedback to adapt to dynamic environments, by
selecting the appropriate retrieval method based on input queries and the
historical multi-objective performance of each arm. Extensive experiments
conducted on two benchmark KGQA datasets demonstrate that our method
significantly outperforms baseline methods in non-stationary settings while
achieving state-of-the-art performance in stationary environments. Code and
data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",Xiaqiang Tang
2024-12-16T17:32:38Z,http://arxiv.org/abs/2412.12006v2,"Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A
  Novel Weighted Retrieval-Augmented Generation Paradigm","Technical troubleshooting in enterprise environments often involves
navigating diverse, heterogeneous data sources to resolve complex issues
effectively. This paper presents a novel agentic AI solution built on a
Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise
technical troubleshooting. By dynamically weighting retrieval sources such as
product manuals, internal knowledge bases, FAQs, and troubleshooting guides
based on query context, the framework prioritizes the most relevant data. For
instance, it gives precedence to product manuals for SKU-specific queries while
incorporating general FAQs for broader issues. The system employs FAISS for
efficient dense vector search, coupled with a dynamic aggregation mechanism to
seamlessly integrate results from multiple sources. A Llama-based
self-evaluator ensures the contextual accuracy and confidence of the generated
responses before delivering them. This iterative cycle of retrieval and
validation enhances precision, diversity, and reliability in response
generation. Preliminary evaluations on large enterprise datasets demonstrate
the framework's efficacy in improving troubleshooting accuracy, reducing
resolution times, and adapting to varied technical challenges. Future research
aims to enhance the framework by integrating advanced conversational AI
capabilities, enabling more interactive and intuitive troubleshooting
experiences. Efforts will also focus on refining the dynamic weighting
mechanism through reinforcement learning to further optimize the relevance and
precision of retrieved information. By incorporating these advancements, the
proposed framework is poised to evolve into a comprehensive, autonomous AI
solution, redefining technical service workflows across enterprise settings.",Rajat Khanda
2024-12-17T05:38:27Z,http://arxiv.org/abs/2412.12559v2,"EXIT: Context-Aware Extractive Compression for Enhancing
  Retrieval-Augmented Generation","We introduce EXIT, an extractive context compression framework that enhances
both the effectiveness and efficiency of retrieval-augmented generation (RAG)
in question answering (QA). Current RAG systems often struggle when retrieval
models fail to rank the most relevant documents, leading to the inclusion of
more context at the expense of latency and accuracy. While abstractive
compression methods can drastically reduce token counts, their token-by-token
generation process significantly increases end-to-end latency. Conversely,
existing extractive methods reduce latency but rely on independent,
non-adaptive sentence selection, failing to fully utilize contextual
information. EXIT addresses these limitations by classifying sentences from
retrieved documents - while preserving their contextual dependencies - enabling
parallelizable, context-aware extraction that adapts to query complexity and
retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks
show that EXIT consistently surpasses existing compression methods and even
uncompressed baselines in QA accuracy, while also delivering substantial
reductions in inference time and token count. By improving both effectiveness
and efficiency, EXIT provides a promising direction for developing scalable,
high-quality QA solutions in RAG pipelines. Our code is available at
https://github.com/ThisIsHwang/EXIT",Taeho Hwang
2024-12-19T04:18:51Z,http://arxiv.org/abs/2412.14510v1,PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization,"The emergence of Retrieval-augmented generation (RAG) has alleviated the
issues of outdated and hallucinatory content in the generation of large
language models (LLMs), yet it still reveals numerous limitations. When a
general-purpose LLM serves as the RAG generator, it often suffers from
inadequate response informativeness, response robustness, and citation quality.
Past approaches to tackle these limitations, either by incorporating additional
steps beyond generating responses or optimizing the generator through
supervised fine-tuning (SFT), still failed to align with the RAG requirement
thoroughly. Consequently, optimizing the RAG generator from multiple preference
perspectives while maintaining its end-to-end LLM form remains a challenge. To
bridge this gap, we propose Multiple Perspective Preference Alignment for
Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator
of RAG systems to align with RAG requirements comprehensively. Specifically, we
construct high-quality instruction fine-tuning data and multi-perspective
preference data by sampling varied quality responses from the generator across
different prompt documents quality scenarios. Subsequently, we optimize the
generator using SFT and Direct Preference Optimization (DPO). Extensive
experiments conducted on four question-answer datasets across three LLMs
demonstrate that PA-RAG can significantly enhance the performance of RAG
generators. Our code and datasets are available at
https://github.com/wujwyi/PA-RAG.",Jiayi Wu
2024-12-12T01:21:03Z,http://arxiv.org/abs/2412.15235v1,"OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large
  Language Models","This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented
Generation method designed to enhance LLM-generated responses by anchoring
retrieval processes in domain-specific ontologies. While LLMs are widely used
for tasks like question answering and search, they struggle to adapt to
specialized knowledge, such as industrial workflows or knowledge work, without
expensive fine-tuning or sub-optimal retrieval methods. Existing
retrieval-augmented models, such as RAG, offer improvements but fail to account
for structured domain knowledge, leading to suboptimal context generation.
Ontologies, which conceptually organize domain knowledge by defining entities
and their interrelationships, offer a structured representation to address this
gap. OG-RAG constructs a hypergraph representation of domain documents, where
each hyperedge encapsulates clusters of factual knowledge grounded using
domain-specific ontology. An optimization algorithm then retrieves the minimal
set of hyperedges that constructs a precise, conceptually grounded context for
the LLM. This method enables efficient retrieval while preserving the complex
relationships between entities. OG-RAG applies to domains where fact-based
reasoning is essential, particularly in tasks that require workflows or
decision-making steps to follow predefined rules and procedures. These include
industrial workflows in healthcare, legal, and agricultural sectors, as well as
knowledge-driven tasks such as news journalism, investigative research,
consulting and more. Our evaluations demonstrate that OG-RAG increases the
recall of accurate facts by 55% and improves response correctness by 40% across
four different LLMs. Additionally, OG-RAG enables 30% faster attribution of
responses to context and boosts fact-based reasoning accuracy by 27% compared
to baseline methods.",Kartik Sharma
2024-12-14T06:47:56Z,http://arxiv.org/abs/2412.15246v1,Accelerating Retrieval-Augmented Generation,"An evolving solution to address hallucination and enhance accuracy in large
language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves
augmenting LLMs with information retrieved from an external knowledge source,
such as the web. This paper profiles several RAG execution pipelines and
demystifies the complex interplay between their retrieval and generation
phases. We demonstrate that while exact retrieval schemes are expensive, they
can reduce inference time compared to approximate retrieval variants because an
exact retrieval model can send a smaller but more accurate list of documents to
the generative model while maintaining the same end-to-end accuracy. This
observation motivates the acceleration of the exact nearest neighbor search for
RAG.
  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL
device that implements a scale-out near-memory acceleration architecture with a
novel cache-coherent interface between the host CPU and near-memory
accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a
512GB vector database compared with executing the search on Intel Sapphire
Rapids CPUs. This higher search performance translates to 1.7-26.3x lower
end-to-end inference time for representative RAG applications. IKS is
inherently a memory expander; its internal DRAM can be disaggregated and used
for other applications running on the server to prevent DRAM, which is the most
expensive component in today's servers, from being stranded.",Derrick Quinn
2024-12-20T03:37:07Z,http://arxiv.org/abs/2412.15529v2,"XRAG: eXamining the Core -- Benchmarking Foundational Components in
  Advanced Retrieval-Augmented Generation","Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent
data with the generative capabilities of Large Language Models (LLMs), ensuring
that the generated output is not only contextually relevant but also accurate
and current. We introduce XRAG, an open-source, modular codebase that
facilitates exhaustive evaluation of the performance of foundational components
of advanced RAG modules. These components are systematically categorized into
four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We
systematically analyse them across reconfigured datasets, providing a
comprehensive benchmark for their effectiveness. As the complexity of RAG
systems continues to escalate, we underscore the critical need to identify
potential failure points in RAG systems. We formulate a suite of experimental
methodologies and diagnostic testing protocols to dissect the failure points
inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed
at bolstering the overall performance of these modules. Our work thoroughly
evaluates the performance of advanced core components in RAG systems, providing
insights into optimizations for prevalent failure points.",Qianren Mao
2024-12-20T19:49:12Z,http://arxiv.org/abs/2412.16311v1,"HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational
  Knowledge Bases","Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as ""hybrid"" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.",Meng-Chieh Lee
2024-12-21T06:16:04Z,http://arxiv.org/abs/2412.16500v1,"Speech Retrieval-Augmented Generation without Automatic Speech
  Recognition","One common approach for question answering over speech data is to first
transcribe speech using automatic speech recognition (ASR) and then employ
text-based retrieval-augmented generation (RAG) on the transcriptions. While
this cascaded pipeline has proven effective in many practical settings, ASR
errors can propagate to the retrieval and generation steps. To overcome this
limitation, we introduce SpeechRAG, a novel framework designed for
open-question answering over spoken data. Our proposed approach fine-tunes a
pre-trained speech encoder into a speech adapter fed into a frozen large
language model (LLM)--based retrieval model. By aligning the embedding spaces
of text and speech, our speech retriever directly retrieves audio passages from
text-based queries, leveraging the retrieval capacity of the frozen text
retriever. Our retrieval experiments on spoken question answering datasets show
that direct speech retrieval does not degrade over the text-based baseline, and
outperforms the cascaded systems using ASR. For generation, we use a speech
language model (SLM) as a generator, conditioned on audio passages rather than
transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded
text-based models when there is high WER in the transcripts.",Do June Min
2024-12-22T14:16:38Z,http://arxiv.org/abs/2412.17031v1,"A Reality Check on Context Utilisation for Retrieval-Augmented
  Generation","Retrieval-augmented generation (RAG) helps address the limitations of the
parametric knowledge embedded within a language model (LM). However,
investigations of how LMs utilise retrieved information of varying complexity
in real-world scenarios have been limited to synthetic contexts. We introduce
DRUID (Dataset of Retrieved Unreliable, Insufficient and
Difficult-to-understand contexts) with real-world queries and contexts manually
annotated for stance. The dataset is based on the prototypical task of
automated claim verification, for which automated retrieval of real-world
evidence is crucial. We compare DRUID to synthetic datasets (CounterFact,
ConflictQA) and find that artificial datasets often fail to represent the
complex and diverse real-world context settings. We show that synthetic
datasets exaggerate context characteristics rare in real retrieved data, which
leads to inflated context utilisation results, as measured by our novel ACU
score. Moreover, while previous work has mainly focused on singleton context
characteristics to explain context utilisation, correlations between singleton
context properties and ACU on DRUID are surprisingly small compared to other
properties related to context source. Overall, our work underscores the need
for real-world aligned context utilisation studies to represent and improve
performance in real-world RAG settings.",Lovisa Hagström
2020-05-22T21:34:34Z,http://arxiv.org/abs/2005.11401v4,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when fine-tuned on
downstream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their
performance lags behind task-specific architectures. Additionally, providing
provenance for their decisions and updating their world knowledge remain open
research problems. Pre-trained models with a differentiable access mechanism to
explicit non-parametric memory can overcome this issue, but have so far been
only investigated for extractive downstream tasks. We explore a general-purpose
fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
combine pre-trained parametric and non-parametric memory for language
generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index
of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
formulations, one which conditions on the same retrieved passages across the
whole generated sequence, the other can use different passages per token. We
fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
tasks and set the state-of-the-art on three open domain QA tasks, outperforming
parametric seq2seq models and task-specific retrieve-and-extract architectures.
For language generation tasks, we find that RAG models generate more specific,
diverse and factual language than a state-of-the-art parametric-only seq2seq
baseline.",Patrick Lewis
2024-01-24T06:50:20Z,http://arxiv.org/abs/2401.13256v3,"UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for
  Personalized Dialogue Systems","Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.",Hongru Wang
2024-01-30T14:25:32Z,http://arxiv.org/abs/2401.17043v3,"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
  Generation of Large Language Models","Retrieval-Augmented Generation (RAG) is a technique that enhances the
capabilities of large language models (LLMs) by incorporating external
knowledge sources. This method addresses common LLM limitations, including
outdated information and the tendency to produce inaccurate ""hallucinated""
content. However, the evaluation of RAG systems is challenging, as existing
benchmarks are limited in scope and diversity. Most of the current benchmarks
predominantly assess question-answering applications, overlooking the broader
spectrum of situations where RAG could prove advantageous. Moreover, they only
evaluate the performance of the LLM component of the RAG pipeline in the
experiments, and neglect the influence of the retrieval component and the
external knowledge database. To address these issues, this paper constructs a
large-scale and more comprehensive benchmark, and evaluates all the components
of RAG systems in various RAG application scenarios. Specifically, we have
categorized the range of RAG applications into four distinct types-Create,
Read, Update, and Delete (CRUD), each representing a unique use case. ""Create""
refers to scenarios requiring the generation of original, varied content.
""Read"" involves responding to intricate questions in knowledge-intensive
situations. ""Update"" focuses on revising and rectifying inaccuracies or
inconsistencies in pre-existing texts. ""Delete"" pertains to the task of
summarizing extensive texts into more concise forms. For each of these CRUD
categories, we have developed comprehensive datasets to evaluate the
performance of RAG systems. We also analyze the effects of various components
of the RAG system, such as the retriever, the context length, the knowledge
base construction, and the LLM. Finally, we provide useful insights for
optimizing the RAG technology for different scenarios.",Yuanjie Lyu
2024-02-01T00:07:23Z,http://arxiv.org/abs/2402.00247v2,Towards AI-Assisted Synthesis of Verified Dafny Methods,"Large language models show great promise in many domains, including
programming. A promise is easy to make but hard to keep, and language models
often fail to keep their promises, generating erroneous code. A promising
avenue to keep models honest is to incorporate formal verification: generating
programs' specifications as well as code so that the code can be proved correct
with respect to the specifications. Unfortunately, existing large language
models show a severe lack of proficiency in verified programming.
  In this paper, we demonstrate how to improve two pretrained models'
proficiency in the Dafny verification-aware language. Using 178 problems from
the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to
synthesize Dafny methods. We use three different types of prompts: a direct
Contextless prompt; a Signature prompt that includes a method signature and
test cases, and a Chain of Thought (CoT) prompt that decomposes the problem
into steps and includes retrieval augmentation generated example problems and
solutions. Our results show that GPT-4 performs better than PaLM-2 on these
tasks and that both models perform best with the retrieval augmentation
generated CoT prompt. GPT-4 was able to generate verified, human-evaluated,
Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the
problems with the Contextless prompt, and even fewer (10%) for the Signature
prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP
problems, 50 that we wrote manually, and 103 synthesized by GPT-4.
  Our results demonstrate that the benefits of formal program verification are
now within reach of code generating large language models...",Md Rakib Hossain Misu
2024-01-29T06:49:53Z,http://arxiv.org/abs/2402.01733v1,"Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report","Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.",YuHe Ke
2024-02-12T14:53:28Z,http://arxiv.org/abs/2402.07688v2,"CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation
  for Evaluating LLMs in Cybersecurity Knowledge","Large Language Models (LLMs) are increasingly used across various domains,
from software development to cyber threat intelligence. Understanding all the
different fields of cybersecurity, which includes topics such as cryptography,
reverse engineering, and risk assessment, poses a challenge even for human
experts. To accurately test the general knowledge of LLMs in cybersecurity, the
research community needs a diverse, accurate, and up-to-date dataset. To
address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000,
and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets
comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing
GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents,
including NIST standards, research papers, publicly accessible books, RFCs, and
other publications in the cybersecurity domain, to generate questions, each
with four possible answers. The results underwent several rounds of error
checking and refinement. Human experts invested over 200 hours validating the
questions and solutions to ensure their accuracy and relevance, and to filter
out any questions unrelated to cybersecurity. We have evaluated and compared 25
state-of-the-art LLM models on the CyberMetric datasets. In addition to our
primary goal of evaluating LLMs, we involved 30 human participants to solve
CyberMetric-80 in a closed-book scenario. The results can serve as a reference
for comparing the general cybersecurity knowledge of humans and LLMs. The
findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct,
Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs.
Additionally, the top LLMs were more accurate than humans on CyberMetric-80,
although highly experienced human experts still outperformed small models such
as Llama-3-8B, Phi-2 or Gemma-7b.",Norbert Tihanyi
2024-02-12T18:28:36Z,http://arxiv.org/abs/2402.07867v3,"PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented
  Generation of Large Language Models","Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate these limitations. The key idea of RAG is to ground the answer
generation of an LLM on external knowledge retrieved from a knowledge database.
Existing studies mainly focus on improving the accuracy or efficiency of RAG,
leaving its security largely unexplored. We aim to bridge the gap in this work.
We find that the knowledge database in a RAG system introduces a new and
practical attack surface. Based on this attack surface, we propose PoisonedRAG,
the first knowledge corruption attack to RAG, where an attacker could inject a
few malicious texts into the knowledge database of a RAG system to induce an
LLM to generate an attacker-chosen target answer for an attacker-chosen target
question. We formulate knowledge corruption attacks as an optimization problem,
whose solution is a set of malicious texts. Depending on the background
knowledge (e.g., black-box and white-box settings) of an attacker on a RAG
system, we propose two solutions to solve the optimization problem,
respectively. Our results show PoisonedRAG could achieve a 90% attack success
rate when injecting five malicious texts for each target question into a
knowledge database with millions of texts. We also evaluate several defenses
and our results show they are insufficient to defend against PoisonedRAG,
highlighting the need for new defenses.",Wei Zou
2024-02-29T18:59:01Z,http://arxiv.org/abs/2402.19473v6,Retrieval-Augmented Generation for AI-Generated Content: A Survey,"Advancements in model algorithms, the growth of foundational models, and
access to high-quality datasets have propelled the evolution of Artificial
Intelligence Generated Content (AIGC). Despite its notable successes, AIGC
still faces hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and inference costs.
Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to
address such challenges. In particular, RAG introduces the information
retrieval process, which enhances the generation process by retrieving relevant
objects from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing efforts that
integrate RAG technique into AIGC scenarios. We first classify RAG foundations
according to how the retriever augments the generator, distilling the
fundamental abstractions of the augmentation methodologies for various
retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",Penghao Zhao
2024-06-03T02:25:33Z,http://arxiv.org/abs/2406.00083v2,"BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of
  Large Language Models","Large Language Models (LLMs) are constrained by outdated information and a
tendency to generate incorrect data, commonly referred to as ""hallucinations.""
Retrieval-Augmented Generation (RAG) addresses these limitations by combining
the strengths of retrieval-based methods and generative models. This approach
involves retrieving relevant information from a large, up-to-date dataset and
using it to enhance the generation process, leading to more accurate and
contextually appropriate responses. Despite its benefits, RAG introduces a new
attack surface for LLMs, particularly because RAG databases are often sourced
from public data, such as the web. In this paper, we propose \TrojRAG{} to
identify the vulnerabilities and attacks on retrieval parts (RAG database) and
their indirect attacks on generative parts (LLMs). Specifically, we identify
that poisoning several customized content passages could achieve a retrieval
backdoor, where the retrieval works well for clean queries but always returns
customized poisoned adversarial queries. Triggers and poisoned passages can be
highly customized to implement various attacks. For example, a trigger could be
a semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial
passages can be tailored to different contents, not only linked to the triggers
but also used to indirectly attack generative LLMs without modifying them.
These attacks can include denial-of-service attacks on RAG and semantic
steering attacks on LLM generations conditioned by the triggers. Our
experiments demonstrate that by just poisoning 10 adversarial passages can
induce 98.2\% success rate to retrieve the adversarial passages. Then, these
passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\%
or increase the rate of negative responses from 0.22\% to 72\% for targeted
queries.",Jiaqi Xue
2024-06-11T19:20:27Z,http://arxiv.org/abs/2406.10273v5,"Beyond Words: On Large Language Models Actionability in Mission-Critical
  Risk Analysis","Context. Risk analysis assesses potential risks in specific scenarios. Risk
analysis principles are context-less; the same methodology can be applied to a
risk connected to health and information technology security. Risk analysis
requires a vast knowledge of national and international regulations and
standards and is time and effort-intensive. A large language model can quickly
summarize information in less time than a human and can be fine-tuned to
specific tasks.
  Aim. Our empirical study aims to investigate the effectiveness of
Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our
knowledge, no prior study has explored its capabilities in risk analysis.
  Method. We manually curated 193 unique scenarios leading to 1283
representative samples from over 50 mission-critical analyses archived by the
industrial context team in the last five years. We compared the base GPT-3.5
and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned
counterparts. We employ two human experts as competitors of the models and
three other human experts to review the models and the former human experts'
analysis. The reviewers analyzed 5,000 scenario analyses.
  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs
are quicker and more actionable. Moreover, our findings show that RAG-assisted
LLMs have the lowest hallucination rates, effectively uncovering hidden risks
and complementing human expertise. Thus, the choice of model depends on
specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and
base models for comprehensiveness and actionability. Therefore, experts can
leverage LLMs as an effective complementing companion in risk analysis within a
condensed timeframe. They can also save costs by averting unnecessary expenses
associated with implementing unwarranted countermeasures.",Matteo Esposito
2024-07-10T17:20:59Z,http://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key
applications to enhance employee productivity. Retrieval Augmented Generation
(RAG), Large Language Models (LLMs), and orchestration frameworks like
Langchain and Llamaindex are crucial for building these chatbots. However,
creating effective enterprise chatbots is challenging and requires meticulous
RAG pipeline engineering. This includes fine-tuning embeddings and LLMs,
extracting documents from vector databases, rephrasing queries, reranking
results, designing prompts, honoring document access controls, providing
concise responses, including references, safeguarding personal information, and
building orchestration agents. We present a framework for building RAG-based
chatbots based on our experience with three NVIDIA chatbots: for IT/HR
benefits, financial earnings, and general content. Our contributions are
three-fold: introducing the FACTS framework (Freshness, Architectures, Cost,
Testing, Security), presenting fifteen RAG pipeline control points, and
providing empirical results on accuracy-latency tradeoffs between large and
small LLMs. To the best of our knowledge, this is the first paper of its kind
that provides a holistic view of the factors as well as solutions for building
secure enterprise-grade chatbots.""",Rama Akkiraju
2024-07-15T15:20:40Z,http://arxiv.org/abs/2407.10805v6,"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation","Retrieval-augmented generation (RAG) has improved large language models
(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.
However, current RAG methods often fall short of ensuring the depth and
completeness of retrieved information, which is necessary for complex reasoning
tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG
framework that iteratively retrieves information from both unstructured and
structured knowledge sources in a tight-coupling manner. Specifically, ToG-2
leverages knowledge graphs (KGs) to link documents via entities, facilitating
deep and knowledge-guided context retrieval. Simultaneously, it utilizes
documents as entity contexts to achieve precise and efficient graph retrieval.
ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate answers. We
conduct a series of well-designed experiments to highlight the following
advantages of ToG-2: 1) ToG-2 tightly couples the processes of context
retrieval and graph retrieval, deepening context retrieval via the KG while
enabling reliable graph retrieval based on contexts; 2) it achieves deep and
faithful reasoning in LLMs through an iterative knowledge retrieval process of
collaboration between contexts and the KG; and 3) ToG-2 is training-free and
plug-and-play compatible with various LLMs. Extensive experiments demonstrate
that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7
knowledge-intensive datasets with GPT-3.5, and can elevate the performance of
smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
The source code is available on https://github.com/IDEA-FinAI/ToG-2.",Shengjie Ma
2024-07-22T13:29:56Z,http://arxiv.org/abs/2407.15621v1,"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in
  Radiology Using Dynamic Retrieval Augmented Generation","Large language models (LLMs) have advanced the field of artificial
intelligence (AI) in medicine. However LLMs often generate outdated or
inaccurate information based on static training datasets. Retrieval augmented
generation (RAG) mitigates this by integrating outside data sources. While
previous RAG systems used pre-assembled, fixed databases with limited
flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end
framework that retrieves data from authoritative radiologic online sources in
real-time. RadioRAG is evaluated using a dedicated radiologic
question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of
various LLMs when answering radiology-specific questions with and without
access to additional online information via RAG. Using 80 questions from RSNA
Case Collection across radiologic subspecialties and 24 additional
expert-curated questions, for which the correct gold-standard answers were
available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B
and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved
context-specific information from www.radiopaedia.org in real-time and
incorporated them into its reply. RadioRAG consistently improved diagnostic
accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It
matched or exceeded question answering without RAG across radiologic
subspecialties, particularly in breast imaging and emergency radiology.
However, degree of improvement varied among models; GPT-3.5-turbo and
Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2
showed no improvement, highlighting variability in its effectiveness. LLMs
benefit when provided access to domain-specific data beyond their training
data. For radiology, RadioRAG establishes a robust framework that substantially
improves diagnostic accuracy and factuality in radiological question answering.",Soroosh Tayebi Arasteh
2024-08-01T17:18:17Z,http://arxiv.org/abs/2408.00727v3,"Improving Retrieval-Augmented Generation in Medicine with Iterative
  Follow-up Questions","The emergent abilities of large language models (LLMs) have demonstrated
great potential in solving medical questions. They can possess considerable
medical knowledge, but may still hallucinate and are inflexible in the
knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed
to enhance the medical question-answering capabilities of LLMs with external
knowledge bases, it may still fail in complex cases where multiple rounds of
information-seeking are required. To address such an issue, we propose
iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up
queries based on previous information-seeking attempts. In each iteration of
i-MedRAG, the follow-up queries will be answered by a conventional RAG system
and they will be further used to guide the query generation in the next
iteration. Our experiments show the improved performance of various LLMs
brought by i-MedRAG compared with conventional RAG on complex questions from
clinical vignettes in the United States Medical Licensing Examination (USMLE),
as well as various knowledge tests in the Massive Multitask Language
Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all
existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an
accuracy of 69.68% on the MedQA dataset. In addition, we characterize the
scaling properties of i-MedRAG with different iterations of follow-up queries
and different numbers of queries per iteration. Our case studies show that
i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing
an in-depth analysis of medical questions. To the best of our knowledge, this
is the first-of-its-kind study on incorporating follow-up queries into medical
RAG. The implementation of i-MedRAG is available at
https://github.com/Teddy-XiongGZ/MedRAG.",Guangzhi Xiong
2024-08-03T22:14:13Z,http://arxiv.org/abs/2408.01869v1,"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented
  Generation for Pharmacovigilance","In the era of Large Language Models (LLMs), given their remarkable text
understanding and generation abilities, there is an unprecedented opportunity
to develop new, LLM-based methods for trustworthy medical knowledge synthesis,
extraction and summarization. This paper focuses on the problem of
Pharmacovigilance (PhV), where the significance and challenges lie in
identifying Adverse Drug Events (ADEs) from diverse text sources, such as
medical literature, clinical notes, and drug labels. Unfortunately, this task
is hindered by factors including variations in the terminologies of drugs and
outcomes, and ADE descriptions often being buried in large amounts of narrative
text. We present MALADE, the first effective collaborative multi-agent system
powered by LLM with Retrieval Augmented Generation for ADE extraction from drug
label data. This technique involves augmenting a query to an LLM with relevant
information extracted from text resources, and instructing the LLM to compose a
response consistent with the augmented data. MALADE is a general LLM-agnostic
architecture, and its unique capabilities are: (1) leveraging a variety of
external sources, such as medical literature, drug labels, and FDA tools (e.g.,
OpenFDA drug information API), (2) extracting drug-outcome association in a
structured format along with the strength of the association, and (3) providing
explanations for established associations. Instantiated with GPT-4 Turbo or
GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area
Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our
implementation leverages the Langroid multi-agent LLM framework and can be
found at https://github.com/jihyechoi77/malade.",Jihye Choi
2024-08-14T13:22:14Z,http://arxiv.org/abs/2408.07542v1,"New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson
  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation","Introduction: Poor educational quality in Secondary Schools is still regarded
as one of the major struggles in 21st century Uganda - especially in rural
areas. Research identifies several problems, including low quality or absent
teacher lesson planning. As the government pushes towards the implementation of
a new curriculum, exiting lesson plans become obsolete and the problem is
worsened. Using a Retrieval Augmented Generation approach, we developed a
prototype that generates customized lesson plans based on the
government-accredited textbooks. This helps teachers create lesson plans more
efficiently and with better quality, ensuring they are fully aligned the new
curriculum and the competence-based learning approach.
  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,
and LangChain Framework - and thereafter made available on a public website.
Vector stores were trained for three new curriculum textbooks (ICT,
Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were
generated following a pseudo-random generation protocol, based on the suggested
periods in the textbooks. The lesson plans were analyzed regarding their
technical quality by three independent raters following the Lesson Plan
Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically
designed for East Africa and competence-based curriculums.
  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average
quality of between 75 and 80%, corresponding to ""very good lesson plan"". None
of the lesson plans scored below 65%, although one lesson plan could be argued
to have been missing the topic. In conclusion, the quality of the generated
lesson plans is at least comparable, if not better, than those created by
humans, as demonstrated in a study in Rwanda, whereby no lesson plan even
reached the benchmark of 50%.",Simon Kloker
2024-08-31T16:14:42Z,http://arxiv.org/abs/2409.00494v2,"GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:
  Opportunities and Challenges for Integrating Large Language Models (LLMs) and
  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems","Leveraging recent advances in generative AI, multi-agent systems are
increasingly being developed to enhance the functionality and efficiency of
smart city applications. This paper explores the transformative potential of
large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)
technologies in Intelligent Transportation Systems (ITS), paving the way for
innovative solutions to address critical challenges in urban mobility. We begin
by providing a comprehensive overview of the current state-of-the-art in
mobility data, ITS, and Connected Vehicles (CV) applications. Building on this
review, we discuss the rationale behind RAG and examine the opportunities for
integrating these Generative AI (GenAI) technologies into the smart mobility
sector. We propose a conceptual framework aimed at developing multi-agent
systems capable of intelligently and conversationally delivering smart mobility
services to urban commuters, transportation operators, and decision-makers. Our
approach seeks to foster an autonomous and intelligent approach that (a)
promotes science-based advisory to reduce traffic congestion, accidents, and
carbon emissions at multiple scales, (b) facilitates public education and
engagement in participatory mobility management, and (c) automates specialized
transportation management tasks and the development of critical ITS platforms,
such as data analytics and interpretation, knowledge representation, and
traffic simulations. By integrating LLM and RAG, our approach seeks to overcome
the limitations of traditional rule-based multi-agent systems, which rely on
fixed knowledge bases and limited reasoning capabilities. This integration
paves the way for a more scalable, intuitive, and automated multi-agent
paradigm, driving advancements in ITS and urban mobility.",Haowen Xu
2024-09-15T15:21:45Z,http://arxiv.org/abs/2409.10576v2,"Language Models and Retrieval Augmented Generation for Automated
  Structured Data Extraction from Diagnostic Reports","Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.",Mohamed Sobhi Jabal
2024-09-20T21:06:00Z,http://arxiv.org/abs/2409.13902v1,"Enhancing Large Language Models with Domain-specific Retrieval Augment
  Generation: A Case Study on Long-form Consumer Health Question Answering in
  Ophthalmology","Despite the potential of Large Language Models (LLMs) in medicine, they may
generate responses lacking supporting evidence or based on hallucinated
evidence. While Retrieval Augment Generation (RAG) is popular to address this
issue, few studies implemented and evaluated RAG in downstream domain-specific
applications. We developed a RAG pipeline with 70,000 ophthalmology-specific
documents that retrieve relevant documents to augment LLMs during inference
time. In a case study on long-form consumer health questions, we systematically
evaluated the responses including over 500 references of LLMs with and without
RAG on 100 questions with 10 healthcare professionals. The evaluation focuses
on factuality of evidence, selection and ranking of evidence, attribution of
evidence, and answer accuracy and completeness. LLMs without RAG provided 252
references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor
errors, and 20.6% were correct. In contrast, LLMs with RAG significantly
improved accuracy (54.5% being correct) and reduced error rates (18.8% with
minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents
retrieved by RAG were selected as the top references in the LLM response, with
an average ranking of 4.9. The use of RAG also improved evidence attribution
(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight
decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47
to 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited
hallucinated and erroneous evidence in the responses, raising concerns for
downstream applications in the medical domain. RAG substantially reduced the
proportion of such evidence but encountered challenges.",Aidan Gilson
2024-09-23T11:20:20Z,http://arxiv.org/abs/2409.14924v1,"Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey
  on How to Make your LLMs use External Data More Wisely","Large language models (LLMs) augmented with external data have demonstrated
remarkable capabilities in completing real-world tasks. Techniques for
integrating external data into LLMs, such as Retrieval-Augmented Generation
(RAG) and fine-tuning, are gaining increasing attention and widespread
application. Nonetheless, the effective deployment of data-augmented LLMs
across various specialized fields presents substantial challenges. These
challenges encompass a wide range of issues, from retrieving relevant data and
accurately interpreting user intent to fully harnessing the reasoning
capabilities of LLMs for complex tasks. We believe that there is no
one-size-fits-all solution for data-augmented LLM applications. In practice,
underperformance often arises from a failure to correctly identify the core
focus of a task or because the task inherently requires a blend of multiple
capabilities that must be disentangled for better resolution. In this survey,
we propose a RAG task categorization method, classifying user queries into four
levels based on the type of external data required and primary focus of the
task: explicit fact queries, implicit fact queries, interpretable rationale
queries, and hidden rationale queries. We define these levels of queries,
provide relevant datasets, and summarize the key challenges and most effective
techniques for addressing these challenges. Finally, we discuss three main
forms of integrating external data into LLMs: context, small model, and
fine-tuning, highlighting their respective strengths, limitations, and the
types of problems they are suited to solve. This work aims to help readers
thoroughly understand and decompose the data requirements and key bottlenecks
in building LLM applications, offering solutions to the different challenges
and serving as a guide to systematically developing such applications.",Siyun Zhao
2024-10-03T17:40:55Z,http://arxiv.org/abs/2410.02721v1,"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization","Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.",Ryan C. Barron
2024-10-12T10:21:00Z,http://arxiv.org/abs/2410.09472v1,"DRCap: Decoding CLAP Latents with Retrieval-augmented Generation for
  Zero-shot Audio Captioning","While automated audio captioning (AAC) has made notable progress, traditional
fully supervised AAC models still face two critical challenges: the need for
expensive audio-text pair data for training and performance degradation when
transferring across domains. To overcome these limitations, we present DRCap, a
data-efficient and flexible zero-shot audio captioning system that requires
text-only data for training and can quickly adapt to new domains without
additional fine-tuning. DRCap integrates a contrastive language-audio
pre-training (CLAP) model and a large-language model (LLM) as its backbone.
During training, the model predicts the ground-truth caption with a fixed text
encoder from CLAP, whereas, during inference, the text encoder is replaced with
the audio encoder to generate captions for audio clips in a zero-shot manner.
To mitigate the modality gap of the CLAP model, we use both the projection
strategy from the encoder side and the retrieval-augmented generation strategy
from the decoder side. Specifically, audio embeddings are first projected onto
a text embedding support to absorb extensive semantic information within the
joint multi-modal space of CLAP. At the same time, similar captions retrieved
from a datastore are fed as prompts to instruct the LLM, incorporating external
knowledge to take full advantage of its strong generative capability.
Conditioned on both the projected CLAP embedding and the retrieved similar
captions, the model is able to produce a more accurate and semantically rich
textual description. By tailoring the text embedding support and the caption
datastore to the target domain, DRCap acquires a robust ability to adapt to new
domains in a training-free manner. Experimental results demonstrate that DRCap
outperforms all other zero-shot models in in-domain scenarios and achieves
state-of-the-art performance in cross-domain scenarios.",Xiquan Li
2024-11-29T16:09:43Z,http://arxiv.org/abs/2411.19804v1,"Advanced System Integration: Analyzing OpenAPI Chunking for
  Retrieval-Augmented Generation","Integrating multiple (sub-)systems is essential to create advanced
Information Systems (ISs). Difficulties mainly arise when integrating dynamic
environments across the IS lifecycle. A traditional approach is a registry that
provides the API documentation of the systems' endpoints. Large Language Models
(LLMs) have shown to be capable of automatically creating system integrations
(e.g., as service composition) based on this documentation but require concise
input due to input token limitations, especially regarding comprehensive API
descriptions. Currently, it is unknown how best to preprocess these API
descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented
Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing,
of OpenAPIs to reduce the input token length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints and retrieves details on
demand. We evaluate RAG for endpoint discovery using the RestBench benchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval recall, precision, and F1 score. Then, we assess the
Discovery Agent using the same test set. With our prototype, we demonstrate how
to successfully employ RAG for endpoint discovery to reduce the token count.
While revealing high values for recall, precision, and F1, further research is
necessary to retrieve all requisite endpoints. Our experiments show that for
preprocessing, LLM-based and format-specific approaches outperform na\""ive
chunking methods. Relying on an agent further enhances these results as the
agent splits the tasks into multiple fine granular subtasks, improving the
overall RAG performance in the token count, precision, and F1 score.",Robin D. Pesl
2024-12-18T20:18:03Z,http://arxiv.org/abs/2412.14304v1,"Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing
  LLM Ophthalmological QA in LMICs","Current ophthalmology clinical workflows are plagued by over-referrals, long
waits, and complex and heterogeneous medical records. Large language models
(LLMs) present a promising solution to automate various procedures such as
triaging, preliminary tests like visual acuity assessment, and report
summaries. However, LLMs have demonstrated significantly varied performance
across different languages in natural language question-answering tasks,
potentially exacerbating healthcare disparities in Low and Middle-Income
Countries (LMICs). This study introduces the first multilingual
ophthalmological question-answering benchmark with manually curated questions
parallel across languages, allowing for direct cross-lingual comparisons. Our
evaluation of 6 popular LLMs across 7 different languages reveals substantial
bias across different languages, highlighting risks for clinical deployment of
LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought
or Retrieval-augmented generation (RAG) by themselves fall short of closing
this performance gap, often failing to improve performance across all languages
and lacking specificity for the medical domain. To address this issue, We
propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time
de-biasing method leveraging retrieval augmented generation and
self-verification. Our approach not only improves performance across all
languages but also significantly reduces the multilingual bias gap,
facilitating equitable LLM application across the globe.",David Restrepo
2021-09-08T16:15:50Z,http://arxiv.org/abs/2109.03754v2,"Memory and Knowledge Augmented Language Models for Inferring Salience in
  Long-Form Stories","Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.",David Wilmot
2021-09-15T12:58:51Z,http://arxiv.org/abs/2109.07263v2,End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs,"We propose a novel problem within end-to-end learning of task-oriented
dialogs (TOD), in which the dialog system mimics a troubleshooting agent who
helps a user by diagnosing their problem (e.g., car not starting). Such dialogs
are grounded in domain-specific flowcharts, which the agent is supposed to
follow during the conversation. Our task exposes novel technical challenges for
neural TOD, such as grounding an utterance to the flowchart without explicit
annotation, referring to additional manual pages when user asks a clarification
question, and ability to follow unseen flowcharts at test time. We release a
dataset (FloDial) consisting of 2,738 dialogs grounded on 12 different
troubleshooting flowcharts. We also design a neural model, FloNet, which uses a
retrieval-augmented generation architecture to train the dialog agent. Our
experiments find that FloNet can do zero-shot transfer to unseen flowcharts,
and sets a strong baseline for future research.",Dinesh Raghu
2021-12-16T08:18:47Z,http://arxiv.org/abs/2112.08688v2,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,"Retrieval-augmented generation models have shown state-of-the-art performance
across many knowledge-intensive NLP tasks such as open question answering and
fact verification. These models are trained to generate the final output given
the retrieved passages, which can be irrelevant to the original query, leading
to learning spurious cues or answer memorization. This work introduces a method
to incorporate the evidentiality of passages -- whether a passage contains
correct evidence to support the output -- into training the generator. We
introduce a multi-task learning framework to jointly generate the final output
and predict the evidentiality of each passage, leveraging a new task-agnostic
method to obtain silver evidentiality labels for supervision. Our experiments
on five datasets across three knowledge-intensive tasks show that our new
evidentiality-guided generator significantly outperforms its direct counterpart
with the same-size model and advances the state of the art on FaVIQ-Ambig. We
attribute these improvements to both the auxiliary multi-task learning and
silver evidentiality mining techniques.",Akari Asai
2022-03-30T23:30:16Z,http://arxiv.org/abs/2203.16714v1,End-to-End Table Question Answering via Retrieval-Augmented Generation,"Most existing end-to-end Table Question Answering (Table QA) models consist
of a two-stage framework with a retriever to select relevant table candidates
from a corpus and a reader to locate the correct answers from table candidates.
Even though the accuracy of the reader models is significantly improved with
the recent transformer-based approaches, the overall performance of such
frameworks still suffers from the poor accuracy of using traditional
information retrieval techniques as retrievers. To alleviate this problem, we
introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense
vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence
model to generate answer tokens. Given any natural language question, T-RAG
utilizes a unified pipeline to automatically search through a table corpus to
directly locate the correct answer from the table cells. We apply T-RAG to
recent open-domain Table QA benchmarks and demonstrate that the fine-tuned
T-RAG model is able to achieve state-of-the-art performance in both the
end-to-end Table QA and the table retrieval tasks.",Feifei Pan
2022-04-08T10:36:21Z,http://arxiv.org/abs/2204.03985v2,KGI: An Integrated Framework for Knowledge Intensive Language Tasks,"In this paper, we present a system to showcase the capabilities of the latest
state-of-the-art retrieval augmented generation models trained on
knowledge-intensive language tasks, such as slot filling, open domain question
answering, dialogue, and fact-checking. Moreover, given a user query, we show
how the output from these different models can be combined to cross-examine the
outputs of each other. Particularly, we show how accuracy in dialogue can be
improved using the question answering model. We are also releasing all models
used in the demo as a contribution of this paper. A short video demonstrating
the system is available at https://ibm.box.com/v/emnlp2022-demo.",Md Faisal Mahbub Chowdhury
2022-07-07T00:57:02Z,http://arxiv.org/abs/2207.03030v1,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,"This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.",Sebastian Hofstätter
2022-09-28T17:54:55Z,http://arxiv.org/abs/2209.14290v1,FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation,"Retrieval-augmented generation models offer many benefits over standalone
language models: besides a textual answer to a given query they provide
provenance items retrieved from an updateable knowledge base. However, they are
also more complex systems and need to handle long inputs. In this work, we
introduce FiD-Light to strongly increase the efficiency of the state-of-the-art
retrieval-augmented FiD model, while maintaining the same level of
effectiveness. Our FiD-Light model constrains the information flow from the
encoder (which encodes passages separately) to the decoder (using concatenated
encoded representations). Furthermore, we adapt FiD-Light with re-ranking
capabilities through textual source pointers, to improve the top-ranked
provenance precision. Our experiments on a diverse set of seven knowledge
intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier
between query latency and effectiveness. FiD-Light with source pointing sets
substantial new state-of-the-art results on six KILT tasks for combined text
generation and provenance retrieval evaluation, while maintaining reasonable
efficiency.",Sebastian Hofstätter
2022-10-10T17:45:38Z,http://arxiv.org/abs/2210.04873v2,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",Tanay Dixit
2023-05-02T15:33:01Z,http://arxiv.org/abs/2305.01526v1,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset","In this paper, we release a largest ever medical Question Answering (QA)
dataset with 26 million QA pairs. We benchmark many existing approaches in our
dataset in terms of both retrieval and generation. Experimental results show
that the existing models perform far lower than expected and the released
dataset is still challenging in the pre-trained language model era. Moreover,
we also experimentally show the benefit of the proposed dataset in many
aspects: (i) trained models for other QA datasets in a zero-shot fashion; and
(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)
improving existing pre-trained language models by using the QA pairs as a
pre-training corpus in continued training manner. We believe that this dataset
will not only contribute to medical research but also facilitate both the
patients and clinical doctors. See
\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",Jianquan Li
2023-05-05T16:28:03Z,http://arxiv.org/abs/2305.03660v1,"Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT
  models","We propose Retrieval Augmented Generation (RAG) as an approach for automated
radiology report writing that leverages multimodally aligned embeddings from a
contrastively pretrained vision language model for retrieval of relevant
candidate radiology text for an input radiology image and a general domain
generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for
report generation using the relevant radiology text retrieved. This approach
keeps hallucinated generations under check and provides capabilities to
generate report content in the format we desire leveraging the instruction
following capabilities of these generative models. Our approach achieves better
clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score
of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different
clinical settings as it allows to augment the automated radiology report
generation process with content relevant for that setting while also having the
ability to inject user intents and requirements in the prompts as part of the
report generation process to modulate the content and format of the generated
reports as applicable for that clinical setting.",Mercy Ranjit
2023-05-10T22:36:27Z,http://arxiv.org/abs/2305.06488v4,A Platform for the Biomedical Application of Large Language Models,"Current-generation Large Language Models (LLMs) have stirred enormous
interest in recent months, yielding great potential for accessibility and
automation, while simultaneously posing significant challenges and risk of
misuse. To facilitate interfacing with LLMs in the biomedical space, while at
the same time safeguarding their functionalities through sensible constraints,
we propose a dedicated, open-source framework: BioChatter. Based on open-source
software packages, we synergise the many functionalities that are currently
developing around LLMs, such as knowledge integration / retrieval-augmented
generation, model chaining, and benchmarking, resulting in an easy-to-use and
inclusive framework for application in many use cases of biomedicine. We focus
on robust and user-friendly implementation, including ways to deploy
privacy-preserving local open-source LLMs. We demonstrate use cases via two
multi-purpose web apps (https://chat.biocypher.org), and provide documentation,
support, and an open community.",Sebastian Lobentanzer
2023-05-24T16:57:04Z,http://arxiv.org/abs/2305.15344v1,"Learning Answer Generation using Supervision from Automatic Question
  Answering Evaluators","Recent studies show that sentence-level extractive QA, i.e., based on Answer
Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA)
models, which generate answers using the top-k answer sentences ranked by AS2
models (a la retrieval-augmented generation style). In this paper, we propose a
novel training paradigm for GenQA using supervision from automatic QA
evaluation models (GAVA). Specifically, we propose three strategies to transfer
knowledge from these QA evaluation models to a GenQA model: (i) augmenting
training data with answers generated by the GenQA model and labelled by GAVA
(either statically, before training, or (ii) dynamically, at every training
epoch); and (iii) using the GAVA score for weighting the generator loss during
the learning of the GenQA model. We evaluate our proposed methods on two
academic and one industrial dataset, obtaining a significant improvement in
answering accuracy over the previous state of the art.",Matteo Gabburo
2023-05-30T08:36:45Z,http://arxiv.org/abs/2305.18846v1,"Knowledge Graph-Augmented Language Models for Knowledge-Grounded
  Dialogue Generation","Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",Minki Kang
2023-06-12T03:54:04Z,http://arxiv.org/abs/2306.06851v2,"UniPoll: A Unified Social Media Poll Generation Framework via
  Multi-Objective Optimization","Social media platforms are vital for expressing opinions and understanding
public sentiment, yet many analytical tools overlook passive users who mainly
consume content without engaging actively. To address this, we introduce
UniPoll, an advanced framework designed to automatically generate polls from
social media posts using sophisticated natural language generation (NLG)
techniques. Unlike traditional methods that struggle with social media's
informal and context-sensitive nature, UniPoll leverages enriched contexts from
user comments and employs multi-objective optimization to enhance poll
relevance and engagement. To tackle the inherently noisy nature of social media
data, UniPoll incorporates Retrieval-Augmented Generation (RAG) and synthetic
data generation, ensuring robust performance across real-world scenarios. The
framework surpasses existing models, including T5, ChatGLM3, and GPT-3.5, in
generating coherent and contextually appropriate question-answer pairs.
Evaluated on the Chinese WeiboPolls dataset and the newly introduced English
RedditPolls dataset, UniPoll demonstrates superior cross-lingual and
cross-platform capabilities, making it a potent tool to boost user engagement
and create a more inclusive environment for interaction.",Yixia Li
2023-07-07T02:42:06Z,http://arxiv.org/abs/2307.04642v2,"TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal
  Prediction","When applied to open-domain question answering, large language models (LLMs)
frequently generate incorrect responses based on made-up facts, which are
called $\textit{hallucinations}$. Retrieval augmented generation (RAG) is a
promising strategy to avoid hallucinations, but it does not provide guarantees
on its correctness. To address this challenge, we propose the Trustworthy
Retrieval Augmented Question Answering, or $\textit{TRAQ}$, which provides the
first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal
prediction, a statistical technique for constructing prediction sets that are
guaranteed to contain the semantically correct response with high probability.
Additionally, TRAQ leverages Bayesian optimization to minimize the size of the
constructed sets. In an extensive experimental evaluation, we demonstrate that
TRAQ provides the desired correctness guarantee while reducing prediction set
size by 16.2% on average compared to an ablation. The implementation is
available at $\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.",Shuo Li
2023-08-24T05:26:54Z,http://arxiv.org/abs/2308.12574v2,"Modeling Uncertainty and Using Post-fusion as Fallback Improves
  Retrieval Augmented Generation with LLMs","The integration of retrieved passages and large language models (LLMs), such
as ChatGPTs, has significantly contributed to improving open-domain question
answering. However, there is still a lack of exploration regarding the optimal
approach for incorporating retrieved passages into the answer generation
process. This paper aims to fill this gap by investigating different methods of
combining retrieved passages with LLMs to enhance answer generation. We begin
by examining the limitations of a commonly-used concatenation approach.
Surprisingly, this approach often results in generating ""unknown"" outputs, even
when the correct document is among the top-k retrieved passages. To address
this issue, we explore four alternative strategies for integrating the
retrieved passages with the LLMs. These strategies include two single-round
methods that utilize chain-of-thought reasoning and two multi-round strategies
that incorporate feedback loops. Through comprehensive analyses and
experiments, we provide insightful observations on how to effectively leverage
retrieved passages to enhance the answer generation capability of LLMs.",Ye Liu
2023-09-27T06:33:29Z,http://arxiv.org/abs/2309.15427v2,Graph Neural Prompting with Large Language Models,"Large language models (LLMs) have shown remarkable generalization capability
with exceptional performance in various language modeling tasks. However, they
still exhibit inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored utilizing knowledge graphs
(KGs) to enhance language modeling via joint training and customized model
architectures, applying this to LLMs is problematic owing to their large number
of parameters and high computational cost. Therefore, how to enhance
pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented
generation, remains an open question. In this work, we propose Graph Neural
Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in
learning beneficial knowledge from KGs. GNP encompasses various designs,
including a standard graph neural network encoder, a cross-modality pooling
module, a domain projector, and a self-supervised link prediction objective.
Extensive experiments on multiple datasets demonstrate the superiority of GNP
on both commonsense and biomedical reasoning tasks across different LLM sizes
and settings. Code is available at https://github.com/meettyj/GNP.",Yijun Tian
2023-09-28T05:19:06Z,http://arxiv.org/abs/2310.01427v1,Attention Sorting Combats Recency Bias In Long Context Language Models,"Current language models often fail to incorporate long contexts efficiently
during generation. We show that a major contributor to this issue are attention
priors that are likely learned during pre-training: relevant information
located earlier in context is attended to less on average. Yet even when models
fail to use the information from a relevant document in their response, they
still pay preferential attention to that document compared to an irrelevant
document at the same position. We leverage this fact to introduce ``attention
sorting'': perform one step of decoding, sort documents by the attention they
receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves
performance of long context models. Our findings highlight some challenges in
using off-the-shelf language models for retrieval augmented generation.",Alexander Peysakhovich
2023-10-11T15:19:50Z,http://arxiv.org/abs/2310.07581v2,"Qlarify: Recursively Expandable Abstracts for Directed Information
  Retrieval over Scientific Papers","Navigating the vast scientific literature often starts with browsing a
paper's abstract. However, when a reader seeks additional information, not
present in the abstract, they face a costly cognitive chasm during their dive
into the full text. To bridge this gap, we introduce recursively expandable
abstracts, a novel interaction paradigm that dynamically expands abstracts by
progressively incorporating additional information from the papers' full text.
This lightweight interaction allows scholars to specify their information needs
by quickly brushing over the abstract or selecting AI-suggested expandable
entities. Relevant information is synthesized using a retrieval-augmented
generation approach, presented as a fluid, threaded expansion of the abstract,
and made efficiently verifiable via attribution to relevant source-passages in
the paper. Through a series of user studies, we demonstrate the utility of
recursively expandable abstracts and identify future opportunities to support
low-effort and just-in-time exploration of long-form information contexts
through LLM-powered interactions.",Raymond Fok
2023-10-16T18:45:38Z,http://arxiv.org/abs/2310.10760v1,"Towards reducing hallucination in extracting information from financial
  reports using Large Language Models","For a financial analyst, the question and answer (Q\&A) segment of the
company financial report is a crucial piece of information for various analysis
and investment decisions. However, extracting valuable insights from the Q\&A
section has posed considerable challenges as the conventional methods such as
detailed reading and note-taking lack scalability and are susceptible to human
errors, and Optical Character Recognition (OCR) and similar techniques
encounter difficulties in accurately processing unstructured transcript text,
often missing subtle linguistic nuances that drive investor decisions. Here, we
demonstrate the utilization of Large Language Models (LLMs) to efficiently and
rapidly extract information from earnings report transcripts while ensuring
high accuracy transforming the extraction process as well as reducing
hallucination by combining retrieval-augmented generation technique as well as
metadata. We evaluate the outcomes of various LLMs with and without using our
proposed approach based on various objective metrics for evaluating Q\&A
systems, and empirically demonstrate superiority of our method.",Bhaskarjit Sarmah
2023-10-23T11:33:41Z,http://arxiv.org/abs/2310.15205v2,"DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple
  Experts Fine-tuning","We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.",Wei Chen
2023-10-22T14:45:14Z,http://arxiv.org/abs/2310.18344v1,Chainpoll: A high efficacy method for LLM hallucination detection,"Large language models (LLMs) have experienced notable advancements in
generating coherent and contextually relevant responses. However,
hallucinations - incorrect or unfounded claims - are still prevalent, prompting
the creation of automated metrics to detect these in LLM outputs. Our
contributions include: introducing ChainPoll, an innovative hallucination
detection method that excels compared to its counterparts, and unveiling
RealHall, a refined collection of benchmark datasets to assess hallucination
detection metrics from recent studies. While creating RealHall, we assessed
tasks and datasets from previous hallucination detection studies and observed
that many are not suitable for the potent LLMs currently in use. Overcoming
this, we opted for four datasets challenging for modern LLMs and pertinent to
real-world scenarios. Using RealHall, we conducted a comprehensive comparison
of ChainPoll with numerous hallucination metrics from recent studies. Our
findings indicate that ChainPoll outperforms in all RealHall benchmarks,
achieving an overall AUROC of 0.781. This surpasses the next best theoretical
method by 11% and exceeds industry standards by over 23%. Additionally,
ChainPoll is cost-effective and offers greater transparency than other metrics.
We introduce two novel metrics to assess LLM hallucinations: Adherence and
Correctness. Adherence is relevant to Retrieval Augmented Generation workflows,
evaluating an LLM's analytical capabilities within given documents and
contexts. In contrast, Correctness identifies logical and reasoning errors.",Robert Friel
2023-11-07T19:27:28Z,http://arxiv.org/abs/2311.04310v1,"KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI
  Integration using Retrieval-Augmented Generation","Academic researchers face challenges keeping up with exponentially growing
published findings in their field. Performing comprehensive literature reviews
to synthesize knowledge is time-consuming and labor-intensive using manual
approaches. Recent advances in artificial intelligence provide promising
solutions, yet many require coding expertise, limiting accessibility.
KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the
KNIME visual programming platform to automate literature review tasks for users
with no coding experience. By leveraging KNIME's intuitive graphical interface,
researchers can create workflows to search their Zotero libraries and utilize
OpenAI models to extract key information without coding. Users simply provide
API keys and configure settings through a user-friendly interface in a locally
stored copy of the workflow. KNIMEZoBot then allows asking natural language
questions via a chatbot and retrieves relevant passages from papers to generate
synthesized answers. This system has significant potential to expedite
literature reviews for researchers unfamiliar with coding by automating
retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot
demonstrates how thoughtfully designed AI tools can expand accessibility and
accelerate knowledge building across diverse research domains.",Suad Alshammari
2023-11-09T06:45:04Z,http://arxiv.org/abs/2311.05169v1,"Large Language Models and Prompt Engineering for Biomedical Query
  Focused Multi-Document Summarisation","This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",Diego Mollá
2023-11-09T10:40:04Z,http://arxiv.org/abs/2311.05261v1,RAGLog: Log Anomaly Detection using Retrieval Augmented Generation,"The ability to detect log anomalies from system logs is a vital activity
needed to ensure cyber resiliency of systems. It is applied for fault
identification or facilitate cyber investigation and digital forensics.
However, as logs belonging to different systems and components differ
significantly, the challenge to perform such analysis is humanly challenging
from the volume, variety and velocity of logs. This is further complicated by
the lack or unavailability of anomalous log entries to develop trained machine
learning or artificial intelligence models for such purposes. In this research
work, we explore the use of a Retrieval Augmented Large Language Model that
leverages a vector database to detect anomalies from logs. We used a Question
and Answer configuration pipeline. To the best of our knowledge, our experiment
which we called RAGLog is a novel one and the experimental results show much
promise.",Jonathan Pan
2023-11-10T15:10:36Z,http://arxiv.org/abs/2311.06102v1,"Making LLMs Worth Every Penny: Resource-Limited Text Classification in
  Banking","Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",Lefteris Loukas
2023-11-14T18:41:54Z,http://arxiv.org/abs/2311.08377v1,Learning to Filter Context for Retrieval-Augmented Generation,"On-the-fly retrieval of relevant knowledge has proven an essential element of
reliable systems for tasks such as open-domain question answering and fact
verification. However, because retrieval systems are not perfect, generation
models are required to generate outputs given partially or entirely irrelevant
passages. This can cause over- or under-reliance on context, and result in
problems in the generated output such as hallucinations. To alleviate these
problems, we propose FILCO, a method that improves the quality of the context
provided to the generator by (1) identifying useful context based on lexical
and information-theoretic approaches, and (2) training context filtering models
that can filter retrieved contexts at test time. We experiment on six
knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our
method outperforms existing approaches on extractive question answering (QA),
complex multi-hop and long-form QA, fact verification, and dialog generation
tasks. FILCO effectively improves the quality of context, whether or not it
supports the canonical output.",Zhiruo Wang
2023-11-15T17:04:56Z,http://arxiv.org/abs/2311.09114v2,"Ever: Mitigating Hallucination in Large Language Models through
  Real-Time Verification and Rectification","Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating fluent text. However, they often encounter the challenge of
generating inaccurate or hallucinated content. This issue is common in both
non-retrieval-based generation and retrieval-augmented generation approaches,
and existing post-hoc rectification methods may not address the accumulated
hallucination errors that may be caused by the ""snowballing"" issue, especially
in reasoning tasks. To tackle these challenges, we introduce a novel approach
called Real-time Verification and Rectification (Ever). Instead of waiting
until the end of the generation process to rectify hallucinations, Ever employs
a real-time, step-wise generation and hallucination rectification strategy. The
primary objective is to detect and rectify hallucinations as they occur during
the text generation process. When compared to both retrieval-based and
non-retrieval-based baselines, Ever demonstrates a significant improvement in
generating trustworthy and factually accurate text across a diverse range of
tasks, including short-form QA, biography generation, and multi-hop reasoning.",Haoqiang Kang
2023-11-21T19:41:46Z,http://arxiv.org/abs/2311.12955v1,"Don't forget private retrieval: distributed private similarity search
  for large language models","While the flexible capabilities of large language models (LLMs) allow them to
answer a range of queries based on existing learned knowledge, information
retrieval to augment generation is an important tool to allow LLMs to answer
questions on information not included in pre-training data. Such private
information is increasingly being generated in a wide array of distributed
contexts by organizations and individuals. Performing such information
retrieval using neural embeddings of queries and documents always leaked
information about queries and database content unless both were stored locally.
We present Private Retrieval Augmented Generation (PRAG), an approach that uses
multi-party computation (MPC) to securely transmit queries to a distributed set
of servers containing a privately constructed database to return top-k and
approximate top-k documents. This is a first-of-its-kind approach to dense
information retrieval that ensures no server observes a client's query or can
see the database content. The approach introduces a novel MPC friendly protocol
for inverted file approximate search (IVF) that allows for fast document search
over distributed and private data in sublinear communication complexity. This
work presents new avenues through which data for use in LLMs can be accessed
and used without needing to centralize or forgo privacy.",Guy Zyskind
2023-11-23T09:58:39Z,http://arxiv.org/abs/2311.13878v1,"Minimizing Factual Inconsistency and Hallucination in Large Language
  Models","Large Language Models (LLMs) are widely used in critical fields such as
healthcare, education, and finance due to their remarkable proficiency in
various language-related tasks. However, LLMs are prone to generating factually
incorrect responses or ""hallucinations,"" which can lead to a loss of
credibility and trust among users. To address this issue, we propose a
multi-stage framework that generates the rationale first, verifies and refines
incorrect ones, and uses them as supporting references to generate the answer.
The generated rationale enhances the transparency of the answer and our
framework provides insights into how the model arrived at this answer, by using
this rationale and the references to the context. In this paper, we demonstrate
its effectiveness in improving the quality of responses to drug-related
inquiries in the life sciences industry. Our framework improves traditional
Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be
14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,
fine-tuning samples based on our framework improves the accuracy of smaller
open-access LLMs by 33-42% and competes with RAG on commercial models.",Muneeswaran I
2023-11-27T05:27:13Z,http://arxiv.org/abs/2311.15548v1,"Deficiency of Large Language Models in Finance: An Empirical Examination
  of Hallucination","The hallucination issue is recognized as a fundamental deficiency of large
language models (LLMs), especially when applied to fields such as finance,
education, and law. Despite the growing concerns, there has been a lack of
empirical investigation. In this paper, we provide an empirical examination of
LLMs' hallucination behaviors in financial tasks. First, we empirically
investigate LLM model's ability of explaining financial concepts and
terminologies. Second, we assess LLM models' capacity of querying historical
stock prices. Third, to alleviate the hallucination issue, we evaluate the
efficacy of four practical methods, including few-shot learning, Decoding by
Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method
and the prompt-based tool learning method for a function to generate a query
command. Finally, our major finding is that off-the-shelf LLMs experience
serious hallucination behaviors in financial tasks. Therefore, there is an
urgent need to call for research efforts in mitigating LLMs' hallucination.",Haoqiang Kang
2023-11-27T19:17:39Z,http://arxiv.org/abs/2311.16267v2,"Novel Preprocessing Technique for Data Embedding in Engineering Code
  Generation Using Large Language Model","We present four main contributions to enhance the performance of Large
Language Models (LLMs) in generating domain-specific code: (i) utilizing
LLM-based data splitting and data renovation techniques to improve the semantic
representation of embeddings' space; (ii) introducing the Chain of Density for
Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text
Renovation (ATR) algorithm for assessing data renovation reliability; (iii)
developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt
technique; and (iv) effectively refactoring existing scripts to generate new
and high-quality scripts with LLMs. By using engineering simulation software
RedHawk-SC as a case study, we demonstrate the effectiveness of our data
pre-processing method for expanding and categorizing scripts. When combined
with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG)
method in retrieving more relevant information, ultimately achieving a 73.33%
""Percentage of Correct Lines"" for code generation problems in MapReduce
applications.",Yu-Chen Lin
2023-11-28T06:18:54Z,http://arxiv.org/abs/2311.16543v3,"RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language
  Models","This paper presents RTLFixer, a novel framework enabling automatic syntax
errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's
promising capabilities, our analysis indicates that approximately 55% of errors
in LLM-generated Verilog are syntax-related, leading to compilation failures.
To tackle this issue, we introduce a novel debugging framework that employs
Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act
as autonomous agents in interactively debugging the code with feedback. This
framework demonstrates exceptional proficiency in resolving syntax errors,
successfully correcting about 98.5% of compilation errors in our debugging
dataset, comprising 212 erroneous implementations derived from the VerilogEval
benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates
in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.",Yun-Da Tsai
2023-12-10T02:29:05Z,http://arxiv.org/abs/2312.05733v1,DevBots can co-design APIs,"DevBots are automated tools that perform various tasks in order to support
software development. They are a growing trend and have been used in
repositories to automate repetitive tasks, as code generators, and as
collaborators in eliciting requirements and defining architectures. In this
study, we analyzed 24 articles to investigate the state of the art of using
DevBots in software development, trying to understand their characteristics,
identify use cases, learn the relationship between DevBots and conversational
software development, and discuss how prompt engineering can enable
collaboration between human developers and bots. Additionally, we identified a
gap to address by applying prompt engineering to collaborative API design
between human designers and DevBots and proposed an experiment to assess what
approach, between using Retrieval Augmented Generation or not, is more
suitable. Our conclusion is that DevBots can collaborate with human API
designers, but the two approaches have advantages and disadvantages.",Vinicius Soares Silva Marques
2023-12-10T16:52:00Z,http://arxiv.org/abs/2312.05934v3,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,"Large language models (LLMs) encapsulate a vast amount of factual information
within their pre-trained weights, as evidenced by their ability to answer
diverse questions across different domains. However, this knowledge is
inherently limited, relying heavily on the characteristics of the training
data. Consequently, using external datasets to incorporate new information or
refine the capabilities of LLMs on previously seen information poses a
significant challenge. In this study, we compare two common approaches:
unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate
both approaches on a variety of knowledge-intensive tasks across different
topics. Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing knowledge
encountered during training and entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information through unsupervised
fine-tuning, and that exposing them to numerous variations of the same fact
during training could alleviate this problem.",Oded Ovadia
2023-12-12T09:34:27Z,http://arxiv.org/abs/2312.07104v2,SGLang: Efficient Execution of Structured Language Model Programs,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",Lianmin Zheng
2023-12-15T16:58:52Z,http://arxiv.org/abs/2312.09948v1,"GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading
  Scholarly Article Searches for Systematic Reviews","Systematic reviews (SRs) - the librarian-assisted literature survey of
scholarly articles takes time and requires significant human resources. Given
the ever-increasing volume of published studies, applying existing computing
and informatics technology can decrease this time and resource burden. Due to
the revolutionary advances in (1) Generative AI such as ChatGPT, and (2)
External knowledge-augmented information extraction efforts such as
Retrieval-Augmented Generation, In this work, we explore the use of techniques
from (1) and (2) for SR. We demonstrate a system that takes user queries,
performs query expansion to obtain enriched context (includes additional terms
and definitions by querying language models and knowledge graphs), and uses
this context to search for articles on scholarly databases to retrieve
articles. We perform qualitative evaluations of our system through comparison
against sentinel (ground truth) articles provided by an in-house librarian. The
demo can be found at: https://youtu.be/zMdP56GJ9mU.",Kaushik Roy
2023-12-21T22:52:44Z,http://arxiv.org/abs/2312.14327v1,"Parameter Efficient Tuning Allows Scalable Personalization of LLMs for
  Text Entry: A Case Study on Abbreviation Expansion","Abbreviation expansion is a strategy used to speed up communication by
limiting the amount of typing and using a language model to suggest expansions.
Here we look at personalizing a Large Language Model's (LLM) suggestions based
on prior conversations to enhance the relevance of predictions, particularly
when the user data is small (~1000 samples). Specifically, we compare
fine-tuning, prompt-tuning, and retrieval augmented generation of expanded text
suggestions for abbreviated inputs. Our case study with a deployed 8B parameter
LLM on a real user living with ALS, and experiments on movie character
personalization indicates that (1) customization may be necessary in some
scenarios and prompt-tuning generalizes well to those, (2) fine-tuning on
in-domain data (with as few as 600 samples) still shows some gains, however (3)
retrieval augmented few-shot selection also outperforms fine-tuning. (4)
Parameter efficient tuning allows for efficient and scalable personalization.
For prompt-tuning, we also find that initializing the learned ""soft-prompts"" to
user relevant concept tokens leads to higher accuracy than random
initialization.",Katrin Tomanek
2024-01-03T02:32:55Z,http://arxiv.org/abs/2401.01511v1,"Enhancing Multilingual Information Retrieval in Mixed Human Resources
  Environments: A RAG Model Implementation for Multicultural Enterprise","The advent of Large Language Models has revolutionized information retrieval,
ushering in a new era of expansive knowledge accessibility. While these models
excel in providing open-world knowledge, effectively extracting answers in
diverse linguistic environments with varying levels of literacy remains a
formidable challenge. Retrieval Augmented Generation (RAG) emerges as a
promising solution, bridging the gap between information availability and
multilingual comprehension. However, deploying RAG models in real-world
scenarios demands careful consideration of various factors. This paper
addresses the critical challenges associated with implementing RAG models in
multicultural environments. We delve into essential considerations, including
data feeding strategies, timely updates, mitigation of hallucinations,
prevention of erroneous responses, and optimization of delivery speed. Our work
involves the integration of a diverse array of tools, meticulously combined to
facilitate the seamless adoption of RAG models across languages and literacy
levels within a multicultural organizational context. Through strategic tweaks
in our approaches, we achieve not only effectiveness but also efficiency,
ensuring the accelerated and accurate delivery of information in a manner that
is tailored to the unique requirements of multilingual and multicultural
settings.",Syed Rameel Ahmad
2024-01-04T16:16:14Z,http://arxiv.org/abs/2401.02333v3,"Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models","The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.",Uday Allu
2024-01-13T02:20:17Z,http://arxiv.org/abs/2401.06954v2,Bridging the Preference Gap between Retrievers and LLMs,"Large Language Models (LLMs) have demonstrated superior results across a wide
range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to
enhance the performance by locating relevant information and placing it into
the context window of the LLM. However, the relationship between retrievers and
LLMs in a RAG is still under-investigated. Most existing work treats the
retriever and the LLM as independent components and leaves a gap between
retrieving human-""friendly"" information and assembling a LLM-""friendly""
context. In this work, we examine a novel bridge mechanism. We validate the
ranking and selection assumptions of retrievers in the context of RAG and
propose a framework that chains together supervised and reinforcement learning
to train a bridge model that optimizes the connection between the retriever and
the LLM. Empirical results demonstrate the effectiveness of our method in both
question-answering and personalized generation tasks.",Zixuan Ke
2024-01-16T02:11:35Z,http://arxiv.org/abs/2401.10286v3,"Code-Based English Models Surprising Performance on Chinese QA Pair
  Extraction Task","In previous studies, code-based models have consistently outperformed
text-based models in reasoning-intensive scenarios. When generating our
knowledge base for Retrieval-Augmented Generation (RAG), we observed that
code-based models also perform exceptionally well in Chinese QA Pair Extraction
task. Further, our experiments and the metrics we designed discovered that
code-based models containing a certain amount of Chinese data achieve even
better performance. Additionally, the capabilities of code-based English models
in specified Chinese tasks offer a distinct perspective for discussion on the
philosophical ""Chinese Room"" thought experiment.",Linghan Zheng
2024-02-02T12:34:09Z,http://arxiv.org/abs/2402.01364v2,Continual Learning for Large Language Models: A Survey,"Large language models (LLMs) are not amenable to frequent re-training, due to
high training costs arising from their massive scale. However, updates are
necessary to endow LLMs with new skills and keep them up-to-date with rapidly
evolving human knowledge. This paper surveys recent works on continual learning
for LLMs. Due to the unique nature of LLMs, we catalog continue learning
techniques in a novel multi-staged categorization scheme, involving continual
pretraining, instruction tuning, and alignment. We contrast continual learning
for LLMs with simpler adaptation methods used in smaller models, as well as
with other enhancement strategies like retrieval-augmented generation and model
editing. Moreover, informed by a discussion of benchmarks and evaluation, we
identify several challenges and future work directions for this crucial task.",Tongtong Wu
2024-01-27T00:18:07Z,http://arxiv.org/abs/2402.01722v1,"Enhancing Large Language Model Performance To Answer Questions and
  Extract Information More Accurately","Large Language Models (LLMs) generate responses to questions; however, their
effectiveness is often hindered by sub-optimal quality of answers and
occasional failures to provide accurate responses to questions. To address
these challenges, a fine-tuning process is employed, involving feedback and
examples to refine models. The objective is to enhance AI models through
continuous feedback loops, utilizing metrics such as cosine similarity, LLM
evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like
GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on
financial datasets, including the FinanceBench and RAG Instruct Benchmark
Tester Dataset, illustrating the necessity of fine-tuning. The results showcase
the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,
providing superior question and answering capabilities. Notably, the
combination of fine-tuning the LLM with a process known as Retrieval Augmented
Generation (RAG) proves to generate responses with improved accuracy.",Liang Zhang
2024-02-01T02:24:15Z,http://arxiv.org/abs/2402.01767v2,HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA,"Retrieval-augmented generation (RAG) has rapidly advanced the language model
field, particularly in question-answering (QA) systems. By integrating external
documents during the response generation phase, RAG significantly enhances the
accuracy and reliability of language models. This method elevates the quality
of responses and reduces the frequency of hallucinations, where the model
generates incorrect or misleading information. However, these methods exhibit
limited retrieval accuracy when faced with numerous indistinguishable
documents, presenting notable challenges in their practical application. In
response to these emerging challenges, we present HiQA, an advanced
multi-document question-answering (MDQA) framework that integrates cascading
metadata into content and a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.",Xinyue Chen
2024-02-05T14:36:51Z,http://arxiv.org/abs/2402.03053v1,"Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for
  Semantic Representations","In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
  For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
  In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the ""Melayu""
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
  These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99",Husein Zolkepli
2024-02-06T18:01:29Z,http://arxiv.org/abs/2402.04206v1,"Explaining Autonomy: Enhancing Human-Robot Interaction through
  Explanation Generation with Large Language Models","This paper introduces a system designed to generate explanations for the
actions performed by an autonomous robot in Human-Robot Interaction (HRI).
Explainability in robotics, encapsulated within the concept of an eXplainable
Autonomous Robot (XAR), is a growing research area. The work described in this
paper aims to take advantage of the capabilities of Large Language Models
(LLMs) in performing natural language processing tasks. This study focuses on
the possibility of generating explanations using such models in combination
with a Retrieval Augmented Generation (RAG) method to interpret data gathered
from the logs of autonomous systems. In addition, this work also presents a
formalization of the proposed explanation system. It has been evaluated through
a navigation test from the European Robotics League (ERL), a Europe-wide social
robotics competition. Regarding the obtained results, a validation
questionnaire has been conducted to measure the quality of the explanations
from the perspective of technical users. The results obtained during the
experiment highlight the potential utility of LLMs in achieving explanatory
capabilities in robots.",David Sobrín-Hidalgo
2024-02-06T21:14:45Z,http://arxiv.org/abs/2402.04411v2,"DFA-RAG: Conversational Semantic Router for Large Language Model with
  Definite Finite Automaton","This paper introduces the retrieval-augmented large language model with
Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the
capabilities of conversational agents using large language models (LLMs).
Traditional LLMs face challenges in generating regulated and compliant
responses in special scenarios with predetermined response guidelines, like
emotional support and customer service. Our framework addresses these
challenges by embedding a Definite Finite Automaton (DFA), learned from
training dialogues, within the LLM. This structured approach acts as a semantic
router which enables the LLM to adhere to a deterministic response pathway. The
routing is achieved by the retrieval-augmentation generation (RAG) strategy,
which carefully selects dialogue examples aligned with the current
conversational context. The advantages of DFA-RAG include an interpretable
structure through human-readable DFA, context-aware retrieval for responses in
conversations, and plug-and-play compatibility with existing LLMs. Extensive
benchmarks validate DFA-RAG's effectiveness, indicating its potential as a
valuable contribution to the conversational agent.",Yiyou Sun
2024-02-06T11:29:44Z,http://arxiv.org/abs/2402.05135v1,"CADReN: Contextual Anchor-Driven Relational Network for Controllable
  Cross-Graphs Node Importance Estimation","Node Importance Estimation (NIE) is crucial for integrating external
information into Large Language Models through Retriever-Augmented Generation.
Traditional methods, focusing on static, single-graph characteristics, lack
adaptability to new graphs and user-specific requirements. CADReN, our proposed
method, addresses these limitations by introducing a Contextual Anchor (CA)
mechanism. This approach enables the network to assess node importance relative
to the CA, considering both structural and semantic features within Knowledge
Graphs (KGs). Extensive experiments show that CADReN achieves better
performance in cross-graph NIE task, with zero-shot prediction ability. CADReN
is also proven to match the performance of previous models on single-graph NIE
task. Additionally, we introduce and opensource two new datasets, RIC200 and
WK1K, specifically designed for cross-graph NIE research, providing a valuable
resource for future developments in this domain.",Zijie Zhong
2024-02-15T12:12:19Z,http://arxiv.org/abs/2402.09906v2,Generative Representational Instruction Tuning,"All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.",Niklas Muennighoff
2024-02-16T19:28:52Z,http://arxiv.org/abs/2402.11035v3,Dense Passage Retrieval: Is it Retrieving?,"Dense passage retrieval (DPR) is the first step in the retrieval augmented
generation (RAG) paradigm for improving the performance of large language
models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of
the embeddings between queries and relevant textual data. A deeper
understanding of DPR fine-tuning will be required to fundamentally unlock the
full potential of this approach. In this work, we explore DPR-trained models
mechanistically by using a combination of probing, layer activation analysis,
and model editing. Our experiments show that DPR training decentralizes how
knowledge is stored in the network, creating multiple access pathways to the
same information. We also uncover a limitation in this training style: the
internal knowledge of the pre-trained model bounds what the retrieval model can
retrieve. These findings suggest a few possible directions for dense retrieval:
(1) expose the DPR training process to more knowledge so more can be
decentralized, (2) inject facts as decentralized representations, (3) model and
incorporate knowledge uncertainty in the retrieval process, and (4) directly
map internal model knowledge to a knowledge base.",Benjamin Reichman
2024-02-19T14:33:24Z,http://arxiv.org/abs/2402.12177v4,Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning,"Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.",Mingtian Zhang
2024-02-21T05:41:34Z,http://arxiv.org/abs/2402.13542v2,"ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling","Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.",Lingxi Zhang
2024-02-21T06:04:53Z,http://arxiv.org/abs/2402.13547v2,"ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through
  Retrieval-Augmented Agents","Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to
leverage external knowledge, enhancing their performance on knowledge-intensive
tasks. However, existing RAG models often treat LLMs as passive recipients of
information, which can lead to interference from noisy retrieved content. In
this paper, we introduce ActiveRAG, a multi-agent framework that mimics human
learning behavior to help LLMs actively engage with and learn from retrieved
evidence. ActiveRAG designs a knowledge assimilation agent to form the
knowledge understanding by associating external knowledge with the parametric
memory of LLMs. Then our model employs the thought accommodation agent to
calibrate the internal thought of LLMs for response refinement. Our experiments
show that ActiveRAG achieves a 10\% improvement over vanilla RAG on various
question-answering benchmarks. Further analysis reveals that ActiveRAG
mitigates the impact of noisy retrievals, alleviates conflicts between external
knowledge and parametric memory and improves the self-consistency of LLMs in
answering the question. All data and codes are available at
https://github.com/OpenMatch/ActiveRAG.",Zhipeng Xu
2024-02-21T08:54:47Z,http://arxiv.org/abs/2402.13625v2,MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning,"Since commonsense information has been recorded significantly less frequently
than its existence, language models pre-trained by text generation have
difficulty to learn sufficient commonsense knowledge. Several studies have
leveraged text retrieval to augment the models' commonsense ability. Unlike
text, images capture commonsense information inherently but little effort has
been paid to effectively utilize them. In this work, we propose a novel
Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and
images to enhance the commonsense ability of language models. Extensive
experiments on the Common-Gen task have demonstrated the efficacy of MORE based
on the pre-trained models of both single and multiple modalities.",Wanqing Cui
2024-02-22T06:21:41Z,http://arxiv.org/abs/2402.14318v1,Assessing generalization capability of text ranking models in Polish,"Retrieval-augmented generation (RAG) is becoming an increasingly popular
technique for integrating internal knowledge bases with large language models.
In a typical RAG pipeline, three models are used, responsible for the
retrieval, reranking, and generation stages. In this article, we focus on the
reranking problem for the Polish language, examining the performance of
rerankers and comparing their results with available retrieval models. We
conduct a comprehensive evaluation of existing models and those trained by us,
utilizing a benchmark of 41 diverse information retrieval tasks for the Polish
language. The results of our experiments show that most models struggle with
out-of-domain generalization. However, a combination of effective optimization
method and a large training dataset allows for building rerankers that are both
compact in size and capable of generalization. The best of our models
establishes a new state-of-the-art for reranking in the Polish language,
outperforming existing models with up to 30 times more parameters.",Sławomir Dadas
2024-02-23T13:02:10Z,http://arxiv.org/abs/2402.15301v2,"Causal Graph Discovery with Retrieval-Augmented Generation based Large
  Language Models","Causal graph recovery is traditionally done using statistical
estimation-based methods or based on individual's knowledge about variables of
interests. They often suffer from data collection biases and limitations of
individuals' knowledge. The advance of large language models (LLMs) provides
opportunities to address these problems. We propose a novel method that
leverages LLMs to deduce causal relationships in general causal graph recovery
tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs
extracted from scientific publication database as well as experiment data about
factors of interest to achieve this goal. Our method gives a prompting strategy
to extract associational relationships among those factors and a mechanism to
perform causality verification for these associations. Comparing to other
LLM-based methods that directly instruct LLMs to do the highly complex causal
reasoning, our method shows clear advantage on causal graph quality on
benchmark datasets. More importantly, as causality among some factors may
change as new research results emerge, our method show sensitivity to new
evidence in the literature and can provide useful information for updating
causal graphs accordingly.",Yuzhe Zhang
2024-02-26T08:59:05Z,http://arxiv.org/abs/2402.16406v1,"From RAGs to riches: Using large language models to write documents for
  clinical trials","Clinical trials require numerous documents to be written -- protocols,
consent forms, clinical study reports and others. Large language models (LLMs)
offer the potential to rapidly generate first versions of these documents,
however there are concerns about the quality of their output Here we report an
evaluation of LLMs in generating parts of one such document, clinical trial
protocols. We find that an offthe-shelf LLM delivers reasonable results,
especially when assessing content relevance and the correct use of terminology.
However, deficiencies remain: specifically clinical thinking and logic, and
appropriate use of references. To improve performance, we used
retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date
information. As a result of using RAG, the writing quality of the LLM improves
substantially, which has implications for the practical useability of LLMs in
clinical trial-related writing.",Nigel Markey
2024-02-26T18:55:15Z,http://arxiv.org/abs/2402.16829v1,"GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning","Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.",Aivin V. Solatorio
2024-02-06T13:19:53Z,http://arxiv.org/abs/2402.16874v1,"Enhancing Retrieval Processes for Language Generation with Augmented
  Queries","In the rapidly changing world of smart technology, searching for documents
has become more challenging due to the rise of advanced language models. These
models sometimes face difficulties, like providing inaccurate information,
commonly known as ""hallucination."" This research focuses on addressing this
issue through Retrieval-Augmented Generation (RAG), a technique that guides
models to give accurate responses based on real facts. To overcome scalability
issues, the study explores connecting user queries with sophisticated language
models such as BERT and Orca2, using an innovative query optimization process.
The study unfolds in three scenarios: first, without RAG, second, without
additional assistance, and finally, with extra help. Choosing the compact yet
efficient Orca2 7B model demonstrates a smart use of computing resources. The
empirical results indicate a significant improvement in the initial language
model's performance under RAG, particularly when assisted with prompts
augmenters. Consistency in document retrieval across different encodings
highlights the effectiveness of using language model-generated queries. The
introduction of UMAP for BERT further simplifies document retrieval while
maintaining strong results.",Julien Pierre Edmond Ghali
2024-02-28T17:38:06Z,http://arxiv.org/abs/2402.18510v4,"RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval","This paper investigates the gap in representation powers of Recurrent Neural
Networks (RNNs) and Transformers in the context of solving algorithmic
problems. We focus on understanding whether RNNs, known for their memory
efficiency in handling long sequences, can match the performance of
Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.
Our theoretical analysis reveals that CoT improves RNNs but is insufficient to
close the gap with Transformers. A key bottleneck lies in the inability of RNNs
to perfectly retrieve information from the context, even with CoT: for several
tasks that explicitly or implicitly require this capability, such as
associative recall and determining if a graph is a tree, we prove that RNNs are
not expressive enough to solve the tasks while Transformers can solve them with
ease. Conversely, we prove that adopting techniques to enhance the in-context
retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)
and adding a single Transformer layer, can elevate RNNs to be capable of
solving all polynomial-time solvable problems with CoT, hence closing the
representation gap with Transformers.",Kaiyue Wen
2024-03-02T12:19:04Z,http://arxiv.org/abs/2403.01193v3,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,"Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.",Philip Feldman
2024-03-06T15:40:30Z,http://arxiv.org/abs/2403.03792v2,"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt
  Injection Attacks","We introduce a new family of prompt injection attacks, termed Neural Exec.
Unlike known attacks that rely on handcrafted strings (e.g., ""Ignore previous
instructions and...""), we show that it is possible to conceptualize the
creation of execution triggers as a differentiable search problem and use
learning-based methods to autonomously generate them.
  Our results demonstrate that a motivated adversary can forge triggers that
are not only drastically more effective than current handcrafted ones but also
exhibit inherent flexibility in shape, properties, and functionality. In this
direction, we show that an attacker can design and generate Neural Execs
capable of persisting through multi-stage preprocessing pipelines, such as in
the case of Retrieval-Augmented Generation (RAG)-based applications. More
critically, our findings show that attackers can produce triggers that deviate
markedly in form and shape from any known attack, sidestepping existing
blacklist-based detection and sanitation approaches.",Dario Pasquini
2024-03-06T17:48:06Z,http://arxiv.org/abs/2403.03888v3,FaaF: Facts as a Function for the evaluation of generated text,"The demand for accurate and efficient verification of information in texts
generated by large language models (LMs) is at an all-time high, but remains
unresolved. Recent efforts have focused on extracting and verifying atomic
facts from these texts via prompting LM evaluators. However, we demonstrate
that this method of prompting is unreliable when faced with incomplete or
inaccurate reference information. We introduce Facts as a Function (FaaF), a
new approach to the fact verification task that leverages the function-calling
capabilities of LMs. FaaF significantly enhances the ability of LMs to identify
unsupported facts in texts, while also improving efficiency and significantly
lowering costs compared to prompt-based methods. Additionally, we propose a
framework for evaluating factual recall in Retrieval Augmented Generation (RAG)
systems, which we employ to compare prompt-based and FaaF methods using various
LMs under challenging conditions.",Vasileios Katranidis
2024-03-07T08:25:46Z,http://arxiv.org/abs/2403.04307v3,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,"Hallucinations pose a significant challenge to the reliability of large
language models (LLMs) in critical domains. Recent benchmarks designed to
assess LLM hallucinations within conventional NLP tasks, such as
knowledge-intensive question answering (QA) and summarization, are insufficient
for capturing the complexities of user-LLM interactions in dynamic, real-world
settings. To address this gap, we introduce HaluEval-Wild, the first benchmark
specifically designed to evaluate LLM hallucinations in the wild. We
meticulously collect challenging (adversarially filtered by Alpaca) user
queries from ShareGPT, an existing real-world user-LLM interaction datasets, to
evaluate the hallucination rates of various LLMs. Upon analyzing the collected
queries, we categorize them into five distinct types, which enables a
fine-grained analysis of the types of hallucinations LLMs exhibit, and
synthesize the reference answers with the powerful GPT-4 model and
retrieval-augmented generation (RAG). Our benchmark offers a novel approach
towards enhancing our comprehension of and improving LLM reliability in
scenarios reflective of real-world interactions. Our benchmark is available at
https://github.com/HaluEval-Wild/HaluEval-Wild.",Zhiying Zhu
2024-03-07T17:13:12Z,http://arxiv.org/abs/2403.04666v2,Telecom Language Models: Must They Be Large?,"The increasing interest in Large Language Models (LLMs) within the
telecommunications sector underscores their potential to revolutionize
operational efficiency. However, the deployment of these sophisticated models
is often hampered by their substantial size and computational demands, raising
concerns about their viability in resource-constrained environments. Addressing
this challenge, recent advancements have seen the emergence of small language
models that surprisingly exhibit performance comparable to their larger
counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a
compact yet powerful model, exemplifies this new wave of efficient small
language models. This paper conducts a comprehensive evaluation of Phi-2's
intrinsic understanding of the telecommunications domain. Recognizing the
scale-related limitations, we enhance Phi-2's capabilities through a
Retrieval-Augmented Generation approach, meticulously integrating an extensive
knowledge base specifically curated with telecom standard specifications. The
enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering
questions about telecom standards with a precision that closely rivals the more
resource-intensive GPT-3.5. The paper further explores the refined capabilities
of Phi-2 in addressing problem-solving scenarios within the telecom sector,
highlighting its potential and limitations.",Nicola Piovesan
2024-03-11T16:01:05Z,http://arxiv.org/abs/2403.06840v2,"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation
  via Iterative Self-Feedback","Large language models (LLMs) demonstrate exceptional performance in numerous
tasks but still heavily rely on knowledge stored in their parameters. Moreover,
updating this knowledge incurs high training costs. Retrieval-augmented
generation (RAG) methods address this issue by integrating external knowledge.
The model can answer questions it couldn't previously by retrieving knowledge
relevant to the query. This approach improves performance in certain scenarios
for specific tasks. However, if irrelevant texts are retrieved, it may impair
model performance. In this paper, we propose Retrieval Augmented Iterative
Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and
processes them in three submodules to enhance the model's problem-solving
capabilities. Experiments show that our method outperforms existing benchmarks,
performing well on models like GPT3.5, Llama2, significantly enhancing factual
reasoning capabilities and reducing hallucinations.",Yanming Liu
2024-03-14T06:17:20Z,http://arxiv.org/abs/2403.09125v5,"Exploring the Capabilities and Limitations of Large Language Models in
  the Electric Energy Sector","Large Language Models (LLMs) as chatbots have drawn remarkable attention
thanks to their versatile capability in natural language processing as well as
in a wide range of tasks. While there has been great enthusiasm towards
adopting such foundational model-based artificial intelligence tools in all
sectors possible, the capabilities and limitations of such LLMs in improving
the operation of the electric energy sector need to be explored, and this
article identifies fruitful directions in this regard. Key future research
directions include data collection systems for fine-tuning LLMs, embedding
power system-specific tools in the LLMs, and retrieval augmented generation
(RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in
safety-critical use cases.",Subir Majumder
2024-03-14T09:45:05Z,http://arxiv.org/abs/2403.09226v2,"Retrieval augmented text-to-SQL generation for epidemiological question
  answering using electronic health records","Electronic health records (EHR) and claims data are rich sources of
real-world data that reflect patient health status and healthcare utilization.
Querying these databases to answer epidemiological questions is challenging due
to the intricacy of medical terminology and the need for complex SQL queries.
Here, we introduce an end-to-end methodology that combines text-to-SQL
generation with retrieval augmented generation (RAG) to answer epidemiological
questions using EHR and claims data. We show that our approach, which
integrates a medical coding step into the text-to-SQL process, significantly
improves the performance over simple prompting. Our findings indicate that
although current language models are not yet sufficiently accurate for
unsupervised use, RAG offers a promising direction for improving their
capabilities, as shown in a realistic industry setting.",Angelo Ziletti
2024-03-15T09:54:04Z,http://arxiv.org/abs/2403.10153v3,"Improving Medical Multi-modal Contrastive Learning with Expert
  Annotations","We introduce eCLIP, an enhanced version of the CLIP model that integrates
expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key
challenges in contrastive multi-modal medical imaging analysis, notably data
scarcity and the ""modality gap"" -- a significant disparity between image and
text embeddings that diminishes the quality of representations and hampers
cross-modal interoperability. eCLIP integrates a heatmap processor and
leverages mixup augmentation to efficiently utilize the scarce expert
annotations, thus boosting the model's learning effectiveness. eCLIP is
designed to be generally applicable to any variant of CLIP without requiring
any modifications of the core architecture. Through detailed evaluations across
several tasks, including zero-shot inference, linear probing, cross-modal
retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using
a frozen Large Language Model, eCLIP showcases consistent improvements in
embedding quality. The outcomes reveal enhanced alignment and uniformity,
affirming eCLIP's capability to harness high-quality annotations for enriched
multi-modal analysis in the medical imaging domain.",Yogesh Kumar
2024-03-15T16:30:14Z,http://arxiv.org/abs/2403.10446v1,"Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A
  Case Study on Domain-Specific Queries in Private Knowledge-Bases","We proposed an end-to-end system design towards utilizing Retrieval Augmented
Generation (RAG) to improve the factual accuracy of Large Language Models
(LLMs) for domain-specific and time-sensitive queries related to private
knowledge-bases. Our system integrates RAG pipeline with upstream datasets
processing and downstream performance evaluation. Addressing the challenge of
LLM hallucinations, we finetune models with a curated dataset which originates
from CMU's extensive resources and annotated with the teacher model. Our
experiments demonstrate the system's effectiveness in generating more accurate
answers to domain-specific and time-sensitive inquiries. The results also
revealed the limitations of fine-tuning LLMs with small-scale and skewed
datasets. This research highlights the potential of RAG systems in augmenting
LLMs with external datasets for improved performance in knowledge-intensive
tasks. Our code and models are available on Github.",Jiarui Li
2024-03-17T23:02:04Z,http://arxiv.org/abs/2403.11366v2,"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning","The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.",Anique Tahir
2024-03-18T12:58:16Z,http://arxiv.org/abs/2403.11747v2,Embedded Named Entity Recognition using Probing Classifiers,"Streaming text generation has become a common way of increasing the
responsiveness of language model powered applications, such as chat assistants.
At the same time, extracting semantic information from generated text is a
useful tool for applications such as automated fact checking or retrieval
augmented generation. Currently, this requires either separate models during
inference, which increases computational cost, or destructive fine-tuning of
the language model. Instead, we propose an approach called EMBER which enables
streaming named entity recognition in decoder-only language models without
fine-tuning them and while incurring minimal additional computational cost at
inference time. Specifically, our experiments show that EMBER maintains high
token generation rates, with only a negligible decrease in speed of around 1%
compared to a 43.64% slowdown measured for a baseline. We make our code and
data available online, including a toolkit for training, testing, and deploying
efficient token classification models optimized for streaming text generation.",Nicholas Popovič
2024-03-03T03:01:14Z,http://arxiv.org/abs/2403.14666v2,SyllabusQA: A Course Logistics Question Answering Dataset,"Automated teaching assistants and chatbots have significant potential to
reduce the workload of human instructors, especially for logistics-related
question answering, which is important to students yet repetitive for
instructors. However, due to privacy concerns, there is a lack of publicly
available datasets. We introduce SyllabusQA, an open-source dataset with 63
real course syllabi covering 36 majors, containing 5,078 open-ended course
logistics-related question-answer pairs that are diverse in both question types
and answer formats. Since many logistics-related questions contain critical
information like the date of an exam, it is important to evaluate the
factuality of answers. We benchmark several strong baselines on this task, from
large language model prompting to retrieval-augmented generation. We introduce
Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of
predicted answers. We find that despite performing close to humans on
traditional metrics of textual similarity, there remains a significant gap
between automated approaches and humans in terms of fact precision.",Nigel Fernandez
2024-03-27T04:20:18Z,http://arxiv.org/abs/2403.18243v1,"Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check","Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.",Linhao Ye
2024-03-29T00:14:46Z,http://arxiv.org/abs/2403.19889v1,Towards a Robust Retrieval-Based Summarization System,"This paper describes an investigation of the robustness of large language
models (LLMs) for retrieval augmented generation (RAG)-based summarization
tasks. While LLMs provide summarization capabilities, their performance in
complex, real-world scenarios remains under-explored. Our first contribution is
LogicSumm, an innovative evaluation framework incorporating realistic scenarios
to assess LLM robustness during RAG-based summarization. Based on limitations
identified by LogiSumm, we then developed SummRAG, a comprehensive system to
create training dialogues and fine-tune a model to enhance robustness within
LogicSumm's scenarios. SummRAG is an example of our goal of defining structured
methods to test the capabilities of an LLM, rather than addressing issues in a
one-off fashion. Experimental results confirm the power of SummRAG, showcasing
improved logical coherence and summarization quality. Data, corresponding model
weights, and Python code are available online.",Shengjie Liu
2024-03-29T03:56:19Z,http://arxiv.org/abs/2403.19964v3,FairRAG: Fair Human Generation via Fair Retrieval Augmentation,"Existing text-to-image generative models reflect or even amplify societal
biases ingrained in their training data. This is especially concerning for
human image generation where models are biased against certain demographic
groups. Existing attempts to rectify this issue are hindered by the inherent
limitations of the pre-trained models and fail to substantially improve
demographic diversity. In this work, we introduce Fair Retrieval Augmented
Generation (FairRAG), a novel framework that conditions pre-trained generative
models on reference images retrieved from an external image database to improve
fairness in human generation. FairRAG enables conditioning through a
lightweight linear module that projects reference images into the textual
space. To enhance fairness, FairRAG applies simple-yet-effective debiasing
strategies, providing images from diverse demographic groups during the
generative process. Extensive experiments demonstrate that FairRAG outperforms
existing methods in terms of demographic diversity, image-text alignment, and
image fidelity while incurring minimal computational overhead during inference.",Robik Shrestha
2024-04-01T10:43:52Z,http://arxiv.org/abs/2404.01037v1,ARAGOG: Advanced RAG Output Grading,"Retrieval-Augmented Generation (RAG) is essential for integrating external
knowledge into Large Language Model (LLM) outputs. While the literature on RAG
is growing, it primarily focuses on systematic reviews and comparisons of new
state-of-the-art (SoTA) techniques against their predecessors, with a gap in
extensive experimental comparisons. This study begins to address this gap by
assessing various RAG methods' impacts on retrieval precision and answer
similarity. We found that Hypothetical Document Embedding (HyDE) and LLM
reranking significantly enhance retrieval precision. However, Maximal Marginal
Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a
baseline Naive RAG system, and Multi-query approaches underperformed. Sentence
Window Retrieval emerged as the most effective for retrieval precision, despite
its variable performance on answer similarity. The study confirms the potential
of the Document Summary Index as a competent retrieval approach. All resources
related to this research are publicly accessible for further investigation
through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We
welcome the community to further this exploratory study in RAG systems.",Matouš Eibich
2024-04-02T15:10:11Z,http://arxiv.org/abs/2404.02022v3,"Improving Retrieval Augmented Open-Domain Question-Answering with
  Vectorized Contexts","In the era of large language models, applying techniques such as Retrieval
Augmented Generation can better address Open-Domain Question-Answering
problems. Due to constraints including model sizes and computing resources, the
length of context is often limited, and it becomes challenging to empower the
model to cover overlong contexts while answering questions from open domains.
This paper proposes a general and convenient method to covering longer contexts
in Open-Domain Question-Answering tasks. It leverages a small encoder language
model that effectively encodes contexts, and the encoding applies
cross-attention with origin inputs. With our method, the origin language models
can cover several times longer contexts while keeping the computing
requirements close to the baseline. Our experiments demonstrate that after
fine-tuning, there is improved performance across two held-in datasets, four
held-out datasets, and also in two In Context Learning settings.",Zhuo Chen
2024-04-02T17:00:11Z,http://arxiv.org/abs/2404.02103v2,"CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions
  for RAG systems","Retrieval Augmented Generation (RAG) has become a popular application for
large language models. It is preferable that successful RAG systems provide
accurate answers that are supported by being grounded in a passage without any
hallucinations. While considerable work is required for building a full RAG
pipeline, being able to benchmark performance is also necessary. We present
ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG
pipeline. ClapNQ includes long answers with grounded gold passages from Natural
Questions (NQ) and a corpus to perform either retrieval, generation, or the
full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full
passage, and cohesive, meaning that the answer is composed fluently, often by
integrating multiple pieces of the passage that are not contiguous. RAG models
must adapt to these properties to be successful at ClapNQ. We present baseline
experiments and analysis for ClapNQ that highlight areas where there is still
significant room for improvement in grounded RAG. CLAPNQ is publicly available
at https://github.com/primeqa/clapnq",Sara Rosenthal
2024-04-02T21:35:54Z,http://arxiv.org/abs/2404.02319v2,"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization","In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .",Tobias Schnabel
2024-04-03T05:31:59Z,http://arxiv.org/abs/2404.02474v1,uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?,"Inspired by human cognition, Jiang et al.(2023c) create a benchmark for
assessing LLMs' lateral thinking-thinking outside the box. Building upon this
benchmark, we investigate how different prompting methods enhance LLMs'
performance on this task to reveal their inherent power for outside-the-box
thinking ability. Through participating in SemEval-2024, task 9, Sentence
Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)
and direct prompting, enhancing with informative descriptions, and employing
contextualizing prompts using a retrieval augmented generation (RAG) pipeline.
Our experiments involve three LLMs including GPT-3.5, GPT-4, and
Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and
options using GPT-4, validated by humans for quality. Findings indicate that
compressed informative prompts enhance performance. Dynamic in-context learning
enhances model performance significantly. Furthermore, fine-tuning Zephyr on
our dataset enhances performance across other commonsense datasets,
underscoring the value of innovative thinking.",Pouya Sadeghi
2024-04-04T00:10:39Z,http://arxiv.org/abs/2404.03122v1,"Towards Standards-Compliant Assistive Technology Product Specifications
  via LLMs","In the rapidly evolving field of assistive technology (AT), ensuring that
products meet national and international standards is essential for user
safety, efficacy, and accessibility. In this vision paper, we introduce
CompliAT, a pioneering framework designed to streamline the compliance process
of AT product specifications with these standards through the innovative use of
Large Language Models (LLMs). CompliAT addresses three critical tasks: checking
terminology consistency, classifying products according to standards, and
tracing key product specifications to standard requirements. We tackle the
challenge of terminology consistency to ensure that the language used in
product specifications aligns with relevant standards, reducing
misunderstandings and non-compliance risks. We propose a novel approach for
product classification, leveraging a retrieval-augmented generation model to
accurately categorize AT products aligning to international standards, despite
the sparse availability of training data. Finally, CompliAT implements a
traceability and compliance mechanism from key product specifications to
standard requirements, ensuring all aspects of an AT product are thoroughly
vetted against the corresponding standards. By semi-automating these processes,
CompliAT aims to significantly reduce the time and effort required for AT
product standards compliance and uphold quality and safety standards. We
outline our planned implementation and evaluation plan for CompliAT.",Chetan Arora
2024-04-05T18:44:54Z,http://arxiv.org/abs/2404.04351v2,"Assisting humans in complex comparisons: automated information
  comparison at scale","Generative Large Language Models enable efficient analytics across knowledge
domains, rivalling human experts in information comparisons. However, the
applications of LLMs for information comparisons face scalability challenges
due to the difficulties in maintaining information across large contexts and
overcoming model token limitations. To address these challenges, we developed
the novel Abstractive Summarization & Criteria-driven Comparison Endpoint
(ASC$^2$End) system to automate information comparison at scale. Our system
employs Semantic Text Similarity comparisons for generating evidence-supported
analyses. We utilize proven data-handling strategies such as abstractive
summarization and retrieval augmented generation to overcome token limitations
and retain relevant information during model inference. Prompts were designed
using zero-shot strategies to contextualize information for improved model
reasoning. We evaluated abstractive summarization using ROUGE scoring and
assessed the generated comparison quality using survey responses. Models
evaluated on the ASC$^2$End system show desirable results providing insights on
the expected performance of the system. ASC$^2$End is a novel system and tool
that enables accurate, automated information comparison at scale across
knowledge domains, overcoming limitations in context length and retrieval.",Truman Yuen
2024-04-06T05:44:53Z,http://arxiv.org/abs/2404.04510v1,"IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe
  Biomedical Natural Language Inference for Clinical Trials","Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.",Shreyasi Mandal
2024-04-08T15:00:36Z,http://arxiv.org/abs/2404.05587v2,"Enhancing Software-Related Information Extraction via Single-Choice
  Question Answering with Large Language Models","This paper describes our participation in the Shared Task on Software
Mentions Disambiguation (SOMD), with a focus on improving relation extraction
in scholarly texts through generative Large Language Models (LLMs) using
single-choice question-answering. The methodology prioritises the use of
in-context learning capabilities of GLMs to extract software-related entities
and their descriptive attributes, such as distributive information. Our
approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for
Named Entity Recognition (NER) and Attributive NER to identify relationships
between extracted software entities, providing a structured solution for
analysing software citations in academic literature. The paper provides a
detailed description of our approach, demonstrating how using GLMs in a
single-choice QA paradigm can greatly enhance IE methodologies. Our
participation in the SOMD shared task highlights the importance of precise
software citation practices and showcases our system's ability to overcome the
challenges of disambiguating and extracting relationships between software
mentions. This sets the groundwork for future research and development in this
field.",Wolfgang Otto
2024-04-09T04:20:27Z,http://arxiv.org/abs/2404.06004v1,"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free
  Information Retrieval","In approximate nearest neighbor search (ANNS) methods based on approximate
proximity graphs, DiskANN achieves good recall-speed balance for large-scale
datasets using both of RAM and storage. Despite it claims to save memory usage
by loading compressed vectors by product quantization (PQ), its memory usage
increases in proportion to the scale of datasets. In this paper, we propose
All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the
compressed vectors to storage. Our method achieves $\sim$10 MB memory usage in
query search even with billion-scale datasets with minor performance
degradation. AiSAQ also reduces the index load time before query search, which
enables the index switch between muitiple billion-scale datasets and
significantly enhances the flexibility of retrieval-augmented generation (RAG).
This method is applicable to all graph-based ANNS algorithms and can be
combined with higher-spec ANNS methods in the future.",Kento Tatsuno
2024-04-09T13:02:22Z,http://arxiv.org/abs/2404.06278v1,"Dimensionality Reduction in Sentence Transformer Vector Databases with
  Fast Fourier Transform","Dimensionality reduction in vector databases is pivotal for streamlining AI
data management, enabling efficient storage, faster computation, and improved
model performance. This paper explores the benefits of reducing vector database
dimensions, with a focus on computational efficiency and overcoming the curse
of dimensionality. We introduce a novel application of Fast Fourier Transform
(FFT) to dimensionality reduction, a method previously underexploited in this
context. By demonstrating its utility across various AI domains, including
Retrieval-Augmented Generation (RAG) models and image processing, this
FFT-based approach promises to improve data retrieval processes and enhance the
efficiency and scalability of AI solutions. The incorporation of FFT may not
only optimize operations in real-time processing and recommendation systems but
also extend to advanced image processing techniques, where dimensionality
reduction can significantly improve performance and analysis efficiency. This
paper advocates for the broader adoption of FFT in vector database management,
marking a significant stride towards addressing the challenges of data volume
and complexity in AI research and applications. Unlike many existing
approaches, we directly handle the embedding vectors produced by the model
after processing a test input.",Vitaly Bulgakov
2024-04-10T02:02:34Z,http://arxiv.org/abs/2404.06680v1,"Onco-Retriever: Generative Classifier for Retrieval of EHR Records in
  Oncology","Retrieving information from EHR systems is essential for answering specific
questions about patient journeys and improving the delivery of clinical care.
Despite this fact, most EHR systems still rely on keyword-based searches. With
the advent of generative large language models (LLMs), retrieving information
can lead to better search and summarization capabilities. Such retrievers can
also feed Retrieval-augmented generation (RAG) pipelines to answer any query.
However, the task of retrieving information from EHR real-world clinical data
contained within EHR systems in order to solve several downstream use cases is
challenging due to the difficulty in creating query-document support pairs. We
provide a blueprint for creating such datasets in an affordable manner using
large language models. Our method results in a retriever that is 30-50 F-1
points better than propriety counterparts such as Ada and Mistral for oncology
data elements. We further compare our model, called Onco-Retriever, against
fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation
on real-world EHR data along with latency analysis of the different models and
provide a path forward for healthcare organizations to build domain-specific
retrievers.",Shashi Kant Gupta
2024-03-23T00:49:40Z,http://arxiv.org/abs/2404.07221v2,"Improving Retrieval for RAG based Question Answering Models on Financial
  Documents","The effectiveness of Large Language Models (LLMs) in generating accurate
responses relies heavily on the quality of input provided, particularly when
employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by
sourcing the most relevant text chunk(s) to base queries upon. Despite the
significant advancements in LLMs' response quality in recent years, users may
still encounter inaccuracies or irrelevant answers; these issues often stem
from suboptimal text chunk retrieval by RAG rather than the inherent
capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine
the RAG process. This paper explores the existing constraints of RAG pipelines
and introduces methodologies for enhancing text retrieval. It delves into
strategies such as sophisticated chunking techniques, query expansion, the
incorporation of metadata annotations, the application of re-ranking
algorithms, and the fine-tuning of embedding algorithms. Implementing these
approaches can substantially improve the retrieval quality, thereby elevating
the overall performance and reliability of LLMs in processing and responding to
queries.",Spurthi Setty
2024-04-10T22:26:26Z,http://arxiv.org/abs/2404.07376v2,LLMs in Biomedicine: A study on clinical Named Entity Recognition,"Large Language Models (LLMs) demonstrate remarkable versatility in various
NLP tasks but encounter distinct challenges in biomedical due to the
complexities of language and data scarcity. This paper investigates LLMs
application in the biomedical domain by exploring strategies to enhance their
performance for the NER task. Our study reveals the importance of meticulously
designed prompts in the biomedical. Strategic selection of in-context examples
yields a marked improvement, offering ~15-20\% increase in F1 score across all
benchmark datasets for biomedical few-shot NER. Additionally, our results
indicate that integrating external biomedical knowledge via prompting
strategies can enhance the proficiency of general-purpose LLMs to meet the
specialized needs of biomedical NER. Leveraging a medical knowledge base, our
proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can
boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at
\url{https://github.com/masoud-monajati/LLM_Bio_NER}",Masoud Monajatipoor
2024-04-13T09:33:00Z,http://arxiv.org/abs/2404.08940v1,Introducing Super RAGs in Mistral 8x7B-v1,"The relentless pursuit of enhancing Large Language Models (LLMs) has led to
the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel
approach designed to elevate the performance of LLMs by integrating external
knowledge sources with minimal structural modifications. This paper presents
the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,
and examines the resultant improvements in accuracy, speed, and user
satisfaction. Our methodology uses a fine-tuned instruct model setup and a
cache tuning fork system, ensuring efficient and relevant data retrieval. The
evaluation, conducted over several epochs, demonstrates significant
enhancements across all metrics. The findings suggest that Super RAGs can
effectively augment LLMs, paving the way for more sophisticated and reliable AI
systems. This research contributes to the field by providing empirical evidence
of the benefits of Super RAGs and offering insights into their potential
applications.",Ayush Thakur
2024-04-16T17:59:10Z,http://arxiv.org/abs/2404.10774v2,MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents,"Recognizing if LLM output can be grounded in evidence is central to many
tasks in NLP: retrieval-augmented generation, summarization, document-grounded
dialogue, and more. Current approaches to this kind of fact-checking are based
on verifying each piece of a model generation against potential evidence using
an LLM. However, this process can be very computationally expensive, requiring
many calls to a model to check a single response. In this work, we show how to
build small fact-checking models that have GPT-4-level performance but for 400x
lower cost. We do this by constructing synthetic training data with GPT-4,
which involves creating realistic yet challenging instances of factual errors
via a structured generation procedure. Training on this data teaches models to
check each fact in the claim and recognize synthesis of information across
sentences. For evaluation, we unify datasets from recent work on fact-checking
and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best
system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable
size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data
synthesis, and models.",Liyan Tang
2024-04-17T01:27:42Z,http://arxiv.org/abs/2404.10981v2,"A Survey on Retrieval-Augmented Text Generation for Large Language
  Models","Retrieval-Augmented Generation (RAG) merges retrieval methods with deep
learning advancements to address the static limitations of large language
models (LLMs) by enabling the dynamic integration of up-to-date external
information. This methodology, focusing primarily on the text domain, provides
a cost-effective solution to the generation of plausible but possibly incorrect
responses by LLMs, thereby enhancing the accuracy and reliability of their
outputs through the use of real-world data. As RAG grows in complexity and
incorporates multiple concepts that can influence its performance, this paper
organizes the RAG paradigm into four categories: pre-retrieval, retrieval,
post-retrieval, and generation, offering a detailed perspective from the
retrieval viewpoint. It outlines RAG's evolution and discusses the field's
progression through the analysis of significant studies. Additionally, the
paper introduces evaluation methods for RAG, addressing the challenges faced
and proposing future research directions. By offering an organized framework
and categorization, the study aims to consolidate existing research on RAG,
clarify its technological underpinnings, and highlight its potential to broaden
the adaptability and applications of LLMs.",Yizheng Huang
2024-04-17T10:00:56Z,http://arxiv.org/abs/2404.11216v2,"Position Engineering: Boosting Large Language Models through Positional
  Information Manipulation","The performance of large language models (LLMs) is significantly influenced
by the quality of the prompts provided. In response, researchers have developed
enormous prompt engineering strategies aimed at modifying the prompt text to
enhance task performance. In this paper, we introduce a novel technique termed
position engineering, which offers a more efficient way to guide large language
models. Unlike prompt engineering, which requires substantial effort to modify
the text provided to LLMs, position engineering merely involves altering the
positional information in the prompt without modifying the text itself. We have
evaluated position engineering in two widely-used LLM scenarios:
retrieval-augmented generation (RAG) and in-context learning (ICL). Our
findings show that position engineering substantially improves upon the
baseline in both cases. Position engineering thus represents a promising new
strategy for exploiting the capabilities of large language models.",Zhiyuan He
2024-04-17T18:13:16Z,http://arxiv.org/abs/2404.11672v1,MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory,"While current large language models (LLMs) demonstrate some capabilities in
knowledge-intensive tasks, they are limited by relying on their parameters as
an implicit storage mechanism. As a result, they struggle with infrequent
knowledge and temporal degradation. In addition, the uninterpretable nature of
parametric memorization makes it challenging to understand and prevent
hallucination. Parametric memory pools and model editing are only partial
solutions. Retrieval Augmented Generation (RAG) $\unicode{x2013}$ though
non-parametric $\unicode{x2013}$ has its own limitations: it lacks structure,
complicates interpretability and makes it hard to effectively manage stored
knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs
by integrating a structured and explicit read-and-write memory module. MemLLM
tackles the aforementioned challenges by enabling dynamic interaction with the
memory and improving the LLM's capabilities in using stored knowledge. Our
experiments indicate that MemLLM enhances the LLM's performance and
interpretability, in language modeling in general and knowledge-intensive tasks
in particular. We see MemLLM as an important step towards making LLMs more
grounded and factual through memory augmentation.",Ali Modarressi
2024-04-18T08:01:20Z,http://arxiv.org/abs/2404.11973v1,"Exploring the landscape of large language models: Foundations,
  techniques, and challenges","In this review paper, we delve into the realm of Large Language Models
(LLMs), covering their foundational principles, diverse applications, and
nuanced training processes. The article sheds light on the mechanics of
in-context learning and a spectrum of fine-tuning approaches, with a special
focus on methods that optimize efficiency in parameter usage. Additionally, it
explores how LLMs can be more closely aligned with human preferences through
innovative reinforcement learning frameworks and other novel methods that
incorporate human feedback. The article also examines the emerging technique of
retrieval augmented generation, integrating external knowledge into LLMs. The
ethical dimensions of LLM deployment are discussed, underscoring the need for
mindful and responsible application. Concluding with a perspective on future
research trajectories, this review offers a succinct yet comprehensive overview
of the current state and emerging trends in the evolving landscape of LLMs,
serving as an insightful guide for both researchers and practitioners in
artificial intelligence.",Milad Moradi
2024-04-18T10:25:42Z,http://arxiv.org/abs/2404.12065v2,"RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political
  Fact-Checking using Multimodal Large Language Models","The escalating challenge of misinformation, particularly in political
discourse, requires advanced fact-checking solutions; this is even clearer in
the more complex scenario of multimodal claims. We tackle this issue using a
multimodal large language model in conjunction with retrieval-augmented
generation (RAG), and introduce two novel reasoning techniques: Chain of RAG
(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by
extracting both textual and image content, retrieving external information, and
reasoning subsequent questions to be answered based on prior evidence. We
achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique
by 0.14 points. Human evaluation confirms that the vast majority of our
generated fact-check explanations contain all information from gold standard
data.",M. Abdul Khaliq
2024-04-22T05:46:40Z,http://arxiv.org/abs/2404.13892v2,Retrieval-Augmented Audio Deepfake Detection,"With recent advances in speech synthesis including text-to-speech (TTS) and
voice conversion (VC) systems enabling the generation of ultra-realistic audio
deepfakes, there is growing concern about their potential misuse. However, most
deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a
single model, resulting in performance bottlenecks and transparency issues.
Inspired by retrieval-augmented generation (RAG), we propose a
retrieval-augmented detection (RAD) framework that augments test samples with
similar retrieved samples for enhanced detection. We also extend the
multi-fusion attentive classifier to integrate it with our proposed RAD
framework. Extensive experiments show the superior performance of the proposed
RAD framework over baseline methods, achieving state-of-the-art results on the
ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.
Further sample analysis indicates that the retriever consistently retrieves
samples mostly from the same speaker with acoustic characteristics highly
consistent with the query audio, thereby improving detection performance.",Zuheng Kang
2024-04-23T05:51:45Z,http://arxiv.org/abs/2404.14760v2,Retrieval Augmented Generation for Domain-specific Question Answering,"Question answering (QA) has become an important application in the advanced
development of large language models. General pre-trained large language models
for question-answering are not trained to properly understand the knowledge or
terminology for a specific domain, such as finance, healthcare, education, and
customer service for a product. To better cater to domain-specific
understanding, we build an in-house question-answering system for Adobe
products. We propose a novel framework to compile a large question-answer
database and develop the approach for retrieval-aware finetuning of a Large
Language model. We showcase that fine-tuning the retriever leads to major
improvements in the final generation. Our overall approach reduces
hallucinations during generation while keeping in context the latest retrieval
information for contextual grounding.",Sanat Sharma
2024-04-23T18:00:09Z,http://arxiv.org/abs/2404.15406v2,"Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal
  LLMs","Multimodal LLMs are the natural evolution of LLMs, and enlarge their
capabilities so as to work beyond the pure textual modality. As research is
being carried out to design novel architectures and vision-and-language
adapters, in this paper we concentrate on endowing such models with the
capability of answering questions that require external knowledge. Our
approach, termed Wiki-LLaVA, aims at integrating an external knowledge source
of multimodal documents, which is accessed through a hierarchical retrieval
pipeline. Relevant passages, using this approach, are retrieved from the
external knowledge source and employed as additional context for the LLM,
augmenting the effectiveness and precision of generated dialogues. We conduct
extensive experiments on datasets tailored for visual question answering with
external data and demonstrate the appropriateness of our approach.",Davide Caffagni
2024-04-24T15:58:59Z,http://arxiv.org/abs/2404.15939v3,"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language
  Models for Telecommunications","The application of Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems in the telecommunication domain presents unique
challenges, primarily due to the complex nature of telecom standard documents
and the rapid evolution of the field. The paper introduces Telco-RAG, an
open-source RAG framework designed to handle the specific needs of
telecommunications standards, particularly 3rd Generation Partnership Project
(3GPP) documents. Telco-RAG addresses the critical challenges of implementing a
RAG pipeline on highly technical content, paving the way for applying LLMs in
telecommunications and offering guidelines for RAG implementation in other
technical domains.",Andrei-Laurentiu Bornea
2024-04-24T17:59:36Z,http://arxiv.org/abs/2404.16032v2,"Studying Large Language Model Behaviors Under Context-Memory Conflicts
  With Real Documents","Retrieval-augmented generation (RAG) mitigates many problems of fully
parametric language models, such as temporal degradation, hallucinations, and
lack of grounding. In RAG, the model's knowledge can be updated from documents
provided in context. This leads to cases of conflict between the model's
parametric knowledge and the contextual information, where the model may not
always update its knowledge. Previous work studied context-memory knowledge
conflicts by creating synthetic documents that contradict the model's correct
parametric answers. We present a framework for studying such knowledge
conflicts in a realistic setup. We update incorrect parametric knowledge using
real conflicting documents. This reflects how knowledge conflicts arise in
practice. In this realistic scenario, we find that knowledge updates fail less
often than previously reported. In cases where the models still fail to update
their answers, we find a parametric bias: the incorrect parametric answer
appearing in context makes the knowledge update likelier to fail. These results
suggest that the factual parametric knowledge of LLMs can negatively influence
their reading abilities and behaviors. Our code is available at
https://github.com/kortukov/realistic_knowledge_conflicts/ .",Evgenii Kortukov
2024-04-24T19:30:18Z,http://arxiv.org/abs/2404.16160v2,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,"Large language models (LLMs) have demonstrated impressive generalization
capabilities on specific tasks with human-written instruction data. However,
the limited quantity, diversity, and professional expertise of such instruction
data raise concerns about the performance of LLMs in psychotherapy tasks when
provided with domain-specific instructions. To address this, we firstly propose
Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and
secondly, we use an adaption fine-tuning method and retrieval augmented
generation method to improve pre-trained LLMs. Through quantitative evaluation
of linguistic quality using automatic and human evaluation, we observe that
pre-trained LLMs on Psychotherapy Assistant Instructions outperform
state-of-the-art LLMs response baselines. Our Assistant-Instruction approach
offers a half-annotation method to align pre-trained LLMs with instructions and
provide pre-trained LLMs with more psychotherapy knowledge.",Cheng Kang
2024-04-25T13:10:48Z,http://arxiv.org/abs/2404.16587v1,"Understanding Privacy Risks of Embeddings Induced by Large Language
  Models","Large language models (LLMs) show early signs of artificial general
intelligence but struggle with hallucinations. One promising solution to
mitigate these hallucinations is to store external knowledge as embeddings,
aiding LLMs in retrieval-augmented generation. However, such a solution risks
compromising privacy, as recent studies experimentally showed that the original
text can be partially reconstructed from text embeddings by pre-trained
language models. The significant advantage of LLMs over traditional pre-trained
models may exacerbate these concerns. To this end, we investigate the
effectiveness of reconstructing original knowledge and predicting entity
attributes from these embeddings when LLMs are employed. Empirical findings
indicate that LLMs significantly improve the accuracy of two evaluated tasks
over those from pre-trained models, regardless of whether the texts are
in-distribution or out-of-distribution. This underscores a heightened potential
for LLMs to jeopardize user privacy, highlighting the negative consequences of
their widespread use. We further discuss preliminary strategies to mitigate
this risk.",Zhihao Zhu
2024-04-26T07:11:18Z,http://arxiv.org/abs/2404.17196v1,"Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
  Applications","Presently, with the assistance of advanced LLM application development
frameworks, more and more LLM-powered applications can effortlessly augment the
LLMs' knowledge with external content using the retrieval augmented generation
(RAG) technique. However, these frameworks' designs do not have sufficient
consideration of the risk of external content, thereby allowing attackers to
undermine the applications developed with these frameworks. In this paper, we
reveal a new threat to LLM-powered applications, termed retrieval poisoning,
where attackers can guide the application to yield malicious responses during
the RAG process. Specifically, through the analysis of LLM application
frameworks, attackers can craft documents visually indistinguishable from
benign ones. Despite the documents providing correct information, once they are
used as reference sources for RAG, the application is misled into generating
incorrect responses. Our preliminary experiments indicate that attackers can
mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate
in the real-world application, demonstrating the potential impact of retrieval
poisoning.",Quan Zhang
2024-04-26T11:51:53Z,http://arxiv.org/abs/2404.17347v1,InspectorRAGet: An Introspection Platform for RAG Evaluation,"Large Language Models (LLM) have become a popular approach for implementing
Retrieval Augmented Generation (RAG) systems, and a significant amount of
effort has been spent on building good models and metrics. In spite of
increased recognition of the need for rigorous evaluation of RAG systems, few
tools exist that go beyond the creation of model output and automatic
calculation. We present InspectorRAGet, an introspection platform for RAG
evaluation. InspectorRAGet allows the user to analyze aggregate and
instance-level performance of RAG systems, using both human and algorithmic
metrics as well as annotator quality. InspectorRAGet is suitable for multiple
use cases and is available publicly to the community. The demo video is
available at https://youtu.be/MJhe8QIXcEc",Kshitij Fadnis
2024-04-30T19:51:37Z,http://arxiv.org/abs/2405.00175v1,"Towards a Search Engine for Machines: Unified Ranking for Multiple
  Retrieval-Augmented Large Language Models","This paper introduces uRAG--a framework with a unified retrieval engine that
serves multiple downstream retrieval-augmented generation (RAG) systems. Each
RAG system consumes the retrieval results for a unique purpose, such as
open-domain question answering, fact verification, entity linking, and relation
extraction. We introduce a generic training guideline that standardizes the
communication between the search engine and the downstream RAG systems that
engage in optimizing the retrieval model. This lays the groundwork for us to
build a large-scale experimentation ecosystem consisting of 18 RAG systems that
engage in training and 18 unknown RAG systems that use the uRAG as the new
users of the search engine. Using this experimentation ecosystem, we answer a
number of fundamental research questions that improve our understanding of
promises and challenges in developing search engines for machines.",Alireza Salemi
2024-05-02T15:06:18Z,http://arxiv.org/abs/2405.01359v1,GAIA: A General AI Assistant for Intelligent Accelerator Operations,"Large-scale machines like particle accelerators are usually run by a team of
experienced operators. In case of a particle accelerator, these operators
possess suitable background knowledge on both accelerator physics and the
technology comprising the machine. Due to the complexity of the machine,
particular subsystems of the machine are taken care of by experts, who the
operators can turn to. In this work the reasoning and action (ReAct) prompting
paradigm is used to couple an open-weights large language model (LLM) with a
high-level machine control system framework and other tools, e.g. the
electronic logbook or machine design documentation. By doing so, a multi-expert
retrieval augmented generation (RAG) system is implemented, which assists
operators in knowledge retrieval tasks, interacts with the machine directly if
needed, or writes high level control system scripts. This consolidation of
expert knowledge and machine interaction can simplify and speed up machine
operation tasks for both new and experienced human operators.",Frank Mayet
2024-04-28T14:58:55Z,http://arxiv.org/abs/2405.01585v1,"Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular
  RAG Applications","In recent times Large Language Models have exhibited tremendous capabilities,
especially in the areas of mathematics, code generation and general-purpose
reasoning. However for specialized domains especially in applications that
require parsing and analyzing large chunks of numeric or tabular data even
state-of-the-art (SOTA) models struggle. In this paper, we introduce a new
approach to solving domain-specific tabular data analysis tasks by presenting a
unique RAG workflow that mitigates the scalability issues of existing tabular
LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel
approach to fine-tune embedding models for tabular Retrieval-Augmentation
Generation (RAG) applications. Embedding models form a crucial component in the
RAG workflow and even current SOTA embedding models struggle as they are
predominantly trained on textual datasets and thus underperform in scenarios
involving complex tabular data. The evaluation results showcase that our
approach not only outperforms current SOTA embedding models in this domain but
also does so with a notably smaller and more efficient model structure.",Sujit Khanna
2024-05-08T04:07:38Z,http://arxiv.org/abs/2405.06697v1,Automated Conversion of Static to Dynamic Scheduler via Natural Language,"In this paper, we explore the potential application of Large Language Models
(LLMs) that will automatically model constraints and generate code for dynamic
scheduling problems given an existing static model. Static scheduling problems
are modelled and coded by optimization experts. These models may be easily
obsoleted as the underlying constraints may need to be fine-tuned in order to
reflect changes in the scheduling rules. Furthermore, it may be necessary to
turn a static model into a dynamic one in order to cope with disturbances in
the environment. In this paper, we propose a Retrieval-Augmented Generation
(RAG) based LLM model to automate the process of implementing constraints for
Dynamic Scheduling (RAGDyS), without seeking help from an optimization modeling
expert. Our framework aims to minimize technical complexities related to
mathematical modelling and computational workload for end-users, thereby
allowing end-users to quickly obtain a new schedule close to the original
schedule with changes reflected by natural language constraint descriptions.",Paul Mingzheng Tang
2024-05-15T07:48:10Z,http://arxiv.org/abs/2405.09161v2,"Exploring the Potential of Large Language Models for Automation in
  Technical Customer Service","Purpose: The purpose of this study is to investigate the potential of Large
Language Models (LLMs) in transforming technical customer service (TCS) through
the automation of cognitive tasks. Design/Methodology/Approach: Using a
prototyping approach, the research assesses the feasibility of automating
cognitive tasks in TCS with LLMs, employing real-world technical incident data
from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks
such as translation, summarization, and content generation can be effectively
automated with LLMs like GPT-4, while higher-level tasks such as reasoning
require more advanced technological approaches such as Retrieval-Augmented
Generation (RAG) or finetuning ; furthermore, the study underscores the
significance of data ecosystems in enabling more complex cognitive tasks by
fostering data sharing among various actors involved. Originality/Value: This
study contributes to the emerging theory on LLM potential and technical
feasibility in service management, providing concrete insights for operators of
TCS units and highlighting the need for further research to address limitations
and validate the applicability of LLMs across different domains.",Jochen Wulf
2024-05-16T10:53:31Z,http://arxiv.org/abs/2405.09980v1,FinTextQA: A Dataset for Long-form Financial Question Answering,"Accurate evaluation of financial question answering (QA) systems necessitates
a comprehensive dataset encompassing diverse question types and contexts.
However, current financial QA datasets lack scope diversity and question
complexity. This work introduces FinTextQA, a novel dataset for long-form
question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,
source-attributed QA pairs extracted and selected from finance textbooks and
government agency websites.Moreover, we developed a Retrieval-Augmented
Generation (RAG)-based LFQA system, comprising an embedder, retriever,
reranker, and generator. A multi-faceted evaluation approach, including human
ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the
performance of different LFQA system configurations under heightened noisy
conditions. The results indicate that: (1) Among all compared generators,
Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The
most effective system configuration on our dataset involved setting the
embedder, retriever, reranker, and generator as Ada2, Automated Merged
Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are
less susceptible to noise after the length of contexts reaching a specific
threshold.",Jian Chen
2024-05-20T14:03:05Z,http://arxiv.org/abs/2405.12035v1,KG-RAG: Bridging the Gap Between Knowledge and Creativity,"Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.",Diego Sanmartin
2024-05-20T20:27:00Z,http://arxiv.org/abs/2405.12363v2,Question-Based Retrieval using Atomic Units for Enterprise RAG,"Enterprise retrieval augmented generation (RAG) offers a highly flexible
framework for combining powerful large language models (LLMs) with internal,
possibly temporally changing, documents. In RAG, documents are first chunked.
Relevant chunks are then retrieved for a user query, which are passed as
context to a synthesizer LLM to generate the query response. However, the
retrieval step can limit performance, as incorrect chunks can lead the
synthesizer LLM to generate a false response. This work applies a zero-shot
adaptation of standard dense retrieval steps for more accurate chunk recall.
Specifically, a chunk is first decomposed into atomic statements. A set of
synthetic questions are then generated on these atoms (with the chunk as the
context). Dense retrieval involves finding the closest set of synthetic
questions, and associated chunks, to the user query. It is found that retrieval
with the atoms leads to higher recall than retrieval with chunks. Further
performance gain is observed with retrieval using the synthetic questions
generated over the atoms. Higher recall at the retrieval step enables higher
performance of the enterprise LLM using the RAG pipeline.",Vatsal Raina
2024-05-23T11:00:19Z,http://arxiv.org/abs/2405.14431v1,RaFe: Ranking Feedback Improves Query Rewriting for RAG,"As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)
techniques have evolved, query rewriting has been widely incorporated into the
RAG system for downstream tasks like open-domain QA. Many works have attempted
to utilize small models with reinforcement learning rather than costly LLMs to
improve query rewriting. However, current methods require annotations (e.g.,
labeled relevant documents or downstream answers) or predesigned rewards for
feedback, which lack generalization, and fail to utilize signals tailored for
query rewriting. In this paper, we propose ours, a framework for training query
rewriting models free of annotations. By leveraging a publicly available
reranker, ours~provides feedback aligned well with the rewriting objectives.
Experimental results demonstrate that ours~can obtain better performance than
baselines.",Shengyu Mao
2024-05-23T20:04:54Z,http://arxiv.org/abs/2405.15028v1,AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings,"Ranking is a fundamental and popular problem in search. However, existing
ranking algorithms usually restrict the granularity of ranking to full passages
or require a specific dense index for each desired level of granularity. Such
lack of flexibility in granularity negatively affects many applications that
can benefit from more granular ranking, such as sentence-level ranking for
open-domain question-answering, or proposition-level ranking for attribution.
In this work, we introduce the idea of any-granularity ranking, which leverages
multi-vector embeddings to rank at varying levels of granularity while
maintaining encoding at a single (coarser) level of granularity. We propose a
multi-granular contrastive loss for training multi-vector approaches, and
validate its utility with both sentences and propositions as ranking units.
Finally, we demonstrate the application of proposition-level ranking to
post-hoc citation addition in retrieval-augmented generation, surpassing the
performance of prompt-driven citation generation.",Revanth Gangi Reddy
2024-05-24T13:44:25Z,http://arxiv.org/abs/2405.15556v1,Certifiably Robust RAG against Retrieval Corruption,"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval
corruption attacks: an attacker can inject malicious passages into retrieval
results to induce inaccurate responses. In this paper, we propose RobustRAG as
the first defense framework against retrieval corruption attacks. The key
insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM
responses from each passage in isolation and then securely aggregate these
isolated responses. To instantiate RobustRAG, we design keyword-based and
decoding-based algorithms for securely aggregating unstructured text responses.
Notably, RobustRAG can achieve certifiable robustness: we can formally prove
and certify that, for certain queries, RobustRAG can always return accurate
responses, even when the attacker has full knowledge of our defense and can
arbitrarily inject a small number of malicious passages. We evaluate RobustRAG
on open-domain QA and long-form text generation datasets and demonstrate its
effectiveness and generalizability across various tasks and datasets.",Chong Xiang
2024-05-25T05:45:55Z,http://arxiv.org/abs/2405.16072v4,"SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS
  Design Generation","In this paper, we introduce SynthAI, a new method for the automated creation
of High-Level Synthesis (HLS) designs. SynthAI integrates ReAct agents,
Chain-of-Thought (CoT) prompting, web search technologies, and the
Retrieval-Augmented Generation (RAG) framework within a structured decision
graph. This innovative approach enables the systematic decomposition of complex
hardware design tasks into multiple stages and smaller, manageable modules. As
a result, SynthAI produces synthesizable designs that closely adhere to
user-specified design objectives and functional requirements. We further
validate the capabilities of SynthAI through several case studies, highlighting
its proficiency in generating complex, multi-module logic designs from a single
initial prompt. The SynthAI code is provided via the following repo:
\url{https://github.com/sarashs/FPGA_AGI}",Seyed Arash Sheikholeslam
2024-05-27T11:18:25Z,http://arxiv.org/abs/2405.17053v2,"WirelessLLM: Empowering Large Language Models Towards Wireless
  Intelligence","The rapid evolution of wireless technologies and the growing complexity of
network infrastructures necessitate a paradigm shift in how communication
networks are designed, configured, and managed. Recent advancements in Large
Language Models (LLMs) have sparked interest in their potential to
revolutionize wireless communication systems. However, existing studies on LLMs
for wireless systems are limited to a direct application for telecom language
understanding. To empower LLMs with knowledge and expertise in the wireless
domain, this paper proposes WirelessLLM, a comprehensive framework for adapting
and enhancing LLMs to address the unique challenges and requirements of
wireless communication networks. We first identify three foundational
principles that underpin WirelessLLM: knowledge alignment, knowledge fusion,
and knowledge evolution. Then, we investigate the enabling technologies to
build WirelessLLM, including prompt engineering, retrieval augmented
generation, tool usage, multi-modal pre-training, and domain-specific
fine-tuning. Moreover, we present three case studies to demonstrate the
practical applicability and benefits of WirelessLLM for solving typical
problems in wireless networks. Finally, we conclude this paper by highlighting
key challenges and outlining potential avenues for future research.",Jiawei Shao
2024-05-27T13:16:29Z,http://arxiv.org/abs/2405.17147v1,"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability","The rapid advancement of Large Language Models (LLMs) has significantly
impacted human-computer interaction, epitomized by the release of GPT-4o, which
introduced comprehensive multi-modality capabilities. In this paper, we first
explored the deployment strategies, economic considerations, and sustainability
challenges associated with the state-of-the-art LLMs. More specifically, we
discussed the deployment debate between Retrieval-Augmented Generation (RAG)
and fine-tuning, highlighting their respective advantages and limitations.
After that, we quantitatively analyzed the requirement of xPUs in training and
inference. Additionally, for the tokenomics of LLM services, we examined the
balance between performance and cost from the quality of experience (QoE)'s
perspective of end users. Lastly, we envisioned the future hybrid architecture
of LLM processing and its corresponding sustainability concerns, particularly
in the environmental carbon footprint impact. Through these discussions, we
provided a comprehensive overview of the operational and strategic
considerations essential for the responsible development and deployment of
LLMs.",Haiwei Dong
2024-05-27T18:40:49Z,http://arxiv.org/abs/2405.17587v2,RAGSys: Item-Cold-Start Recommender as RAG System,"Large Language Models (LLM) hold immense promise for real-world applications,
but their generic knowledge often falls short of domain-specific needs.
Fine-tuning, a common approach, can suffer from catastrophic forgetting and
hinder generalizability. In-Context Learning (ICL) offers an alternative, which
can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant
demonstrations for few-shot learning tasks. This paper explores the desired
qualities of a demonstration retrieval system for ICL. We argue that ICL
retrieval in this context resembles item-cold-start recommender systems,
prioritizing discovery and maximizing information gain over strict relevance.
We propose a novel evaluation method that measures the LLM's subsequent
performance on NLP tasks, eliminating the need for subjective diversity scores.
Our findings demonstrate the critical role of diversity and quality bias in
retrieved demonstrations for effective ICL, and highlight the potential of
recommender system techniques in this domain.",Emile Contal
2024-05-28T17:56:46Z,http://arxiv.org/abs/2405.18414v1,Don't Forget to Connect! Improving RAG with Graph-based Reranking,"Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.",Jialin Dong
2024-05-29T01:12:53Z,http://arxiv.org/abs/2405.18682v2,"Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical
  Machine Reading Comprehension","Large language models (LLMs) have shown remarkable performance on many tasks
in different domains. However, their performance in closed-book biomedical
machine reading comprehension (MRC) has not been evaluated in depth. In this
work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We
experiment with different conventional prompting techniques as well as
introduce our own novel prompting method. To solve some of the retrieval
problems inherent to LLMs, we propose a prompting strategy named Implicit
Retrieval Augmented Generation (RAG) that alleviates the need for using vector
databases to retrieve important chunks in traditional RAG setups. Moreover, we
report qualitative assessments on the natural language generation outputs from
our approach. The results show that our new prompting technique is able to get
the best performance in two out of four datasets and ranks second in rest of
them. Experiments show that modern-day LLMs like GPT even in a zero-shot
setting can outperform supervised models, leading to new state-of-the-art
(SoTA) results on two of the benchmarks.",Shubham Vatsal
2024-05-29T15:47:57Z,http://arxiv.org/abs/2405.19207v1,A Multi-Source Retrieval Question Answering Framework Based on RAG,"With the rapid development of large-scale language models,
Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing
RAG paradigms are inevitably influenced by erroneous retrieval information,
thereby reducing the reliability and correctness of generated results.
Therefore, to improve the relevance of retrieval information, this study
proposes a method that replaces traditional retrievers with GPT-3.5, leveraging
its vast corpus knowledge to generate retrieval information. We also propose a
web retrieval based method to implement fine-grained knowledge retrieval,
Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic
partitioning of problem.In order to mitigate the illusion of GPT retrieval and
reduce noise in Web retrieval,we proposes a multi-source retrieval framework,
named MSRAG, which combines GPT retrieval with web retrieval. Experiments on
multiple knowledge-intensive QA datasets demonstrate that the proposed
framework in this study performs better than existing RAG framework in
enhancing the overall efficiency and accuracy of QA systems.",Ridong Wu
2024-05-26T06:45:39Z,http://arxiv.org/abs/2405.19366v2,"ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with
  LLM-Enhanced Cardiological Text","The utilization of deep learning on electrocardiogram (ECG) analysis has
brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.
By leveraging the capabilities of deep learning in semantic understanding,
especially in feature extraction and representation learning, this study
introduces a new multimodal contrastive pretaining framework that aims to
improve the quality and robustness of learned representations of 12-lead ECG
signals. Our framework comprises two key components, including Cardio Query
Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a
retrieval-augmented generation (RAG) pipeline to leverage large language models
(LLMs) and external medical knowledge to generate detailed textual descriptions
of ECGs. The generated text is enriched with information about demographics and
waveform patterns. ESI integrates both contrastive and captioning loss to
pretrain ECG encoders for enhanced representations. We validate our approach
through various downstream tasks, including arrhythmia detection and ECG-based
subject identification. Our experimental results demonstrate substantial
improvements over strong baselines in these tasks. These baselines encompass
supervised and self-supervised learning methods, as well as prior multimodal
pretraining approaches.",Han Yu
