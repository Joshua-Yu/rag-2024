Published Date,Link,Title,Summary,First Author
2024-05-29T23:11:53Z,http://arxiv.org/abs/2405.19563v1,Unlearning Climate Misinformation in Large Language Models,"Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.",Michael Fore
2024-05-30T03:44:54Z,http://arxiv.org/abs/2405.19670v4,"One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for
  Retrieval-Augmented Large Language Models","Retrieval-augmented generation (RAG) is a promising way to improve large
language models (LLMs) for generating more factual, accurate, and up-to-date
content. Existing methods either optimize prompts to guide LLMs in leveraging
retrieved information or directly fine-tune LLMs to adapt to RAG scenarios.
Although fine-tuning can yield better performance, it often compromises the
LLMs' general generation capabilities by modifying their parameters. This
limitation poses challenges in practical applications, especially when LLMs are
already deployed, as parameter adjustments may affect their original
functionality. To address this, we propose a novel method that involves
learning scalable and pluggable virtual tokens for RAG. By maintaining the
LLMs' original parameters and fine-tuning only the embeddings of these
pluggable tokens, our approach not only enhances LLMs' performance but also
preserves their general generation capabilities. Furthermore, we design several
training strategies to improve the scalability, flexibility, and
generalizability of our method. Comprehensive experiments across 12
question-answering tasks demonstrate the superiority of our approach.",Yutao Zhu
2024-05-27T17:55:36Z,http://arxiv.org/abs/2406.00041v2,"QUB-Cirdan at ""Discharge Me!"": Zero shot discharge letter generation by
  open-source LLM","The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to
reduce the administrative burden on clinicians by automating the creation of
critical sections of patient discharge letters. This paper presents our
approach using the Llama3 8B quantized model to generate the ""Brief Hospital
Course"" and ""Discharge Instructions"" sections. We employ a zero-shot method
combined with Retrieval-Augmented Generation (RAG) to produce concise,
contextually accurate summaries. Our contributions include the development of a
curated template-based approach to ensure reliability and consistency, as well
as the integration of RAG for word count prediction. We also describe several
unsuccessful experiments to provide insights into our pathway for the
competition. Our results demonstrate the effectiveness and efficiency of our
approach, achieving high scores across multiple evaluation metrics.",Rui Guo
2024-06-02T06:48:43Z,http://arxiv.org/abs/2406.00638v1,"COS-Mix: Cosine Similarity and Distance Fusion for Improved Information
  Retrieval","This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented
Generation (RAG) that integrates cosine similarity and cosine distance measures
to improve retrieval performance, particularly for sparse data. The traditional
cosine similarity measure is widely used to capture the similarity between
vectors in high-dimensional spaces. However, it has been shown that this
measure can yield arbitrary results in certain scenarios. To address this
limitation, we incorporate cosine distance measures to provide a complementary
perspective by quantifying the dissimilarity between vectors. Our approach is
experimented on proprietary data, unlike recent publications that have used
open-source datasets. The proposed method demonstrates enhanced retrieval
performance and provides a more comprehensive understanding of the semantic
relationships between documents or items. This hybrid strategy offers a
promising solution for efficiently and accurately retrieving relevant
information in knowledge-intensive applications, leveraging techniques such as
BM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based
retrieval to facilitate efficient information retrieval.",Kush Juvekar
2024-06-03T04:14:21Z,http://arxiv.org/abs/2406.00975v2,"Luna: An Evaluation Foundation Model to Catch Language Model
  Hallucinations with High Accuracy and Low Cost","Retriever Augmented Generation (RAG) systems have become pivotal in enhancing
the capabilities of language models by incorporating external knowledge
retrieval mechanisms. However, a significant challenge in deploying these
systems in industry applications is the detection and mitigation of
hallucinations: instances where the model generates information that is not
grounded in the retrieved context. Addressing this issue is crucial for
ensuring the reliability and accuracy of responses generated by large language
models (LLMs) in diverse industry settings. Current hallucination detection
techniques fail to deliver accuracy, low latency, and low cost simultaneously.
We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination
detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and
commercial evaluation frameworks on the hallucination detection task, with 97%
and 91% reduction in cost and latency, respectively. Luna is lightweight and
generalizes across multiple industry verticals and out-of-domain data, making
it an ideal candidate for industry LLM applications.",Masha Belyi
2024-06-03T06:55:10Z,http://arxiv.org/abs/2406.01045v1,"Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs","Large Language Models (LLMs) demonstrate significant capabilities in
processing natural language data, promising efficient knowledge extraction from
diverse textual sources to enhance situational awareness and support
decision-making. However, concerns arise due to their susceptibility to
hallucination, resulting in contextually inaccurate content. This work focuses
on harnessing LLMs for automated Event Extraction, introducing a new method to
address hallucination by decomposing the task into Event Detection and Event
Argument Extraction. Moreover, the proposed method integrates dynamic
schema-aware augmented retrieval examples into prompts tailored for each
specific inquiry, thereby extending and adapting advanced prompting techniques
such as Retrieval-Augmented Generation. Evaluation findings on prominent event
extraction benchmarks and results from a synthesized benchmark illustrate the
method's superior performance compared to baseline approaches.",Fatemeh Shiri
2024-06-03T12:39:04Z,http://arxiv.org/abs/2406.01273v2,SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper introduces SoccerRAG, an innovative framework designed to
harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. Our evaluations indicate that SoccerRAG
effectively handles complex queries, offering significant improvements over
traditional retrieval systems in terms of accuracy and user engagement. The
results underscore the potential of using RAG and LLMs in sports analytics,
paving the way for future advancements in the accessibility and real-time
processing of sports data.",Aleksander Theo Strand
2024-06-03T12:48:38Z,http://arxiv.org/abs/2406.01280v2,Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG,"The rapid evolution of digital sports media necessitates sophisticated
information retrieval systems that can efficiently parse extensive multimodal
datasets. This paper demonstrates SoccerRAG, an innovative framework designed
to harness the power of Retrieval Augmented Generation (RAG) and Large Language
Models (LLMs) to extract soccer-related information through natural language
queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic
querying and automatic data validation, enhancing user interaction and
accessibility to sports archives. We present a novel interactive user interface
(UI) based on the Chainlit framework which wraps around the core functionality,
and enable users to interact with the SoccerRAG framework in a chatbot-like
visual manner.",Aleksander Theo Strand
2024-06-03T20:18:56Z,http://arxiv.org/abs/2406.01768v1,"TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP
  Specifications","Understanding telecom standards involves sorting through numerous technical
documents, such as those produced by the 3rd Generation Partnership Project
(3GPP), which is time-consuming and labor-intensive. While large language
models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive
dataset is crucial for their effective pre-training and fine-tuning. In this
paper, we introduce \textit{TSpec-LLM}, an open-source comprehensive dataset
covering all 3GPP documents from Release 8 to Release 19 (1999--2023). To
evaluate its efficacy, we first select a representative sample of 3GPP
documents, create corresponding technical questions, and assess the baseline
performance of various LLMs. We then incorporate a retrieval-augmented
generation (RAG) framework to enhance LLM capabilities by retrieving relevant
context from the \textit{TSpec-LLM} dataset. Our evaluation shows that using a
naive-RAG framework on \textit{TSpec-LLM} improves the accuracy of GPT-3.5,
Gemini 1.0 Pro, and GPT-4 from 44\%, 46\%, and 51\% to 71\%, 75\%, and 72\%,
respectively.",Rasoul Nikbakht
2024-06-04T16:42:17Z,http://arxiv.org/abs/2406.02472v1,"Analyzing Temporal Complex Events with Large Language Models? A
  Benchmark towards Temporal, Long Context Understanding","The digital landscape is rapidly evolving with an ever-increasing volume of
online news, emphasizing the need for swift and precise analysis of complex
events. We refer to the complex events composed of many news articles over an
extended period as Temporal Complex Event (TCE). This paper proposes a novel
approach using Large Language Models (LLMs) to systematically extract and
analyze the event chain within TCE, characterized by their key points and
timestamps. We establish a benchmark, named TCELongBench, to evaluate the
proficiency of LLMs in handling temporal dynamics and understanding extensive
text. This benchmark encompasses three distinct tasks - reading comprehension,
temporal sequencing, and future event forecasting. In the experiment, we
leverage retrieval-augmented generation (RAG) method and LLMs with long context
window to deal with lengthy news articles of TCE. Our findings indicate that
models with suitable retrievers exhibit comparable performance with those
utilizing long context window.",Zhihan Zhang
2024-06-10T16:46:22Z,http://arxiv.org/abs/2406.06458v1,"Evaluating the Retrieval Component in LLM-Based Question Answering
  Systems","Question answering systems (QA) utilizing Large Language Models (LLMs)
heavily depend on the retrieval component to provide them with domain-specific
information and reduce the risk of generating inaccurate responses or
hallucinations. Although the evaluation of retrievers dates back to the early
research in Information Retrieval, assessing their performance within LLM-based
chatbots remains a challenge.
  This study proposes a straightforward baseline for evaluating retrievers in
Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate
that this evaluation framework provides a better image of how the retriever
performs and is more aligned with the overall performance of the QA system.
Although conventional metrics such as precision, recall, and F1 score may not
fully capture LLMs' capabilities - as they can yield accurate responses despite
imperfect retrievers - our method considers LLMs' strengths to ignore
irrelevant contexts, as well as potential errors and hallucinations in their
responses.",Ashkan Alinejad
2024-06-03T19:40:28Z,http://arxiv.org/abs/2406.06575v1,"Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and
  Abbreviation De-hallucination","Electronic design engineers are challenged to find relevant information
efficiently for a myriad of tasks within design construction, verification and
technology development. Large language models (LLM) have the potential to help
improve productivity by serving as conversational agents that effectively
function as subject-matter experts. In this paper we demonstrate Ask-EDA, a
chat agent designed to serve as a 24x7 expert available to provide guidance to
design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation
(RAG) and abbreviation de-hallucination (ADH) techniques to deliver more
relevant and accurate responses. We curated three evaluation datasets, namely
q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct
aspect: general design question answering, design command handling and
abbreviation resolution. We demonstrated that hybrid RAG offers over a 40%
improvement in Recall on the q2a-100 dataset and over a 60% improvement on the
cmds-100 dataset compared to not using RAG, while ADH yields over a 70%
enhancement in Recall on the abbr-100 dataset. The evaluation results show that
Ask-EDA can effectively respond to design-related inquiries.",Luyao Shi
2024-06-11T13:36:19Z,http://arxiv.org/abs/2406.07257v1,"Scholarly Question Answering using Large Language Models in the
  NFDI4DataScience Gateway","This paper introduces a scholarly Question Answering (QA) system on top of
the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based
(RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a
unified and intuitive interface for querying various scientific databases using
federated search. The RAG-based scholarly QA, powered by a Large Language Model
(LLM), facilitates dynamic interaction with search results, enhancing filtering
capabilities and fostering a conversational engagement with the Gateway search.
The effectiveness of both the Gateway and the scholarly QA system is
demonstrated through experimental analysis.",Hamed Babaei Giglou
2024-06-12T07:55:32Z,http://arxiv.org/abs/2406.07973v2,"Unique Security and Privacy Threats of Large Language Model: A
  Comprehensive Survey","With the rapid development of artificial intelligence, large language models
(LLMs) have made remarkable advancements in natural language processing. These
models are trained on vast datasets to exhibit powerful language understanding
and generation capabilities across various applications, including machine
translation, chatbots, and agents. However, LLMs have revealed a variety of
privacy and security issues throughout their life cycle, drawing significant
academic and industrial attention. Moreover, the risks faced by LLMs differ
significantly from those encountered by traditional language models. Given that
current surveys lack a clear taxonomy of unique threat models across diverse
scenarios, we emphasize the unique privacy and security threats associated with
five specific scenarios: pre-training, fine-tuning, retrieval-augmented
generation systems, deployment, and LLM-based agents. Addressing the
characteristics of each risk, this survey outlines potential threats and
countermeasures. Research on attack and defense situations can offer feasible
research directions, enabling more areas to benefit from LLMs.",Shang Wang
2024-06-12T08:26:30Z,http://arxiv.org/abs/2406.07990v1,"Blowfish: Topological and statistical signatures for quantifying
  ambiguity in semantic search","This works reports evidence for the topological signatures of ambiguity in
sentence embeddings that could be leveraged for ranking and/or explanation
purposes in the context of vector search and Retrieval Augmented Generation
(RAG) systems. We proposed a working definition of ambiguity and designed an
experiment where we have broken down a proprietary dataset into collections of
chunks of varying size - 3, 5, and 10 lines and used the different collections
successively as queries and answers sets. It allowed us to test the signatures
of ambiguity with removal of confounding factors. Our results show that proxy
ambiguous queries (size 10 queries against size 3 documents) display different
distributions of homologies 0 and 1 based features than proxy clear queries
(size 5 queries against size 10 documents). We then discuss those results in
terms increased manifold complexity and/or approximately discontinuous
embedding submanifolds. Finally we propose a strategy to leverage those
findings as a new scoring strategy of semantic similarities.",Thomas Roland Barillot
2024-06-14T08:21:42Z,http://arxiv.org/abs/2406.09818v3,"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from
  Corporate Climate Disclosures","To handle the vast amounts of qualitative data produced in corporate climate
communication, stakeholders increasingly rely on Retrieval Augmented Generation
(RAG) systems. However, a significant gap remains in evaluating domain-specific
information retrieval - the basis for answer generation. To address this
challenge, this work simulates the typical tasks of a sustainability analyst by
examining 30 sustainability reports with 16 detailed climate-related questions.
As a result, we obtain a dataset with over 8.5K unique question-source-answer
pairs labeled by different levels of relevance. Furthermore, we develop a use
case with the dataset to investigate the integration of expert knowledge into
information retrieval with embeddings. Although we show that incorporating
expert knowledge works, we also outline the critical limitations of embeddings
in knowledge-intensive downstream domains like climate change communication.",Tobias Schimanski
2024-06-14T12:41:07Z,http://arxiv.org/abs/2406.09979v2,HIRO: Hierarchical Information Retrieval Optimization,"Retrieval-Augmented Generation (RAG) has revolutionized natural language
processing by dynamically integrating external knowledge into Large Language
Models (LLMs), addressing their limitation of static training datasets. Recent
implementations of RAG leverage hierarchical data structures, which organize
documents at various levels of summarization and information density. This
complexity, however, can cause LLMs to ""choke"" on information overload,
necessitating more sophisticated querying mechanisms. In this context, we
introduce Hierarchical Information Retrieval Optimization (HIRO), a novel
querying approach that employs a Depth-First Search (DFS)-based recursive
similarity score calculation and branch pruning. This method uniquely minimizes
the context delivered to the LLM without informational loss, effectively
managing the challenge of excessive data. HIRO's refined approach is validated
by a 10.85% improvement in performance on the NarrativeQA dataset.",Krish Goel
2024-06-11T02:37:06Z,http://arxiv.org/abs/2406.10263v1,"A Lightweight Framework for Adaptive Retrieval In Code Completion With
  Critique Model","Recent advancements in Retrieval-Augmented Generation have significantly
enhanced code completion at the repository level. Various RAG-based code
completion systems are proposed based on different design choices. For
instance, gaining more effectiveness at the cost of repeating the
retrieval-generation process multiple times. However, the indiscriminate use of
retrieval in current methods reveals issues in both efficiency and
effectiveness, as a considerable portion of retrievals are unnecessary and may
introduce unhelpful or even harmful suggestions to code language models. To
address these challenges, we introduce CARD, a lightweight critique method
designed to provide insights into the necessity of retrievals and select the
optimal answer from multiple predictions. CARD can seamlessly integrate into
any RAG-based code completion system. Our evaluation shows that CARD saves 21%
to 46% times of retrieval for Line completion, 14% to 40% times of retrieval
for API completion, and 6% to 46.5% times of retrieval for function completion
respectively, while improving the accuracy. CARD reduces latency ranging from
16% to 83%. CARD is generalizable to different LMs, retrievers, and programming
languages. It is lightweight with training in few seconds and inference in few
milliseconds.",Wenrui Zhang
2024-06-17T03:29:14Z,http://arxiv.org/abs/2406.11177v1,TIFG: Text-Informed Feature Generation with Large Language Models,"Textual information of data is of vital importance for data mining and
feature engineering. However, existing methods focus on learning the data
structures and overlook the textual information along with the data.
Consequently, they waste this valuable resource and miss out on the deeper data
relationships embedded within the texts. In this paper, we introduce
Text-Informed Feature Generation (TIFG), a novel LLM-based text-informed
feature generation framework. TIFG utilizes the textual information to generate
features by retrieving possible relevant features within external knowledge
with Retrieval Augmented Generation (RAG) technology. In this approach, the
TIFG can generate new explainable features to enrich the feature space and
further mine feature relationships. We design the TIFG to be an automated
framework that continuously optimizes the feature generation process, adapts to
new data inputs, and improves downstream task performance over iterations. A
broad range of experiments in various downstream tasks showcases that our
approach can generate high-quality and meaningful features, and is
significantly superior to existing methods.",Xinhao Zhang
2024-06-17T07:52:42Z,http://arxiv.org/abs/2406.11290v1,"Iterative Utility Judgment Framework via LLMs Inspired by Relevance in
  Philosophy","Utility and topical relevance are critical measures in information retrieval
(IR), reflecting system and user perspectives, respectively. While topical
relevance has long been emphasized, utility is a higher standard of relevance
and is more useful for facilitating downstream tasks, e.g., in
Retrieval-Augmented Generation (RAG). When we incorporate utility judgments
into RAG, we realize that the topical relevance, utility, and answering in RAG
are closely related to the three types of relevance that Schutz discussed from
a philosophical perspective. They are topical relevance, interpretational
relevance, and motivational relevance, respectively. Inspired by the dynamic
iterations of the three types of relevance, we propose an Iterative utiliTy
judgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We
conducted extensive experiments on multi-grade passage retrieval and factoid
question-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental
results demonstrate significant improvements in utility judgments, ranking of
topical relevance, and answer generation upon representative baselines,
including multiple single-shot utility judging approaches. Our code and
benchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.",Hengran Zhang
2024-06-17T11:22:25Z,http://arxiv.org/abs/2406.11424v1,"Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG
  Systems: A Comparative Study of Performance and Scalability","This paper presents an analysis of open-source large language models (LLMs)
and their application in Retrieval-Augmented Generation (RAG) tasks, specific
for enterprise-specific data sets scraped from their websites. With the
increasing reliance on LLMs in natural language processing, it is crucial to
evaluate their performance, accessibility, and integration within specific
organizational contexts. This study examines various open-source LLMs, explores
their integration into RAG frameworks using enterprise-specific data, and
assesses the performance of different open-source embeddings in enhancing the
retrieval and generation process. Our findings indicate that open-source LLMs,
combined with effective embedding techniques, can significantly improve the
accuracy and efficiency of RAG systems, offering a viable alternative to
proprietary solutions for enterprises.",Gautam B
2024-06-17T13:01:12Z,http://arxiv.org/abs/2406.11497v3,"CrAM: Credibility-Aware Attention Modification in LLMs for Combating
  Misinformation in RAG","Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of ""credibility-aware RAG"", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen1.5-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.",Boyi Deng
2024-06-17T20:14:16Z,http://arxiv.org/abs/2406.12069v2,Satyrn: A Platform for Analytics Augmented Generation,"Large language models (LLMs) are capable of producing documents, and
retrieval augmented generation (RAG) has shown itself to be a powerful method
for improving accuracy without sacrificing fluency. However, not all
information can be retrieved from text. We propose an approach that uses the
analysis of structured data to generate fact sets that are used to guide
generation in much the same way that retrieved documents are used in RAG. This
analytics augmented generation (AAG) approach supports the ability to utilize
standard analytic techniques to generate facts that are then converted to text
and passed to an LLM. We present a neurosymbolic platform, Satyrn, that
leverages AAG to produce accurate, fluent, and coherent reports grounded in
large scale databases. In our experiments, we find that Satyrn generates
reports in which over 86% of claims are accurate while maintaining high levels
of fluency and coherence, even when using smaller language models such as
Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims
are accurate.",Marko Sterbentz
2024-06-18T00:41:41Z,http://arxiv.org/abs/2406.12169v1,"Intermediate Distillation: Data-Efficient Distillation from Black-Box
  LLMs for Information Retrieval","Recent research has explored distilling knowledge from large language models
(LLMs) to optimize retriever models, especially within the retrieval-augmented
generation (RAG) framework. However, most existing training methods rely on
extracting supervision signals from LLMs' weights or their output
probabilities, which is not only resource-intensive but also incompatible with
black-box LLMs. In this paper, we introduce \textit{Intermediate Distillation},
a data-efficient knowledge distillation training scheme that treats LLMs as
black boxes and distills their knowledge via an innovative LLM-ranker-retriever
pipeline, solely using LLMs' ranking generation as the supervision signal.
Extensive experiments demonstrate that our proposed method can significantly
improve the performance of retriever models with only 1,000 training instances.
Moreover, our distilled retriever model significantly boosts performance in
question-answering tasks within the RAG framework, demonstrating the potential
of LLMs to economically and effectively train smaller models.",Zizhong Li
2024-06-18T06:54:28Z,http://arxiv.org/abs/2406.12331v1,"Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text
  Understanding","Current Large Language Models (LLMs) face inherent limitations due to their
pre-defined context lengths, which impede their capacity for multi-hop
reasoning within extensive textual contexts. While existing techniques like
Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by
sourcing external information, they fall short when direct answers are not
readily available. We introduce a novel approach that re-imagines information
retrieval through dynamic in-context editing, inspired by recent breakthroughs
in knowledge editing. By treating lengthy contexts as malleable external
knowledge, our method interactively gathers and integrates relevant
information, thereby enabling LLMs to perform sophisticated reasoning steps.
Experimental results demonstrate that our method effectively empowers
context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with
improved performance, which outperforms state-of-the-art context window
extrapolation methods and even compares favorably to more advanced commercial
long-context models. Our interactive method not only enhances reasoning
capabilities but also mitigates the associated training and computational
costs, making it a pragmatic solution for enhancing LLMs' reasoning within
expansive contexts.",Weizhi Fei
2024-06-18T17:22:48Z,http://arxiv.org/abs/2406.12806v1,"Identifying Performance-Sensitive Configurations in Software Systems
  through Code Analysis with LLM Agents","Configuration settings are essential for tailoring software behavior to meet
specific performance requirements. However, incorrect configurations are
widespread, and identifying those that impact system performance is challenging
due to the vast number and complexity of possible settings. In this work, we
present PerfSense, a lightweight framework that leverages Large Language Models
(LLMs) to efficiently identify performance-sensitive configurations with
minimal overhead. PerfSense employs LLM agents to simulate interactions between
developers and performance engineers using advanced prompting techniques such
as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of
seven open-source Java systems demonstrates that PerfSense achieves an average
accuracy of 64.77% in classifying performance-sensitive configurations,
outperforming both our LLM baseline (50.36%) and the previous state-of-the-art
method (61.75%). Notably, our prompt chaining technique improves recall by 10%
to 30% while maintaining similar precision levels. Additionally, a manual
analysis of 362 misclassifications reveals common issues, including LLMs'
misunderstandings of requirements (26.8%). In summary, PerfSense significantly
reduces manual effort in classifying performance-sensitive configurations and
offers valuable insights for future LLM-based code analysis research.",Zehao Wang
2024-06-18T17:46:08Z,http://arxiv.org/abs/2406.12824v1,"From RAGs to rich parameters: Probing how language models utilize
  external knowledge over parametric information for factual queries","Retrieval Augmented Generation (RAG) enriches the ability of language models
to reason using external context to augment responses for a given user prompt.
This approach has risen in popularity due to practical applications in various
applications of language models in search, question/answering, and chat-bots.
However, the exact nature of how this approach works isn't clearly understood.
In this paper, we mechanistically examine the RAG pipeline to highlight that
language models take shortcut and have a strong bias towards utilizing only the
context information to answer the question, while relying minimally on their
parametric memory. We probe this mechanistic behavior in language models with:
(i) Causal Mediation Analysis to show that the parametric memory is minimally
utilized when answering a question and (ii) Attention Contributions and
Knockouts to show that the last token residual stream do not get enriched from
the subject token in the question, but gets enriched from other informative
tokens in the context. We find this pronounced shortcut behaviour true across
both LLaMa and Phi family of models.",Hitesh Wadhwa
2024-06-19T04:53:48Z,http://arxiv.org/abs/2406.13213v2,"Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database
  Filtering with LLM-Extracted Metadata","The retrieval-augmented generation (RAG) enables retrieval of relevant
information from an external knowledge source and allows large language models
(LLMs) to answer queries over previously unseen document collections. However,
it was demonstrated that traditional RAG applications perform poorly in
answering multi-hop questions, which require retrieving and reasoning over
multiple elements of supporting evidence. We introduce a new method called
Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to
improve the RAG selection of the relevant documents from various sources,
relevant to the question. While database filtering is specific to a set of
questions from a particular domain and format, we found out that Multi-Meta-RAG
greatly improves the results on the MultiHop-RAG benchmark. The code is
available at https://github.com/mxpoliakov/Multi-Meta-RAG.",Mykhailo Poliakov
2024-06-19T08:29:54Z,http://arxiv.org/abs/2406.13331v2,Improving Zero-shot LLM Re-Ranker with Risk Minimization,"In the Retrieval-Augmented Generation (RAG) system, advanced Large Language
Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an
unsupervised way, which re-rank documents based on the probability of
generating the query given the content of a document. However, directly
prompting LLMs to approximate QLMs inherently is biased, where the estimated
distribution might diverge from the actual document-specific distribution. In
this study, we introduce a novel framework, $\mathrm{UR^3}$, which leverages
Bayesian decision theory to both quantify and mitigate this estimation bias.
Specifically, $\mathrm{UR^3}$ reformulates the problem as maximizing the
probability of document generation, thereby harmonizing the optimization of
query and document generation probabilities under a unified risk minimization
objective. Our empirical results indicate that $\mathrm{UR^3}$ significantly
enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits
the QA tasks by achieving higher accuracy with fewer input documents.",Xiaowei Yuan
2024-06-19T21:07:35Z,http://arxiv.org/abs/2406.13840v1,"StackRAG Agent: Improving Developer Answers with Retrieval-Augmented
  Generation","Developers spend much time finding information that is relevant to their
questions. Stack Overflow has been the leading resource, and with the advent of
Large Language Models (LLMs), generative models such as ChatGPT are used
frequently. However, there is a catch in using each one separately. Searching
for answers is time-consuming and tedious, as shown by the many tools developed
by researchers to address this issue. On the other, using LLMs is not reliable,
as they might produce irrelevant or unreliable answers (i.e., hallucination).
In this work, we present StackRAG, a retrieval-augmented Multiagent generation
tool based on LLMs that combines the two worlds: aggregating the knowledge from
SO to enhance the reliability of the generated answers. Initial evaluations
show that the generated answers are correct, accurate, relevant, and useful.",Davit Abrahamyan
2024-06-20T12:59:27Z,http://arxiv.org/abs/2406.14277v2,"QPaug: Question and Passage Augmentation for Open-Domain Question
  Answering of LLMs","Retrieval-augmented generation (RAG) has received much attention for
Open-domain question-answering (ODQA) tasks as a means to compensate for the
parametric knowledge of large language models (LLMs). While previous approaches
focused on processing retrieved passages to remove irrelevant context, they
still rely heavily on the quality of retrieved passages which can degrade if
the question is ambiguous or complex. In this paper, we propose a simple yet
efficient method called question and passage augmentation (QPaug) via LLMs for
open-domain QA. QPaug first decomposes the original questions into
multiple-step sub-questions. By augmenting the original question with detailed
sub-questions and planning, we are able to make the query more specific on what
needs to be retrieved, improving the retrieval performance. In addition, to
compensate for the case where the retrieved passages contain distracting
information or divided opinions, we augment the retrieved passages with
self-generated passages by LLMs to guide the answer extraction. Experimental
results show that QPaug outperforms the previous state-of-the-art and achieves
significant performance gain over existing RAG methods. The source code is
available at \url{https://github.com/kmswin1/QPaug}.",Minsang Kim
2024-06-20T15:12:41Z,http://arxiv.org/abs/2406.14394v1,SEC-QA: A Systematic Evaluation Corpus for Financial QA,"The financial domain frequently deals with large numbers of long documents
that are essential for daily operations. Significant effort is put towards
automating financial data analysis. However, a persistent challenge, not
limited to the finance domain, is the scarcity of datasets that accurately
reflect real-world tasks for model evaluation. Existing datasets are often
constrained by size, context, or relevance to practical applications. Moreover,
LLMs are currently trained on trillions of tokens of text, limiting access to
novel data or documents that models have not encountered during training for
unbiased evaluation. We propose SEC-QA, a continuous dataset generation
framework with two key features: 1) the semi-automatic generation of
Question-Answer (QA) pairs spanning multiple long context financial documents,
which better represent real-world financial scenarios; 2) the ability to
continually refresh the dataset using the most recent public document
collections, not yet ingested by LLMs. Our experiments show that current
retrieval augmented generation methods systematically fail to answer these
challenging multi-document questions. In response, we introduce a QA system
based on program-of-thought that improves the ability to perform complex
information retrieval and quantitative reasoning pipelines, thereby increasing
QA accuracy.",Viet Dac Lai
2024-06-20T20:55:38Z,http://arxiv.org/abs/2406.14732v2,"TTQA-RS- A break-down prompting approach for Multi-hop Table-Text
  Question Answering with Reasoning and Summarization","Question answering (QA) over tables and text has gained much popularity over
the years. Multi-hop table-text QA requires multiple hops between the table and
text, making it a challenging QA task. Although several works have attempted to
solve the table-text QA task, most involve training the models and requiring
labeled data. In this paper, we have proposed a Retrieval Augmented Generation
(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop
Table-Text Question Answering with Reasoning and Summarization. Our model uses
an enhanced retriever for table-text information retrieval and uses augmented
knowledge, including table-text summary with decomposed sub-questions with
answers for a reasoning-based table-text QA. Using open-source language models,
our model outperformed all existing prompting methods for table-text QA tasks
on existing table-text QA datasets, such as HybridQA and OTT-QA's development
set. Our experiments demonstrate the potential of prompt-based approaches using
open-source LLMs. Additionally, by using LLaMA3-70B, our model achieved
state-of-the-art performance for prompting-based methods on multi-hop
table-text QA.",Jayetri Bardhan
2024-06-21T08:31:02Z,http://arxiv.org/abs/2406.14972v1,A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems,"Retrieval Augmented Generation (RAG) represents a significant advancement in
artificial intelligence combining a retrieval phase with a generative phase,
with the latter typically being powered by large language models (LLMs). The
current common practices in RAG involve using ""instructed"" LLMs, which are
fine-tuned with supervised training to enhance their ability to follow
instructions and are aligned with human preferences using state-of-the-art
techniques. Contrary to popular belief, our study demonstrates that base models
outperform their instructed counterparts in RAG tasks by 20% on average under
our experimental settings. This finding challenges the prevailing assumptions
about the superiority of instructed LLMs in RAG applications. Further
investigations reveal a more nuanced situation, questioning fundamental aspects
of RAG and suggesting the need for broader discussions on the topic; or, as
Fromm would have it, ""Seldom is a glance at the statistics enough to understand
the meaning of the figures"".",Florin Cuconasu
2024-06-21T10:48:21Z,http://arxiv.org/abs/2406.15045v2,"Integrating Knowledge Retrieval and Large Language Models for Clinical
  Report Correction","This study proposes an approach for error correction in radiology reports,
leveraging large language models (LLMs) and retrieval-augmented generation
(RAG) techniques. The proposed framework employs a novel internal+external
retrieval mechanism to extract relevant medical entities and relations from the
report of interest and an external knowledge source. A three-stage inference
process is introduced, decomposing the task into error detection, localization,
and correction subtasks, which enhances the explainability and performance of
the system. The effectiveness of the approach is evaluated using a benchmark
dataset created by corrupting real-world radiology reports with realistic
errors, guided by domain experts. Experimental results demonstrate the benefits
of the proposed methods, with the combination of internal and external
retrieval significantly improving the accuracy of error detection,
localization, and correction across various state-of-the-art LLMs. The findings
contribute to the development of more robust and reliable error correction
systems for clinical documentation.",Jinge Wu
2024-06-24T07:17:59Z,http://arxiv.org/abs/2406.16367v1,"On the Role of Long-tail Knowledge in Retrieval Augmented Large Language
  Models","Retrieval augmented generation (RAG) exhibits outstanding performance in
promoting the knowledge capabilities of large language models (LLMs) with
retrieved documents related to user queries. However, RAG only focuses on
improving the response quality of LLMs via enhancing queries indiscriminately
with retrieved information, paying little attention to what type of knowledge
LLMs really need to answer original queries more accurately. In this paper, we
suggest that long-tail knowledge is crucial for RAG as LLMs have already
remembered common world knowledge during large-scale pre-training. Based on our
observation, we propose a simple but effective long-tail knowledge detection
method for LLMs. Specifically, the novel Generative Expected Calibration Error
(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on
both statistics and semantics. Hence, we retrieve relevant documents and infuse
them into the model for patching knowledge loopholes only when the input query
relates to long-tail knowledge. Experiments show that, compared to existing RAG
pipelines, our method achieves over 4x speedup in average inference time and
consistent performance improvement in downstream tasks.",Dongyang Li
2024-06-24T07:52:05Z,http://arxiv.org/abs/2406.16383v2,"Context-augmented Retrieval: A Novel Framework for Fast Information
  Retrieval based Response Generation using Large Language Model","Generating high-quality answers consistently by providing contextual
information embedded in the prompt passed to the Large Language Model (LLM) is
dependent on the quality of information retrieval. As the corpus of contextual
information grows, the answer/inference quality of Retrieval Augmented
Generation (RAG) based Question Answering (QA) systems declines. This work
solves this problem by combining classical text classification with the Large
Language Model (LLM) to enable quick information retrieval from the vector
store and ensure the relevancy of retrieved information. For the same, this
work proposes a new approach Context Augmented retrieval (CAR), where
partitioning of vector database by real-time classification of information
flowing into the corpus is done. CAR demonstrates good quality answer
generation along with significant reduction in information retrieval and answer
generation time.",Sai Ganesh
2024-06-26T07:38:24Z,http://arxiv.org/abs/2406.18134v1,"Assessing ""Implicit"" Retrieval Robustness of Large Language Models","Retrieval-augmented generation has gained popularity as a framework to
enhance large language models with external knowledge. However, its
effectiveness hinges on the retrieval robustness of the model. If the model
lacks retrieval robustness, its performance is constrained by the accuracy of
the retriever, resulting in significant compromises when the retrieved context
is irrelevant. In this paper, we evaluate the ""implicit"" retrieval robustness
of various large language models, instructing them to directly output the final
answer without explicitly judging the relevance of the retrieved context. Our
findings reveal that fine-tuning on a mix of gold and distracting context
significantly enhances the model's robustness to retrieval inaccuracies, while
still maintaining its ability to extract correct answers when retrieval is
accurate. This suggests that large language models can implicitly handle
relevant or irrelevant retrieved context by learning solely from the
supervision of the final answer in an end-to-end manner. Introducing an
additional process for explicit relevance judgment can be unnecessary and
disrupts the end-to-end approach.",Xiaoyu Shen
2024-06-27T05:14:34Z,http://arxiv.org/abs/2406.18894v1,"Assessing the Effectiveness of LLMs in Android Application Vulnerability
  Analysis","The increasing frequency of attacks on Android applications coupled with the
recent popularity of large language models (LLMs) necessitates a comprehensive
understanding of the capabilities of the latter in identifying potential
vulnerabilities, which is key to mitigate the overall risk. To this end, the
work at hand compares the ability of nine state-of-the-art LLMs to detect
Android code vulnerabilities listed in the latest Open Worldwide Application
Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open
dataset of over 100 vulnerable code samples, including obfuscated ones,
assessing each model's ability to identify key vulnerabilities. Our analysis
reveals the strengths and weaknesses of each LLM, identifying important factors
that contribute to their performance. Additionally, we offer insights into
context augmentation with retrieval-augmented generation (RAG) for detecting
Android code vulnerabilities, which in turn may propel secure application
development. Finally, while the reported findings regarding code vulnerability
analysis show promise, they also reveal significant discrepancies among the
different LLMs.",Vasileios Kouliaridis
2024-06-27T13:08:35Z,http://arxiv.org/abs/2406.19150v1,RAVEN: Multitask Retrieval Augmented Vision-Language Learning,"The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.",Varun Nagaraj Rao
2024-06-27T14:38:33Z,http://arxiv.org/abs/2406.19215v1,"SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented
  Generation","This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel
adaptive RAG model that extracts self-aware uncertainty of LLMs from their
internal states. SeaKR activates retrieval when the LLMs present high
self-aware uncertainty for generation. To effectively integrate retrieved
knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty
to preserve the snippet that reduces their uncertainty to the utmost. To
facilitate solving complex tasks that require multiple retrievals, SeaKR
utilizes their self-aware uncertainty to choose among different reasoning
strategies. Our experiments on both complex and simple Question Answering
datasets show that SeaKR outperforms existing adaptive RAG methods. We release
our code at https://github.com/THU-KEG/SeaKR.",Zijun Yao
2024-06-27T16:33:40Z,http://arxiv.org/abs/2406.19309v2,"Which Neurons Matter in IR? Applying Integrated Gradients-based Methods
  to Understand Cross-Encoders","With the recent addition of Retrieval-Augmented Generation (RAG), the scope
and importance of Information Retrieval (IR) has expanded. As a result, the
importance of a deeper understanding of IR models also increases. However,
interpretability in IR remains under-explored, especially when it comes to the
models' inner mechanisms. In this paper, we explore the possibility of adapting
Integrated Gradient-based methods in an IR context to identify the role of
individual neurons within the model. In particular, we provide new insights
into the role of what we call ""relevance"" neurons, as well as how they deal
with unseen data. Finally, we carry out an in-depth pruning study to validate
our findings.",Mathias Vast
2024-06-29T10:46:01Z,http://arxiv.org/abs/2407.00396v1,"A Study on Effect of Reference Knowledge Choice in Generating Technical
  Content Relevant to SAPPhIRE Model Using Large Language Model","Representation of systems using the SAPPhIRE model of causality can be an
inspirational stimulus in design. However, creating a SAPPhIRE model of a
technical or a natural system requires sourcing technical knowledge from
multiple technical documents regarding how the system works. This research
investigates how to generate technical content accurately relevant to the
SAPPhIRE model of causality using a Large Language Model, also called LLM. This
paper, which is the first part of the two-part research, presents a method for
hallucination suppression using Retrieval Augmented Generating with LLM to
generate technical content supported by the scientific information relevant to
a SAPPhIRE con-struct. The result from this research shows that the selection
of reference knowledge used in providing context to the LLM for generating the
technical content is very important. The outcome of this research is used to
build a software support tool to generate the SAPPhIRE model of a given
technical system.",Kausik Bhattacharya
2024-07-01T15:53:29Z,http://arxiv.org/abs/2407.01403v1,"Optimization of Retrieval-Augmented Generation Context with Outlier
  Detection","In this paper, we focus on methods to reduce the size and improve the quality
of the prompt context required for question-answering systems. Attempts to
increase the number of retrieved chunked documents and thereby enlarge the
context related to the query can significantly complicate the processing and
decrease the performance of a Large Language Model (LLM) when generating
responses to queries. It is well known that a large set of documents retrieved
from a database in response to a query may contain irrelevant information,
which often leads to hallucinations in the resulting answers. Our goal is to
select the most semantically relevant documents, treating the discarded ones as
outliers. We propose and evaluate several methods for identifying outliers by
creating features that utilize the distances of embedding vectors, retrieved
from the vector database, to both the centroid and the query vectors. The
methods were evaluated by comparing the similarities of the retrieved LLM
responses to ground-truth answers obtained using the OpenAI GPT-4o model. It
was found that the greatest improvements were achieved with increasing
complexity of the questions and answers.",Vitaly Bulgakov
2024-06-27T15:45:29Z,http://arxiv.org/abs/2407.01449v3,ColPali: Efficient Document Retrieval with Vision Language Models,"Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.",Manuel Faysse
2024-07-01T20:47:47Z,http://arxiv.org/abs/2407.01796v1,"Ground Every Sentence: Improving Retrieval-Augmented LLMs with
  Interleaved Reference-Claim Generation","Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large
Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text
Generation (ATG) has attracted growing attention, which provides citations to
support the model's responses in RAG, so as to enhance the credibility of
LLM-generated content and facilitate verification. Prior methods mainly adopt
coarse-grained attributions, linking to passage-level references or providing
paragraph-level citations. However, these methods still fall short in
verifiability and require certain time costs for fact checking. This paper
proposes a fine-grained ATG method called ReClaim(Refer & Claim), which
alternates the generation of references and answers step by step. Unlike
traditional coarse-grained attribution, ReClaim allows the model to add
sentence-level fine-grained citations to each answer sentence in long-form
question-answering tasks. Our experiments encompass various training and
inference methods and multiple LLMs, verifying the effectiveness of our
approach.",Sirui Xia
2024-07-02T12:57:42Z,http://arxiv.org/abs/2407.02233v2,Synthetic Multimodal Question Generation,"Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to
question-answering over multimodal documents. A key challenge with evaluating
MMRAG is the paucity of high-quality datasets matching the question styles and
modalities of interest. In light of this, we propose SMMQG, a synthetic data
generation framework. SMMQG leverages interplay between a retriever, large
language model (LLM) and large multimodal model (LMM) to generate question and
answer pairs directly from multimodal documents, with the questions conforming
to specified styles and modalities. We use SMMQG to generate an MMRAG dataset
of 1024 questions over Wikipedia documents and evaluate state-of-the-art models
using it, revealing insights into model performance that are attainable only
through style- and modality-specific evaluation data. Next, we measure the
quality of data produced by SMMQG via a human study. We find that the quality
of SMMQG-generated synthetic data is on par with the quality of the
crowdsourced benchmark MMQA and that downstream evaluation results using both
datasets strongly concur.",Ian Wu
2024-07-04T13:52:23Z,http://arxiv.org/abs/2407.03937v2,"TongGu: Mastering Classical Chinese Understanding with
  Knowledge-Grounded Large Language Models","Classical Chinese is a gateway to the rich heritage and wisdom of ancient
China, yet its complexities pose formidable comprehension barriers for most
modern people without specialized knowledge. While Large Language Models (LLMs)
have shown remarkable capabilities in Natural Language Processing (NLP), they
struggle with Classical Chinese Understanding (CCU), especially in
data-demanding and knowledge-intensive tasks. In response to this dilemma, we
propose \textbf{TongGu} (mean understanding ancient and modern), the first
CCU-specific LLM, underpinned by three core contributions. First, we construct
a two-stage instruction-tuning dataset ACCN-INS derived from rich classical
Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we
propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting,
enabling TongGu to acquire new capabilities while preserving its foundational
knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG)
technique to reduce hallucinations based on knowledge-grounding. Extensive
experiments across 24 diverse CCU tasks validate TongGu's superior ability,
underscoring the effectiveness of RAT and CCU-RAG. The model and dataset are
available at \url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",Jiahuan Cao
2024-07-05T14:16:47Z,http://arxiv.org/abs/2407.04528v4,"GPT vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning","Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.",Aleksander Ficek
2024-07-06T17:25:11Z,http://arxiv.org/abs/2407.05138v1,Vortex under Ripplet: An Empirical Study of RAG-enabled Applications,"Large language models (LLMs) enhanced by retrieval-augmented generation (RAG)
provide effective solutions in various application scenarios. However,
developers face challenges in integrating RAG-enhanced LLMs into software
systems, due to lack of interface specification, requirements from software
context, and complicated system management. In this paper, we manually studied
100 open-source applications that incorporate RAG-enhanced LLMs, and their
issue reports. We have found that more than 98% of applications contain
multiple integration defects that harm software functionality, efficiency, and
security. We have also generalized 19 defect patterns and proposed guidelines
to tackle them. We hope this work could aid LLM-enabled software development
and motivate future research.",Yuchen Shao
2024-07-07T21:26:36Z,http://arxiv.org/abs/2407.05502v2,"Faux Polyglot: A Study on Information Disparity in Multilingual Large
  Language Models","With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are
playing a pivotal role in information search and are being adopted globally.
Although the multilingual capability of LLMs offers new opportunities to bridge
the language barrier, do these capabilities translate into real-life scenarios
where linguistic divide and knowledge conflicts between multilingual sources
are known occurrences? In this paper, we studied LLM's linguistic preference in
a RAG-based information search setting. We found that LLMs displayed systemic
bias towards information in the same language as the query language in both
information retrieval and answer generation. Furthermore, in scenarios where
there is little information in the language of the query, LLMs prefer documents
in high-resource languages, reinforcing the dominant views. Such bias exists
for both factual and opinion-based queries. Our results highlight the
linguistic divide within multilingual LLMs in information search systems. The
seemingly beneficial multilingual capability of LLMs may backfire on
information parity by reinforcing language-specific information cocoons or
filter bubbles further marginalizing low-resource views.",Nikhil Sharma
2024-07-11T13:22:17Z,http://arxiv.org/abs/2407.08488v2,Lynx: An Open Source Hallucination Evaluation Model,"Retrieval Augmented Generation (RAG) techniques aim to mitigate
hallucinations in Large Language Models (LLMs). However, LLMs can still produce
information that is unsupported or contradictory to the retrieved contexts. We
introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced
reasoning on challenging real-world hallucination scenarios. To evaluate LYNX,
we present HaluBench, a comprehensive hallucination evaluation benchmark,
consisting of 15k samples sourced from various real-world domains. Our
experiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and
closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,
HaluBench and our evaluation code for public access.",Selvan Sunitha Ravi
2024-07-11T13:29:28Z,http://arxiv.org/abs/2407.08495v2,"Investigating LLMs as Voting Assistants via Contextual Augmentation: A
  Case Study on the European Parliament Elections 2024","In light of the recent 2024 European Parliament elections, we are
investigating if LLMs can be used as Voting Advice Applications (VAAs). We
audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the
stance of political parties based on the latest ""EU and I"" voting assistance
questionnaire. Furthermore, we explore alternatives to improve models'
performance by augmenting the input context via Retrieval-Augmented Generation
(RAG) relying on web search, and Self-Reflection using staged conversations
that aim to re-collect relevant content from the model's internal memory. We
find that MIXTRAL is highly accurate with an 82% accuracy on average with a
significant performance disparity across different political groups (50-95%).
Augmenting the input context with expert-curated information can lead to a
significant boost of approx. 9%, which remains an open challenge for automated
RAG approaches, even considering curated content.",Ilias Chalkidis
2024-07-12T06:06:54Z,http://arxiv.org/abs/2407.09014v3,CompAct: Compressing Retrieved Documents Actively for Question Answering,"Retrieval-augmented generation supports language models to strengthen their
factual groundings by providing external contexts. However, language models
often face challenges when given extensive information, diminishing their
effectiveness in solving questions. Context compression tackles this issue by
filtering out irrelevant information, but current methods still struggle in
realistic scenarios where crucial information cannot be captured with a
single-step approach. To overcome this limitation, we introduce CompAct, a
novel framework that employs an active strategy to condense extensive documents
without losing key information. Our experiments demonstrate that CompAct brings
significant improvements in both performance and compression rate on multi-hop
question-answering benchmarks. CompAct flexibly operates as a cost-efficient
plug-in module with various off-the-shelf retrievers or readers, achieving
exceptionally high compression rates (47x).",Chanwoong Yoon
2024-07-12T13:30:44Z,http://arxiv.org/abs/2407.09252v3,Context Embeddings for Efficient Answer Generation in RAG,"Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge
of LLMs by extending the input with external information. As a consequence, the
contextual inputs to the model become much longer which slows down decoding
time directly translating to the time a user has to wait for an answer. We
address this challenge by presenting COCOM, an effective context compression
method, reducing long contexts to only a handful of Context Embeddings speeding
up the generation time by a large margin. Our method allows for different
compression rates trading off decoding time for answer quality. Compared to
earlier methods, COCOM allows for handling multiple contexts more effectively,
significantly reducing decoding time for long inputs. Our method demonstrates a
speed-up of up to 5.69 $\times$ while achieving higher performance compared to
existing efficient context compression methods.",David Rau
2024-05-17T12:23:19Z,http://arxiv.org/abs/2407.09977v1,"Mitigating Interpretation Bias in Rock Records with Large Language
  Models: Insights from Paleoenvironmental Analysis","The reconstruction of Earth's history faces significant challenges due to the
nonunique interpretations often derived from rock records. The problem has long
been recognized but there are no systematic solutions in practice. This study
introduces an innovative approach that leverages Large Language Models (LLMs)
along with retrieval augmented generation and real-time search capabilities to
counteract interpretation biases, thereby enhancing the accuracy and
reliability of geological analyses. By applying this framework to sedimentology
and paleogeography, we demonstrate its effectiveness in mitigating
interpretations biases through the generation and evaluation of multiple
hypotheses for the same data, which can effectively reduce human bias. Our
research illuminates the transformative potential of LLMs in refining
paleoenvironmental studies and extends their applicability across various
sub-disciplines of Earth sciences, enabling a deeper and more accurate
depiction of Earth's evolution.",Luoqi Wang
2024-07-14T15:25:08Z,http://arxiv.org/abs/2407.10245v1,"GenSco: Can Question Decomposition based Passage Alignment improve
  Question Answering?","Retrieval augmented generation (RAG) with large language models (LLMs) for
Question Answering (QA) entails furnishing relevant context within the prompt
to facilitate the LLM in answer generation. During the generation, inaccuracies
or hallucinations frequently occur due to two primary factors: inadequate or
distracting context in the prompts, and the inability of LLMs to effectively
reason through the facts. In this paper, we investigate whether providing
aligned context via a carefully selected passage sequence leads to better
answer generation by the LLM for multi-hop QA. We introduce, ""GenSco"", a novel
approach of selecting passages based on the predicted decomposition of the
multi-hop questions}. The framework consists of two distinct LLMs: (i)
Generator LLM, which is used for question decomposition and final answer
generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to
semantically guide the Generator for passage selection. The generator is
invoked only once for the answer generation, resulting in a cost-effective and
efficient approach. We evaluate on three broadly established multi-hop question
answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve
an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect
to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.",Barah Fazili
2024-07-15T17:30:31Z,http://arxiv.org/abs/2407.10930v2,"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better
  Together","Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai",Dilara Soylu
2024-06-28T10:57:50Z,http://arxiv.org/abs/2407.12025v1,"LLM4DESIGN: An Automated Multi-Modal System for Architectural and
  Environmental Design","This study introduces LLM4DESIGN, a highly automated system for generating
architectural and environmental design proposals. LLM4DESIGN, relying solely on
site conditions and design requirements, employs Multi-Agent systems to foster
creativity, Retrieval Augmented Generation (RAG) to ground designs in realism,
and Visual Language Models (VLM) to synchronize all information. This system
resulting in coherent, multi-illustrated, and multi-textual design schemes. The
system meets the dual needs of narrative storytelling and objective drawing
presentation in generating architectural and environmental design proposals.
Extensive comparative and ablation experiments confirm the innovativeness of
LLM4DESIGN's narrative and the grounded applicability of its plans,
demonstrating its superior performance in the field of urban renewal design.
Lastly, we have created the first cross-modal design scheme dataset covering
architecture, landscape, interior, and urban design, providing rich resources
for future research.",Ran Chen
2024-07-01T05:37:17Z,http://arxiv.org/abs/2407.12036v2,Exploring Advanced Large Language Models with LLMsuite,"This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.",Giorgio Roffo
2024-07-11T05:04:44Z,http://arxiv.org/abs/2407.12057v1,"NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker
  and AWS Trainium and Inferentia2","Retrieval-augmented generation (RAG) techniques are widely used today to
retrieve and present information in a conversational format. This paper
presents a set of enhancements to traditional RAG techniques, focusing on large
language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI
chips via SageMaker. These chips are characterized by their elasticity,
affordability, and efficient performance for AI compute tasks. Besides enabling
deployment on these chips, this work aims to improve tool usage, add citation
capabilities, and mitigate the risks of hallucinations and unsafe responses due
to context bias. We benchmark our RAG system's performance on the Natural
Questions and HotPotQA datasets, achieving an accuracy of 62% and 59%
respectively, exceeding other models such as DBRX and Mixtral Instruct.",Tengfei Xue
2024-07-17T13:11:28Z,http://arxiv.org/abs/2407.12529v2,Crafting the Path: Robust Query Rewriting for Information Retrieval,"Query rewriting aims to generate a new query that can complement the original
query to improve the information retrieval system. Recent studies on query
rewriting, such as query2doc, query2expand and querey2cot, rely on the internal
knowledge of Large Language Models (LLMs) to generate a relevant passage to add
information to the query. Nevertheless, the efficacy of these methodologies may
markedly decline in instances where the requisite knowledge is not encapsulated
within the model's intrinsic parameters. In this paper, we propose a novel
structured query rewriting method called Crafting the Path tailored for
retrieval systems. Crafting the Path involves a three-step process that crafts
query-related information necessary for finding the passages to be searched in
each step. Specifically, the Crafting the Path begins with Query Concept
Comprehension, proceeds to Query Type Identification, and finally conducts
Expected Answer Extraction. Experimental results show that our method
outperforms previous rewriting methods, especially in less familiar domains for
LLMs. We demonstrate that our method is less dependent on the internal
parameter knowledge of the model and generates queries with fewer factual
inaccuracies. Furthermore, we observe that \name{} demonstrates superior
performance in the retrieval-augmented generation scenarios.",Ingeol Baek
2024-07-17T16:55:42Z,http://arxiv.org/abs/2407.12735v4,EchoSight: Advancing Visual-Language Models with Wiki Knowledge,"Knowledge-based Visual Question Answering (KVQA) tasks require answering
questions about images using extensive background knowledge. Despite
significant advancements, generative models often struggle with these tasks due
to the limited integration of external knowledge. In this paper, we introduce
EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework
that enables large language models (LLMs) to answer visual questions requiring
fine-grained encyclopedic knowledge. To strive for high-performing retrieval,
EchoSight first searches wiki articles by using visual-only information,
subsequently, these candidate articles are further reranked according to their
relevance to the combined text-image query. This approach significantly
improves the integration of multimodal knowledge, leading to enhanced retrieval
outcomes and more accurate VQA responses. Our experimental results on the
Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes
new state-of-the-art results in knowledge-based VQA, achieving an accuracy of
41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",Yibin Yan
2024-07-18T02:19:00Z,http://arxiv.org/abs/2407.13101v1,"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach","Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.",Zhouyu Jiang
2024-07-18T21:49:32Z,http://arxiv.org/abs/2407.13909v1,PRAGyan -- Connecting the Dots in Tweets,"As social media platforms grow, understanding the underlying reasons behind
events and statements becomes crucial for businesses, policymakers, and
researchers. This research explores the integration of Knowledge Graphs (KGs)
with Large Language Models (LLMs) to perform causal analysis of tweets dataset.
The LLM aided analysis techniques often lack depth in uncovering the causes
driving observed effects. By leveraging KGs and LLMs, which encode rich
semantic relationships and temporal information, this study aims to uncover the
complex interplay of factors influencing causal dynamics and compare the
results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented
Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data
format, to retrieve relevant context for causal reasoning. Our approach
demonstrates that the KG-enhanced LLM RAG can provide improved results when
compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus
increases in size. Our qualitative analysis highlights the advantages of
combining KGs with LLMs for improved interpretability and actionable insights,
facilitating informed decision-making across various domains. Whereas,
quantitative analysis using metrics such as BLEU and cosine similarity show
that our approach outperforms the baseline by 10\%.",Rahul Ravi
2024-07-19T03:02:51Z,http://arxiv.org/abs/2407.13998v2,"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval
  Augmented Question Answering","Question answering based on retrieval augmented generation (RAG-QA) is an
important research topic in NLP and has a wide range of real-world
applications. However, most existing datasets for this task are either
constructed using a single source corpus or consist of short extractive
answers, which fall short of evaluating large language model (LLM) based RAG-QA
systems on cross-domain generalization. To address these limitations, we create
Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form
answers that integrate short extractive answers from multiple documents into a
single, coherent narrative, covering 26K queries and large corpora across seven
different domains. We further propose RAG-QA Arena by directly comparing
model-generated answers against LFRQA's answers using LLMs as evaluators. We
show via extensive experiments that RAG-QA Arena and human judgments on answer
quality are highly correlated. Moreover, only 41.3% of the most competitive
LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a
challenging evaluation platform for future research.",Rujun Han
2024-07-19T08:33:07Z,http://arxiv.org/abs/2407.14116v1,AuditNet: A Conversational AI-based Security Assistant [DEMO],"In the age of information overload, professionals across various fields face
the challenge of navigating vast amounts of documentation and ever-evolving
standards. Ensuring compliance with standards, regulations, and contractual
obligations is a critical yet complex task across various professional fields.
We propose a versatile conversational AI assistant framework designed to
facilitate compliance checking on the go, in diverse domains, including but not
limited to network infrastructure, legal contracts, educational standards,
environmental regulations, and government policies. By leveraging
retrieval-augmented generation using large language models, our framework
automates the review, indexing, and retrieval of relevant, context-aware
information, streamlining the process of verifying adherence to established
guidelines and requirements. This AI assistant not only reduces the manual
effort involved in compliance checks but also enhances accuracy and efficiency,
supporting professionals in maintaining high standards of practice and ensuring
regulatory compliance in their respective fields. We propose and demonstrate
AuditNet, the first conversational AI security assistant designed to assist IoT
network security experts by providing instant access to security standards,
policies, and regulations.",Shohreh Deldari
2024-07-20T17:37:51Z,http://arxiv.org/abs/2407.14944v1,"Automatic Generation of Fashion Images using Prompting in Generative
  Machine Learning Models","The advent of artificial intelligence has contributed in a groundbreaking
transformation of the fashion industry, redefining creativity and innovation in
unprecedented ways. This work investigates methodologies for generating
tailored fashion descriptions using two distinct Large Language Models and a
Stable Diffusion model for fashion image creation. Emphasizing adaptability in
AI-driven fashion creativity, we depart from traditional approaches and focus
on prompting techniques, such as zero-shot and few-shot learning, as well as
Chain-of-Thought (CoT), which results in a variety of colors and textures,
enhancing the diversity of the outputs. Central to our methodology is
Retrieval-Augmented Generation (RAG), enriching models with insights from
fashion sources to ensure contemporary representations. Evaluation combines
quantitative metrics such as CLIPscore with qualitative human judgment,
highlighting strengths in creativity, coherence, and aesthetic appeal across
diverse styles. Among the participants, RAG and few-shot learning techniques
are preferred for their ability to produce more relevant and appealing fashion
descriptions. Our code is provided at https://github.com/georgiarg/AutoFashion.",Georgia Argyrou
2024-07-22T07:15:49Z,http://arxiv.org/abs/2407.15428v1,"Decoding BACnet Packets: A Large Language Model Approach for Packet
  Interpretation","The Industrial Control System (ICS) environment encompasses a wide range of
intricate communication protocols, posing substantial challenges for Security
Operations Center (SOC) analysts tasked with monitoring, interpreting, and
addressing network activities and security incidents. Conventional monitoring
tools and techniques often struggle to provide a clear understanding of the
nature and intent of ICS-specific communications. To enhance comprehension, we
propose a software solution powered by a Large Language Model (LLM). This
solution currently focused on BACnet protocol, processes a packet file data and
extracts context by using a mapping database, and contemporary context
retrieval methods for Retrieval Augmented Generation (RAG). The processed
packet information, combined with the extracted context, serves as input to the
LLM, which generates a concise packet file summary for the user. The software
delivers a clear, coherent, and easily understandable summary of network
activities, enabling SOC analysts to better assess the current state of the
control system.",Rashi Sharma
2024-07-22T15:37:41Z,http://arxiv.org/abs/2407.15734v1,"TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON","TaskGen is an open-sourced agentic framework which uses an Agent to solve an
arbitrary task by breaking them down into subtasks. Each subtask is mapped to
an Equipped Function or another Agent to execute. In order to reduce verbosity
(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from
the Large Language Model (LLM), along with additional features such as type
checking and iterative error correction. Key to the philosophy of TaskGen is
the management of information/memory on a need-to-know basis. We empirically
evaluate TaskGen on various environments such as 40x40 dynamic maze navigation
with changing obstacle locations (100% solve rate), TextWorld escape room
solving with dense rewards and detailed goals (96% solve rate), web browsing
(69% of actions successful), solving the MATH dataset (71% solve rate over 100
Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset
(F1 score of 47.03%)",John Chong Min Tan
2024-07-22T17:50:31Z,http://arxiv.org/abs/2407.15831v1,"NV-Retriever: Improving text embedding models with effective
  hard-negative mining","Text embedding models have been popular for information retrieval
applications such as semantic search and Question-Answering systems based on
Retrieval-Augmented Generation (RAG). Those models are typically Transformer
models that are fine-tuned with contrastive learning objectives. Many papers
introduced new embedding model architectures and training approaches, however,
one of the key ingredients, the process of mining negative passages, remains
poorly explored or described. One of the challenging aspects of fine-tuning
embedding models is the selection of high quality hard-negative passages for
contrastive learning. In this paper we propose a family of positive-aware
mining methods that leverage the positive relevance score for more effective
false negatives removal. We also provide a comprehensive ablation study on
hard-negative mining methods over their configurations, exploring different
teacher and base models. We demonstrate the efficacy of our proposed methods by
introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval
(BEIR) benchmark and 0.65 points higher than previous methods. The model placed
1st when it was published to MTEB Retrieval on July 07, 2024.",Gabriel de Souza P. Moreira
2024-07-21T16:42:45Z,http://arxiv.org/abs/2407.18333v1,"AutoVCoder: A Systematic Framework for Automated Verilog Code Generation
  using LLMs","Recently, the use of large language models (LLMs) for software code
generation, e.g., C/C++ and Python, has proven a great success. However, LLMs
still suffer from low syntactic and functional correctness when it comes to the
generation of register-transfer level (RTL) code, such as Verilog. To address
this issue, in this paper, we develop AutoVCoder, a systematic open-source
framework that significantly improves the LLMs' correctness of generating
Verilog code and enhances the quality of its output at the same time. Our
framework integrates three novel techniques, including a high-quality hardware
dataset generation approach, a two-round LLM fine-tuning method and a
domain-specific retrieval-augmented generation (RAG) mechanism. Experimental
results demonstrate that AutoVCoder outperforms both industrial and academic
LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%
improvement in functional correctness on the EvalMachine and EvalHuman
benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax
correctness and a 3.4% increase in functional correctness on the RTLLM
benchmark compared with RTLCoder.",Mingzhe Gao
2024-07-29T08:38:14Z,http://arxiv.org/abs/2407.19794v2,Introducing a new hyper-parameter for RAG: Context Window Utilization,"This paper introduces a new hyper-parameter for Retrieval-Augmented
Generation (RAG) systems called Context Window Utilization. RAG systems enhance
generative models by incorporating relevant information retrieved from external
knowledge bases, improving the factual accuracy and contextual relevance of
generated responses. The size of the text chunks retrieved and processed is a
critical factor influencing RAG performance. This study aims to identify the
optimal chunk size that maximizes answer generation quality. Through systematic
experimentation, we analyze the effects of varying chunk sizes on the
efficiency and effectiveness of RAG frameworks. Our findings reveal that an
optimal chunk size balances the trade-off between providing sufficient context
and minimizing irrelevant information. These insights are crucial for enhancing
the design and implementation of RAG systems, underscoring the importance of
selecting an appropriate chunk size to achieve superior performance.",Kush Juvekar
2024-07-29T13:26:43Z,http://arxiv.org/abs/2407.19994v3,"A Study on the Implementation Method of an Agent-Based Advanced RAG
  System Using Graph","This study aims to improve knowledge-based question-answering (QA) systems by
overcoming the limitations of existing Retrieval-Augmented Generation (RAG)
models and implementing an advanced RAG system based on Graph technology to
develop high-quality generative AI services. While existing RAG models
demonstrate high accuracy and fluency by utilizing retrieved information, they
may suffer from accuracy degradation as they generate responses using
pre-loaded knowledge without reprocessing. Additionally, they cannot
incorporate real-time data after the RAG configuration stage, leading to issues
with contextual understanding and biased information. To address these
limitations, this study implemented an enhanced RAG system utilizing Graph
technology. This system is designed to efficiently search and utilize
information. Specifically, it employs LangGraph to evaluate the reliability of
retrieved information and synthesizes diverse data to generate more accurate
and enhanced responses. Furthermore, the study provides a detailed explanation
of the system's operation, key implementation steps, and examples through
implementation code and validation results, thereby enhancing the understanding
of advanced RAG technology. This approach offers practical guidelines for
implementing advanced RAG systems in corporate services, making it a valuable
resource for practical application.",Cheonsu Jeong
2024-07-30T09:53:55Z,http://arxiv.org/abs/2407.20700v1,"Industrial-Grade Smart Troubleshooting through Causal Technical Language
  Processing: a Proof of Concept","This paper describes the development of a causal diagnosis approach for
troubleshooting an industrial environment on the basis of the technical
language expressed in Return on Experience records. The proposed method
leverages the vectorized linguistic knowledge contained in the distributed
representation of a Large Language Model, and the causal associations entailed
by the embedded failure modes and mechanisms of the industrial assets. The
paper presents the elementary but essential concepts of the solution, which is
conceived as a causality-aware retrieval augmented generation system, and
illustrates them experimentally on a real-world Predictive Maintenance setting.
Finally, it discusses avenues of improvement for the maturity of the utilized
causal technology to meet the robustness challenges of increasingly complex
scenarios in the industry.",Alexandre Trilla
2024-07-26T03:45:30Z,http://arxiv.org/abs/2407.21059v1,"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable
  Frameworks","Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.",Yunfan Gao
2024-07-31T01:51:24Z,http://arxiv.org/abs/2407.21276v2,Multi-Level Querying using A Knowledge Pyramid,"This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.",Rubing Chen
2024-07-31T03:00:59Z,http://arxiv.org/abs/2407.21300v3,Implementing Streaming algorithm and k-means clusters to RAG,"Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.",Haoyu Kang
2024-07-31T21:33:56Z,http://arxiv.org/abs/2408.00167v2,Finch: Prompt-guided Key-Value Cache Compression,"Recent large language model applications, such as Retrieval-Augmented
Generation and chatbots, have led to an increased need to process longer input
contexts. However, this requirement is hampered by inherent limitations.
Architecturally, models are constrained by a context window defined during
training. Additionally, processing extensive texts requires substantial GPU
memory. We propose a novel approach, Finch, to compress the input context by
leveraging the pre-trained model weights of the self-attention. Given a prompt
and a long text, Finch iteratively identifies the most relevant Key (K) and
Value (V) pairs over chunks of the text conditioned on the prompt. Only such
pairs are stored in the KV cache, which, within the space constrained by the
context window, ultimately contains a compressed version of the long text. Our
proposal enables models to consume large inputs even with high compression (up
to 93x) while preserving semantic integrity without the need for fine-tuning.",Giulio Corallo
2024-07-20T06:10:46Z,http://arxiv.org/abs/2408.00798v1,"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation
  for Industrial Knowledge Base","This paper introduces Golden-Retriever, designed to efficiently navigate vast
industrial knowledge bases, overcoming challenges in traditional LLM
fine-tuning and RAG frameworks with domain-specific jargon and context
interpretation. Golden-Retriever incorporates a reflection-based question
augmentation step before document retrieval, which involves identifying jargon,
clarifying its meaning based on context, and augmenting the question
accordingly. Specifically, our method extracts and lists all jargon and
abbreviations in the input question, determines the context against a
pre-defined list, and queries a jargon dictionary for extended definitions and
descriptions. This comprehensive augmentation ensures the RAG framework
retrieves the most relevant documents by providing clear context and resolving
ambiguities, significantly improving retrieval accuracy. Evaluations using
three open-source LLMs on a domain-specific question-answer dataset demonstrate
Golden-Retriever's superior performance, providing a robust solution for
efficiently integrating and querying industrial knowledge bases.",Zhiyu An
2024-08-02T08:37:03Z,http://arxiv.org/abs/2408.01107v2,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.",Chengrui Wang
2024-08-02T13:35:11Z,http://arxiv.org/abs/2408.01262v4,RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,"Retrieval-Augmented Generation (RAG) is a powerful approach that enables
large language models (LLMs) to incorporate external knowledge. However,
evaluating the effectiveness of RAG systems in specialized scenarios remains
challenging due to the high costs of data construction and the lack of suitable
evaluation metrics. This paper introduces RAGEval, a framework designed to
assess RAG systems across diverse scenarios by generating high-quality
documents, questions, answers, and references through a schema-based pipeline.
With a focus on factual accuracy, we propose three novel metrics Completeness,
Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses.
Experimental results show that RAGEval outperforms zero-shot and one-shot
methods in terms of clarity, safety, conformity, and richness of generated
samples. Furthermore, the use of LLMs for scoring the proposed metrics
demonstrates a high level of consistency with human evaluations. RAGEval
establishes a new paradigm for evaluating RAG systems in real-world
applications.",Kunlun Zhu
2024-08-02T17:54:34Z,http://arxiv.org/abs/2408.01419v1,DebateQA: Evaluating Question Answering on Debatable Knowledge,"The rise of large language models (LLMs) has enabled us to seek answers to
inherently debatable questions on LLM chatbots, necessitating a reliable way to
evaluate their ability. However, traditional QA benchmarks assume fixed answers
are inadequate for this purpose. To address this, we introduce DebateQA, a
dataset of 2,941 debatable questions, each accompanied by multiple
human-annotated partial answers that capture a variety of perspectives. We
develop two metrics: Perspective Diversity, which evaluates the
comprehensiveness of perspectives, and Dispute Awareness, which assesses if the
LLM acknowledges the question's debatable nature. Experiments demonstrate that
both metrics align with human preferences and are stable across different
underlying models. Using DebateQA with two metrics, we assess 12 popular LLMs
and retrieval-augmented generation methods. Our findings reveal that while LLMs
generally excel at recognizing debatable issues, their ability to provide
comprehensive answers encompassing diverse perspectives varies considerably.",Rongwu Xu
2024-08-07T14:42:13Z,http://arxiv.org/abs/2408.03811v1,"Generative Language Models with Retrieval Augmented Generation for
  Automated Short Answer Scoring","Automated Short Answer Scoring (ASAS) is a critical component in educational
assessment. While traditional ASAS systems relied on rule-based algorithms or
complex deep learning methods, recent advancements in Generative Language
Models (GLMs) offer new opportunities for improvement. This study explores the
application of GLMs to ASAS, leveraging their off-the-shelf capabilities and
performance in various domains. We propose a novel pipeline that combines
vector databases, transformer-based encoders, and GLMs to enhance short answer
scoring accuracy. Our approach stores training responses in a vector database,
retrieves semantically similar responses during inference, and employs a GLM to
analyze these responses and determine appropriate scores. We further optimize
the system through fine-tuned retrieval processes and prompt engineering.
Evaluation on the SemEval 2013 dataset demonstrates a significant improvement
on the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,
highlighting the potential of GLMs in advancing ASAS technology.",Zifan Wang
2024-08-08T06:57:49Z,http://arxiv.org/abs/2408.04259v2,EfficientRAG: Efficient Retriever for Multi-Hop Question Answering,"Retrieval-augmented generation (RAG) methods encounter difficulties when
addressing complex questions like multi-hop queries. While iterative retrieval
methods improve performance by gathering additional information, current
approaches often rely on multiple calls of large language models (LLMs). In
this paper, we introduce EfficientRAG, an efficient retriever for multi-hop
question answering. EfficientRAG iteratively generates new queries without the
need for LLM calls at each iteration and filters out irrelevant information.
Experimental results demonstrate that EfficientRAG surpasses existing RAG
methods on three open-domain multi-hop question-answering datasets.",Ziyuan Zhuang
2024-08-08T09:59:30Z,http://arxiv.org/abs/2408.04342v1,"Towards Explainable Network Intrusion Detection using Large Language
  Models","Large Language Models (LLMs) have revolutionised natural language processing
tasks, particularly as chat agents. However, their applicability to threat
detection problems remains unclear. This paper examines the feasibility of
employing LLMs as a Network Intrusion Detection System (NIDS), despite their
high computational requirements, primarily for the sake of explainability.
Furthermore, considerable resources have been invested in developing LLMs, and
they may offer utility for NIDS. Current state-of-the-art NIDS rely on
artificial benchmarking datasets, resulting in skewed performance when applied
to real-world networking environments. Therefore, we compare the GPT-4 and
LLama3 models against traditional architectures and transformer-based models to
assess their ability to detect malicious NetFlows without depending on
artificially skewed datasets, but solely on their vast pre-trained acquired
knowledge. Our results reveal that, although LLMs struggle with precise attack
detection, they hold significant potential for a path towards explainable NIDS.
Our preliminary exploration shows that LLMs are unfit for the detection of
Malicious NetFlows. Most promisingly, however, these exhibit significant
potential as complementary agents in NIDS, particularly in providing
explanations and aiding in threat response when integrated with Retrieval
Augmented Generation (RAG) and function calling capabilities.",Paul R. B. Houssel
2024-08-09T23:17:56Z,http://arxiv.org/abs/2408.05379v1,Temporal Analysis and Repair of Flaky Dockerfiles,"Dockerfile flakiness, characterized by inconsistent build behavior without
Dockerfile or project source code changes, poses significant challenges in
Continuous Integration and Delivery (CI/CD) pipelines. This issue can lead to
unreliable deployments and increased debugging efforts, yet it remains
underexplored in current research. We conduct a systematic analysis of
Dockerfile flakiness, presenting a comprehensive taxonomy of common flakiness
categories, including dependency-related errors and server connectivity issues.
Furthermore, we introduce FlakiDock, a tool leveraging large language models
and retrieval-augmented generation techniques with dynamic analysis and an
iterative feedback loop to automatically repair flaky Dockerfiles. Our
evaluation shows that FlakiDock achieves a 73.55% repair accuracy,
outperforming existing tools such as PARFUM by 12,581% and GPT-4-based
prompting by 94.63%. These results underscore the effectiveness of FlakiDock in
addressing Dockerfile flakiness and improving build reliability.",Taha Shabani
2024-08-13T14:59:44Z,http://arxiv.org/abs/2408.06941v2,OpenResearcher: Unleashing AI for Accelerated Scientific Research,"The rapid growth of scientific literature imposes significant challenges for
researchers endeavoring to stay updated with the latest advancements in their
fields and delve into new areas. We introduce OpenResearcher, an innovative
platform that leverages Artificial Intelligence (AI) techniques to accelerate
the research process by answering diverse questions from researchers.
OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to
integrate Large Language Models (LLMs) with up-to-date, domain-specific
knowledge. Moreover, we develop various tools for OpenResearcher to understand
researchers' queries, search from the scientific literature, filter retrieved
information, provide accurate and comprehensive answers, and self-refine these
answers. OpenResearcher can flexibly use these tools to balance efficiency and
effectiveness. As a result, OpenResearcher enables researchers to save time and
increase their potential to discover new insights and drive scientific
breakthroughs. Demo, video, and code are available at:
https://github.com/GAIR-NLP/OpenResearcher.",Yuxiang Zheng
2024-08-15T10:54:55Z,http://arxiv.org/abs/2408.08073v1,Extracting Sentence Embeddings from Pretrained Transformer Models,"Background/introduction: Pre-trained transformer models shine in many natural
language processing tasks and therefore are expected to bear the representation
of the input sentence or text meaning. These sentence-level embeddings are also
important in retrieval-augmented generation. But do commonly used plain
averaging or prompt templates surface it enough?
  Methods: Given 110M parameters BERT's hidden representations from multiple
layers and multiple tokens we tried various ways to extract optimal sentence
representations. We tested various token aggregation and representation
post-processing techniques. We also tested multiple ways of using a general
Wikitext dataset to complement BERTs sentence representations. All methods were
tested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12
classification tasks. We also evaluated our representation-shaping techniques
on other static models, including random token representations.
  Results: Proposed representation extraction methods improved the performance
on STS and clustering tasks for all models considered. Very high improvements
for static token-based models, especially random embeddings for STS tasks
almost reach the performance of BERT-derived representations.
  Conclusions: Our work shows that for multiple tasks simple baselines with
representation shaping techniques reach or even outperform more complex
BERT-based models or are able to contribute to their performance.",Lukas Stankevičius
2024-08-15T22:34:44Z,http://arxiv.org/abs/2408.08444v1,"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question
  Answering","In knowledge-intensive tasks such as open-domain question answering (OpenQA),
Large Language Models (LLMs) often struggle to generate factual answers relying
solely on their internal (parametric) knowledge. To address this limitation,
Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving
relevant information from external sources, thereby positioning the retriever
as a pivotal component. Although dense retrieval demonstrates state-of-the-art
performance, its training poses challenges due to the scarcity of ground-truth
evidence, largely attributed to the high costs of human annotation. In this
paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create
weakly labeled data for training dense retrievers. Specifically, we rerank the
top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will
generate the correct answer based on the question and each passage. The
highest-ranking passages are then used as positive training examples for dense
retrieval. Our comprehensive experiments across four publicly available OpenQA
datasets demonstrate that our approach enhances both retrieval and OpenQA
performance compared to baseline models.",Jinming Nian
2024-08-16T04:32:10Z,http://arxiv.org/abs/2408.08521v1,"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement
  Framework for Multimodal Question Answering","Recent advancements in retrieval-augmented generation (RAG) have demonstrated
impressive performance in the question-answering (QA) task. However, most
previous works predominantly focus on text-based answers. While some studies
address multimodal data, they still fall short in generating comprehensive
multimodal answers, particularly for explaining concepts or providing
step-by-step tutorials on how to accomplish specific goals. This capability is
especially valuable for applications such as enterprise chatbots and settings
such as customer service and educational systems, where the answers are sourced
from multimodal data. In this paper, we introduce a simple and effective
framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR
enhances text-based answers by retrieving relevant multimodal data and refining
the responses to create coherent multimodal answers. This framework can be
easily extended to support multimodal answers in enterprise chatbots with
minimal modifications. Human evaluation results indicate that multimodal
answers generated by MuRAR are more useful and readable compared to plain text
answers.",Zhengyuan Zhu
2024-08-15T16:53:05Z,http://arxiv.org/abs/2408.08925v1,"Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building
  E-commerce Chat Assistants","This work presents Retail-GPT, an open-source RAG-based chatbot designed to
enhance user engagement in retail e-commerce by guiding users through product
recommendations and assisting with cart operations. The system is
cross-platform and adaptable to various e-commerce domains, avoiding reliance
on specific chat applications or commercial activities. Retail-GPT engages in
human-like conversations, interprets user demands, checks product availability,
and manages cart operations, aiming to serve as a virtual sales agent and test
the viability of such assistants across different retail businesses.",Bruno Amaral Teixeira de Freitas
2024-08-16T22:00:00Z,http://arxiv.org/abs/2408.09031v1,A Primer on Generative AI for Telecom: From Theory to Practice,"The rise of generative artificial intelligence (GenAI) is transforming the
telecom industry. GenAI models, particularly large language models (LLMs), have
emerged as powerful tools capable of driving innovation, improving efficiency,
and delivering superior customer services in telecom. This paper provides an
overview of GenAI for telecom from theory to practice. We review GenAI models
and discuss their practical applications in telecom. Furthermore, we describe
the key technology enablers and best practices for applying GenAI to telecom
effectively. We highlight the importance of retrieval augmented generation
(RAG) in connecting LLMs to telecom domain specific data sources to enhance the
accuracy of the LLMs' responses. We present a real-world use case on RAG-based
chatbot that can answer open radio access network (O-RAN) specific questions.
The demonstration of the chatbot to the O-RAN Alliance has triggered immense
interest in the industry. We have made the O-RAN RAG chatbot publicly
accessible on GitHub.",Xingqin Lin
2024-08-17T13:32:32Z,http://arxiv.org/abs/2408.09199v1,TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems,"In the pursuit of enhancing domain-specific Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) emerges as a promising solution to
mitigate issues such as hallucinations, outdated knowledge, and limited
expertise in highly specialized queries. However, existing approaches to RAG
fall short by neglecting system state variables, which are crucial for ensuring
adaptive control, retrieval halting, and system convergence. In this paper, we
introduce the TC-RAG through rigorous proof, a novel framework that addresses
these challenges by incorporating a Turing Complete System to manage state
variables, thereby enabling more efficient and accurate knowledge retrieval. By
leveraging a memory stack system with adaptive retrieval, reasoning, and
planning capabilities, TC-RAG not only ensures the controlled halting of
retrieval processes but also mitigates the accumulation of erroneous knowledge
via Push and Pop actions. In the case study of the medical domain, our
extensive experiments on real-world healthcare datasets demonstrate the
superiority of TC-RAG over existing methods in accuracy by over 7.20\%. Our
dataset and code have been available at
https://https://github.com/Artessay/SAMA.git.",Xinke Jiang
2024-08-17T19:17:00Z,http://arxiv.org/abs/2408.09277v1,"Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case
  Study at Ericsson","This paper presents our experience developing a Llama-based chatbot for
question answering about continuous integration and continuous delivery (CI/CD)
at Ericsson, a multinational telecommunications company. Our chatbot is
designed to handle the specificities of CI/CD documents at Ericsson, employing
a retrieval-augmented generation (RAG) model to enhance accuracy and relevance.
Our empirical evaluation of the chatbot on industrial CI/CD-related questions
indicates that an ensemble retriever, combining BM25 and embedding retrievers,
yields the best performance. When evaluated against a ground truth of 72 CI/CD
questions and answers at Ericsson, our most accurate chatbot configuration
provides fully correct answers for 61.11% of the questions, partially correct
answers for 26.39%, and incorrect answers for 12.50%. Through an error analysis
of the partially correct and incorrect answers, we discuss the underlying
causes of inaccuracies and provide insights for further refinement. We also
reflect on lessons learned and suggest future directions for further improving
our chatbot's accuracy.",Daksh Chaudhary
2024-08-05T00:43:56Z,http://arxiv.org/abs/2408.11058v1,LLM Agents Improve Semantic Code Search,"Code Search is a key task that many programmers often have to perform while
developing solutions to problems. Current methodologies suffer from an
inability to perform accurately on prompts that contain some ambiguity or ones
that require additional context relative to a code-base. We introduce the
approach of using Retrieval Augmented Generation (RAG) powered agents to inject
information into user prompts allowing for better inputs into embedding models.
By utilizing RAG, agents enhance user queries with relevant details from GitHub
repositories, making them more informative and contextually aligned.
Additionally, we introduce a multi-stream ensemble approach which when paired
with agentic workflow can obtain improved retrieval accuracy, which we deploy
on application called repo-rift.com. Experimental results on the CodeSearchNet
dataset demonstrate that RepoRift significantly outperforms existing methods,
achieving an 78.2% success rate at Success@10 and a 34.6% success rate at
Success@1. This research presents a substantial advancement in semantic code
search, highlighting the potential of agentic LLMs and RAG to enhance code
retrieval systems.",Sarthak Jain
2024-08-21T18:00:21Z,http://arxiv.org/abs/2408.11903v2,"Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for
  Ancient Indian Philosophy","LLMs have revolutionized the landscape of information retrieval and knowledge
dissemination. However, their application in specialized areas is often
hindered by factual inaccuracies and hallucinations, especially in long-tail
knowledge distributions. We explore the potential of retrieval-augmented
generation (RAG) models for long-form question answering (LFQA) in a
specialized knowledge domain. We present VedantaNY-10M, a dataset curated from
extensive public discourses on the ancient Indian philosophy of Advaita
Vedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,
focusing on transcription, retrieval, and generation performance. Human
evaluations by computational linguists and domain experts show that the RAG
model significantly outperforms the standard model in producing factual and
comprehensive responses having fewer hallucinations. In addition, a
keyword-based hybrid retriever that emphasizes unique low-frequency terms
further improves results. Our study provides insights into effectively
integrating modern large language models with ancient knowledge systems.
Project page with dataset and code: https://sites.google.com/view/vedantany-10m",Priyanka Mandikal
2024-08-21T21:34:01Z,http://arxiv.org/abs/2408.12003v1,"RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and
  Personalization","With the development of the modern social economy, tourism has become an
important way to meet people's spiritual needs, bringing development
opportunities to the tourism industry. However, existing large language models
(LLMs) face challenges in personalized recommendation capabilities and the
generation of content that can sometimes produce hallucinations. This study
proposes an optimization scheme for Tibet tourism LLMs based on
retrieval-augmented generation (RAG) technology. By constructing a database of
tourist viewpoints and processing the data using vectorization techniques, we
have significantly improved retrieval accuracy. The application of RAG
technology effectively addresses the hallucination problem in content
generation. The optimized model shows significant improvements in fluency,
accuracy, and relevance of content generation. This research demonstrates the
potential of RAG technology in the standardization of cultural tourism
information and data analysis, providing theoretical and technical support for
the development of intelligent cultural tourism service systems.",Jinhu Qi
2024-08-22T12:21:22Z,http://arxiv.org/abs/2408.12333v2,Graph Retrieval Augmented Trustworthiness Reasoning,"Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game ""Werewolf,"" comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.",Ying Zhu
2024-08-23T20:51:04Z,http://arxiv.org/abs/2408.13366v1,"CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations
  of Research Papers","This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.",Ekaterina Trofimova
2024-08-24T03:18:42Z,http://arxiv.org/abs/2408.13450v1,vitaLITy 2: Reviewing Academic Literature Using Large Language Models,"Academic literature reviews have traditionally relied on techniques such as
keyword searches and accumulation of relevant back-references, using databases
like Google Scholar or IEEEXplore. However, both the precision and accuracy of
these search techniques is limited by the presence or absence of specific
keywords, making literature review akin to searching for needles in a haystack.
We present vitaLITy 2, a solution that uses a Large Language Model or LLM-based
approach to identify semantically relevant literature in a textual embedding
space. We include a corpus of 66,692 papers from 1970-2023 which are searchable
through text embeddings created by three language models. vitaLITy 2
contributes a novel Retrieval Augmented Generation (RAG) architecture and can
be interacted with through an LLM with augmented prompts, including
summarization of a collection of papers. vitaLITy 2 also provides a chat
interface that allow users to perform complex queries without learning any new
programming language. This also enables users to take advantage of the
knowledge captured in the LLM from its enormous training corpus. Finally, we
demonstrate the applicability of vitaLITy 2 through two usage scenarios.
vitaLITy 2 is available as open-source software at
https://vitality-vis.github.io.",Hongye An
2024-08-24T09:23:01Z,http://arxiv.org/abs/2408.13533v1,"Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the
  Role of RAG Noise in Large Language Models","Retrieval-Augmented Generation (RAG) has emerged as a crucial method for
addressing hallucinations in large language models (LLMs). While recent
research has extended RAG models to complex noisy scenarios, these explorations
often confine themselves to limited noise types and presuppose that noise is
inherently detrimental to LLMs, potentially deviating from real-world retrieval
environments and restricting practical applicability. In this paper, we define
seven distinct noise types from a linguistic perspective and establish a Noise
RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing
multiple datasets and reasoning tasks. Through empirical evaluation of eight
representative LLMs with diverse architectures and scales, we reveal that these
noises can be further categorized into two practical groups: noise that is
beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs
(aka harmful noise). While harmful noise generally impairs performance,
beneficial noise may enhance several aspects of model capabilities and overall
performance. Our analysis offers insights for developing more robust, adaptable
RAG solutions and mitigating hallucinations across diverse retrieval scenarios.",Jinyang Wu
2024-08-25T11:09:15Z,http://arxiv.org/abs/2408.13808v1,"Towards Reliable Medical Question Answering: Techniques and Challenges
  in Mitigating Hallucinations in Language Models","The rapid advancement of large language models (LLMs) has significantly
impacted various domains, including healthcare and biomedicine. However, the
phenomenon of hallucination, where LLMs generate outputs that deviate from
factual accuracy or context, poses a critical challenge, especially in
high-stakes domains. This paper conducts a scoping study of existing techniques
for mitigating hallucinations in knowledge-based task in general and especially
for medical domains. Key methods covered in the paper include
Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback
loops, supervised fine-tuning, and prompt engineering. These techniques, while
promising in general contexts, require further adaptation and optimization for
the medical domain due to its unique demands for up-to-date, specialized
knowledge and strict adherence to medical guidelines. Addressing these
challenges is crucial for developing trustworthy AI systems that enhance
clinical decision-making and patient safety as well as accuracy of biomedical
scientific research.",Duy Khoa Pham
2024-08-26T14:45:03Z,http://arxiv.org/abs/2408.14317v1,Claim Verification in the Age of Large Language Models: A Survey,"The large and ever-increasing amount of data available on the Internet
coupled with the laborious task of manual claim and fact verification has
sparked the interest in the development of automated claim verification
systems. Several deep learning and transformer-based models have been proposed
for this task over the years. With the introduction of Large Language Models
(LLMs) and their superior performance in several NLP tasks, we have seen a
surge of LLM-based approaches to claim verification along with the use of novel
methods such as Retrieval Augmented Generation (RAG). In this survey, we
present a comprehensive account of recent claim verification frameworks using
LLMs. We describe the different components of the claim verification pipeline
used in these frameworks in detail including common approaches to retrieval,
prompting, and fine-tuning. Finally, we describe publicly available English
datasets created for this task.",Alphaeus Dmonte
2024-08-26T16:00:41Z,http://arxiv.org/abs/2408.14380v1,Probing Causality Manipulation of Large Language Models,"Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.",Chenyang Zhang
2024-08-11T18:04:56Z,http://arxiv.org/abs/2408.15264v1,"Validation Requirements for AI-based Intervention-Evaluation in Aging
  and Longevity Research and Practice","The field of aging and longevity research is overwhelmed by vast amounts of
data, calling for the use of Artificial Intelligence (AI), including Large
Language Models (LLMs), for the evaluation of geroprotective interventions.
Such evaluations should be correct, useful, comprehensive, explainable, and
they should consider causality, interdisciplinarity, adherence to standards,
longitudinal data and known aging biology. In particular, comprehensive
analyses should go beyond comparing data based on canonical biomedical
databases, suggesting the use of AI to interpret changes in biomarkers and
outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and
dedicated workflows employing, e.g., Retrieval-Augmented Generation. While
naive trust in the responses of AI tools can cause harm, adding our
requirements to LLM queries can improve response quality, calling for
benchmarking efforts and justifying the informed use of LLMs for advice on
longevity interventions.",Georg Fuellen
2024-08-30T08:26:55Z,http://arxiv.org/abs/2408.17095v2,"RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation
  and Retrieval-Guidance","Diffusion-based models demonstrate impressive generation capabilities.
However, they also have a massive number of parameters, resulting in enormous
model sizes, thus making them unsuitable for deployment on resource-constraint
devices. Block-wise generation can be a promising alternative for designing
compact-sized (parameter-efficient) deep generative models since the model can
generate one block at a time instead of generating the whole image at once.
However, block-wise generation is also considerably challenging because
ensuring coherence across generated blocks can be non-trivial. To this end, we
design a retrieval-augmented generation (RAG) approach and leverage the
corresponding blocks of the images retrieved by the RAG module to condition the
training and generation stages of a block-wise denoising diffusion model. Our
conditioning schemes ensure coherence across the different blocks during
training and, consequently, during generation. While we showcase our approach
using the latent diffusion model (LDM) as the base model, it can be used with
other variants of denoising diffusion models. We validate the solution of the
coherence problem through the proposed approach by reporting substantive
experiments to demonstrate our approach's effectiveness in compact model size
and excellent generation quality.",Avideep Mukherjee
2024-09-04T01:14:04Z,http://arxiv.org/abs/2409.02361v1,"Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented
  Ambiguous Question Answering","The retrieval augmented generation (RAG) framework addresses an ambiguity in
user queries in QA systems by retrieving passages that cover all plausible
interpretations and generating comprehensive responses based on the passages.
However, our preliminary studies reveal that a single retrieval process often
suffers from low quality results, as the retrieved passages frequently fail to
capture all plausible interpretations. Although the iterative RAG approach has
been proposed to address this problem, it comes at the cost of significantly
reduced efficiency. To address these issues, we propose the
diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved
passages to encompass diverse interpretations. Subsequently, DIVA verifies the
quality of the passages and adapts the most suitable approach tailored to their
quality. This approach improves the QA systems accuracy and robustness by
handling low quality retrieval issue in ambiguous questions, while enhancing
efficiency.",Yeonjun In
2024-09-05T05:34:16Z,http://arxiv.org/abs/2409.03258v3,"GraphInsight: Unlocking Insights in Large Language Models for Graph
  Structure Understanding","Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.",Yukun Cao
2024-09-10T15:39:32Z,http://arxiv.org/abs/2409.06595v1,"GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question
  Answering","Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.",Sacha Muller
2024-09-11T08:56:27Z,http://arxiv.org/abs/2409.07110v1,"Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and
  Education","This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed
to enhance user interaction for educational and research purposes. Leveraging
cutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as
a sophisticated AI assistant, exploiting the capabilities of traditional models
like ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval
Augmented Generation (RAG) through three primary methods: integration of
preprocessed documents, real-time processing of user-uploaded files, and
information retrieval from any specified website. Additionally, the chatbot
incorporates image generation via a Stable Diffusion Model (SDM), image
understanding and response generation through LLAVA, and search functionality
on the internet powered by secure search engine such as DuckDuckGo. To provide
comprehensive support, Bio-Eng-LMM offers text summarization, website content
summarization, and both text and voice interaction. The chatbot maintains
session memory to ensure contextually relevant and coherent responses. This
integrated platform builds upon the strengths of RAG-GPT and Web-Based RAG
Query (WBRQ) where the system fetches relevant information directly from the
web to enhance the LLMs response generation.",Ali Forootani
2024-09-04T19:00:59Z,http://arxiv.org/abs/2409.07487v2,MoA is All You Need: Building LLM Research Team using Mixture of Agents,"Large Language Models (LLMs) research in the financial domain is particularly
complex due to the sheer number of approaches proposed in literature.
Retrieval-Augmented Generation (RAG) has emerged as one of the leading methods
in the sector due to its inherent groundedness and data source variability. In
this work, we introduce a RAG framework called Mixture of Agents (MoA) and
demonstrate its viability as a practical, customizable, and highly effective
approach for scaling RAG applications. MoA is essentially a layered network of
individually customized small language models (Hoffmann et al., 2022)
collaborating to answer questions and extract information. While there are many
theoretical propositions for such an architecture and even a few libraries for
generally applying the structure in practice, there are limited documented
studies evaluating the potential of this framework considering real business
constraints such as cost and speed. We find that the MoA framework, consisting
of small language models (Hoffmann et al., 2022), produces higher quality and
more grounded responses across various financial domains that are core to
Vanguard's business while simultaneously maintaining low costs.",Sandy Chen
2024-09-12T02:40:28Z,http://arxiv.org/abs/2409.07713v1,"Experimenting with Legal AI Solutions: The Case of Question-Answering
  for Access to Justice","Generative AI models, such as the GPT and Llama series, have significant
potential to assist laypeople in answering legal questions. However, little
prior work focuses on the data sourcing, inference, and evaluation of these
models in the context of laypersons. To this end, we propose a human-centric
legal NLP pipeline, covering data sourcing, inference, and evaluation. We
introduce and release a dataset, LegalQA, with real and specific legal
questions spanning from employment law to criminal law, corresponding answers
written by legal experts, and citations for each answer. We develop an
automatic evaluation protocol for this dataset, then show that
retrieval-augmented generation from only 850 citations in the train set can
match or outperform internet-wide retrieval, despite containing 9 orders of
magnitude less data. Finally, we propose future directions for open-sourced
efforts, which fall behind closed-sourced models.",Jonathan Li
2024-09-13T02:08:47Z,http://arxiv.org/abs/2409.08479v2,"Exploring Information Retrieval Landscapes: An Investigation of a Novel
  Evaluation Techniques and Comparative Document Splitting Methods","The performance of Retrieval-Augmented Generation (RAG) systems in
information retrieval is significantly influenced by the characteristics of the
documents being processed. In this study, the structured nature of textbooks,
the conciseness of articles, and the narrative complexity of novels are shown
to require distinct retrieval strategies. A comparative evaluation of multiple
document-splitting methods reveals that the Recursive Character Splitter
outperforms the Token-based Splitter in preserving contextual integrity. A
novel evaluation technique is introduced, utilizing an open-source model to
generate a comprehensive dataset of question-and-answer pairs, simulating
realistic retrieval scenarios to enhance testing efficiency and metric
reliability. The evaluation employs weighted scoring metrics, including
SequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy
and relevance. This approach establishes a refined standard for evaluating the
precision of RAG systems, with future research focusing on optimizing chunk and
overlap sizes to improve retrieval accuracy and efficiency.",Esmaeil Narimissa
2024-09-13T13:34:32Z,http://arxiv.org/abs/2409.08820v1,"A RAG Approach for Generating Competency Questions in Ontology
  Engineering","Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.",Xueli Pan
2024-09-14T03:11:00Z,http://arxiv.org/abs/2409.09281v1,"Language Models ""Grok"" to Copy","We examine the pre-training dynamics of language models, focusing on their
ability to copy text from preceding context--a fundamental skill for various
LLM applications, including in-context learning (ICL) and retrieval-augmented
generation (RAG). We propose a novel perspective that Transformer-based
language models develop copying abilities similarly to grokking, which refers
to sudden generalization on test set long after the model fit to the training
set. Our experiments yield three arguments: (1) The pre-training loss decreases
rapidly, while the context copying ability of models initially lags and then
abruptly saturates. (2) The speed of developing copying ability is independent
of the number of tokens trained, similarly to how grokking speed is unaffected
by dataset size as long as the data distribution is preserved. (3) Induction
heads, the attention heads responsible for copying, form from shallow to deep
layers during training, mirroring the development of circuits in deeper layers
during grokking. We contend that the connection between grokking and context
copying can provide valuable insights for more effective language model
training, ultimately improving in-context performance. For example, we
demonstrated that techniques that enhance grokking, such as regularization,
either accelerate or enhance the development of context copying.",Ang Lv
2024-09-14T17:40:35Z,http://arxiv.org/abs/2409.09493v1,"Hacking, The Lazy Way: LLM Augmented Pentesting","Security researchers are continually challenged by the need to stay current
with rapidly evolving cybersecurity research, tools, and techniques. This
constant cycle of learning, unlearning, and relearning, combined with the
repetitive tasks of sifting through documentation and analyzing data, often
hinders productivity and innovation. This has led to a disparity where only
organizations with substantial resources can access top-tier security experts,
while others rely on firms with less skilled researchers who focus primarily on
compliance rather than actual security.
  We introduce ""LLM Augmented Pentesting,"" demonstrated through a tool named
""Pentest Copilot,"" to address this gap. This approach integrates Large Language
Models into penetration testing workflows. Our research includes a ""chain of
thought"" mechanism to streamline token usage and boost performance, as well as
unique Retrieval Augmented Generation implementation to minimize hallucinations
and keep models aligned with the latest techniques. Additionally, we propose a
novel file analysis approach, enabling LLMs to understand files. Furthermore,
we highlight a unique infrastructure system that supports if implemented, can
support in-browser assisted penetration testing, offering a robust platform for
cybersecurity professionals, These advancements mark a significant step toward
bridging the gap between automated tools and human expertise, offering a
powerful solution to the challenges faced by modern cybersecurity teams.",Dhruva Goyal
2024-09-17T01:37:57Z,http://arxiv.org/abs/2409.10825v3,"Unveiling and Mitigating Bias in Large Language Model Recommendations: A
  Path to Fairness","excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.",Anindya Bijoy Das
2024-09-17T07:44:06Z,http://arxiv.org/abs/2409.10955v1,"Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style","Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.",Yuepei Li
2024-09-17T14:47:33Z,http://arxiv.org/abs/2409.11242v2,"Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded
  Attributions and Learning to Refuse","LLMs are an integral component of retrieval-augmented generation (RAG)
systems. While many studies focus on evaluating the overall quality of
end-to-end RAG systems, there is a gap in understanding the appropriateness of
LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic
metric that evaluates the trustworthiness of LLMs within the RAG framework. Our
results show that various prompting methods, such as in-context learning, fail
to effectively adapt LLMs to the RAG task as measured by Trust-Score.
Consequently, we propose Trust-Align, a method to align LLMs for improved
Trust-Score performance. The LLaMA-3 family, aligned using our method,
significantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0),
QAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of
Trust-Align across different open-weight models, including the LLaMA series (1b
to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at
\url{https://anonymous.4open.science/r/trust-align}",Maojia Song
2024-09-19T07:39:22Z,http://arxiv.org/abs/2409.12524v1,"Should RAG Chatbots Forget Unimportant Conversations? Exploring
  Importance and Forgetting with Psychological Insights","While Retrieval-Augmented Generation (RAG) has shown promise in enhancing
long-term conversations, the increasing memory load as conversations progress
degrades retrieval accuracy. Drawing on psychological insights, we propose
LUFY, a simple yet effective method that focuses on emotionally arousing
memories and retains less than 10% of the conversation. In the user experiment,
participants interacted with three types of RAG chatbots, each for 2 hours over
4 sessions, marking the most extensive assessment of a chatbot's long-term
capabilities to date -- more than four times longer than any existing
benchmark. The results demonstrate that prioritizing arousing memories while
forgetting the majority of the conversation significantly enhances user
experience. This study pushes the frontier of long-term conversations and
highlights the importance of forgetting unimportant parts of conversations.
Code and Dataset: https://github.com/ryuichi-sumida/LUFY",Ryuichi Sumida
2024-09-20T14:30:45Z,http://arxiv.org/abs/2409.13537v1,"ShizishanGPT: An Agricultural Large Language Model Integrating Tools and
  Resources","Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",Shuting Yang
2024-09-10T17:51:21Z,http://arxiv.org/abs/2409.13741v1,Knowing When to Ask -- Bridging Large Language Models and Data,"Large Language Models (LLMs) are prone to generating factually incorrect
information when responding to queries that involve numerical and statistical
data or other timely facts. In this paper, we present an approach for enhancing
the accuracy of LLMs by integrating them with Data Commons, a vast, open-source
repository of public statistics from trusted organizations like the United
Nations (UN), Center for Disease Control and Prevention (CDC) and global census
bureaus. We explore two primary methods: Retrieval Interleaved Generation
(RIG), where the LLM is trained to produce natural language queries to retrieve
data from Data Commons, and Retrieval Augmented Generation (RAG), where
relevant data tables are fetched from Data Commons and used to augment the
LLM's prompt. We evaluate these methods on a diverse set of queries,
demonstrating their effectiveness in improving the factual accuracy of LLM
outputs. Our work represents an early step towards building more trustworthy
and reliable LLMs that are grounded in verifiable statistical data and capable
of complex factual reasoning.",Prashanth Radhakrishnan
2024-09-21T03:03:09Z,http://arxiv.org/abs/2409.13992v1,"SMART-RAG: Selection using Determinantal Matrices for Augmented
  Retrieval","Retrieval-Augmented Generation (RAG) has greatly improved large language
models (LLMs) by enabling them to generate accurate, contextually grounded
responses through the integration of external information. However,
conventional RAG approaches, which prioritize top-ranked documents based solely
on query-context relevance, often introduce redundancy and conflicting
information. This issue is particularly evident in unsupervised retrieval
settings, where there are no mechanisms to effectively mitigate these problems,
leading to suboptimal context selection. To address this, we propose Selection
using Matrices for Augmented Retrieval (SMART) in question answering tasks, a
fully unsupervised and training-free framework designed to optimize context
selection in RAG. SMART leverages Determinantal Point Processes (DPPs) to
simultaneously model relevance, diversity and conflict, ensuring the selection
of potentially high-quality contexts. Experimental results across multiple
datasets demonstrate that SMART significantly enhances QA performance and
surpasses previous unsupervised context selection methods, showing a promising
strategy for RAG.",Jiatao Li
2024-09-21T09:36:14Z,http://arxiv.org/abs/2409.14083v1,"SURf: Teaching Large Vision-Language Models to Selectively Utilize
  Retrieved Information","Large Vision-Language Models (LVLMs) have become pivotal at the intersection
of computer vision and natural language processing. However, the full potential
of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains
underutilized. Existing works either focus solely on the text modality or are
limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize
retrieved information and are sensitive to irrelevant or misleading references.
To address these challenges, we propose a self-refinement framework designed to
teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically,
when given questions that are incorrectly answered by the LVLM backbone, we
obtain references that help correct the answers (positive references) and those
that do not (negative references). We then fine-tune the LVLM backbone using a
combination of these positive and negative references. Our experiments across
three tasks and seven datasets demonstrate that our framework significantly
enhances LVLMs ability to effectively utilize retrieved multimodal references
and improves their robustness against irrelevant or misleading information. The
source code is available at https://github.com/GasolSun36/SURf.",Jiashuo Sun
2024-09-21T15:32:10Z,http://arxiv.org/abs/2409.14175v1,"QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and
  Option Shuffling","Large Language models (LLMs) have brought about substantial advancements in
the field of Question Answering (QA) systems. These models do remarkably well
in addressing intricate inquiries in a variety of disciplines. However, because
of domain-specific vocabulary, complex technological concepts, and the
requirement for exact responses applying LLMs to specialized sectors like
telecommunications presents additional obstacles. GPT-3.5 has been used in
recent work, to obtain noteworthy accuracy for telecom-related questions in a
Retrieval Augmented Generation (RAG) framework. Notwithstanding these
developments, the practical use of models such as GPT-3.5 is restricted by
their proprietary nature and high computing demands. This paper introduces
QMOS, an innovative approach which uses a Question-Masked loss and Option
Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice
Questions in the telecommunications domain. Our focus was on using opensource,
smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.
Our multi-faceted approach involves several enhancements to the whole LLM-RAG
pipeline of finetuning, retrieval, prompt engineering and inference. Our
approaches significantly outperform existing results, achieving accuracy
improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%
to 84.65% with Phi-2.",Blessed Guda
2024-09-21T16:46:15Z,http://arxiv.org/abs/2409.14192v2,"Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic
  Extraction","Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.",Hossein Sholehrasa
2024-09-23T00:09:34Z,http://arxiv.org/abs/2409.14634v2,"Scideator: Human-LLM Scientific Idea Generation Grounded in
  Research-Paper Facet Recombination","The scientific ideation process often involves blending salient aspects of
existing papers to create new ideas. To see if large language models (LLMs) can
assist this process, we contribute Scideator, a novel mixed-initiative tool for
scientific ideation. Starting from a user-provided set of papers, Scideator
extracts key facets (purposes, mechanisms, and evaluations) from these and
relevant papers, allowing users to explore the idea space by interactively
recombining facets to synthesize inventive ideas. Scideator also helps users to
gauge idea novelty by searching the literature for potential overlaps and
showing automated novelty assessments and explanations. To support these tasks,
Scideator introduces four LLM-powered retrieval-augmented generation (RAG)
modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty
Checker, and Idea Novelty Iterator. In a within-subjects user study, 19
computer-science researchers identified significantly more interesting ideas
using Scideator compared to a strong baseline combining a scientific search
engine with LLM interaction.",Marissa Radensky
2024-09-23T10:23:19Z,http://arxiv.org/abs/2409.14878v1,"InterMind: A Doctor-Patient-Family Interactive Depression Assessment
  System Empowered by Large Language Models","Depression poses significant challenges to patients and healthcare
organizations, necessitating efficient assessment methods. Existing paradigms
typically focus on a patient-doctor way that overlooks multi-role interactions,
such as family involvement in the evaluation and caregiving process. Moreover,
current automatic depression detection (ADD) methods usually model depression
detection as a classification or regression task, lacking interpretability for
the decision-making process. To address these issues, we developed InterMind, a
doctor-patient-family interactive depression assessment system empowered by
large language models (LLMs). Our system enables patients and families to
contribute descriptions, generates assistive diagnostic reports for doctors,
and provides actionable insights, improving diagnostic precision and
efficiency. To enhance LLMs' performance in psychological counseling and
diagnostic interpretability, we integrate retrieval-augmented generation (RAG)
and chain-of-thoughts (CoT) techniques for data augmentation, which mitigates
the hallucination issue of LLMs in specific scenarios after instruction
fine-tuning. Quantitative experiments and professional assessments by
clinicians validate the effectiveness of our system.",Zhiyuan Zhou
2024-09-09T07:28:14Z,http://arxiv.org/abs/2409.15337v1,Revisiting the Solution of Meta KDD Cup 2024: CRAG,"This paper presents the solution of our team APEX in the Meta KDD CUP 2024:
CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the
limitations of existing QA benchmarks in evaluating the diverse and dynamic
challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a
more comprehensive assessment of RAG performance and contributes to advancing
research in this field. We propose a routing-based domain and dynamic adaptive
RAG pipeline, which performs specific processing for the diverse and dynamic
nature of the question in all three stages: retrieval, augmentation, and
generation. Our method achieved superior performance on CRAG and ranked 2nd for
Task 2&3 on the final competition leaderboard. Our implementation is available
at this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.",Jie Ouyang
2024-09-23T20:05:12Z,http://arxiv.org/abs/2409.15515v1,"Learning When to Retrieve, What to Rewrite, and How to Respond in
  Conversational QA","Augmenting Large Language Models (LLMs) with information retrieval
capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial
for knowledge-intensive tasks. However, understanding users' contextual search
intent when generating responses is an understudied topic for conversational
question answering (QA). This conversational extension leads to additional
concerns when compared to single-turn QA as it is more challenging for systems
to comprehend conversational context and manage retrieved passages over
multiple turns. In this work, we propose a method for enabling LLMs to decide
when to retrieve in RAG settings given a conversational context. When retrieval
is deemed necessary, the LLM then rewrites the conversation for passage
retrieval and judges the relevance of returned passages before response
generation. Operationally, we build on the single-turn SELF-RAG framework (Asai
et al., 2023) and propose SELF-multi-RAG for conversational settings.
SELF-multi-RAG demonstrates improved capabilities over single-turn variants
with respect to retrieving relevant passages (by using summarized
conversational context) and assessing the quality of generated responses.
Experiments on three conversational QA datasets validate the enhanced response
generation capabilities of SELF-multi-RAG, with improvements of ~13% measured
by human annotation.",Nirmal Roy
2024-09-24T15:20:39Z,http://arxiv.org/abs/2409.16176v1,Cyber Knowledge Completion Using Large Language Models,"The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.",Braden K Webb
2024-09-24T17:37:54Z,http://arxiv.org/abs/2409.16266v1,"REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial
  Task Allocation in Multi-Human Multi-Robot Teams","Multi-human multi-robot teams combine the complementary strengths of humans
and robots to tackle complex tasks across diverse applications. However, the
inherent heterogeneity of these teams presents significant challenges in
initial task allocation (ITA), which involves assigning the most suitable tasks
to each team member based on their individual capabilities before task
execution. While current learning-based methods have shown promising results,
they are often computationally expensive to train, and lack the flexibility to
incorporate user preferences in multi-objective optimization and adapt to
last-minute changes in real-world dynamic environments. To address these
issues, we propose REBEL, an LLM-based ITA framework that integrates rule-based
and experience-enhanced learning. By leveraging Retrieval-Augmented Generation,
REBEL dynamically retrieves relevant rules and past experiences, enhancing
reasoning efficiency. Additionally, REBEL can complement pre-trained RL-based
ITA policies, improving situational awareness and overall team performance.
Extensive experiments validate the effectiveness of our approach across various
settings. More details are available at https://sites.google.com/view/ita-rebel .",Arjun Gupte
2024-09-25T09:41:46Z,http://arxiv.org/abs/2409.16779v1,LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ,"Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements.",Marc-Antoine Allard
2024-09-26T06:53:29Z,http://arxiv.org/abs/2409.17580v1,"Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case
  Study","Extracting meaningful insights from large and complex datasets poses
significant challenges, particularly in ensuring the accuracy and relevance of
retrieved information. Traditional data retrieval methods such as sequential
search and index-based retrieval often fail when handling intricate and
interconnected data structures, resulting in incomplete or misleading outputs.
To overcome these limitations, we introduce Structured-GraphRAG, a versatile
framework designed to enhance information retrieval across structured datasets
in natural language queries. Structured-GraphRAG utilizes multiple knowledge
graphs, which represent data in a structured format and capture complex
relationships between entities, enabling a more nuanced and comprehensive
retrieval of information. This graph-based approach reduces the risk of errors
in language model outputs by grounding responses in a structured format,
thereby enhancing the reliability of results. We demonstrate the effectiveness
of Structured-GraphRAG by comparing its performance with that of a recently
published method using traditional retrieval-augmented generation. Our findings
show that Structured-GraphRAG significantly improves query processing
efficiency and reduces response times. While our case study focuses on soccer
data, the framework's design is broadly applicable, offering a powerful tool
for data analysis and enhancing language model applications across various
structured domains.",Zahra Sepasdar
2024-09-26T08:55:21Z,http://arxiv.org/abs/2409.17648v3,"Efficient In-Domain Question Answering for Resource-Constrained
  Environments","Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.",Isaac Chung
2024-09-27T09:20:42Z,http://arxiv.org/abs/2409.18575v1,Corpus-informed Retrieval Augmented Generation of Clarifying Questions,"This study aims to develop models that generate corpus informed clarifying
questions for web search, in a way that ensures the questions align with the
available information in the retrieval corpus. We demonstrate the effectiveness
of Retrieval Augmented Language Models (RAG) in this process, emphasising their
ability to (i) jointly model the user query and retrieval corpus to pinpoint
the uncertainty and ask for clarifications end-to-end and (ii) model more
evidence documents, which can be used towards increasing the breadth of the
questions asked. However, we observe that in current datasets search intents
are largely unsupported by the corpus, which is problematic both for training
and evaluation. This causes question generation models to ``hallucinate'', ie.
suggest intents that are not in the corpus, which can have detrimental effects
in performance. To address this, we propose dataset augmentation methods that
align the ground truth clarifications with the retrieval corpus. Additionally,
we explore techniques to enhance the relevance of the evidence pool during
inference, but find that identifying ground truth intents within the corpus
remains challenging. Our analysis suggests that this challenge is partly due to
the bias of current datasets towards clarification taxonomies and calls for
data that can support generating corpus-informed clarifications.",Antonios Minas Krasakis
2024-09-16T20:36:17Z,http://arxiv.org/abs/2409.18986v1,"Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test
  Interpretation in Clinical Medicine","Accurate interpretation of lab results is crucial in clinical medicine, yet
most patient portals use universal normal ranges, ignoring factors like age and
gender. This study introduces Lab-AI, an interactive system that offers
personalized normal ranges using Retrieval-Augmented Generation (RAG) from
credible health sources. Lab-AI has two modules: factor retrieval and normal
range retrieval. We tested these on 68 lab tests-30 with conditional factors
and 38 without. For tests with factors, normal ranges depend on
patient-specific information. Our results show that GPT-4-turbo with RAG
achieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal
range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by
29.1% in factor retrieval and showed 60.9% and 52.9% improvements in
question-level and lab-level performance, respectively, for normal range
retrieval. These findings highlight Lab-AI's potential to enhance patient
understanding of lab results.",Xiaoyu Wang
2024-09-28T23:59:46Z,http://arxiv.org/abs/2409.19487v3,"HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations","In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.",Ziyu Wang
2024-09-29T16:08:45Z,http://arxiv.org/abs/2409.19753v2,"CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex
  Knowledge Graph Question Answering","Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.",Yike Wu
2024-09-30T07:48:55Z,http://arxiv.org/abs/2409.20042v2,"Beyond Scores: A Modular RAG-Based System for Automatic Short Answer
  Scoring with Feedback","Automatic short answer scoring (ASAS) helps reduce the grading burden on
educators but often lacks detailed, explainable feedback. Existing methods in
ASAS with feedback (ASAS-F) rely on fine-tuning language models with limited
datasets, which is resource-intensive and struggles to generalize across
contexts. Recent approaches using large language models (LLMs) have focused on
scoring without extensive fine-tuning. However, they often rely heavily on
prompt engineering and either fail to generate elaborated feedback or do not
adequately evaluate it. In this paper, we propose a modular retrieval augmented
generation based ASAS-F system that scores answers and generates feedback in
strict zero-shot and few-shot learning scenarios. We design our system to be
adaptable to various educational tasks without extensive prompt engineering
using an automatic prompt generation framework. Results show an improvement in
scoring accuracy by 9\% on unseen questions compared to fine-tuning, offering a
scalable and cost-effective solution.",Menna Fateen
2024-09-30T15:53:38Z,http://arxiv.org/abs/2409.20434v1,"QAEncoder: Towards Aligned Representation Learning in Question Answering
  System","Modern QA systems entail retrieval-augmented generation (RAG) for accurate
and trustworthy responses. However, the inherent gap between user queries and
relevant documents hinders precise matching. Motivated by our conical
distribution hypothesis, which posits that potential queries and documents form
a cone-like structure in the embedding space, we introduce QAEncoder, a
training-free approach to bridge this gap. Specifically, QAEncoder estimates
the expectation of potential queries in the embedding space as a robust
surrogate for the document embedding, and attaches document fingerprints to
effectively distinguish these embeddings. Extensive experiments on fourteen
embedding models across six languages and eight datasets validate QAEncoder's
alignment capability, which offers a plug-and-play solution that seamlessly
integrates with existing RAG architectures and training-based methods.",Zhengren Wang
2024-10-01T07:18:34Z,http://arxiv.org/abs/2410.00454v1,UniAdapt: A Universal Adapter for Knowledge Calibration,"Large Language Models (LLMs) require frequent updates to correct errors and
keep pace with continuously evolving knowledge in a timely and effective
manner. Recent research in it model editing has highlighted the challenges in
balancing generalization and locality, especially in the context of lifelong
model editing. We discover that inserting knowledge directly into the model
often causes conflicts and potentially disrupts other unrelated pre-trained
knowledge. To address this problem, we introduce UniAdapt, a universal adapter
for knowledge calibration. Inspired by the Mixture of Experts architecture and
Retrieval-Augmented Generation, UniAdapt is designed with a vector-assisted
router that is responsible for routing inputs to appropriate experts. The
router maintains a vector store, including multiple shards, to construct
routing vectors based on semantic similarity search results. UniAdapt is fully
model-agnostic and designed for seamless plug-and-play integration.
Experimental results show that UniAdapt outperforms existing lifelong model
editors and achieves exceptional results in most metrics.",Tai D. Nguyen
2024-10-03T03:06:42Z,http://arxiv.org/abs/2410.02163v1,"Controlled Generation of Natural Adversarial Documents for Stealthy
  Retrieval Poisoning","Recent work showed that retrieval based on embedding similarity (e.g., for
retrieval-augmented generation) is vulnerable to poisoning: an adversary can
craft malicious documents that are retrieved in response to broad classes of
queries. We demonstrate that previous, HotFlip-based techniques produce
documents that are very easy to detect using perplexity filtering. Even if
generation is constrained to produce low-perplexity text, the resulting
documents are recognized as unnatural by LLMs and can be automatically filtered
from the retrieval corpus.
  We design, implement, and evaluate a new controlled generation technique that
combines an adversarial objective (embedding similarity) with a ""naturalness""
objective based on soft scores computed using an open-source, surrogate LLM.
The resulting adversarial documents (1) cannot be automatically detected using
perplexity filtering and/or other LLMs, except at the cost of significant false
positives in the retrieval corpus, yet (2) achieve similar poisoning efficacy
to easily-detectable documents generated using HotFlip, and (3) are
significantly more effective than prior methods for energy-guided generation,
such as COLD.",Collin Zhang
2024-10-03T09:48:09Z,http://arxiv.org/abs/2410.02338v2,How Much Can RAG Help the Reasoning of LLM?,"Retrieval-Augmented Generation (RAG) has gained significant popularity in
modern Large Language Models (LLMs) due to its effectiveness in introducing new
knowledge and reducing hallucinations. However, the deep understanding of RAG
remains limited, how does RAG help the reasoning process and can RAG help
improve the reasoning capability remains question. While external documents are
typically considered as a method to incorporate domain-specific information,
they also contain intermediate reasoning results related to the query, this
suggests that documents could enhance the reasoning capability of LLMs, which
has not been previously explored. In this paper, we investigate this issue in
depth and find that while RAG can assist with reasoning, the help is limited.
If we conceptualize the reasoning process as a tree with fixed depth, then RAG
struggles to assist LLMs in performing deeper reasoning. Additionally, the
information in the documents requires preprocessing to filter out noise. We
demonstrate that this preprocessing is difficult to achieve simply fine-tuning
of the LLM, it often necessitates numerous additional transformer layers to
solve the problem. To simplify the problem, we propose DPrompt tuning, which
effectively resolves the issue within just limited transformer layers, leading
to improved performance.",Jingyu Liu
2024-10-03T14:55:22Z,http://arxiv.org/abs/2410.02551v1,"ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration","We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.",Zixiang Wang
2024-10-03T16:34:46Z,http://arxiv.org/abs/2410.02650v1,Undesirable Memorization in Large Language Models: A Survey,"While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.",Ali Satvaty
2024-10-03T17:55:09Z,http://arxiv.org/abs/2410.02742v2,"Grounding Large Language Models In Embodied Environment With Imperfect
  World Models","Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.",Haolan Liu
2024-10-03T19:25:05Z,http://arxiv.org/abs/2410.02932v1,Intrinsic Evaluation of RAG Systems for Deep-Logic Questions,"We introduce the Overall Performance Index (OPI), an intrinsic metric to
evaluate retrieval-augmented generation (RAG) mechanisms for applications
involving deep-logic queries. OPI is computed as the harmonic mean of two key
metrics: the Logical-Relation Correctness Ratio and the average of BERT
embedding similarity scores between ground-truth and generated answers. We
apply OPI to assess the performance of LangChain, a popular RAG tool, using a
logical relations classifier fine-tuned from GPT-4o on the RAG-Dataset-12000
from Hugging Face. Our findings show a strong correlation between BERT
embedding similarity scores and extrinsic evaluation scores. Among the commonly
used retrievers, the cosine similarity retriever using BERT-based embeddings
outperforms others, while the Euclidean distance-based retriever exhibits the
weakest performance. Furthermore, we demonstrate that combining multiple
retrievers, either algorithmically or by merging retrieved sentences, yields
superior performance compared to using any single retriever alone.",Junyi Hu
2024-10-04T15:54:49Z,http://arxiv.org/abs/2410.03537v1,Ward: Provable RAG Dataset Inference via LLM Watermarks,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",Nikola Jovanović
2024-09-30T06:27:53Z,http://arxiv.org/abs/2410.03727v2,"FaithEval: Can Your Language Model Stay Faithful to Context, Even If
  ""The Moon is Made of Marshmallows""","Ensuring faithfulness to context in large language models (LLMs) and
retrieval-augmented generation (RAG) systems is crucial for reliable deployment
in real-world applications, as incorrect or unsupported information can erode
user trust. Despite advancements on standard benchmarks, faithfulness
hallucination-where models generate responses misaligned with the provided
context-remains a significant challenge. In this work, we introduce FaithEval,
a novel and comprehensive benchmark tailored to evaluate the faithfulness of
LLMs in contextual scenarios across three diverse tasks: unanswerable,
inconsistent, and counterfactual contexts. These tasks simulate real-world
challenges where retrieval mechanisms may surface incomplete, contradictory, or
fabricated information. FaithEval comprises 4.9K high-quality problems in
total, validated through a rigorous four-stage context construction and
validation framework, employing both LLM-based auto-evaluation and human
validation. Our extensive study across a wide range of open-source and
proprietary models reveals that even state-of-the-art models often struggle to
remain faithful to the given context, and that larger models do not necessarily
exhibit improved faithfulness.Project is available at:
\url{https://github.com/SalesforceAIResearch/FaithEval}.",Yifei Ming
2024-10-02T05:24:49Z,http://arxiv.org/abs/2410.03754v1,Enhancing Retrieval in QA Systems with Derived Feature Association,"Retrieval augmented generation (RAG) has become the standard in long context
question answering (QA) systems. However, typical implementations of RAG rely
on a rather naive retrieval mechanism, in which texts whose embeddings are most
similar to that of the query are deemed most relevant. This has consequences in
subjective QA tasks, where the most relevant text may not directly contain the
answer. In this work, we propose a novel extension to RAG systems, which we
call Retrieval from AI Derived Documents (RAIDD). RAIDD leverages the full
power of the LLM in the retrieval process by deriving inferred features, such
as summaries and example questions, from the documents at ingest. We
demonstrate that this approach significantly improves the performance of RAG
systems on long-context QA tasks.",Keyush Shah
2024-10-03T15:26:50Z,http://arxiv.org/abs/2410.03780v1,Reward-RAG: Enhancing RAG with Reward Driven Supervision,"In this paper, we introduce Reward-RAG, a novel approach designed to enhance
the Retrieval-Augmented Generation (RAG) model through Reward-Driven
Supervision. Unlike previous RAG methodologies, which focus on training
language models (LMs) to utilize external knowledge retrieved from external
sources, our method adapts retrieval information to specific domains by
employing CriticGPT to train a dedicated reward model. This reward model
generates synthesized datasets for fine-tuning the RAG encoder, aligning its
outputs more closely with human preferences. The versatility of our approach
allows it to be effectively applied across various domains through
domain-specific fine-tuning. We evaluate Reward-RAG on publicly available
benchmarks from multiple domains, comparing it to state-of-the-art methods. Our
experimental results demonstrate significant improvements in performance,
highlighting the effectiveness of Reward-RAG in improving the relevance and
quality of generated responses. These findings underscore the potential of
integrating reward models with RAG to achieve superior outcomes in natural
language generation tasks.",Thang Nguyen
2024-10-04T18:22:58Z,http://arxiv.org/abs/2410.03845v2,ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD,"Open-source Electronic Design Automation (EDA) tools are rapidly transforming
chip design by addressing key barriers of commercial EDA tools such as
complexity, costs, and access. Recent advancements in Large Language Models
(LLMs) have further enhanced efficiency in chip design by providing user
assistance across a range of tasks like setup, decision-making, and flow
automation. This paper introduces ORAssistant, a conversational assistant for
OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to
improve the user experience for the OpenROAD flow, from RTL-GDSII by providing
context-specific responses to common user queries, including installation,
command usage, flow setup, and execution, in prose format. Currently,
ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and
KLayout. The data model is built from publicly available documentation and
GitHub resources. The proposed architecture is scalable, supporting extensions
to other open-source tools, operating modes, and LLM models. We use Google
Gemini as the base LLM model to build and test ORAssistant. Early evaluation
results of the RAG-based model show notable improvements in performance and
accuracy compared to non-fine-tuned LLMs.",Aviral Kaintura
2024-10-05T15:13:22Z,http://arxiv.org/abs/2410.04194v1,Consistent Autoformalization for Constructing Mathematical Libraries,"Autoformalization is the task of automatically translating mathematical
content written in natural language to a formal language expression. The
growing language interpretation capabilities of Large Language Models (LLMs),
including in formal languages, are lowering the barriers for autoformalization.
However, LLMs alone are not capable of consistently and reliably delivering
autoformalization, in particular as the complexity and specialization of the
target domain grows. As the field evolves into the direction of systematically
applying autoformalization towards large mathematical libraries, the need to
improve syntactic, terminological and semantic control increases. This paper
proposes the coordinated use of three mechanisms, most-similar retrieval
augmented generation (MS-RAG), denoising steps, and auto-correction with syntax
error feedback (Auto-SEF) to improve autoformalization quality. The empirical
analysis, across different models, demonstrates that these mechanisms can
deliver autoformalizaton results which are syntactically, terminologically and
semantically more consistent. These mechanisms can be applied across different
LLMs and have shown to deliver improve results across different model types.",Lan Zhang
2024-10-06T11:23:56Z,http://arxiv.org/abs/2410.04452v1,"MindScope: Exploring cognitive biases in large language models through
  Multi-Agent Systems","Detecting cognitive biases in large language models (LLMs) is a fascinating
task that aims to probe the existing cognitive biases within these models.
Current methods for detecting cognitive biases in language models generally
suffer from incomplete detection capabilities and a restricted range of
detectable bias types. To address this issue, we introduced the 'MindScope'
dataset, which distinctively integrates static and dynamic elements. The static
component comprises 5,170 open-ended questions spanning 72 cognitive bias
categories. The dynamic component leverages a rule-based, multi-agent
communication framework to facilitate the generation of multi-round dialogues.
This framework is flexible and readily adaptable for various psychological
experiments involving LLMs. In addition, we introduce a multi-agent detection
method applicable to a wide range of detection tasks, which integrates
Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement
learning-based decision module. Demonstrating substantial effectiveness, this
method has shown to improve detection accuracy by as much as 35.10% compared to
GPT-4. Codes and appendix are available at
https://github.com/2279072142/MindScope.",Zhentao Xie
2024-10-07T04:15:02Z,http://arxiv.org/abs/2410.04739v2,TableRAG: Million-Token Table Understanding with Language Models,"Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.",Si-An Chen
2024-10-07T05:27:22Z,http://arxiv.org/abs/2410.04759v1,"Driving with Regulation: Interpretable Decision-Making for Autonomous
  Vehicles with Retrieval-Augmented Reasoning via LLM","This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease.",Tianhui Cai
2024-10-07T13:03:45Z,http://arxiv.org/abs/2410.05004v1,Fast State Restoration in LLM Serving with HCache,"The growing complexity of LLM usage today, e.g., multi-round conversation and
retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)
reusable across user requests. Given the capacity constraints of GPU memory,
only a limited number of contexts can be cached on GPU for reusing. Existing
inference systems typically evict part of the KV cache and restore it by
recomputing it from the original tokens or offloading it to host storage for
later retrieval, both of which introduce substantial computational or I/O
overheads. We propose HCache, a novel LLM state restoration method. Its key
idea is to restore LLM states from intermediate activations and thus utilize
computational and I/O resources with low overhead. We enhance HCache with two
techniques, including i) a bubble-free restoration scheduler that integrates
resource-complementary methods to optimize the balance between computation and
IO tasks; and ii) a chunk-based storage manager to address the layout mismatch
issue (i.e., layer-before-token saving versus token-before-layer restoration).
Our evaluations, conducted using real-world tasks, show that HCache reduces the
TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less
storage space; compared to token recomputation, HCache achieves up to 5.73X
reduction in TTFT.",Shiwei Gao
2024-10-07T16:14:47Z,http://arxiv.org/abs/2410.05162v1,"Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models","Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.",Mehrdad Farahani
2024-10-08T11:33:09Z,http://arxiv.org/abs/2410.05930v1,"Fortify Your Foundations: Practical Privacy and Security for Foundation
  Model Deployments In The Cloud","Foundation Models (FMs) display exceptional performance in tasks such as
natural language processing and are being applied across a growing range of
disciplines. Although typically trained on large public datasets, FMs are often
fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,
which rely on private data. This access, along with their size and costly
training, heightens the risk of intellectual property theft. Moreover,
multimodal FMs may expose sensitive information. In this work, we examine the
FM threat model and discuss the practicality and comprehensiveness of various
approaches for securing against them, such as ML-based methods and trusted
execution environments (TEEs). We demonstrate that TEEs offer an effective
balance between strong security properties, usability, and performance.
Specifically, we present a solution achieving less than 10\% overhead versus
bare metal for the full Llama2 7B and 13B inference pipelines running inside
\intel\ SGX and \intel\ TDX. We also share our configuration files and insights
from our implementation. To our knowledge, our work is the first to show the
practicality of TEEs for securing FMs.",Marcin Chrapek
2024-10-08T15:22:36Z,http://arxiv.org/abs/2410.06121v1,"Less is More: Making Smaller Language Models Competent Subgraph
  Retrievers for Multi-hop KGQA","Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.",Wenyu Huang
2024-10-09T15:10:00Z,http://arxiv.org/abs/2410.06972v1,"Diamond of Thought: A Design Thinking-Based Framework for LLMs in
  Wearable Design","Wearable design is an interdisciplinary field that balances technological
innovation, human factors, and human-computer interactions. Despite
contributions from various disciplines, many projects lack stable
interdisciplinary teams, which often leads to design failures. Large language
models (LLMs) integrate diverse information and generate innovative solutions,
making them a valuable tool for enhancing design processes. Thus, we have
explored the use of LLMs in wearable design by combining design-thinking
principles with LLM capabilities. We have developed the ""Diamond of Thought""
framework and analysed 1,603 prototypes and 1,129 products from a body-centric
perspective to create a comprehensive database. We employed retrieval-augmented
generation to input database details into the LLMs, ensuring applicability to
wearable design challenges and integration of embodied cognition into the
process. Our LLM-based methodology for wearables has been experimentally
validated, demonstrating the potential of LLMs for the advancement of design
practices. This study offers new tools and methods for future wearable designs.",Qiyang Miao
2024-10-10T02:48:06Z,http://arxiv.org/abs/2410.07551v1,KRAG Framework for Enhancing LLMs in the Legal Domain,"This paper introduces Knowledge Representation Augmented Generation (KRAG), a
novel framework designed to enhance the capabilities of Large Language Models
(LLMs) within domain-specific applications. KRAG points to the strategic
inclusion of critical knowledge entities and relationships that are typically
absent in standard data sets and which LLMs do not inherently learn. In the
context of legal applications, we present Soft PROLEG, an implementation model
under KRAG, which uses inference graphs to aid LLMs in delivering structured
legal reasoning, argumentation, and explanations tailored to user inquiries.
The integration of KRAG, either as a standalone framework or in tandem with
retrieval augmented generation (RAG), markedly improves the ability of language
models to navigate and solve the intricate challenges posed by legal texts and
terminologies. This paper details KRAG's methodology, its implementation
through Soft PROLEG, and potential broader applications, underscoring its
significant role in advancing natural language understanding and processing in
specialized knowledge domains.",Nguyen Ha Thanh
2024-10-10T02:58:52Z,http://arxiv.org/abs/2410.07561v2,"AI-Press: A Multi-Agent News Generating and Feedback Simulation System
  Powered by Large Language Models","The rise of various social platforms has transformed journalism. The growing
demand for news content has led to the increased use of large language models
(LLMs) in news production due to their speed and cost-effectiveness. However,
LLMs still encounter limitations in professionalism and ethical judgment in
news generation. Additionally, predicting public feedback is usually difficult
before news is released. To tackle these challenges, we introduce AI-Press, an
automated news drafting and polishing system based on multi-agent collaboration
and Retrieval-Augmented Generation. We develop a feedback simulation system
that generates public feedback considering demographic distributions. Through
extensive quantitative and qualitative evaluations, our system shows
significant improvements in news-generating capabilities and verifies the
effectiveness of public feedback simulation.",Xiawei Liu
2024-10-11T11:41:02Z,http://arxiv.org/abs/2410.08731v1,"Developing a Pragmatic Benchmark for Assessing Korean Legal Language
  Understanding in Large Language Models","Large language models (LLMs) have demonstrated remarkable performance in the
legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However
their efficacy remains limited for non-standardized tasks and tasks in
languages other than English. This underscores the need for careful evaluation
of LLMs within each legal system before application. Here, we introduce KBL, a
benchmark for assessing the Korean legal language understanding of LLMs,
consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning
tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510
examples). First two datasets were developed in close collaboration with
lawyers to evaluate LLMs in practical scenarios in a certified manner.
Furthermore, considering legal practitioners' frequent use of extensive legal
documents for research, we assess LLMs in both a closed book setting, where
they rely solely on internal knowledge, and a retrieval-augmented generation
(RAG) setting, using a corpus of Korean statutes and precedents. The results
indicate substantial room and opportunities for improvement.",Yeeun Kim
2024-10-11T13:52:44Z,http://arxiv.org/abs/2410.08815v2,"StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via
  Inference-time Hybrid Information Structurization","Retrieval-augmented generation (RAG) is a key means to effectively enhance
large language models (LLMs) in many knowledge-based tasks. However, existing
RAG methods struggle with knowledge-intensive reasoning tasks, because useful
information required to these tasks are badly scattered. This characteristic
makes it difficult for existing RAG methods to accurately identify key
information and perform global reasoning with such noisy augmentation. In this
paper, motivated by the cognitive theories that humans convert raw information
into various structured knowledge when tackling knowledge-intensive reasoning,
we proposes a new framework, StructRAG, which can identify the optimal
structure type for the task at hand, reconstruct original documents into this
structured format, and infer answers based on the resulting structure.
Extensive experiments across various knowledge-intensive tasks show that
StructRAG achieves state-of-the-art performance, particularly excelling in
challenging scenarios, demonstrating its potential as an effective solution for
enhancing LLMs in complex real-world applications.",Zhuoqun Li
2024-10-11T17:32:59Z,http://arxiv.org/abs/2410.09019v1,"MedMobile: A mobile-sized language model with expert-level clinical
  capabilities","Language models (LMs) have demonstrated expert-level reasoning and recall
abilities in medicine. However, computational costs and privacy concerns are
mounting barriers to wide-scale implementation. We introduce a parsimonious
adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of
running on a mobile device, for medical applications. We demonstrate that
MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for
physicians (~60%), and approaching the scores of models 100 times its size. We
subsequently perform a careful set of ablations, and demonstrate that chain of
thought, ensembling, and fine-tuning lead to the greatest performance gains,
while unexpectedly retrieval augmented generation fails to demonstrate
significant improvements",Krithik Vishwanath
2024-10-11T17:10:10Z,http://arxiv.org/abs/2410.09136v1,"Underutilized land and sustainable development: effects on employment,
  economic output, and mitigation of CO2 emissions","Climate change, deforestation, and biodiversity loss are calling for
innovative approaches to effective reforestation and afforestation. This paper
explores the integration of artificial intelligence and remote sensing
technologies for optimizing tree planting strategies, estimating labor
requirements, and determining space needs for various tree species in Gabala
District of Azerbaijan. The study employs YOLOv8 for precise identification of
potential planting sites and a Retrieval-Augmented Generation approach,
combined with the Gemini API, to provide tailored species recommendations. The
methodology incorporates time-series modeling to forecast the impact of
reforestation on CO2 emissions reduction, utilizing Holt-Winters for
predictions. Our results indicate that the AI model can effectively identify
suitable locations and species, offering valuable insights into the potential
economic and environmental benefits of large-scale tree planting thus fostering
sustainable economic development and helping to mitigate the adverse effects of
global warming and climate change.",Seymur Garibov
2024-10-12T19:38:09Z,http://arxiv.org/abs/2410.09629v1,"Synthetic Knowledge Ingestion: Towards Knowledge Refinement and
  Injection for Enhancing Large Language Models","Large language models (LLMs) are proficient in capturing factual knowledge
across various domains. However, refining their capabilities on previously seen
knowledge or integrating new knowledge from external sources remains a
significant challenge. In this work, we propose a novel synthetic knowledge
ingestion method called Ski, which leverages fine-grained synthesis,
interleaved generation, and assemble augmentation strategies to construct
high-quality data representations from raw knowledge sources. We then integrate
Ski and its variations with three knowledge injection techniques: Retrieval
Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual
Pre-training (CPT) to inject and refine knowledge in language models. Extensive
empirical experiments are conducted on various question-answering tasks
spanning finance, biomedicine, and open-generation domains to demonstrate that
Ski significantly outperforms baseline methods by facilitating effective
knowledge injection. We believe that our work is an important step towards
enhancing the factual accuracy of LLM outputs by refining knowledge
representation and injection capabilities.",Jiaxin Zhang
2024-10-13T17:53:50Z,http://arxiv.org/abs/2410.09942v1,"Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization","This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and retrieval-augmentation strategy.
We introduce an iterative approach where the search engine generates retrieval
results for these RAG agents and gathers feedback on the quality of the
retrieved documents during an offline phase. This feedback is then used to
iteratively optimize the search engine using a novel expectation-maximization
algorithm, with the goal of maximizing each agent's utility function.
Additionally, we adapt this approach to an online setting, allowing the search
engine to refine its behavior based on real-time individual agents feedback to
better serve the results for each of them. Experiments on diverse datasets from
the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our
approach significantly on average outperforms competitive baselines across 18
RAG models. We also demonstrate that our method effectively ``personalizes''
the retrieval process for each RAG agent based on the collected feedback.
Finally, we provide a comprehensive ablation study to explore various aspects
of our method.",Alireza Salemi
2024-10-14T04:06:22Z,http://arxiv.org/abs/2410.10136v1,"Beyond-RAG: Question Identification and Answer Generation in Real-Time
  Conversations","In customer contact centers, human agents often struggle with long average
handling times (AHT) due to the need to manually interpret queries and retrieve
relevant knowledge base (KB) articles. While retrieval augmented generation
(RAG) systems using large language models (LLMs) have been widely adopted in
industry to assist with such tasks, RAG faces challenges in real-time
conversations, such as inaccurate query formulation and redundant retrieval of
frequently asked questions (FAQs). To address these limitations, we propose a
decision support system that can look beyond RAG by first identifying customer
questions in real time. If the query matches an FAQ, the system retrieves the
answer directly from the FAQ database; otherwise, it generates answers via RAG.
Our approach reduces reliance on manual queries, providing responses to agents
within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva
CQ, this system improves efficiency, reduces AHT, and lowers operational costs.
We also introduce an automated LLM-agentic workflow to identify FAQs from
historical transcripts when no predefined FAQs exist.",Garima Agrawal
2024-10-14T08:47:21Z,http://arxiv.org/abs/2410.10293v1,FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG,"Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the retrieval
paradigm that we design and use remains flat, which treats the retrieval
procedures as a one-off deal with constant granularity. Despite effectiveness,
we argue that they suffer from two limitations: (1) flat retrieval exerts a
significant burden on one retriever; (2) constant granularity limits the
ceiling of retrieval performance. In this work, we propose a progressive
retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,
so as to balance effectiveness and efficiency. Specifically, FunnelRAG
establishes a progressive retrieval pipeline by collaborating coarse-to-fine
granularity, large-to-small quantity, and low-to-high capacity, which can
relieve the burden on one retriever and also promote the ceiling of retrieval
performance. Extensive experiments manifest that FunnelRAG achieves comparable
retrieval performance while the time overhead is reduced by nearly 40 percent.",Xinping Zhao
2024-10-14T10:26:57Z,http://arxiv.org/abs/2410.10360v2,"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented
  Language Models with Parameter Decoupling and Tailored Tuning","Retrieval-Augmented Generation (RAG) offers an effective solution to the
issues faced by Large Language Models (LLMs) in hallucination generation and
knowledge obsolescence by incorporating externally retrieved knowledge.
However, existing methods lack effective control mechanisms for integrating
internal and external knowledge. Inspired by human cognitive processes, we
propose Parenting, a novel framework that decouples, identifies, and
purposefully optimizes parameter subspaces related to adherence and robustness.
Specifically, Parenting utilizes a key parameter mining method that combines
forward and backward propagation signals to localize subspaces representing
different capabilities. Then, Parenting employs a type-tailored tuning
strategy, applying specific and appropriate optimizations to different
subspaces, aiming to achieve a balanced enhancement of both adherence and
robustness. Extensive experiments on various datasets and models validate the
effectiveness and generalizability of our method.",Yongxin Xu
2024-10-14T12:45:10Z,http://arxiv.org/abs/2410.10450v1,KBLaM: Knowledge Base augmented Language Model,"In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a
new method for augmenting Large Language Models (LLMs) with external knowledge.
KBLaM works with a knowledge base (KB) constructed from a corpus of documents,
transforming each piece of knowledge in the KB into continuous key-value vector
pairs via pre-trained sentence encoders with linear adapters and integrating
them into pre-trained LLMs via a specialized rectangular attention mechanism.
Unlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval
modules, and unlike in-context learning, its computational overhead scales
linearly with KB size rather than quadratically. Our approach enables
integrating a large KB of more than 10K triples into an 8B pre-trained LLM of
only 8K context window on one single A100 80GB GPU and allows for dynamic
updates without model fine-tuning or retraining. Experiments demonstrate
KBLaM's effectiveness in various tasks, including question-answering and
open-ended reasoning, while providing interpretable insights into its use of
the augmented knowledge.",Xi Wang
2024-10-14T13:18:20Z,http://arxiv.org/abs/2410.10481v1,"Model-Based Differentially Private Knowledge Transfer for Large Language
  Models","As large language models (LLMs) become increasingly prevalent in web
services, effectively leveraging domain-specific knowledge while ensuring
privacy has become critical. Existing methods, such as retrieval-augmented
generation (RAG) and differentially private data synthesis, often compromise
either the utility of domain knowledge or the privacy of sensitive data,
limiting their applicability in specialized domains. To address these
challenges, we propose \textit{Llamdex}, a novel framework that integrates
privacy-preserving, domain-specific models into LLMs. Our approach
significantly enhances the accuracy of domain-specific tasks, achieving up to a
26\% improvement compared to existing methods under the same differential
privacy constraints. Experimental results show that Llamdex not only improves
the accuracy of LLM responses but also maintains comparable inference
efficiency to the original LLM, highlighting its potential for real-world
applications.",Zhaomin Wu
2024-10-14T14:56:01Z,http://arxiv.org/abs/2410.10584v1,"STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with
  FeedBack","Large Language Models (LLMs) often generate incorrect or outdated
information, especially in low-resource settings or when dealing with private
data. To address this, Retrieval-Augmented Generation (RAG) uses external
knowledge bases (KBs), but these can also suffer from inaccuracies. We
introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base
editing with FEEDback approach that iteratively refines the KB based on expert
feedback using a multi-actor, centralized critic reinforcement learning
framework. Each document is assigned to an actor, modeled as a ReACT agent,
which performs structured edits based on document-specific targeted
instructions from a centralized critic. Experimental results show that
STACKFEED significantly improves KB quality and RAG system performance,
enhancing accuracy by up to 8% over baselines.",Naman Gupta
2024-10-14T04:57:32Z,http://arxiv.org/abs/2410.10913v2,"Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined
  Knowledge Base","Recent advances in audio understanding tasks leverage the reasoning
capabilities of LLMs. However, adapting LLMs to learn audio concepts requires
massive training data and substantial computational resources. To address these
challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs
from a knowledge base (KB) and augments them with query audio to generate
accurate textual responses. In RAG, the relevance of the retrieved information
plays a crucial role in effectively processing the input. In this paper, we
analyze how different retrieval methods and knowledge bases impact the
relevance of audio-text pairs and the performance of audio captioning with RAG.
We propose generative pair-to-pair retrieval, which uses the generated caption
as a text query to accurately find relevant audio-text pairs to the query
audio, thereby improving the relevance and accuracy of retrieved information.
Additionally, we refine the large-scale knowledge base to retain only
audio-text pairs that align with the contextualized intents. Our approach
achieves state-of-the-art results on benchmarks including AudioCaps, Clotho,
and Auto-ACD, with detailed ablation studies validating the effectiveness of
our retrieval and KB construction methods.",Choi Changin
2024-10-14T23:38:51Z,http://arxiv.org/abs/2410.11141v1,Can Structured Data Reduce Epistemic Uncertainty?,"In this work, we present a framework that utilizes ontology alignment to
improve the learning process of deep learning models. With this approach we
show that models fine-tuned using ontologies learn a downstream task at a
higher rate with better performance on a sequential classification task
compared to the native version of the model. Additionally, we extend our work
to showcase how subsumption mappings retrieved during the process of ontology
alignment can help enhance Retrieval-Augmented Generation in Large Language
Models. The results show that the responses obtained by using subsumption
mappings show an increase of 8.97% in contextual similarity and a 1% increase
in factual accuracy. We also use these scores to define our Hallucination Index
and show that this approach reduces hallucination in LLMs by 4.847%.",Shriram M S
2024-10-15T02:18:01Z,http://arxiv.org/abs/2410.11195v1,"Athena: Retrieval-augmented Legal Judgment Prediction with Large
  Language Models","Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have
prevailed in countless domains, including legal scenarios. With LLMs' rapid
technological progress, the development of prompt engineering (PE) as an
interface between the LLMs and real-world applications has drawn the attention
of all developers. Various PE methods have been proposed to overcome real-world
challenges, such as few-shot prompting, chain-of-thought, and
retrieval-augmented generation (RAG). However, RAG for legal judgment
prediction (LJP) is still underexplored. To address this, we propose ""Athena"",
a novel framework cultivating RAG as a core preprocess component to enhance
LLMs' performance on specialized tasks. Athena constructs a knowledge base for
accusations, attached with a semantic retrieval mechanism through
vectorization. Our experiments show that Athena's overall performance has
improved significantly, achieving state-of-the-art results on the CAIL2018
dataset. Our ablation study on the in-context window size parameter further
reproduces LLMs' ""lost-in-the-middle"" phenomenon with a relative positional
variation. And with moderate hyper-parameter-tuning, we can achieve at most 95%
of accuracy accordingly. We also study the impact of query rewriting and data
distribution, providing possible directions for future research based on former
analyses.",Xiao Peng
2024-10-15T03:04:26Z,http://arxiv.org/abs/2410.11217v1,On the Capacity of Citation Generation by Large Language Models,"Retrieval-augmented generation (RAG) appears as a promising method to
alleviate the ""hallucination"" problem in large language models (LLMs), since it
can incorporate external traceable resources for response generation. The
essence of RAG in combating the hallucination issue lies in accurately
attributing claims in responses to the corresponding retrieved documents.
However, most of existing works focus on improving the quality of generated
responses from the LLM, while largely overlooked its ability to attribute
sources accurately. In this study, we conduct a systematic analysis about the
capabilities of LLMs in generating citations within response generation, and
further introduce a novel method to enhance their citation generation
abilities. Specifically, we evaluate both the correctness and citation quality
for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce
new citation evaluation metrics to eliminate the over-penalization of
unnecessary and excessive citations in existing metrics. Furthermore, we
propose a Generate-then-Refine method that completes relevant citations and
removes irrelevant ones without altering the response text. The results on
WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves
the quality of citations in responses generated by LLMs.",Haosheng Qian
2024-10-15T08:39:12Z,http://arxiv.org/abs/2410.11395v1,"Synthetic Interlocutors. Experiments with Generative AI to Prolong
  Ethnographic Encounters","This paper introduces ""Synthetic Interlocutors"" for ethnographic research.
Synthetic Interlocutors are chatbots ingested with ethnographic textual
material (interviews and observations) by using Retrieval Augmented Generation
(RAG). We integrated an open-source large language model with ethnographic data
from three projects to explore two questions: Can RAG digest ethnographic
material and act as ethnographic interlocutor? And, if so, can Synthetic
Interlocutors prolong encounters with the field and extend our analysis?
Through reflections on the process of building our Synthetic Interlocutors and
an experimental collaborative workshop, we suggest that RAG can digest
ethnographic materials, and it might lead to prolonged, yet uneasy ethnographic
encounters that allowed us to partially recreate and re-visit fieldwork
interactions while facilitating opportunities for novel analytic insights.
Synthetic Interlocutors can produce collaborative, ambiguous and serendipitous
moments.",Johan Irving Søltoft
2024-10-15T09:50:19Z,http://arxiv.org/abs/2410.11446v1,"AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a
  simple RAG task","This paper describes our $3^{rd}$ place submission in the AVeriTeC shared
task in which we attempted to address the challenge of fact-checking with
evidence retrieved in the wild using a simple scheme of Retrieval-Augmented
Generation (RAG) designed for the task, leveraging the predictive power of
Large Language Models. We release our codebase and explain its two modules -
the Retriever and the Evidence & Label generator - in detail, justifying their
features such as MMR-reranking and Likert-scale confidence estimation. We
evaluate our solution on AVeriTeC dev and test set and interpret the results,
picking the GPT-4o as the most appropriate model for our pipeline at the time
of our publication, with Llama 3.1 70B being a promising open-source
alternative. We perform an empirical error analysis to see that faults in our
predictions often coincide with noise in the data or ambiguous fact-checks,
provoking further research and data augmentation.",Herbert Ullrich
2024-10-15T10:57:12Z,http://arxiv.org/abs/2410.11494v1,DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG,"In the rapidly evolving landscape of language, resolving new linguistic
expressions in continuously updating knowledge bases remains a formidable
challenge. This challenge becomes critical in retrieval-augmented generation
(RAG) with knowledge bases, as emerging expressions hinder the retrieval of
relevant documents, leading to generator hallucinations. To address this issue,
we introduce a novel task aimed at resolving emerging mentions to dynamic
entities and present DynamicER benchmark. Our benchmark includes dynamic entity
mention resolution and entity-centric knowledge-intensive QA task, evaluating
entity linking and RAG model's adaptability to new expressions, respectively.
We discovered that current entity linking models struggle to link these new
expressions to entities. Therefore, we propose a temporal segmented clustering
method with continual adaptation, effectively managing the temporal dynamics of
evolving entities and emerging mentions. Extensive experiments demonstrate that
our method outperforms existing baselines, enhancing RAG model performance on
QA task with resolved mentions.",Jinyoung Kim
2024-10-15T11:20:42Z,http://arxiv.org/abs/2410.11507v2,"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic
  Evaluation Framework for LLMs","While various vertical domain large language models (LLMs) have been
developed, the challenge of automatically evaluating their performance across
different domains remains significant. Current benchmark-based evaluation
methods exhibit rigid, aimless interactions and rely on pre-collected static
datasets that are costly to build, inflexible across domains, and misaligned
with practical user needs. To address this issue, we revisit the evaluation
components and introduce two concepts: Benchmark+, which extends traditional
question-answer benchmark into a more flexible ""strategy-criterion"" format; and
Assessment+, which enhances the interaction process, enabling deeper
exploration and supporting both quantitative metrics and qualitative insights.
These concepts capture the nuanced behaviors of LLMs through richer, multi-turn
interactions. We propose an agent-based evaluation framework called TestAgent,
which implements these concepts through retrieval augmented generation and
reinforcement learning. Experiments on tasks ranging from constructing vertical
domain evaluation to activating existing benchmarks demonstrate the
effectiveness of TestAgent across various scenarios. We believe this work
offers an interesting perspective on automatic evaluation for LLMs.",Wanying Wang
2024-10-06T17:11:29Z,http://arxiv.org/abs/2410.11859v1,"SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health
  Support and Assessment, Based on a Systematic Application Survey","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide accessible, stigma-free, personalized, and real-time
mental health support through cutting-edge AI technologies. It makes the
following contributions: (1) Conducting an extensive survey of recent mental
health support methods to identify prevalent functionalities and unmet needs.
(2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM
technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering,
and domain knowledge. This system offers advanced features such as Suicide Risk
Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized
profile uploads and Conversational Information Extraction. (3) Developing novel
evaluation approaches to assess preliminary assessments and suicide risk
detection, utilizing annotated real-life interview data and professionally
labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator
Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance
model performance and usability through context-sensitive response adjustments
and semantic coherence evaluations. This study contributes to advancing mental
health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-15T21:10:01Z,http://arxiv.org/abs/2410.12069v1,De-jargonizing Science for Journalists with GPT-4: A Pilot Study,"This study offers an initial evaluation of a human-in-the-loop system
leveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented
Generation (RAG) to identify and define jargon terms in scientific abstracts,
based on readers' self-reported knowledge. The system achieves fairly high
recall in identifying jargon and preserves relative differences in readers'
jargon identification, suggesting personalization as a feasible use-case for
LLMs to support sense-making of complex information. Surprisingly, using only
abstracts for context to generate definitions yields slightly more accurate and
higher quality definitions than using RAG-based context from the fulltext of an
article. The findings highlight the potential of generative AI for assisting
science reporters, and can inform future work on developing tools to simplify
dense documents.",Sachita Nishal
2024-10-16T11:43:17Z,http://arxiv.org/abs/2410.12475v2,"Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional
  Safety Engineering","Functional safety is a critical aspect of automotive engineering,
encompassing all phases of a vehicle's lifecycle, including design,
development, production, operation, and decommissioning. This domain involves
highly knowledge-intensive tasks. This paper introduces Aegis: An Advanced
LLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is
specifically designed to support complex functional safety tasks within the
automotive sector. It is tailored to perform Hazard Analysis and Risk
Assessment(HARA), document Functional Safety Requirements(FSR), and plan test
cases for Automatic Emergency Braking(AEB) systems. The most advanced version,
Aegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective
mechanisms to enhance its capability in managing complex, knowledge-intensive
tasks. Additionally, targeted prompt refinement by professional functional
safety practitioners can significantly optimize Aegis's performance in the
functional safety domain. This paper demonstrates the potential of Aegis to
improve the efficiency and effectiveness of functional safety processes in
automotive engineering.",Lu Shi
2024-10-16T13:10:27Z,http://arxiv.org/abs/2410.12532v2,"MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration","Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.",Jinjie Wei
2024-10-11T19:49:05Z,http://arxiv.org/abs/2410.12859v1,"Enhancing Long Context Performance in LLMs Through Inner Loop Query
  Mechanism","Transformers have a quadratic scaling of computational complexity with input
size, which limits the input context window size of large language models
(LLMs) in both training and inference. Meanwhile, retrieval-augmented
generation (RAG) besed models can better handle longer contexts by using a
retrieval system to filter out unnecessary information. However, most RAG
methods only perform retrieval based on the initial query, which may not work
well with complex questions that require deeper reasoning. We introduce a novel
approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving
inner-loop queries, based not only on the query question itself but also on
intermediate findings. At inference time, our model retrieves information from
the RAG system, integrating data from lengthy documents at various levels of
abstraction. Based on the information retrieved, the LLM generates texts stored
in an area named Short-Term Memory (STM) which is then used to formulate the
next query. This retrieval process is repeated until the text in STM converged.
Our experiments demonstrate that retrieval with STM offers improvements over
traditional retrieval-augmented LLMs, particularly in long context tests such
as Multi-Needle In A Haystack (M-NIAH) and BABILong.",Yimin Tang
2024-10-16T08:43:39Z,http://arxiv.org/abs/2410.12890v1,"REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via
  Model Fusion of Embedding Models","Retrieval augmented generation (RAG) pipelines are commonly used in tasks
such as question-answering (QA), relying on retrieving relevant documents from
a vector store computed using a pretrained embedding model. However, if the
retrieved context is inaccurate, the answers generated using the large language
model (LLM) may contain errors or hallucinations. Although pretrained embedding
models have advanced, adapting them to new domains remains challenging.
Fine-tuning is a potential solution, but industry settings often lack the
necessary fine-tuning data. To address these challenges, we propose REFINE, a
novel technique that generates synthetic data from available documents and then
uses a model fusion approach to fine-tune embeddings for improved retrieval
performance in new domains, while preserving out-of-domain capability. We
conducted experiments on the two public datasets: SQUAD and RAG-12000 and a
proprietary TOURISM dataset. Results demonstrate that even the standard
fine-tuning with the proposed data augmentation technique outperforms the
vanilla pretrained model. Furthermore, when combined with model fusion, the
proposed approach achieves superior performance, with a 5.76% improvement in
recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and
RAG-12000 respectively.",Ambuje Gupta
2024-10-17T04:30:46Z,http://arxiv.org/abs/2410.13210v1,"FaithBench: A Diverse Hallucination Benchmark for Summarization by
  Modern LLMs","Summarization is one of the most common tasks performed by large language
models (LLMs), especially in applications like Retrieval-Augmented Generation
(RAG). However, existing evaluations of hallucinations in LLM-generated
summaries, and evaluations of hallucination detection models both suffer from a
lack of diversity and recency in the LLM and LLM families considered. This
paper introduces FaithBench, a summarization hallucination benchmark comprising
challenging hallucinations made by 10 modern LLMs from 8 different families,
with ground truth annotations by human experts. ``Challenging'' here means
summaries on which popular, state-of-the-art hallucination detection models,
including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and
GPT-3.5-Turbo produce the least hallucinations. However, even the best
hallucination detection models have near 50\% accuracies on FaithBench,
indicating lots of room for future improvement. The repo is
https://github.com/vectara/FaithBench",Forrest Sheng Bao
2024-10-17T08:48:54Z,http://arxiv.org/abs/2410.13339v1,"Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval","Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.",Ingeol Baek
2024-10-17T13:51:03Z,http://arxiv.org/abs/2410.13553v1,"Integrating Temporal Representations for Dynamic Memory Retrieval and
  Management in Large Language Models","Conventional dialogue agents often struggle with effective memory recall,
leading to redundant retrieval and inadequate management of unique user
associations. To address this, we propose SynapticRAG, a novel approach
integrating synaptic dynamics into Retrieval-Augmented Generation (RAG).
SynapticRAG integrates temporal representations into memory vectors, mimicking
biological synapses by differentiating events based on occurrence times and
dynamically updating memory significance. This model employs temporal scoring
for memory connections and a synaptic-inspired propagation control mechanism.
Experiments across English, Japanese, and Chinese datasets demonstrate
SynapticRAG's superiority over existing methods, including traditional RAG,
with up to 14.66\% improvement in memory retrieval accuracy. Our approach
advances context-aware dialogue AI systems by enhancing long-term context
maintenance and specific information extraction from conversations.",Yuki Hou
2024-10-17T15:29:57Z,http://arxiv.org/abs/2410.13671v1,"HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World
  Multilingual Settings","Assessing the capabilities and limitations of large language models (LLMs)
has garnered significant interest, yet the evaluation of multiple models in
real-world scenarios remains rare. Multilingual evaluation often relies on
translated benchmarks, which typically do not capture linguistic and cultural
nuances present in the source language. This study provides an extensive
assessment of 24 LLMs on real world data collected from Indian patients
interacting with a medical chatbot in Indian English and 4 other Indic
languages. We employ a uniform Retrieval Augmented Generation framework to
generate responses, which are evaluated using both automated techniques and
human evaluators on four specific metrics relevant to our application. We find
that models vary significantly in their performance and that instruction tuned
Indic models do not always perform well on Indic language queries. Further, we
empirically show that factual correctness is generally lower for responses to
Indic queries compared to English queries. Finally, our qualitative work shows
that code-mixed and culturally relevant queries in our dataset pose challenges
to evaluated models.",Varun Gumma
2024-10-17T21:56:22Z,http://arxiv.org/abs/2410.14057v1,"Towards Cross-Cultural Machine Translation with Retrieval-Augmented
  Generation from Multilingual Knowledge Graphs","Translating text that contains entity names is a challenging task, as
cultural-related references can vary significantly across languages. These
variations may also be caused by transcreation, an adaptation process that
entails more than transliteration and word-for-word translation. In this paper,
we address the problem of cross-cultural translation on two fronts: (i) we
introduce XC-Translate, the first large-scale, manually-created benchmark for
machine translation that focuses on text that contains potentially
culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end
method to integrate information from a multilingual knowledge graph into a
neural machine translation model by leveraging a dense retrieval mechanism. Our
experiments and analyses show that current machine translation systems and
large language models still struggle to translate texts containing entity
names, whereas KG-MT outperforms state-of-the-art approaches by a large margin,
obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4,
respectively.",Simone Conia
2024-10-18T16:11:29Z,http://arxiv.org/abs/2410.14567v2,ScopeQA: A Framework for Generating Out-of-Scope Questions for RAG,"Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{DBLP:conf/emnlp/MinMHZ20}. RAG agents need high-quality data to
improve their responses to confusing questions. This paper presents a novel
guided hallucination-based method to efficiently generate a diverse set of
borderline out-of-scope confusing questions for a given document corpus. We
conduct an empirical comparative evaluation of several large language models as
RAG agents to measure the accuracy of confusion detection and appropriate
response generation. We contribute a benchmark dataset to the public domain.",Zhiyuan Peng
2024-10-19T01:29:12Z,http://arxiv.org/abs/2410.14926v1,"Aligning LLMs with Human Instructions and Stock Market Feedback in
  Financial Sentiment Analysis","Financial sentiment analysis is crucial for trading and investment
decision-making. This study introduces an adaptive retrieval augmented
framework for Large Language Models (LLMs) that aligns with human instructions
through Instruction Tuning and incorporates market feedback to dynamically
adjust weights across various knowledge sources within the Retrieval-Augmented
Generation (RAG) module. Building upon foundational models like LLaMA 2, we
fine-tune a series of LLMs ranging from 7B to 70B in size, enriched with
Instruction Tuning and RAG, and further optimized through direct feedback and
Reinforcement Learning (RL)-based refinement methods applied to the source
weights of RAG.Through extensive evaluation, we demonstrate that the sentiment
outputs from our LLMs more accurately mirror the intrinsic sentiment of textual
data, showcasing a 1% to 6% boost in accuracy and F1 score over existing
state-of-the-art models and leading conversational AI systems. Moreover, the
sentiments extracted are more indicative of the directions in stock price
movements. On top of that, we successfully construct portfolios that yield a
3.61% higher Sharpe ratio compared to the S&P 500 baseline in bullish markets.
These portfolios also demonstrate resilience in bearish markets, with a 5x
reduction in return losses compared to those typically experienced by the S&P
500.",Zijie Zhao
2024-10-19T01:35:26Z,http://arxiv.org/abs/2410.14931v1,"""Ghost of the past"": identifying and resolving privacy leakage from
  LLM's memory through proactive user interaction","Memories, encompassing past inputs in context window and retrieval-augmented
generation (RAG), frequently surface during human-LLM interactions, yet users
are often unaware of their presence and the associated privacy risks. To
address this, we propose MemoAnalyzer, a system for identifying, visualizing,
and managing private information within memories. A semi-structured interview
(N=40) revealed that low privacy awareness was the primary challenge, while
proactive privacy control emerged as the most common user need. MemoAnalyzer
uses a prompt-based method to infer and identify sensitive information from
aggregated past inputs, allowing users to easily modify sensitive content.
Background color temperature and transparency are mapped to inference
confidence and sensitivity, streamlining privacy adjustments. A 5-day
evaluation (N=36) comparing MemoAnalyzer with the default GPT setting and a
manual modification baseline showed MemoAnalyzer significantly improved privacy
awareness and protection without compromising interaction speed. Our study
contributes to privacy-conscious LLM design, offering insights into privacy
protection for Human-AI interactions.",Shuning Zhang
2024-10-20T04:48:12Z,http://arxiv.org/abs/2410.15284v1,Customized FinGPT Search Agents Using Foundation Models,"Current large language models (LLMs) have proven useful for analyzing
financial data, but most existing models, such as BloombergGPT and FinGPT, lack
customization for specific user needs. In this paper, we address this gap by
developing FinGPT Search Agents tailored for two types of users: individuals
and institutions. For individuals, we leverage Retrieval-Augmented Generation
(RAG) to integrate local documents and user-specified data sources. For
institutions, we employ dynamic vector databases and fine-tune models on
proprietary data. There are several key issues to address, including data
privacy, the time-sensitive nature of financial information, and the need for
fast responses. Experiments show that FinGPT agents outperform existing models
in accuracy, relevance, and response time, making them practical for real-world
applications.",Felix Tian
2024-10-20T04:51:24Z,http://arxiv.org/abs/2410.15285v1,"Contextual Augmented Multi-Model Programming (CAMP): A Hybrid
  Local-Cloud Copilot Framework","The advancements in cloud-based Large Languages Models (LLMs) have
revolutionized AI-assisted programming. However, their integration into certain
local development environments like ones within the Apple software ecosystem
(e.g., iOS apps, macOS) remains challenging due to computational demands and
sandboxed constraints. This paper presents CAMP, a multi-model AI-assisted
programming framework that consists of a local model that employs
Retrieval-Augmented Generation (RAG) to retrieve contextual information from
the codebase to facilitate context-aware prompt construction thus optimizing
the performance of the cloud model, empowering LLMs' capabilities in local
Integrated Development Environments (IDEs). The methodology is actualized in
Copilot for Xcode, an AI-assisted programming tool crafted for Xcode that
employs the RAG module to address software constraints and enables diverse
generative programming tasks, including automatic code completion,
documentation, error detection, and intelligent user-agent interaction. The
results from objective experiments on generated code quality and subjective
experiments on user adoption collectively demonstrate the pilot success of the
proposed system and mark its significant contributions to the realm of
AI-assisted programming.",Yuchen Wang
2024-10-20T08:42:29Z,http://arxiv.org/abs/2410.15332v1,"EPIC: Efficient Position-Independent Context Caching for Serving Large
  Language Models","Large Language Models (LLMs) are critical for a wide range of applications,
but serving them efficiently becomes increasingly challenging as inputs become
more complex. Context caching improves serving performance by exploiting
inter-request dependency and reusing key-value (KV) cache across requests, thus
improving time-to-first-token (TTFT). However, existing prefix-based context
caching requires exact token prefix matches, limiting cache reuse in few-shot
learning, multi-document QA, or retrieval-augmented generation, where prefixes
may vary. In this paper, we present EPIC, an LLM serving system that introduces
position-independent context caching (PIC), enabling modular KV cache reuse
regardless of token chunk position (or prefix). EPIC features two key designs:
AttnLink, which leverages static attention sparsity to minimize recomputation
for accuracy recovery, and KVSplit, a customizable chunking method that
preserves semantic coherence. Our experiments demonstrate that Epic delivers up
to 8x improvements in TTFT and 7x throughput over existing systems, with
negligible or no accuracy loss. By addressing the limitations of traditional
caching approaches, Epic enables more scalable and efficient LLM inference.",Junhao Hu
2024-10-20T16:08:54Z,http://arxiv.org/abs/2410.15438v1,"Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based
  LLMs","Retrieval-Augmented Generation (RAG) significantly improved the ability of
Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing
research seeks to enhance RAG performance by retrieving higher-quality
documents or designing RAG-specific LLMs, the internal mechanisms within LLMs
that contribute to the effectiveness of RAG systems remain underexplored. In
this paper, we aim to investigate these internal mechanisms within the popular
Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by
examining expert activations in these LLMs. Our controlled experiments reveal
that several core groups of experts are primarily responsible for RAG-related
behaviors. The activation of these core experts can signify the model's
inclination towards external/internal knowledge and adjust its behavior. For
instance, we identify core experts that can (1) indicate the sufficiency of the
model's internal knowledge, (2) assess the quality of retrieved documents, and
(3) enhance the model's ability to utilize context. Based on these findings, we
propose several strategies to enhance RAG's efficiency and effectiveness
through expert activation. Experimental results across various datasets and
MoE-based LLMs show the effectiveness of our method.",Xin Zhou
2024-10-20T21:17:05Z,http://arxiv.org/abs/2410.15511v1,"ConTReGen: Context-driven Tree-structured Retrieval for Open-domain
  Long-form Text Generation","Open-domain long-form text generation requires generating coherent,
comprehensive responses that address complex queries with both breadth and
depth. This task is challenging due to the need to accurately capture diverse
facets of input queries. Existing iterative retrieval-augmented generation
(RAG) approaches often struggle to delve deeply into each facet of complex
queries and integrate knowledge from various sources effectively. This paper
introduces ConTReGen, a novel framework that employs a context-driven,
tree-structured retrieval approach to enhance the depth and relevance of
retrieved content. ConTReGen integrates a hierarchical, top-down in-depth
exploration of query facets with a systematic bottom-up synthesis, ensuring
comprehensive coverage and coherent integration of multifaceted information.
Extensive experiments on multiple datasets, including LFQA and ODSUM, alongside
a newly introduced dataset, ODSUM-WikiHow, demonstrate that ConTReGen
outperforms existing state-of-the-art RAG models.",Kashob Kumar Roy
2024-10-20T22:59:34Z,http://arxiv.org/abs/2410.15531v1,"Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses
  with Sub-Question Coverage","Evaluating retrieval-augmented generation (RAG) systems remains challenging,
particularly for open-ended questions that lack definitive answers and require
coverage of multiple sub-topics. In this paper, we introduce a novel evaluation
framework based on sub-question coverage, which measures how well a RAG system
addresses different facets of a question. We propose decomposing questions into
sub-questions and classifying them into three types -- core, background, and
follow-up -- to reflect their roles and importance. Using this categorization,
we introduce a fine-grained evaluation protocol that provides insights into the
retrieval and generation characteristics of RAG systems, including three
commercial generative answer engines: You.com, Perplexity AI, and Bing Chat.
Interestingly, we find that while all answer engines cover core sub-questions
more often than background or follow-up ones, they still miss around 50% of
core sub-questions, revealing clear opportunities for improvement. Further,
sub-question coverage metrics prove effective for ranking responses, achieving
82% accuracy compared to human preference annotations. Lastly, we also
demonstrate that leveraging core sub-questions enhances both retrieval and
answer generation in a RAG system, resulting in a 74% win rate over the
baseline that lacks sub-questions.",Kaige Xie
2024-10-21T06:11:38Z,http://arxiv.org/abs/2410.15667v1,RAC: Efficient LLM Factuality Correction with Retrieval Augmentation,"Large Language Models (LLMs) exhibit impressive results across a wide range
of natural language processing (NLP) tasks, yet they can often produce
factually incorrect outputs. This paper introduces a simple but effective
low-latency post-correction method, \textbf{Retrieval Augmented Correction
(RAC)}, aimed at enhancing the factual performance of LLMs without requiring
additional fine-tuning. Our method is general and can be used with any
instruction-tuned LLM, and has greatly reduced latency compared to prior
approaches. RAC decomposes the LLM's output into atomic facts and applies a
fine-grained verification and correction process with retrieved content to
verify and correct the LLM-generated output. Our extensive experiments show
that RAC yields up to 30\% improvements over state-of-the-art baselines across
two popular factuality evaluation datasets, validating its efficacy and
robustness in both with and without the integration of Retrieval-Augmented
Generation (RAG) across different LLMs.\footnote{Our code is at
\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}",Changmao Li
2024-10-21T07:56:45Z,http://arxiv.org/abs/2410.15737v1,Who's Who: Large Language Models Meet Knowledge Conflicts in Practice,"Retrieval-augmented generation (RAG) methods are viable solutions for
addressing the static memory limits of pre-trained language models.
Nevertheless, encountering conflicting sources of information within the
retrieval context is an inevitable practical challenge. In such situations, the
language models are recommended to transparently inform users about the
conflicts rather than autonomously deciding what to present based on their
inherent biases. To analyze how current large language models (LLMs) align with
our recommendation, we introduce WhoQA, a public benchmark dataset to examine
model's behavior in knowledge conflict situations. We induce conflicts by
asking about a common property among entities having the same name, resulting
in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K
questions across 13 Wikidata property types and 150K Wikipedia entities. Our
experiments show that despite the simplicity of WhoQA questions, knowledge
conflicts significantly degrades LLMs' performance in RAG settings.",Quang Hieu Pham
2024-10-21T09:18:30Z,http://arxiv.org/abs/2410.15801v1,Improve Dense Passage Retrieval with Entailment Tuning,"Retrieval module can be plugged into many downstream NLP tasks to improve
their performance, such as open-domain question answering and
retrieval-augmented generation. The key to a retrieval system is to calculate
relevance scores to query and passage pairs. However, the definition of
relevance is often ambiguous. We observed that a major class of relevance
aligns with the concept of entailment in NLI tasks. Based on this observation,
we designed a method called entailment tuning to improve the embedding of dense
retrievers. Specifically, we unify the form of retrieval data and NLI data
using existence claim as a bridge. Then, we train retrievers to predict the
claims entailed in a passage with a variant task of masked prediction. Our
method can be efficiently plugged into current dense retrieval methods, and
experiments show the effectiveness of our method.",Lu Dai
2024-10-21T11:02:18Z,http://arxiv.org/abs/2410.15884v1,"Using GPT Models for Qualitative and Quantitative News Analytics in the
  2024 US Presidental Election Process","The paper considers an approach of using Google Search API and GPT-4o model
for qualitative and quantitative analyses of news through retrieval-augmented
generation (RAG). This approach was applied to analyze news about the 2024 US
presidential election process. Different news sources for different time
periods have been analyzed. Quantitative scores generated by GPT model have
been analyzed using Bayesian regression to derive trend lines. The
distributions found for the regression parameters allow for the analysis of
uncertainty in the election process. The obtained results demonstrate that
using the GPT models for news analysis, one can get informative analytics and
provide key insights that can be applied in further analyses of election
processes.",Bohdan M. Pavlyshenko
2024-10-05T14:37:35Z,http://arxiv.org/abs/2410.16285v1,"Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for
  Your Job?","The current paper presents the development and validation of SelfScore, a
novel benchmark designed to assess the performance of automated Large Language
Model (LLM) agents on help desk and professional consultation tasks. Given the
increasing integration of AI in industries, particularly within customer
service, SelfScore fills a crucial gap by enabling the comparison of automated
agents and human workers. The benchmark evaluates agents on problem complexity
and response helpfulness, ensuring transparency and simplicity in its scoring
system. The study also develops automated LLM agents to assess SelfScore and
explores the benefits of Retrieval-Augmented Generation (RAG) for
domain-specific tasks, demonstrating that automated LLM agents incorporating
RAG outperform those without. All automated LLM agents were observed to perform
better than the human control group. Given these results, the study raises
concerns about the potential displacement of human workers, especially in areas
where AI technologies excel. Ultimately, SelfScore provides a foundational tool
for understanding the impact of AI in help desk environments while advocating
for ethical considerations in the ongoing transition towards automation.",John Mavi
2024-10-22T09:25:21Z,http://arxiv.org/abs/2410.16843v1,"Trustworthy Alignment of Retrieval-Augmented Large Language Models via
  Reinforcement Learning","Trustworthiness is an essential prerequisite for the real-world application
of large language models. In this paper, we focus on the trustworthiness of
language models with respect to retrieval augmentation. Despite being supported
with external evidence, retrieval-augmented generation still suffers from
hallucinations, one primary cause of which is the conflict between contextual
and parametric knowledge. We deem that retrieval-augmented language models have
the inherent capabilities of supplying response according to both contextual
and parametric knowledge. Inspired by aligning language models with human
preference, we take the first step towards aligning retrieval-augmented
language models to a status where it responds relying merely on the external
evidence and disregards the interference of parametric knowledge. Specifically,
we propose a reinforcement learning based algorithm Trustworthy-Alignment,
theoretically and experimentally demonstrating large language models'
capability of reaching a trustworthy status without explicit supervision on how
to respond. Our work highlights the potential of large language models on
exploring its intrinsic abilities by its own and expands the application
scenarios of alignment from fulfilling human preference to creating trustworthy
agents.",Zongmeng Zhang
2024-10-08T16:26:18Z,http://arxiv.org/abs/2410.18104v1,"ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless
  Environment Perception","Large language models (LLMs) hold significant promise in advancing network
management and orchestration in 6G and beyond networks. However, existing LLMs
are limited in domain-specific knowledge and their ability to handle
multi-modal sensory data, which is critical for real-time situational awareness
in dynamic wireless environments. This paper addresses this gap by introducing
ENWAR, an ENvironment-aWARe retrieval augmented generation-empowered
multi-modal LLM framework. ENWAR seamlessly integrates multi-modal sensory
inputs to perceive, interpret, and cognitively process complex wireless
environments to provide human-interpretable situational awareness. ENWAR is
evaluated on the GPS, LiDAR, and camera modality combinations of DeepSense6G
dataset with state-of-the-art LLMs such as Mistral-7b/8x7b and
LLaMa3.1-8/70/405b. Compared to general and often superficial environmental
descriptions of these vanilla LLMs, ENWAR delivers richer spatial analysis,
accurately identifies positions, analyzes obstacles, and assesses line-of-sight
between vehicles. Results show that ENWAR achieves key performance indicators
of up to 70% relevancy, 55% context recall, 80% correctness, and 86%
faithfulness, demonstrating its efficacy in multi-modal perception and
interpretation.",Ahmad M. Nazar
2024-10-08T17:36:48Z,http://arxiv.org/abs/2410.18105v1,"Improving Embedding Accuracy for Document Retrieval Using Entity
  Relationship Maps and Model-Aware Contrastive Sampling","In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic
eXtraction), a 7-billion parameter decoder-only text Feature Extraction Model,
specifically designed for Document Retrieval-Augmented Generation (RAG) tasks.
Our approach employs two training techniques that yield an emergent improvement
in factual focus: (1) Pre-convergence interrupted fine-tuning using Structured
Entity Relationship Maps as training data input: designed to shift the model's
attention and create a bias towards factual content rather than semantic style
- this enhances plain text performance despite not being directly trained for
it; and (2) Model-Aware Contrastive Sampling, creating a balanced and evenly
distributed collation map of hard and soft negatives directly informed by the
base model's competency. This combined methodology yields significant
improvements, enhancing plain text query/document pair retrieval to achieve an
absolute rank@1 accuracy of 90.86% (an increase of 6.26% compared to the next
leading model) in our evaluation, and reducing training data input context size
by an average of 37.71% compared to plain text for both queries and document
texts. Based on our evaluations, our model establishes a new state-of-the-art
standard in text feature extraction for longer context document retrieval
tasks.",Thea Aviss
2024-10-09T16:35:41Z,http://arxiv.org/abs/2410.18251v1,Context-Augmented Code Generation Using Programming Knowledge Graphs,"Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.",Iman Saberi
2024-10-24T17:13:39Z,http://arxiv.org/abs/2410.18926v1,"LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor
  Search","Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets.",Elias Jääsaari
2024-10-25T14:07:53Z,http://arxiv.org/abs/2410.19572v4,ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems,"Retrieval-Augmented Generation (RAG) systems using large language models
(LLMs) often generate inaccurate responses due to the retrieval of irrelevant
or loosely related information. Existing methods, which operate at the document
level, fail to effectively filter out such content. We propose LLM-driven chunk
filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and
filtering retrieved information at the chunk level. Our approach employs
semantic chunking to divide documents into coherent sections and utilizes
LLM-based relevance scoring to assess each chunk's alignment with the user's
query. By filtering out less pertinent chunks before the generation phase, we
significantly reduce hallucinations and improve factual accuracy. Experiments
show that our method outperforms existing RAG models, achieving higher accuracy
on tasks requiring precise information retrieval. This advancement enhances the
reliability of RAG systems, making them particularly beneficial for
applications like fact-checking and multi-hop reasoning.",Ishneet Sukhvinder Singh
2024-10-26T03:07:22Z,http://arxiv.org/abs/2410.20056v1,Multi-Field Adaptive Retrieval,"Document retrieval for tasks such as search and retrieval-augmented
generation typically involves datasets that are unstructured: free-form text
without explicit internal structure in each document. However, documents can
have a structured form, consisting of fields such as an article title, message
body, or HTML header. To address this gap, we introduce Multi-Field Adaptive
Retrieval (MFAR), a flexible framework that accommodates any number of and any
type of document indices on structured data. Our framework consists of two main
steps: (1) the decomposition of an existing document into fields, each indexed
independently through dense and lexical methods, and (2) learning a model which
adaptively predicts the importance of a field by conditioning on the document
query, allowing on-the-fly weighting of the most likely field(s). We find that
our approach allows for the optimized use of dense versus lexical
representations across field types, significantly improves in document ranking
over a number of existing retrievers, and achieves state-of-the-art performance
for multi-field structured data.",Millicent Li
2024-10-27T00:42:21Z,http://arxiv.org/abs/2410.20299v1,"EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge
  Update","Large Language Models are revolutionizing Web, mobile, and Web of Things
systems, driving intelligent and scalable solutions. However, as
Retrieval-Augmented Generation (RAG) systems expand, they encounter significant
challenges related to scalability, including increased delay and communication
overhead. To address these issues, we propose EACO-RAG, an edge-assisted
distributed RAG system that leverages adaptive knowledge updates and inter-node
collaboration. By distributing vector datasets across edge nodes and optimizing
retrieval processes, EACO-RAG significantly reduces delay and resource
consumption while enhancing response accuracy. The system employs a multi-armed
bandit framework with safe online Bayesian methods to balance performance and
cost. Extensive experimental evaluation demonstrates that EACO-RAG outperforms
traditional centralized RAG systems in both response time and resource
efficiency. EACO-RAG effectively reduces delay and resource expenditure to
levels comparable to, or even lower than, those of local RAG systems, while
significantly improving accuracy. This study presents the first systematic
exploration of edge-assisted distributed RAG architectures, providing a
scalable and cost-effective solution for large-scale distributed environments.",Jiaxing Li
2024-10-28T02:55:03Z,http://arxiv.org/abs/2410.20695v2,"Combining Domain-Specific Models and LLMs for Automated Disease
  Phenotyping from Survey Data","This exploratory pilot study investigated the potential of combining a
domain-specific model, BERN2, with large language models (LLMs) to enhance
automated disease phenotyping from research survey data. Motivated by the need
for efficient and accurate methods to harmonize the growing volume of survey
data with standardized disease ontologies, we employed BERN2, a biomedical
named entity recognition and normalization model, to extract disease
information from the ORIGINS birth cohort survey data. After rigorously
evaluating BERN2's performance against a manually curated ground truth dataset,
we integrated various LLMs using prompt engineering, Retrieval-Augmented
Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's
outputs. BERN2 demonstrated high performance in extracting and normalizing
disease mentions, and the integration of LLMs, particularly with Few Shot
Inference and RAG orchestration, further improved accuracy. This approach,
especially when incorporating structured examples, logical reasoning prompts,
and detailed context, offers a promising avenue for developing tools to enable
efficient cohort profiling and data harmonization across large, heterogeneous
research datasets.",Gal Beeri
2024-10-28T12:50:27Z,http://arxiv.org/abs/2410.20975v1,"Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base
  for Geospatial Code Generation Tasks Using Large Language Models","The rise of spatiotemporal data and the need for efficient geospatial
modeling have spurred interest in automating these tasks with large language
models (LLMs). However, general LLMs often generate errors in geospatial code
due to a lack of domain-specific knowledge on functions and operators. To
address this, a retrieval-augmented generation (RAG) approach, utilizing an
external knowledge base of geospatial functions and operators, is proposed.
This study introduces a framework to construct such a knowledge base,
leveraging geospatial script semantics. The framework includes: Function
Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination
Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like
Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and
align geospatial functions. An example knowledge base, Geo-FuB, built from
154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics
show a high accuracy, reaching 88.89% overall, with structural and semantic
accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize
geospatial code generation through the RAG and fine-tuning paradigms is
highlighted.",Shuyang Hou
2024-10-28T14:29:11Z,http://arxiv.org/abs/2410.21067v1,"CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and
  Retrieval-Augmented Translation with Large Language Models","Large language models (LLMs) have shown great promise in machine translation,
but they still struggle with contextually dependent terms, such as new or
domain-specific words. This leads to inconsistencies and errors that are
difficult to address. Existing solutions often depend on manual identification
of such terms, which is impractical given the complexity and evolving nature of
language. While Retrieval-Augmented Generation (RAG) could provide some
assistance, its application to translation is limited by issues such as
hallucinations from information overload. In this paper, we propose CRAT, a
novel multi-agent translation framework that leverages RAG and
causality-enhanced self-reflection to address these challenges. This framework
consists of several specialized agents: the Unknown Terms Identification agent
detects unknown terms within the context, the Knowledge Graph (KG) Constructor
agent extracts relevant internal knowledge about these terms and retrieves
bilingual information from external sources, the Causality-enhanced Judge agent
validates the accuracy of the information, and the Translator agent
incorporates the refined information into the final output. This automated
process allows for more precise and consistent handling of key terms during
translation. Our results show that CRAT significantly improves translation
accuracy, particularly in handling context-sensitive terms and emerging
vocabulary.",Meiqi Chen
2024-10-28T17:04:18Z,http://arxiv.org/abs/2410.21220v1,"Vision Search Assistant: Empower Vision-Language Models as Multimodal
  Search Engines","Search engines enable the retrieval of unknown information with texts.
However, traditional methods fall short when it comes to understanding
unfamiliar visual content, such as identifying an object that the model has
never seen before. This challenge is particularly pronounced for large
vision-language models (VLMs): if the model has not been exposed to the object
depicted in an image, it struggles to generate reliable answers to the user's
question regarding that image. Moreover, as new objects and events continuously
emerge, frequently updating VLMs is impractical due to heavy computational
burdens. To address this limitation, we propose Vision Search Assistant, a
novel framework that facilitates collaboration between VLMs and web agents.
This approach leverages VLMs' visual understanding capabilities and web agents'
real-time information access to perform open-world Retrieval-Augmented
Generation via the web. By integrating visual and textual representations
through this collaboration, the model can provide informed responses even when
the image is novel to the system. Extensive experiments conducted on both
open-set and closed-set QA benchmarks demonstrate that the Vision Search
Assistant significantly outperforms the other models and can be widely applied
to existing VLMs.",Zhixin Zhang
2024-10-29T09:02:37Z,http://arxiv.org/abs/2410.21868v2,Improving In-Context Learning with Small Language Model Ensembles,"Large language models (LLMs) have shown impressive capabilities across
various tasks, but their performance on domain-specific tasks remains limited.
While methods like retrieval augmented generation and fine-tuning can help to
address this, they require significant resources. In-context learning (ICL) is
a cheap and efficient alternative but cannot match the accuracies of advanced
methods. We present Ensemble SuperICL, a novel approach that enhances ICL by
leveraging the expertise of multiple fine-tuned small language models (SLMs).
Ensemble SuperICL achieves state of the art (SoTA) results on several natural
language understanding benchmarks. Additionally, we test it on a medical-domain
labelling task and showcase its practicality by using off-the-shelf SLMs
fine-tuned on a general language task, achieving superior accuracy in
large-scale data labelling compared to all baselines. Finally, we conduct an
ablation study and sensitivity analyses to elucidate the underlying mechanism
of Ensemble SuperICL. Our research contributes to the growing demand for
efficient domain specialisation methods in LLMs, offering a cheap and effective
method for practitioners.",M. Mehdi Mojarradi
2024-10-29T11:03:31Z,http://arxiv.org/abs/2410.21943v1,"Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial
  Applications","Large Language Models (LLMs) have demonstrated impressive capabilities in
answering questions, but they lack domain-specific knowledge and are prone to
hallucinations. Retrieval Augmented Generation (RAG) is one approach to address
these challenges, while multimodal models are emerging as promising AI
assistants for processing both text and images. In this paper we describe a
series of experiments aimed at determining how to best integrate multimodal
models into RAG systems for the industrial domain. The purpose of the
experiments is to determine whether including images alongside text from
documents within the industrial domain increases RAG performance and to find
the optimal configuration for such a multimodal RAG system. Our experiments
include two approaches for image processing and retrieval, as well as two LLMs
(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies
involve the use of multimodal embeddings and the generation of textual
summaries from images. We evaluate our experiments with an LLM-as-a-Judge
approach. Our results reveal that multimodal RAG can outperform single-modality
RAG settings, although image retrieval poses a greater challenge than text
retrieval. Additionally, leveraging textual summaries from images presents a
more promising approach compared to the use of multimodal embeddings, providing
more opportunities for future advancements.",Monica Riedler
2024-10-30T09:15:51Z,http://arxiv.org/abs/2410.22832v1,"HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language
  Models","Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge, making them adaptable and
cost-effective for various applications. However, the growing reliance on these
systems also introduces potential security risks. In this work, we reveal a
novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which
enables attackers to manipulate the retrieval mechanisms of RAG systems by
injecting malicious texts into the knowledge database. When the RAG system
encounters target questions, it generates the attacker's pre-determined answers
instead of the correct ones, undermining the integrity and trustworthiness of
the system. We formalize HijackRAG as an optimization problem and propose both
black-box and white-box attack strategies tailored to different levels of the
attacker's knowledge. Extensive experiments on multiple benchmark datasets show
that HijackRAG consistently achieves high attack success rates, outperforming
existing baseline attacks. Furthermore, we demonstrate that the attack is
transferable across different retriever models, underscoring the widespread
risk it poses to RAG systems. Lastly, our exploration of various defense
mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing
the urgent need for more robust security measures to protect RAG systems in
real-world deployments.",Yucheng Zhang
2024-10-30T10:11:53Z,http://arxiv.org/abs/2410.22874v1,"Eliciting Critical Reasoning in Retrieval-Augmented Language Models via
  Contrastive Explanations","Retrieval-augmented generation (RAG) has emerged as a critical mechanism in
contemporary NLP to support Large Language Models(LLMs) in systematically
accessing richer factual context. However, the integration of RAG mechanisms
brings its inherent challenges, as LLMs need to deal with potentially noisy
contexts. Recent studies have shown that LLMs still struggle to critically
analyse RAG-based in-context information, a limitation that may lead to
incorrect inferences and hallucinations. In this paper, we investigate how to
elicit critical reasoning in RAG via contrastive explanations. In particular,
we propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant
documents given a query, (ii) selects and exemplifies relevant passages, and
(iii) generates explanations that explicitly contrast the relevance of the
passages to (iv) support the final answer. We show the impact of C-RAG building
contrastive reasoning demonstrations from LLMs to instruct smaller models for
retrieval-augmented tasks. Extensive experiments demonstrate that C-RAG
improves state-of-the-art RAG models while (a) requiring significantly fewer
prompts and demonstrations and (b) being robust to perturbations in the
retrieved documents.",Leonardo Ranaldi
2024-10-30T20:28:10Z,http://arxiv.org/abs/2410.23437v1,Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment,"Retrieval-Augmented Generation (RAG) systems enhance text generation by
incorporating external knowledge but often struggle when retrieving context
across different text modalities due to semantic gaps. We introduce a
generalized projection-based method, inspired by adapter modules in transfer
learning, that efficiently bridges these gaps between various text types, such
as programming code and pseudocode, or English and French sentences. Our
approach emphasizes speed, accuracy, and data efficiency, requiring minimal
resources for training and inference. By aligning embeddings from heterogeneous
text modalities into a unified space through a lightweight projection network,
our model significantly outperforms traditional retrieval methods like the
Okapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while
approaching the accuracy of Sentence Transformers. Extensive evaluations
demonstrate the effectiveness and generalizability of our method across
different tasks, highlighting its potential for real-time, resource-constrained
applications.",Arihan Yadav
2024-10-30T23:35:21Z,http://arxiv.org/abs/2410.23511v1,"Dynamic Strategy Planning for Efficient Question Answering with Large
  Language Models","Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),
planning (e.g., SelfAsk), and retrieval augmented generation strategies to
improve the performance of Large Language Models (LLMs) on various tasks, such
as question answering. However, using a single fixed strategy to answer
different kinds of questions is suboptimal in performance and inefficient in
terms of generated output tokens and performed retrievals. In our work, we
propose a novel technique DyPlan, to induce a dynamic strategy selection
process in LLMs, to improve performance and reduce costs in question-answering.
DyPlan incorporates an initial decision step to select the most suitable
strategy conditioned on the input question and guides the LLM's response
generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal
verification and correction process to further enrich the generated answer.
Experiments on three prominent multi-hop question answering (MHQA) datasets
reveal how DyPlan can improve model performance by 7-13% while reducing the
cost by 11-32% relative to the best baseline model.",Tanmay Parekh
2024-10-31T00:18:05Z,http://arxiv.org/abs/2410.23526v1,"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve
  Factualness in Large Language Models","Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.",Hieu Tran
2024-10-31T18:43:12Z,http://arxiv.org/abs/2411.00142v1,"JudgeRank: Leveraging Large Language Models for Reasoning-Intensive
  Reranking","Accurate document retrieval is crucial for the success of retrieval-augmented
generation (RAG) applications, including open-domain question answering and
code completion. While large language models (LLMs) have been employed as dense
encoders or listwise rerankers in RAG systems, they often struggle with
reasoning-intensive tasks because they lack nuanced analysis when judging
document relevance. To address this limitation, we introduce JudgeRank, a novel
agentic reranker that emulates human cognitive processes when assessing
document relevance. Our approach consists of three key steps: (1) query
analysis to identify the core problem, (2) document analysis to extract a
query-aware summary, and (3) relevance judgment to provide a concise assessment
of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT
benchmark, demonstrating substantial performance improvements over first-stage
retrieval methods and outperforming other popular reranking approaches. In
addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers
on the popular BEIR benchmark, validating its zero-shot generalization
capability. Through comprehensive ablation studies, we demonstrate that
JudgeRank's performance generalizes well across LLMs of various sizes while
ensembling them yields even more accurate reranking than individual models.",Tong Niu
2024-11-01T01:11:58Z,http://arxiv.org/abs/2411.00294v2,"LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models","Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.",Kazi Ahmed Asif Fuad
2024-11-01T20:44:59Z,http://arxiv.org/abs/2411.01022v1,"Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM
  Generation Output","We present a light-weight approach for detecting nonfactual outputs from
retrieval-augmented generation (RAG). Given a context and putative output, we
compute a factuality score that can be thresholded to yield a binary decision
to check the results of LLM-based question-answering, summarization, or other
systems. Unlike factuality checkers that themselves rely on LLMs, we use
compact, open-source natural language inference (NLI) models that yield a
freely accessible solution with low latency and low cost at run-time, and no
need for LLM fine-tuning. The approach also enables downstream mitigation and
correction of hallucinations, by tracing them back to specific context chunks.
Our experiments show high area under the ROC curve (AUC) across a wide range of
relevant open source datasets, indicating the effectiveness of our method for
fact-checking RAG output.",Hithesh Sankararaman
2024-11-01T23:03:40Z,http://arxiv.org/abs/2411.01073v1,"AttackQA: Development and Adoption of a Dataset for Assisting
  Cybersecurity Operations using Fine-tuned and Open-Source LLMs","Retrieval-augmented generation (RAG) on specialized domain datasets has shown
improved performance when large language models (LLMs) are fine-tuned for
generating responses to user queries. In this study, we develop a cybersecurity
question-answering (Q\&A) dataset, called AttackQA, and employ it to build a
RAG-based Q\&A system designed for analysts in security operations centers. The
dataset comprises 25,335 Q\&A pairs, accompanied by rationales to facilitate
fine-tuning and evaluation. 80\% of the dataset was generated with help of a
lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per
second with full 16-bit precision on SambaNova System's SN40L specialized
hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and
reject low-quality Q\&A pairs. In using the dataset for RAG, we demonstrate
that fine-tuning open-source embeddings and LLMs can yield superior accuracy
compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).
Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,
enabling the creation of a fully open-source, high-speed RAG and evaluation
pipeline with a benchmark for model accuracy.",Varun Badrinath Krishna
2024-11-02T02:09:01Z,http://arxiv.org/abs/2411.01106v1,"LoRA-Contextualizing Adaptation of Large Multimodal Models for Long
  Document Understanding","Large multimodal models (LMMs) have recently shown great progress in
text-rich image understanding, yet they still struggle with complex,
multi-page, visually-rich documents. Traditional methods using document parsers
for retrieval-augmented generation suffer from performance and efficiency
limitations, while directly presenting all pages to LMMs leads to
inefficiencies, especially with lengthy documents. In this work, we present a
novel framework named LoRA-Contextualizing Adaptation of Large multimodal
models (LoCAL), which broadens the capabilities of any LMM to support
long-document understanding. We demonstrate that LMMs can effectively serve as
multimodal retrievers, fetching relevant pages to answer user questions based
on these pages. LoCAL is implemented with two specific LMM adapters: one for
evidence page retrieval and another for question answering. Empirical results
show state-of-the-art performance on public benchmarks, demonstrating the
effectiveness of LoCAL.",Jian Chen
2024-11-05T06:44:15Z,http://arxiv.org/abs/2411.02850v1,"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene","This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.",Simon Kloker
2024-11-05T22:37:43Z,http://arxiv.org/abs/2411.03538v1,Long Context RAG Performance of Large Language Models,"Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.",Quinn Leng
2024-11-06T00:23:55Z,http://arxiv.org/abs/2411.03572v1,"Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge
  Reasoning and Text Generation","This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.",Yuxin Dong
2024-11-06T22:10:18Z,http://arxiv.org/abs/2411.04284v1,Enhancing Security Control Production With Generative AI,"Security controls are mechanisms or policies designed for cloud based
services to reduce risk, protect information, and ensure compliance with
security regulations. The development of security controls is traditionally a
labor-intensive and time-consuming process. This paper explores the use of
Generative AI to accelerate the generation of security controls. We
specifically focus on generating Gherkin codes which are the domain-specific
language used to define the behavior of security controls in a structured and
understandable format. By leveraging large language models and in-context
learning, we propose a structured framework that reduces the time required for
developing security controls from 2-3 days to less than one minute. Our
approach integrates detailed task descriptions, step-by-step instructions, and
retrieval-augmented generation to enhance the accuracy and efficiency of the
generated Gherkin code. Initial evaluations on AWS cloud services demonstrate
promising results, indicating that GenAI can effectively streamline the
security control development process, thus providing a robust and dynamic
safeguard for cloud-based infrastructures.",Chen Ling
2024-11-07T06:51:24Z,http://arxiv.org/abs/2411.04473v1,ML-Promise: A Multilingual Dataset for Corporate Promise Verification,"Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.",Yohei Seki
2024-11-08T06:12:56Z,http://arxiv.org/abs/2411.05349v1,"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent
  Cluster Diagnosis System and Evaluation Framework","Recent advancements in Large Language Models (LLMs) and related technologies
such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
enabled the creation of autonomous intelligent systems capable of performing
cluster diagnostics and troubleshooting. By integrating these technologies with
self-play methodologies, we have developed an LLM-agent system designed to
autonomously diagnose and resolve issues within AI clusters. Our innovations
include a knowledge base tailored for cluster diagnostics, enhanced LLM
algorithms, practical deployment strategies for agents, and a benchmark
specifically designed for evaluating LLM capabilities in this domain. Through
extensive experimentation across multiple dimensions, we have demonstrated the
superiority of our system in addressing the challenges faced in cluster
diagnostics, particularly in detecting and rectifying performance issues more
efficiently and accurately than traditional methods.",Honghao Shi
2024-11-09T13:17:39Z,http://arxiv.org/abs/2411.06175v2,"Clustering Algorithms and RAG Enhancing Semi-Supervised Text
  Classification with Large LLMs","This paper introduces a novel semi-supervised learning framework specifically
designed for text classification tasks, effectively addressing the challenge of
vast datasets with limited labeled examples. By integrating multi-level
similarity based data augmentation techniques from Retrieval-Augmented
Generation (RAG) to Large Language Model (LLM) rewriting and traditional word
substitution-we constructed an intelligent augmentation pipeline. This
framework innovatively employs the selection of representative landmarks
through clustering, which serve as intermediaries in the retrieval and
rewriting processes, ensuring that the augmented data maintains a distribution
similar to the original dataset. Empirical results show that even in complex
text document classification scenarios with over 100 categories, our method
achieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and
Web of Science datasets, respectively. These findings highlight the
effectiveness and broad applicability of our semi-supervised learning approach
for text classification tasks.",Shan Zhong
2024-11-10T15:21:30Z,http://arxiv.org/abs/2411.06493v2,LProtector: An LLM-driven Vulnerability Detection System,"This paper presents LProtector, an automated vulnerability detection system
for C/C++ codebases driven by the large language model (LLM) GPT-4o and
Retrieval-Augmented Generation (RAG). As software complexity grows, traditional
methods face challenges in detecting vulnerabilities effectively. LProtector
leverages GPT-4o's powerful code comprehension and generation capabilities to
perform binary classification and identify vulnerabilities within target
codebases. We conducted experiments on the Big-Vul dataset, showing that
LProtector outperforms two state-of-the-art baselines in terms of F1 score,
demonstrating the potential of integrating LLMs with vulnerability detection.",Ze Sheng
2024-11-11T17:33:51Z,http://arxiv.org/abs/2411.07156v1,"A Primer on Word Embeddings: AI Techniques for Text Analysis in Social
  Work","Word embeddings represent a transformative technology for analyzing text data
in social work research, offering sophisticated tools for understanding case
notes, policy documents, research literature, and other text-based materials.
This methodological paper introduces word embeddings to social work
researchers, explaining how these mathematical representations capture meaning
and relationships in text data more effectively than traditional keyword-based
approaches. We discuss fundamental concepts, technical foundations, and
practical applications, including semantic search, clustering, and retrieval
augmented generation. The paper demonstrates how embeddings can enhance
research workflows through concrete examples from social work practice, such as
analyzing case notes for housing instability patterns and comparing social work
licensing examinations across languages. While highlighting the potential of
embeddings for advancing social work research, we acknowledge limitations
including information loss, training data constraints, and potential biases. We
conclude that successfully implementing embedding technologies in social work
requires developing domain-specific models, creating accessible tools, and
establishing best practices aligned with social work's ethical principles. This
integration can enhance our ability to analyze complex patterns in text data
while supporting more effective services and interventions.",Brian E. Perron
2024-11-11T22:06:51Z,http://arxiv.org/abs/2411.07396v1,Toward Optimal Search and Retrieval for RAG,"Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.",Alexandria Leto
2024-11-12T12:03:57Z,http://arxiv.org/abs/2411.07739v1,Unlocking Legal Knowledge with Multi-Layered Embedding-Based Retrieval,"This work addresses the challenge of capturing the complexities of legal
knowledge by proposing a multi-layered embedding-based retrieval method for
legal and legislative texts. Creating embeddings not only for individual
articles but also for their components (paragraphs, clauses) and structural
groupings (books, titles, chapters, etc), we seek to capture the subtleties of
legal information through the use of dense vectors of embeddings, representing
it at varying levels of granularity. Our method meets various information needs
by allowing the Retrieval Augmented Generation system to provide accurate
responses, whether for specific segments or entire sections, tailored to the
user's query. We explore the concepts of aboutness, semantic chunking, and
inherent hierarchy within legal texts, arguing that this method enhances the
legal information retrieval. Despite the focus being on Brazil's legislative
methods and the Brazilian Constitution, which follow a civil law tradition, our
findings should in principle be applicable across different legal systems,
including those adhering to common law traditions. Furthermore, the principles
of the proposed method extend beyond the legal domain, offering valuable
insights for organizing and retrieving information in any field characterized
by information encoded in hierarchical text.",João Alberto de Oliveira Lima
2024-11-12T14:12:45Z,http://arxiv.org/abs/2411.07820v2,"Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models","We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.",Youan Cong
2024-11-12T15:26:17Z,http://arxiv.org/abs/2411.07870v6,"Trustful LLMs: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders","Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.",Xiaofeng Zhu
2024-11-12T23:55:11Z,http://arxiv.org/abs/2411.08249v1,Retrieval Augmented Time Series Forecasting,"Retrieval-augmented generation (RAG) is a central component of modern LLM
systems, particularly in scenarios where up-to-date information is crucial for
accurately responding to user queries or when queries exceed the scope of the
training data. The advent of time-series foundation models (TSFM), such as
Chronos, and the need for effective zero-shot forecasting performance across
various time-series domains motivates the question: Do benefits of RAG
similarly carry over to time series forecasting? In this paper, we advocate
that the dynamic and event-driven nature of time-series data makes RAG a
crucial component of TSFMs and introduce a principled RAG framework for
time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within
RAF, we develop efficient strategies for retrieving related time-series
examples and incorporating them into forecast. Through experiments and
mechanistic studies, we demonstrate that RAF indeed improves the forecasting
accuracy across diverse time series domains and the improvement is more
significant for larger TSFM sizes.",Kutay Tire
2024-11-13T04:20:20Z,http://arxiv.org/abs/2411.08324v1,"Are LLMs Prescient? A Continuous Evaluation using Daily News as the
  Oracle","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict ""future"" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.",Hui Dai
2024-11-13T05:40:24Z,http://arxiv.org/abs/2411.08348v1,"Refining Translations with LLMs: A Constraint-Aware Iterative Prompting
  Approach","Large language models (LLMs) have demonstrated remarkable proficiency in
machine translation (MT), even without specific training on the languages in
question. However, translating rare words in low-resource or domain-specific
contexts remains challenging for LLMs. To address this issue, we propose a
multi-step prompt chain that enhances translation faithfulness by prioritizing
key terms crucial for semantic accuracy. Our method first identifies these
keywords and retrieves their translations from a bilingual dictionary,
integrating them into the LLM's context using Retrieval-Augmented Generation
(RAG). We further mitigate potential output hallucinations caused by long
prompts through an iterative self-checking mechanism, where the LLM refines its
translations based on lexical and semantic constraints. Experiments using Llama
and Qwen as base models on the FLORES-200 and WMT datasets demonstrate
significant improvements over baselines, highlighting the effectiveness of our
approach in enhancing translation faithfulness and robustness, particularly in
low-resource scenarios.",Shangfeng Chen
2024-11-13T09:11:56Z,http://arxiv.org/abs/2411.08449v2,Towards Evaluating Large Language Models for Graph Query Generation,"Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.",Siraj Munir
2024-11-13T09:40:37Z,http://arxiv.org/abs/2411.08469v2,"Building Trustworthy AI: Transparent AI Systems via Large Language
  Models, Ontologies, and Logical Reasoning (TranspNet)","Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their ""black box"" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.",Fadi Al Machot
2024-11-13T12:44:41Z,http://arxiv.org/abs/2411.08574v1,"Practitioners' Discussions on Building LLM-based Applications for
  Production","\textit{Background}: Large language models (LLMs) have become a paramount
interest of researchers and practitioners alike, yet a comprehensive overview
of key considerations for those developing LLM-based systems is lacking. This
study addresses this gap by collecting and mapping the topics practitioners
discuss online, offering practical insights into where priorities lie in
developing LLM-based applications. \textit{Method}: We collected 189 videos
from 2022 to 2024 from practitioners actively developing such systems and
discussing various aspects they encounter during development and deployment of
LLMs in production. We analyzed the transcripts using BERTopic, then manually
sorted and merged the generated topics into themes, leading to a total of 20
topics in 8 themes. \textit{Results}: The most prevalent topics fall within the
theme Design \& Architecture, with a strong focus on retrieval-augmented
generation (RAG) systems. Other frequently discussed topics include model
capabilities and enhancement techniques (e.g., fine-tuning, prompt
engineering), infrastructure and tooling, and risks and ethical challenges.
\textit{Implications}: Our results highlight current discussions and challenges
in deploying LLMs in production. This way, we provide a systematic overview of
key aspects practitioners should be aware of when developing LLM-based
applications. We further pale off topics of interest for academics where
further research is needed.",Alina Mailach
2024-11-04T08:15:22Z,http://arxiv.org/abs/2411.08724v1,"QCG-Rerank: Chunks Graph Rerank with Query Expansion in
  Retrieval-Augmented LLMs for Tourism Domain","Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in
Large Language Models (LLMs) by integrating information retrieval techniques.
However, in the tourism domain, since the query is usually brief and the
content in the database is diverse, existing RAG may contain a significant
amount of irrelevant or contradictory information contents after retrieval. To
address this challenge, we propose the QCG-Rerank model. This model first
performs an initial retrieval to obtain candidate chunks and then enhances
semantics by extracting critical information to expand the original query.
Next, we utilize the expanded query and candidate chunks to calculate
similarity scores as the initial transition probability and construct the
chunks graph. Subsequently, We iteratively compute the transition probabilities
based on an initial estimate until convergence. The chunks with the highest
score are selected and input into the LLMs to generate responses. We evaluate
the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.
The experimental results demonstrate the effectiveness and superiority of the
QCG-Rerank method.",Qikai Wei
2024-11-14T17:01:24Z,http://arxiv.org/abs/2411.09590v1,Adopting RAG for LLM-Aided Future Vehicle Design,"In this paper, we explore the integration of Large Language Models (LLMs)
with Retrieval-Augmented Generation (RAG) to enhance automated design and
software development in the automotive industry. We present two case studies: a
standardization compliance chatbot and a design copilot, both utilizing RAG to
provide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,
LLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and
execution time. Our results demonstrate that while GPT-4 offers superior
performance, LLAMA3 and Mistral also show promising capabilities for local
deployment, addressing data privacy concerns in automotive applications. This
study highlights the potential of RAG-augmented LLMs in improving design
workflows and compliance in automotive engineering.",Vahid Zolfaghari
2024-11-19T07:03:19Z,http://arxiv.org/abs/2411.12280v1,"Large Language Models for Material Property Predictions: elastic
  constant tensor prediction and materials design","Efficient and accurate prediction of material properties is critical for
advancing materials design and applications. The rapid-evolution of large
language models (LLMs) presents a new opportunity for material property
predictions, complementing experimental measurements and multi-scale
computational methods. We focus on predicting the elastic constant tensor, as a
case study, and develop domain-specific LLMs for predicting elastic constants
and for materials discovery. The proposed ElaTBot LLM enables simultaneous
prediction of elastic constant tensors, bulk modulus at finite temperatures,
and the generation of new materials with targeted properties. Moreover, the
capabilities of ElaTBot are further enhanced by integrating with general LLMs
(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized
variant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,
reduces the prediction errors by 33.1% compared with domain-specific, material
science LLMs (Darwin) trained on the same dataset. This natural language-based
approach lowers the barriers to computational materials science and highlights
the broader potential of LLMs for material property predictions and inverse
design.",Siyu Liu
2024-11-19T07:16:48Z,http://arxiv.org/abs/2411.12287v2,"CUE-M: Contextual Understanding and Enhanced Search with Multimodal
  Large Language Model","The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large
Language Models (MLLMs) has revolutionized information retrieval and expanded
the practical applications of AI. However, current systems struggle in
accurately interpreting user intent, employing diverse retrieval strategies,
and effectively filtering unintended or inappropriate responses, limiting their
effectiveness. This paper introduces Contextual Understanding and Enhanced
Search with MLLM (CUE-M), a novel multimodal search framework that addresses
these challenges through a multi-stage pipeline comprising image context
enrichment, intent refinement, contextual query generation, external API
integration, and relevance-based filtering. CUE-M incorporates a robust
filtering pipeline combining image-based, text-based, and multimodal
classifiers, dynamically adapting to instance- and category-specific concern
defined by organizational policies. Evaluations on a multimodal Q&A dataset and
a public safety benchmark demonstrate that CUE-M outperforms baselines in
accuracy, knowledge integration, and safety, advancing the capabilities of
multimodal retrieval systems.",Dongyoung Go
2024-11-19T16:54:45Z,http://arxiv.org/abs/2411.12644v2,"CodeXEmbed: A Generalist Embedding Model Family for Multiligual and
  Multi-task Code Retrieval","Despite the success of text retrieval in many NLP tasks, code retrieval
remains a largely underexplored area. Most text retrieval systems are tailored
for natural language queries, often neglecting the specific challenges of
retrieving code. This gap leaves existing models unable to effectively capture
the diversity of programming languages and tasks across different domains,
highlighting the need for more focused research in code retrieval. To address
this, we introduce CodeXEmbed, a family of large-scale code embedding models
ranging from 400M to 7B parameters. Our novel training pipeline unifies
multiple programming languages and transforms various code-related tasks into a
common retrieval framework, enhancing model generalizability and retrieval
performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,
outperforming the previous leading model, Voyage-Code, by over 20% on CoIR
benchmark. In addition to excelling in code retrieval, our models demonstrate
competitive performance on the widely adopted BeIR text retrieval benchmark,
offering versatility across domains. Experimental results demonstrate that
improving retrieval performance significantly enhances end-to-end
Retrieval-Augmented Generation (RAG) performance for code-related tasks.",Ye Liu
2024-11-20T09:43:30Z,http://arxiv.org/abs/2411.13154v1,DMQR-RAG: Diverse Multi-Query Rewriting for RAG,"Large language models often encounter challenges with static knowledge and
hallucinations, which undermine their reliability. Retrieval-augmented
generation (RAG) mitigates these issues by incorporating external information.
However, user queries frequently contain noise and intent deviations,
necessitating query rewriting to improve the relevance of retrieved documents.
In this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework
designed to improve the performance of both document retrieval and final
responses in RAG. Specifically, we investigate how queries with varying
information quantities can retrieve a diverse array of documents, presenting
four rewriting strategies that operate at different levels of information to
enhance the performance of baseline approaches. Additionally, we propose an
adaptive strategy selection method that minimizes the number of rewrites while
optimizing overall performance. Our methods have been rigorously validated
through extensive experiments conducted in both academic and industry settings.",Zhicong Li
2024-11-20T11:41:08Z,http://arxiv.org/abs/2411.13226v1,"AIDBench: A benchmark for evaluating the authorship identification
  capability of large language models","As large language models (LLMs) rapidly advance and integrate into daily
life, the privacy risks they pose are attracting increasing attention. We focus
on a specific privacy risk where LLMs may help identify the authorship of
anonymous texts, which challenges the effectiveness of anonymity in real-world
systems such as anonymous peer review systems. To investigate these risks, we
present AIDBench, a new benchmark that incorporates several author
identification datasets, including emails, blogs, reviews, articles, and
research papers. AIDBench utilizes two evaluation methods: one-to-one
authorship identification, which determines whether two texts are from the same
author; and one-to-many authorship identification, which, given a query text
and a list of candidate texts, identifies the candidate most likely written by
the same author as the query text. We also introduce a Retrieval-Augmented
Generation (RAG)-based method to enhance the large-scale authorship
identification capabilities of LLMs, particularly when input lengths exceed the
models' context windows, thereby establishing a new baseline for authorship
identification using LLMs. Our experiments with AIDBench demonstrate that LLMs
can correctly guess authorship at rates well above random chance, revealing new
privacy risks posed by these powerful models. The source code and data will be
made publicly available after acceptance.",Zichen Wen
2024-11-19T05:15:19Z,http://arxiv.org/abs/2411.14476v1,"StreetviewLLM: Extracting Geographic Information Using a
  Chain-of-Thought Multimodal Large Language Model","Geospatial predictions are crucial for diverse fields such as disaster
management, urban planning, and public health. Traditional machine learning
methods often face limitations when handling unstructured or multi-modal data
like street view imagery. To address these challenges, we propose
StreetViewLLM, a novel framework that integrates a large language model with
the chain-of-thought reasoning and multimodal data sources. By combining street
view imagery with geographic coordinates and textual data, StreetViewLLM
improves the precision and granularity of geospatial predictions. Using
retrieval-augmented generation techniques, our approach enhances geographic
information extraction, enabling a detailed analysis of urban environments. The
model has been applied to seven global cities, including Hong Kong, Tokyo,
Singapore, Los Angeles, New York, London, and Paris, demonstrating superior
performance in predicting urban indicators, including population density,
accessibility to healthcare, normalized difference vegetation index, building
height, and impervious surface. The results show that StreetViewLLM
consistently outperforms baseline models, offering improved predictive accuracy
and deeper insights into the built environment. This research opens new
opportunities for integrating the large language model into urban analytics,
decision-making in urban planning, infrastructure management, and environmental
monitoring.",Zongrong Li
2024-11-21T21:22:58Z,http://arxiv.org/abs/2411.14592v2,G-RAG: Knowledge Expansion in Material Science,"In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.",Radeen Mostafa
2024-11-23T14:47:10Z,http://arxiv.org/abs/2411.15577v1,"From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive
  Grammars","Recent advances in language modeling have demonstrated significant
improvements in zero-shot capabilities, including in-context learning,
instruction following, and machine translation for extremely under-resourced
languages (Tanzer et al., 2024). However, many languages with limited written
resources rely primarily on formal descriptions of grammar and vocabulary.
  In this paper, we introduce a set of benchmarks to evaluate how well models
can extract and classify information from the complex descriptions found in
linguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based
approach that leverages these descriptions for downstream tasks such as machine
translation. Our benchmarks encompass linguistic descriptions for 248 languages
across 142 language families, focusing on typological features from WALS and
Grambank.
  This set of benchmarks offers the first comprehensive evaluation of language
models' in-context ability to accurately interpret and extract linguistic
features, providing a critical resource for scaling NLP to low-resource
languages. The code and data are publicly available at
\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.",Albert Kornilov
2024-11-25T13:53:36Z,http://arxiv.org/abs/2411.16391v2,"Human-Calibrated Automated Testing and Validation of Generative Language
  Models","This paper introduces a comprehensive framework for the evaluation and
validation of generative language models (GLMs), with a focus on
Retrieval-Augmented Generation (RAG) systems deployed in high-stakes domains
such as banking. GLM evaluation is challenging due to open-ended outputs and
subjective quality assessments. Leveraging the structured nature of RAG
systems, where generated responses are grounded in a predefined document
collection, we propose the Human-Calibrated Automated Testing (HCAT) framework.
HCAT integrates a) automated test generation using stratified sampling, b)
embedding-based metrics for explainable assessment of functionality, risk and
safety attributes, and c) a two-stage calibration approach that aligns
machine-generated evaluations with human judgments through probability
calibration and conformal prediction.
  In addition, the framework includes robustness testing to evaluate model
performance against adversarial, out-of-distribution, and varied input
conditions, as well as targeted weakness identification using marginal and
bivariate analysis to pinpoint specific areas for improvement. This
human-calibrated, multi-layered evaluation framework offers a scalable,
transparent, and interpretable approach to GLM assessment, providing a
practical and reliable solution for deploying GLMs in applications where
accuracy, transparency, and regulatory compliance are paramount.",Agus Sudjianto
2024-11-21T19:01:07Z,http://arxiv.org/abs/2411.16707v1,"Enhancing LLMs for Power System Simulations: A Feedback-driven
  Multi-agent Framework","The integration of experimental technologies with large language models
(LLMs) is transforming scientific research, positioning AI as a versatile
research assistant rather than a mere problem-solving tool. In the field of
power systems, however, managing simulations -- one of the essential
experimental technologies -- remains a challenge for LLMs due to their limited
domain-specific knowledge, restricted reasoning capabilities, and imprecise
handling of simulation parameters. To address these limitations, we propose a
feedback-driven, multi-agent framework that incorporates three proposed
modules: an enhanced retrieval-augmented generation (RAG) module, an improved
reasoning module, and a dynamic environmental acting module with an
error-feedback mechanism. Validated on 69 diverse tasks from Daline and
MATPOWER, this framework achieves success rates of 93.13% and 96.85%,
respectively, significantly outperforming the latest LLMs (ChatGPT 4o and
o1-preview), which achieved a 27.77% success rate on standard simulation tasks
and 0% on complex tasks. Additionally, our framework also supports rapid,
cost-effective task execution, completing each simulation in approximately 30
seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable
framework lays a foundation for developing intelligent LLM-based assistants for
human researchers, facilitating power system research and beyond.",Mengshuo Jia
2024-11-27T10:48:37Z,http://arxiv.org/abs/2411.18216v1,"Evaluating and Improving the Robustness of Security Attack Detectors
  Generated by LLMs","Large Language Models (LLMs) are increasingly used in software development to
generate functions, such as attack detectors, that implement security
requirements. However, LLMs struggle to generate accurate code, resulting,
e.g., in attack detectors that miss well-known attacks when used in practice.
This is most likely due to the LLM lacking knowledge about some existing
attacks and to the generated code being not evaluated in real usage scenarios.
We propose a novel approach integrating Retrieval Augmented Generation (RAG)
and Self-Ranking into the LLM pipeline. RAG enhances the robustness of the
output by incorporating external knowledge sources, while the Self-Ranking
technique, inspired to the concept of Self-Consistency, generates multiple
reasoning paths and creates ranks to select the most robust detector. Our
extensive empirical study targets code generated by LLMs to detect two
prevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL
injection (SQLi). Results show a significant improvement in detection
performance compared to baselines, with an increase of up to 71%pt and 37%pt in
the F2-Score for XSS and SQLi detection, respectively.",Samuele Pasini
2024-11-28T06:28:45Z,http://arxiv.org/abs/2411.18947v1,ICLERB: In-Context Learning Embedding and Reranker Benchmark,"In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new
tasks by conditioning on prompts with relevant information. Retrieval-Augmented
Generation (RAG) enhances ICL by incorporating retrieved documents into the
LLM's context at query time. However, traditional retrieval methods focus on
semantic relevance, treating retrieval as a search problem. In this paper, we
propose reframing retrieval for ICL as a recommendation problem, aiming to
select documents that maximize utility in ICL tasks. We introduce the
In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel
evaluation framework that compares retrievers based on their ability to enhance
LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement
Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune
retrieval models using minimal feedback from the LLM. Our experimental results
reveal notable differences between ICLERB and existing benchmarks, and
demonstrate that small models fine-tuned with our RLRAIF algorithm outperform
large state-of-the-art retrieval models. These findings highlight the
limitations of existing evaluation methods and the need for specialized
benchmarks and training strategies adapted to ICL.",Marie Al Ghossein
2024-11-29T04:25:31Z,http://arxiv.org/abs/2411.19463v1,"Towards Understanding Retrieval Accuracy and Prompt Quality in RAG
  Systems","Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the
capability of large language models (LLMs) and has demonstrated promising
efficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show
superior performance, they face unique challenges in stability and reliability.
Their complexity hinders developers' efforts to design, maintain, and optimize
effective RAG systems. Therefore, it is crucial to understand how RAG's
performance is impacted by its design. In this work, we conduct an early
exploratory study toward a better understanding of the mechanism of RAG
systems, covering three code datasets, three QA datasets, and two LLMs. We
focus on four design factors: retrieval document type, retrieval recall,
document selection, and prompt techniques. Our study uncovers how each factor
impacts system correctness and confidence, providing valuable insights for
developing an accurate and reliable RAG system. Based on these findings, we
present nine actionable guidelines for detecting defects and optimizing the
performance of RAG systems. We hope our early exploration can inspire further
advancements in engineering, improving and maintaining LLM-driven intelligent
software systems for greater efficiency and reliability.",Shengming Zhao
2024-11-30T10:58:56Z,http://arxiv.org/abs/2412.00431v2,Multi-Agent System for Cosmological Parameter Analysis,"Multi-agent systems (MAS) utilizing multiple Large Language Model agents with
Retrieval Augmented Generation and that can execute code locally may become
beneficial in cosmological data analysis. Here, we illustrate a first small
step towards AI-assisted analyses and a glimpse of the potential of MAS to
automate and optimize scientific workflows in Cosmology. The system
architecture of our example package, that builds upon the autogen/ag2
framework, can be applied to MAS in any area of quantitative scientific
research. The particular task we apply our methods to is the cosmological
parameter analysis of the Atacama Cosmology Telescope lensing power spectrum
likelihood using Monte Carlo Markov Chains. Our work-in-progress code is open
source and available at https://github.com/CMBAgents/cmbagent.",Andrew Laverick
2024-12-02T16:55:07Z,http://arxiv.org/abs/2412.01709v1,"Query Performance Explanation through Large Language Model for HTAP
  Systems","In hybrid transactional and analytical processing (HTAP) systems, users often
struggle to understand why query plans from one engine (OLAP or OLTP) perform
significantly slower than those from another. Although optimizers provide plan
details via the EXPLAIN function, these explanations are frequently too
technical for non-experts and offer limited insights into performance
differences across engines. To address this, we propose a novel framework that
leverages large language models (LLMs) to explain query performance in HTAP
systems. Built on Retrieval-Augmented Generation (RAG), our framework
constructs a knowledge base that stores historical query executions and
expert-curated explanations. To enable efficient retrieval of relevant
knowledge, query plans are embedded using a lightweight tree-CNN classifier.
This augmentation allows the LLM to generate clear, context-aware explanations
of performance differences between engines. Our approach demonstrates the
potential of LLMs in hybrid engine systems, paving the way for further
advancements in database optimization and user support.",Haibo Xiu
