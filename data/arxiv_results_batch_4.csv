Published Date,Link,Title,Summary,First Author
2024-12-03T00:59:56Z,http://arxiv.org/abs/2412.02065v1,"Leveraging Large Language Models to Democratize Access to Costly
  Financial Datasets for Academic Research","Unequal access to costly datasets essential for empirical research has long
hindered researchers from disadvantaged institutions, limiting their ability to
contribute to their fields and advance their careers. Recent breakthroughs in
Large Language Models (LLMs) have the potential to democratize data access by
automating data collection from unstructured sources. We develop and evaluate a
novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation
(RAG) framework to collect data from corporate disclosures. Our approach
achieves human-level accuracy in collecting CEO pay ratios from approximately
10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000
10-K filings, with LLM processing times of 9 and 40 minutes respectively, each
at a cost under $10. This stands in stark contrast to the hundreds of hours
needed for manual collection or the thousands of dollars required for
commercial database subscriptions. To foster a more inclusive research
community by empowering researchers with limited resources to explore new
avenues of inquiry, we share our methodology and the resulting datasets.",Julian Junyan Wang
2024-12-03T08:34:42Z,http://arxiv.org/abs/2412.02262v1,"Composing Open-domain Vision with RAG for Ocean Monitoring and
  Conservation","Climate change's destruction of marine biodiversity is threatening
communities and economies around the world which rely on healthy oceans for
their livelihoods. The challenge of applying computer vision to niche,
real-world domains such as ocean conservation lies in the dynamic and diverse
environments where traditional top-down learning struggle with long-tailed
distributions, generalization, and domain transfer. Scalable species
identification for ocean monitoring is particularly difficult due to the need
to adapt models to new environments and identify rare or unseen species. To
overcome these limitations, we propose leveraging bottom-up, open-domain
learning frameworks as a resilient, scalable solution for image and video
analysis in marine applications. Our preliminary demonstration uses pretrained
vision-language models (VLMs) combined with retrieval-augmented generation
(RAG) as grounding, leaving the door open for numerous architectural, training
and engineering optimizations. We validate this approach through a preliminary
application in classifying fish from video onboard fishing vessels,
demonstrating impressive emergent retrieval and prediction capabilities without
domain-specific training or knowledge of the task itself.",Sepand Dyanatkar
2024-12-05T14:24:07Z,http://arxiv.org/abs/2412.04185v1,"Leveraging Large Language Models to Generate Course-specific
  Semantically Annotated Learning Objects","Background: Over the past few decades, the process and methodology of
automated question generation (AQG) have undergone significant transformations.
Recent progress in generative natural language models has opened up new
potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs)
for generating computer science questions that are sufficiently annotated for
automatic learner model updates, are fully situated in the context of a
particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT,
our approach involves more targeted strategies such as retrieval-augmented
generation (RAG) to produce contextually relevant and pedagogically meaningful
learning objects.
  Results and Conclusions: Our results show that generating structural,
semantic annotations works well. However, this success was not reflected in the
case of relational annotations. The quality of the generated questions often
did not meet educational standards, highlighting that although LLMs can
contribute to the pool of learning materials, their current level of
performance requires significant human intervention to refine and validate the
generated content.",Dominic Lohr
2024-12-05T15:11:12Z,http://arxiv.org/abs/2412.04235v1,"Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM
  Chatbots","I combine detection and mitigation techniques to addresses hallucinations in
Large Language Models (LLMs). Mitigation is achieved in a question-answering
Retrieval-Augmented Generation (RAG) framework while detection is obtained by
introducing the Negative Missing Information Scoring System (NMISS), which
accounts for contextual relevance in responses. While RAG mitigates
hallucinations by grounding answers in external data, NMISS refines the
evaluation by identifying cases where traditional metrics incorrectly flag
contextually accurate responses as hallucinations. I use Italian health news
articles as context to evaluate LLM performance. Results show that Gemma2 and
GPT-4 outperform the other models, with GPT-4 producing answers closely aligned
with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral
benefit significantly from NMISS, highlighting their ability to provide richer
contextual information. This combined approach offers new insights into the
reduction and more accurate assessment of hallucinations in LLMs, with
applications in real-world healthcare tasks and other domains.",Maria Paola Priola
2024-12-05T17:00:32Z,http://arxiv.org/abs/2412.04342v1,Retrieval-Augmented Machine Translation with Unstructured Knowledge,"Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",Jiaan Wang
2024-12-05T23:10:56Z,http://arxiv.org/abs/2412.04661v1,"HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and
  Representation Learning","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external document retrieval to provide domain-specific or
up-to-date knowledge. The effectiveness of RAG depends on the relevance of
retrieved documents, which is influenced by the semantic alignment of
embeddings with the domain's specialized content. Although full fine-tuning can
align language models to specific domains, it is computationally intensive and
demands substantial data. This paper introduces Hierarchical Embedding
Alignment Loss (HEAL), a novel method that leverages hierarchical fuzzy
clustering with matrix factorization within contrastive learning to efficiently
align LLM embeddings with domain-specific content. HEAL computes
level/depth-wise contrastive losses and incorporates hierarchical penalties to
align embeddings with the underlying relationships in label hierarchies. This
approach enhances retrieval relevance and document classification, effectively
reducing hallucinations in LLM outputs. In our experiments, we benchmark and
evaluate HEAL across diverse domains, including Healthcare, Material Science,
Cyber-security, and Applied Maths.",Manish Bhattarai
2024-12-06T17:04:21Z,http://arxiv.org/abs/2412.05184v1,QueEn: A Large Language Model for Quechua-English Translation,"Recent studies show that large language models (LLMs) are powerful tools for
working with natural language, bringing advances in many areas of computational
linguistics. However, these models face challenges when applied to low-resource
languages due to limited training data and difficulty in understanding cultural
nuances. In this paper, we propose QueEn, a novel approach for Quechua-English
translation that combines Retrieval-Augmented Generation (RAG) with
parameter-efficient fine-tuning techniques. Our method leverages external
linguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for
efficient model adaptation. Experimental results show that our approach
substantially exceeds baseline models, with a BLEU score of 17.6 compared to
1.5 for standard GPT models. The integration of RAG with fine-tuning allows our
system to address the challenges of low-resource language translation while
maintaining computational efficiency. This work contributes to the broader goal
of preserving endangered languages through advanced language technologies.",Junhao Chen
2024-12-06T17:54:54Z,http://arxiv.org/abs/2412.05223v1,100% Hallucination Elimination Using Acurai,"The issue of hallucinations in large language models (LLMs) remains a
critical barrier to the adoption of AI in enterprise and other high-stakes
applications. Despite advancements in retrieval-augmented generation (RAG)
systems, current state-of-the-art methods fail to achieve more than 80%
accuracy in generating faithful and factually correct outputs, even when
provided with relevant and accurate context. In this work, we introduce Acurai,
a novel systematic approach that achieves 100% hallucination-free responses in
LLMs by reformatting queries and context data prior to input. Leveraging a deep
understanding of LLM internal representations, the importance of noun-phrase
dominance, and the role of discrete functional units (DFUs), Acurai ensures
alignment between input context and generated output. We validate this method
using the RAGTruth corpus, demonstrating its ability to eliminate 100%
hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for
achieving consistent, accurate, and faithful AI responses, marking a
significant step forward in the development of trustworthy AI systems.",Michael C. Wood
2024-12-06T22:05:39Z,http://arxiv.org/abs/2412.05447v1,"A Graph-Based Approach for Conversational AI-Driven Personal Memory
  Capture and Retrieval in a Real-world Application","TOBU is a novel mobile application that captures and retrieves `personal
memories' (pictures/videos together with stories and context around those
moments) in a user-engaging AI-guided conversational approach. Our initial
prototype showed that existing retrieval techniques such as retrieval-augmented
generation (RAG) systems fall short due to their limitations in understanding
memory relationships, causing low recall, hallucination, and unsatisfactory
user experience. We design TOBUGraph, a novel graph-based retrieval approach.
During capturing, TOBUGraph leverages large language models (LLMs) to
automatically create a dynamic knowledge graph of memories, establishing
context and relationships of those memories. During retrieval, TOBUGraph
combines LLMs with the memory graph to achieve comprehensive recall through
graph traversal. Our evaluation using real user data demonstrates that
TOBUGraph outperforms multiple RAG implementations in both precision and
recall, significantly improving user experience through improved retrieval
accuracy and reduced hallucination.",Savini Kashmira
2024-12-07T05:49:14Z,http://arxiv.org/abs/2412.05547v1,"KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large
  Language Models","Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.",Weijie Chen
2024-12-08T13:36:42Z,http://arxiv.org/abs/2412.05937v1,"Accelerating Manufacturing Scale-Up from Material Discovery Using
  Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering
  Schematics Design","Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (PIDs)
are critical tools for industrial process design, control, and safety. However,
the generation of precise and regulation-compliant diagrams remains a
significant challenge, particularly in scaling breakthroughs from material
discovery to industrial production in an era of automation and digitalization.
This paper introduces an autonomous agentic framework to address these
challenges through a twostage approach involving knowledge acquisition and
generation. The framework integrates specialized sub-agents for retrieving and
synthesizing multimodal data from publicly available online sources and
constructs ontological knowledge graphs using a Graph Retrieval-Augmented
Generation (Graph RAG) paradigm. These capabilities enable the automation of
diagram generation and open-domain question answering (ODQA) tasks with high
contextual accuracy. Extensive empirical experiments demonstrate the frameworks
ability to deliver regulation-compliant diagrams with minimal expert
intervention, highlighting its practical utility for industrial applications.",Sakhinana Sagar Srinivas
2024-12-08T17:53:43Z,http://arxiv.org/abs/2412.06009v1,"1-800-SHARED-TASKS at RegNLP: Lexical Reranking of Semantic Retrieval
  (LeSeR) for Regulatory Question Answering","This paper presents the system description of our entry for the COLING 2025
RegNLP RIRAG (Regulatory Information Retrieval and Answer Generation)
challenge, focusing on leveraging advanced information retrieval and answer
generation techniques in regulatory domains. We experimented with a combination
of embedding models, including Stella, BGE, CDE, and Mpnet, and leveraged
fine-tuning and reranking for retrieving relevant documents in top ranks. We
utilized a novel approach, LeSeR, which achieved competitive results with a
recall@10 of 0.8201 and map@10 of 0.6655 for retrievals. This work highlights
the transformative potential of natural language processing techniques in
regulatory applications, offering insights into their capabilities for
implementing a retrieval augmented generation system while identifying areas
for future improvement in robustness and domain adaptation.",Jebish Purbey
2024-12-10T11:05:26Z,http://arxiv.org/abs/2412.07412v1,"Generating Knowledge Graphs from Large Language Models: A Comparative
  Study of GPT-4, LLaMA 2, and BERT","Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a
form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks
requiring structured reasoning and semantic understanding. However, creating
KGs for GraphRAGs remains a significant challenge due to accuracy and
scalability limitations of traditional methods. This paper introduces a novel
approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and
BERT to generate KGs directly from unstructured data, bypassing traditional
pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit
Distance, and Semantic Similarity, we evaluate the models' ability to generate
high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic
fidelity and structural accuracy, LLaMA 2 excels in lightweight,
domain-specific graphs, and BERT provides insights into challenges in
entity-relationship modeling. This study underscores the potential of LLMs to
streamline KG creation and enhance GraphRAG accessibility for real-world
applications, while setting a foundation for future advancements.",Ahan Bhatt
2024-12-13T17:53:29Z,http://arxiv.org/abs/2412.10313v1,MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation,"Regulatory documents are rich in nuanced terminology and specialized
semantics. FRAG systems: Frozen retrieval-augmented generators utilizing
pre-trained (or, frozen) components face consequent challenges with both
retriever and answering performance. We present a system that adapts the
retriever performance to the target domain using a multi-stage tuning (MST)
strategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders
used in vector stores using hard negative mining, (b) then uses a hybrid
retriever, combining sparse and dense retrievers using reciprocal rank fusion,
and then (c) adapts the cross-attention encoder by fine-tuning only the top-k
retrieved results. We benchmark the system performance on the dataset released
for the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We
achieve significant performance gains obtaining a top rank on the RegNLP
challenge leaderboard. We also show that a trivial answering approach games the
RePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing
this anomaly, we present important takeaways for future research.",Yash Malviya
2024-12-13T20:39:30Z,http://arxiv.org/abs/2412.10543v1,RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation,"RAG (Retrieval Augmented Generation) allows LLMs (large language models) to
generate better responses with external knowledge, but using more external
knowledge often improves generation quality at the expense of response delay.
Prior work either reduces the response delay (through better scheduling of RAG
queries) or strives to maximize quality (which involves tuning the RAG
workflow), but they fall short in optimizing the tradeoff between the delay and
quality of RAG responses. This paper presents RAGServe, the first RAG system
that jointly schedules queries and adapts the key RAG configurations of each
query, such as the number of retrieved text chunks and synthesis methods, in
order to balance quality optimization and response delay reduction. Using 4
popular RAG-QA datasets, we show that compared with the state-of-the-art RAG
optimization schemes, RAGServe reduces the generation latency by
$1.64-2.54\times$ without sacrificing generation quality.",Siddhant Ray
2024-12-14T05:06:43Z,http://arxiv.org/abs/2412.10684v1,Inference Scaling for Bridging Retrieval and Augmented Generation,"Retrieval-augmented generation (RAG) has emerged as a popular approach to
steering the output of a large language model (LLM) by incorporating retrieved
contexts as inputs. However, existing work observed the generator bias, such
that improving the retrieval results may negatively affect the outcome. In this
work, we show such bias can be mitigated, from inference scaling, aggregating
inference calls from the permuted order of retrieved contexts. The proposed
Mixture-of-Intervention (MOI) explicitly models the debiased utility of each
passage with multiple forward passes to construct a new ranking. We also show
that MOI can leverage the retriever's prior knowledge to reduce the
computational cost by minimizing the number of permutations considered and
lowering the cost per LLM call. We showcase the effectiveness of MOI on diverse
RAG tasks, improving ROUGE-L on MS MARCO and EM on HotpotQA benchmarks by ~7
points.",Youngwon Lee
2024-12-14T17:30:33Z,http://arxiv.org/abs/2412.10906v1,"SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability
  Report Generation","The rapid growth of the financial sector and the rising focus on
Environmental, Social, and Governance (ESG) considerations highlight the need
for advanced NLP tools. However, open-source LLMs proficient in both finance
and ESG domains remain scarce. To address this gap, we introduce SusGen-30K, a
category-balanced dataset comprising seven financial NLP tasks and ESG report
generation, and propose TCFD-Bench, a benchmark for evaluating sustainability
report generation. Leveraging this dataset, we developed SusGen-GPT, a suite of
models achieving state-of-the-art performance across six adapted and two
off-the-shelf tasks, trailing GPT-4 by only 2% despite using 7-8B parameters
compared to GPT-4's 1,700B. Based on this, we propose the SusGen system,
integrated with Retrieval-Augmented Generation (RAG), to assist in
sustainability report generation. This work demonstrates the efficiency of our
approach, advancing research in finance and ESG.",Qilong Wu
2024-12-16T12:44:42Z,http://arxiv.org/abs/2412.11722v2,"GHIssuemarket: A Sandbox Environment for SWE-Agents Economic
  Experimentation","Software engineering agents (swe-agents), as key innovations in intelligent
software engineering, are poised in the industry's end-of-programming debate to
transcend from assistance to primary roles. we argue the importance of
swe-agents' economic viability to their transcendence -- defined as their
capacity to maintain efficient operations in constrained environments -- and
propose its exploration via software engineering economics experimentation.we
introduce ghissuemarket sandbox, a controlled virtual environment for
swe-agents' economic experimentation, simulating the environment of an
envisioned peer-to-peer multiagent system for github issues outsourcing
auctions. in this controlled setting, autonomous swe-agents auction and bid on
github issues, leveraging real-time communication, a built-in
retrieval-augmented generation (rag) interface for effective decision-making,
and instant cryptocurrency micropayments. we open-source our software
artifacts, discuss our sandbox engineering decisions, and advocate towards
swe-agents' economic exploration -- an emerging field we intend to pursue under
the term intelligent software engineering economics (isee).",Mohamed A. Fouad
2024-12-16T16:03:25Z,http://arxiv.org/abs/2412.11919v1,"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained
  Evidence within Generation","Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.",Xiaoxi Li
2024-12-16T19:11:55Z,http://arxiv.org/abs/2412.12300v1,Unanswerability Evaluation for Retreival Augmented Generation,"Existing evaluation frameworks for retrieval-augmented generation (RAG)
systems focus on answerable queries, but they overlook the importance of
appropriately rejecting unanswerable requests. In this paper, we introduce
UAEval4RAG, a framework designed to evaluate whether RAG systems can handle
unanswerable queries effectively. We define a taxonomy with six unanswerable
categories, and UAEval4RAG automatically synthesizes diverse and challenging
queries for any given knowledge base with unanswered ratio and acceptable ratio
metrics. We conduct experiments with various RAG components, including
retrieval models, rewriting methods, rerankers, language models, and prompting
strategies, and reveal hidden trade-offs in performance of RAG systems. Our
findings highlight the critical role of component selection and prompt design
in optimizing RAG systems to balance the accuracy of answerable queries with
high rejection rates of unanswerable ones. UAEval4RAG provides valuable
insights and tools for developing more robust and reliable RAG systems.",Xiangyu Peng
2024-12-16T19:40:26Z,http://arxiv.org/abs/2412.12322v1,"RAG Playground: A Framework for Systematic Evaluation of Retrieval
  Strategies and Prompt Engineering in RAG Systems","We present RAG Playground, an open-source framework for systematic evaluation
of Retrieval-Augmented Generation (RAG) systems. The framework implements and
compares three retrieval approaches: naive vector search, reranking, and hybrid
vector-keyword search, combined with ReAct agents using different prompting
strategies. We introduce a comprehensive evaluation framework with novel
metrics and provide empirical results comparing different language models
(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our
experiments demonstrate significant performance improvements through hybrid
search methods and structured self-evaluation prompting, achieving up to 72.7%
pass rate on our multi-metric evaluation framework. The results also highlight
the importance of prompt engineering in RAG systems, with our custom-prompted
agents showing consistent improvements in retrieval accuracy and response
quality.",Ioannis Papadimitriou
2024-12-16T21:36:03Z,http://arxiv.org/abs/2412.12364v1,"LogBabylon: A Unified Framework for Cross-Log File Integration and
  Analysis","Logs are critical resources that record events, activities, or messages
produced by software applications, operating systems, servers, and network
devices. However, consolidating the heterogeneous logs and cross-referencing
them is challenging and complicated. Manually analyzing the log data is
time-consuming and prone to errors. LogBabylon is a centralized log data
consolidating solution that leverages Large Language Models (LLMs) integrated
with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the
log data in a human-readable way and adds insight analysis of the system
performance and anomaly alerts. It provides a paramount view of the system
landscape, enabling proactive management and rapid incident response.
LogBabylon consolidates diverse log sources and enhances the extracted
information's accuracy and relevancy. This facilitates a deeper understanding
of log data, supporting more effective decision-making and operational
efficiency. Furthermore, LogBabylon streamlines the log analysis process,
significantly reducing the time and effort required to interpret complex
datasets. Its capabilities extend to generating context-aware insights,
offering an invaluable tool for continuous monitoring, performance
optimization, and security assurance in dynamic computing environments.",Rabimba Karanjai
2024-12-17T01:23:45Z,http://arxiv.org/abs/2412.12447v2,"PERC: Plan-As-Query Example Retrieval for Underrepresented Code
  Generation","Code generation with large language models has shown significant promise,
especially when employing retrieval-augmented generation (RAG) with few-shot
examples. However, selecting effective examples that enhance generation quality
remains a challenging task, particularly when the target programming language
(PL) is underrepresented. In this study, we present two key findings: (1)
retrieving examples whose presented algorithmic plans can be referenced for
generating the desired behavior significantly improves generation accuracy, and
(2) converting code into pseudocode effectively captures such algorithmic
plans, enhancing retrieval quality even when the source and the target PLs are
different. Based on these findings, we propose Plan-as-query Example Retrieval
for few-shot prompting in Code generation (PERC), a novel framework that
utilizes algorithmic plans to identify and retrieve effective examples. We
validate the effectiveness of PERC through extensive experiments on the
CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms
the state-of-the-art RAG methods in code generation, both when the source and
target programming languages match or differ, highlighting its adaptability and
robustness in diverse coding environments.",Jaeseok Yoo
2024-12-17T07:49:49Z,http://arxiv.org/abs/2412.12632v1,"What External Knowledge is Preferred by LLMs? Characterizing and
  Exploring Chain of Evidence in Imperfect Context","Incorporating external knowledge into large language models (LLMs) has
emerged as a promising approach to mitigate outdated knowledge and
hallucination in LLMs. However, external knowledge is often imperfect. In
addition to useful knowledge, external knowledge is rich in irrelevant or
misinformation in the context that can impair the reliability of LLM responses.
This paper focuses on LLMs' preferred external knowledge in imperfect contexts
when handling multi-hop QA. Inspired by criminal procedural law's Chain of
Evidence (CoE), we characterize that knowledge preferred by LLMs should
maintain both relevance to the question and mutual support among knowledge
pieces. Accordingly, we propose an automated CoE discrimination approach and
explore LLMs' preferences from their effectiveness, faithfulness and
robustness, as well as CoE's usability in a naive Retrieval-Augmented
Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances
LLMs through more accurate generation, stronger answer faithfulness, better
robustness against knowledge conflict, and improved performance in a popular
RAG case.",Zhiyuan Chang
2024-12-18T08:04:57Z,http://arxiv.org/abs/2412.13582v1,EvoWiki: Evaluating LLMs on Evolving Knowledge,"Knowledge utilization is a critical aspect of LLMs, and understanding how
they adapt to evolving knowledge is essential for their effective deployment.
However, existing benchmarks are predominantly static, failing to capture the
evolving nature of LLMs and knowledge, leading to inaccuracies and
vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an
evolving dataset designed to reflect knowledge evolution by categorizing
information into stable, evolved, and uncharted states. EvoWiki is fully
auto-updatable, enabling precise evaluation of continuously changing knowledge
and newly released LLMs. Through experiments with Retrieval-Augmented
Generation (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs
adapt to evolving knowledge. Our results indicate that current models often
struggle with evolved knowledge, frequently providing outdated or incorrect
responses. Moreover, the dataset highlights a synergistic effect between RAG
and CL, demonstrating their potential to better adapt to evolving knowledge.
EvoWiki provides a robust benchmark for advancing future research on the
knowledge evolution capabilities of large language models.",Wei Tang
2024-12-18T11:00:58Z,http://arxiv.org/abs/2412.13720v1,"Federated Learning and RAG Integration: A Scalable Approach for Medical
  Large Language Models","This study analyzes the performance of domain-specific Large Language Models
(LLMs) for the medical field by integrating Retrieval-Augmented Generation
(RAG) systems within a federated learning framework. Leveraging the inherent
advantages of federated learning, such as preserving data privacy and enabling
distributed computation, this research explores the integration of RAG systems
with models trained under varying client configurations to optimize
performance. Experimental results demonstrate that the federated learning-based
models integrated with RAG systems consistently outperform their non-integrated
counterparts across all evaluation metrics. This study highlights the potential
of combining federated learning and RAG systems for developing domain-specific
LLMs in the medical field, providing a scalable and privacy-preserving solution
for enhancing text generation capabilities.",Jincheol Jung
2024-12-18T15:07:23Z,http://arxiv.org/abs/2412.13924v1,Language verY Rare for All,"In the quest to overcome language barriers, encoder-decoder models like NLLB
have expanded machine translation to rare languages, with some models (e.g.,
NLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform
well in translation, open LLMs prove highly competitive when fine-tuned for
specific tasks involving unknown corpora. We introduce LYRA (Language verY Rare
for All), a novel approach that combines open LLM fine-tuning,
retrieval-augmented generation (RAG), and transfer learning from related
high-resource languages. This study is exclusively focused on single-GPU
training to facilitate ease of adoption. Our study focuses on two-way
translation between French and Mon\'egasque, a rare language unsupported by
existing translation tools due to limited corpus availability. Our results
demonstrate LYRA's effectiveness, frequently surpassing and consistently
matching state-of-the-art encoder-decoder models in rare language translation.",Ibrahim Merad
2024-12-18T16:07:32Z,http://arxiv.org/abs/2412.13988v1,RAG for Effective Supply Chain Security Questionnaire Automation,"In an era where digital security is crucial, efficient processing of
security-related inquiries through supply chain security questionnaires is
imperative. This paper introduces a novel approach using Natural Language
Processing (NLP) and Retrieval-Augmented Generation (RAG) to automate these
responses. We developed QuestSecure, a system that interprets diverse document
formats and generates precise responses by integrating large language models
(LLMs) with an advanced retrieval system. Our experiments show that QuestSecure
significantly improves response accuracy and operational efficiency. By
employing advanced NLP techniques and tailored retrieval mechanisms, the system
consistently produces contextually relevant and semantically rich responses,
reducing cognitive load on security teams and minimizing potential errors. This
research offers promising avenues for automating complex security management
tasks, enhancing organizational security processes.",Zaynab Batool Reza
2024-12-10T21:52:35Z,http://arxiv.org/abs/2412.14191v1,"Ontology-Aware RAG for Improved Question-Answering in Cybersecurity
  Education","Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.",Chengshuai Zhao
2024-12-19T11:30:07Z,http://arxiv.org/abs/2412.14751v1,"Query pipeline optimization for cancer patient question answering
  systems","Retrieval-augmented generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by using query pipelines to retrieve relevant external
information and grounding responses in retrieved knowledge. However, query
pipeline optimization for cancer patient question-answering (CPQA) systems
requires separately optimizing multiple components with domain-specific
considerations. We propose a novel three-aspect optimization approach for the
RAG query pipeline in CPQA systems, utilizing public biomedical databases like
PubMed and PubMed Central. Our optimization includes: (1) document retrieval,
utilizing a comparative analysis of NCBI resources and introducing Hybrid
Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,
identifying optimal pairings of dense retrievers and rerankers; and (3)
semantic representation, introducing Semantic Enhanced Overlap Segmentation
(SEOS) for improved contextual understanding. On a custom-developed dataset
tailored for cancer-related inquiries, our optimized RAG approach improved the
answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and
about 3% over a naive RAG setup. This study highlights the importance of
domain-specific query optimization in realizing the full potential of RAG and
provides a robust framework for building more accurate and reliable CPQA
systems, advancing the development of RAG-based biomedical systems.",Maolin He
2024-12-19T15:44:01Z,http://arxiv.org/abs/2412.14964v1,Knowledge Injection via Prompt Distillation,"In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.",Kalle Kujanpää
2024-12-19T17:48:23Z,http://arxiv.org/abs/2412.15101v1,"Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering
  with Temporal Adaptability","Retrieve-augmented generation (RAG) frameworks have emerged as a promising
solution to multi-hop question answering(QA) tasks since it enables large
language models (LLMs) to incorporate external knowledge and mitigate their
inherent knowledge deficiencies. Despite this progress, existing RAG
frameworks, which usually follows the retrieve-then-read paradigm, often
struggle with multi-hop QA with temporal information since it has difficulty
retrieving and synthesizing accurate time-related information. To address the
challenge, this paper proposes a novel framework called review-then-refine,
which aims to enhance LLM performance in multi-hop QA scenarios with temporal
information. Our approach begins with a review phase, where decomposed
sub-queries are dynamically rewritten with temporal information, allowing for
subsequent adaptive retrieval and reasoning process. In addition, we implement
adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing
the potential for hallucinations. In the subsequent refine phase, the LLM
synthesizes the retrieved information from each sub-query along with its
internal knowledge to formulate a coherent answer. Extensive experimental
results across multiple datasets demonstrate the effectiveness of our proposed
framework, highlighting its potential to significantly improve multi-hop QA
capabilities in LLMs.",Xiangsen Chen
2024-12-19T18:57:11Z,http://arxiv.org/abs/2412.15189v1,"Face the Facts! Evaluating RAG-based Fact-checking Pipelines in
  Realistic Settings","Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under
more realistic scenarios, RAG-based methods for the generation of verdicts -
i.e., short texts discussing the veracity of a claim - evaluating them on
stylistically complex claims and heterogeneous, yet reliable, knowledge bases.
Our findings show a complex landscape, where, for example, LLM-based retrievers
outperform other retrieval techniques, though they still struggle with
heterogeneous knowledge bases; larger models excel in verdict faithfulness,
while smaller models provide better context adherence, with human evaluations
favouring zero-shot and one-shot approaches for informativeness, and fine-tuned
models for emotional alignment.",Daniel Russo
2024-12-16T12:04:22Z,http://arxiv.org/abs/2412.15258v1,DisEmbed: Transforming Disease Understanding through Embeddings,"The medical domain is vast and diverse, with many existing embedding models
focused on general healthcare applications. However, these models often
struggle to capture a deep understanding of diseases due to their broad
generalization across the entire medical field. To address this gap, I present
DisEmbed, a disease-focused embedding model. DisEmbed is trained on a synthetic
dataset specifically curated to include disease descriptions, symptoms, and
disease-related Q\&A pairs, making it uniquely suited for disease-related
tasks. For evaluation, I benchmarked DisEmbed against existing medical models
using disease-specific datasets and the triplet evaluation method. My results
demonstrate that DisEmbed outperforms other models, particularly in identifying
disease-related contexts and distinguishing between similar diseases. This
makes DisEmbed highly valuable for disease-specific use cases, including
retrieval-augmented generation (RAG) tasks, where its performance is
particularly robust.",Salman Faroz
2024-12-17T11:18:14Z,http://arxiv.org/abs/2412.15271v1,"A MapReduce Approach to Effectively Utilize Long Context Information in
  Retrieval Augmented Language Models","While holding great promise for improving and facilitating healthcare, large
language models (LLMs) struggle to produce up-to-date responses on evolving
topics due to outdated knowledge or hallucination. Retrieval-augmented
generation (RAG) is a pivotal innovation that improves the accuracy and
relevance of LLM responses by integrating LLMs with a search engine and
external sources of knowledge. However, the quality of RAG responses can be
largely impacted by the rank and density of key information in the retrieval
results, such as the ""lost-in-the-middle"" problem. In this work, we aim to
improve the robustness and reliability of the RAG workflow in the medical
domain. Specifically, we propose a map-reduce strategy, BriefContext, to combat
the ""lost-in-the-middle"" issue without modifying the model weights. We
demonstrated the advantage of the workflow with various LLM backbones and on
multiple QA datasets. This method promises to improve the safety and
reliability of LLMs deployed in healthcare domains.",Gongbo Zhang
2024-12-18T04:08:18Z,http://arxiv.org/abs/2412.15280v1,Context-DPO: Aligning Language Models for Context-Faithfulness,"Reliable responses from large language models (LLMs) require adherence to
user instructions and retrieved information. While alignment techniques help
LLMs align with human intentions and values, improving context-faithfulness
through alignment remains underexplored. To address this, we propose
$\textbf{Context-DPO}$, the first alignment method specifically designed to
enhance LLMs' context-faithfulness. We introduce $\textbf{ConFiQA}$, a
benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with
knowledge conflicts to evaluate context-faithfulness. By leveraging faithful
and stubborn responses to questions with provided context from ConFiQA, our
Context-DPO aligns LLMs through direct preference optimization. Extensive
experiments demonstrate that our Context-DPO significantly improves
context-faithfulness, achieving 35% to 280% improvements on popular open-source
models. Further analysis demonstrates that Context-DPO preserves LLMs'
generative capabilities while providing interpretable insights into context
utilization. Our code and data are released at
https://github.com/byronBBL/Context-DPO",Baolong Bi
2024-12-20T03:58:27Z,http://arxiv.org/abs/2412.15540v1,"MRAG: A Modular Retrieval Framework for Time-Sensitive Question
  Answering","Understanding temporal relations and answering time-sensitive questions is
crucial yet a challenging task for question-answering systems powered by large
language models (LLMs). Existing approaches either update the parametric
knowledge of LLMs with new facts, which is resource-intensive and often
impractical, or integrate LLMs with external knowledge retrieval (i.e.,
retrieval-augmented generation). However, off-the-shelf retrievers often
struggle to identify relevant documents that require intensive temporal
reasoning. To systematically study time-sensitive question answering, we
introduce the TempRAGEval benchmark, which repurposes existing datasets by
incorporating temporal perturbations and gold evidence labels. As anticipated,
all existing retrieval methods struggle with these temporal reasoning-intensive
questions. We further propose Modular Retrieval (MRAG), a trainless framework
that includes three modules: (1) Question Processing that decomposes question
into a main content and a temporal constraint; (2) Retrieval and Summarization
that retrieves evidence and uses LLMs to summarize according to the main
content; (3) Semantic-Temporal Hybrid Ranking that scores each evidence
summarization based on both semantic and temporal relevance. On TempRAGEval,
MRAG significantly outperforms baseline retrievers in retrieval performance,
leading to further improvements in final answer accuracy.",Zhang Siyue
2024-12-20T06:58:32Z,http://arxiv.org/abs/2412.15605v1,"Don't Do RAG: When Cache-Augmented Generation is All You Need for
  Knowledge Tasks","Retrieval-augmented generation (RAG) has gained traction as a powerful
approach for enhancing language models by integrating external knowledge
sources. However, RAG introduces challenges such as retrieval latency,
potential errors in document selection, and increased system complexity. With
the advent of large language models (LLMs) featuring significantly extended
context windows, this paper proposes an alternative paradigm, cache-augmented
generation (CAG) that bypasses real-time retrieval. Our method involves
preloading all relevant resources, especially when the documents or knowledge
for retrieval are of a limited and manageable size, into the LLM's extended
context and caching its runtime parameters. During inference, the model
utilizes these preloaded parameters to answer queries without additional
retrieval steps. Comparative analyses reveal that CAG eliminates retrieval
latency and minimizes retrieval errors while maintaining context relevance.
Performance evaluations across multiple benchmarks highlight scenarios where
long-context LLMs either outperform or complement traditional RAG pipelines.
These findings suggest that, for certain applications, particularly those with
a constrained knowledge base, CAG provide a streamlined and efficient
alternative to RAG, achieving comparable or superior results with reduced
complexity.",Brian J Chan
2024-12-20T13:54:57Z,http://arxiv.org/abs/2412.15902v1,"On the Suitability of pre-trained foundational LLMs for Analysis in
  German Legal Education","We show that current open-source foundational LLMs possess instruction
capability and German legal background knowledge that is sufficient for some
legal analysis in an educational context. However, model capability breaks down
in very specific tasks, such as the classification of ""Gutachtenstil"" appraisal
style components, or with complex contexts, such as complete legal opinions.
Even with extended context and effective prompting strategies, they cannot
match the Bag-of-Words baseline. To combat this, we introduce a Retrieval
Augmented Generation based prompt example selection method that substantially
improves predictions in high data availability scenarios. We further evaluate
the performance of pre-trained LLMs on two standard tasks for argument mining
and automated essay scoring and find it to be more adequate. Throughout,
pre-trained LLMs improve upon the baseline in scenarios with little or no
labeled data with Chain-of-Thought prompting further helping in the zero-shot
case.",Lorenz Wendlinger
2024-12-20T17:33:50Z,http://arxiv.org/abs/2412.16086v1,"Towards Interpretable Radiology Report Generation via Concept
  Bottlenecks using a Multi-Agentic RAG","Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",Hasan Md Tusfiqur Alam
2024-12-09T17:22:40Z,http://arxiv.org/abs/2412.16176v1,"Efficient VoIP Communications through LLM-based Real-Time Speech
  Reconstruction and Call Prioritization for Emergency Services","Emergency communication systems face disruptions due to packet loss,
bandwidth constraints, poor signal quality, delays, and jitter in VoIP systems,
leading to degraded real-time service quality. Victims in distress often
struggle to convey critical information due to panic, speech disorders, and
background noise, further complicating dispatchers' ability to assess
situations accurately. Staffing shortages in emergency centers exacerbate
delays in coordination and assistance. This paper proposes leveraging Large
Language Models (LLMs) to address these challenges by reconstructing incomplete
speech, filling contextual gaps, and prioritizing calls based on severity. The
system integrates real-time transcription with Retrieval-Augmented Generation
(RAG) to generate contextual responses, using Twilio and AssemblyAI APIs for
seamless implementation. Evaluation shows high precision, favorable BLEU and
ROUGE scores, and alignment with real-world needs, demonstrating the model's
potential to optimize emergency response workflows and prioritize critical
cases effectively.",Danush Venkateshperumal
2024-12-21T00:34:52Z,http://arxiv.org/abs/2412.16412v1,"InfoTech Assistant : A Multimodal Conversational Agent for
  InfoTechnology Web Portal Queries","This pilot study presents the development of the InfoTech Assistant, a
domain-specific, multimodal chatbot engineered to address queries in bridge
evaluation and infrastructure technology. By integrating web data scraping,
large language models (LLMs), and Retrieval-Augmented Generation (RAG), the
InfoTech Assistant provides accurate and contextually relevant responses. Data,
including textual descriptions and images, are sourced from publicly available
documents on the InfoTechnology website and organized in JSON format to
facilitate efficient querying. The architecture of the system includes an
HTML-based interface and a Flask back end connected to the Llama 3.1 model via
LLM Studio. Evaluation results show approximately 95 percent accuracy on
domain-specific tasks, with high similarity scores confirming the quality of
response matching. This RAG-enhanced setup enables the InfoTech Assistant to
handle complex, multimodal queries, offering both textual and visual
information in its responses. The InfoTech Assistant demonstrates strong
potential as a dependable tool for infrastructure professionals, delivering
high accuracy and relevance in its domain-specific outputs.",Sai Surya Gadiraju
2024-12-22T14:17:12Z,http://arxiv.org/abs/2412.17032v1,"MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on
  New and Tail Knowledge","Large language models (LLMs) have demonstrated impressive capabilities in
various reasoning tasks but face significant challenges with complex,
knowledge-intensive multi-hop queries, particularly those involving new or
long-tail knowledge. Existing benchmarks often fail to fully address these
challenges. To bridge this gap, we introduce MINTQA (Multi-hop Question
Answering on New and Tail Knowledge), a comprehensive benchmark to evaluate
LLMs' capabilities in multi-hop reasoning across four critical dimensions:
question handling strategy, sub-question generation, retrieval-augmented
generation, and iterative or dynamic decomposition and retrieval. MINTQA
comprises 10,479 question-answer pairs for evaluating new knowledge and 17,887
pairs for assessing long-tail knowledge, with each question equipped with
corresponding sub-questions and answers. Our systematic evaluation of 22
state-of-the-art LLMs on MINTQA reveals significant limitations in their
ability to handle complex knowledge base queries, particularly in handling new
or unpopular knowledge. Our findings highlight critical challenges and offer
insights for advancing multi-hop reasoning capabilities. The MINTQA benchmark
is available at https://github.com/probe2/multi-hop/.",Jie He
2024-12-23T11:24:04Z,http://arxiv.org/abs/2412.17483v1,"A Silver Bullet or a Compromise for Full Attention? A Comprehensive
  Study of Gist Token-based Context Compression","In this work, we provide a thorough investigation of gist-based context
compression methods to improve long-context processing in large language
models. We focus on two key questions: (1) How well can these methods replace
full attention models? and (2) What potential failure patterns arise due to
compression? Through extensive experiments, we show that while gist-based
compression can achieve near-lossless performance on tasks like
retrieval-augmented generation and long-document QA, it faces challenges in
tasks like synthetic recall. Furthermore, we identify three key failure
patterns: lost by the boundary, lost if surprise, and lost along the way. To
mitigate these issues, we propose two effective strategies: fine-grained
autoencoding, which enhances the reconstruction of original token information,
and segment-wise token importance estimation, which adjusts optimization based
on token dependencies. Our work provides valuable insights into the
understanding of gist token-based context compression and offers practical
strategies for improving compression capabilities.",Chenlong Deng
2024-12-23T13:26:04Z,http://arxiv.org/abs/2412.17558v1,A Survey of Query Optimization in Large Language Models,"\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the
efficiency and quality of Large Language Models (LLMs) in understanding and
answering queries, especially complex ones in scenarios like
Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the
limitations of LLMs by dynamically retrieving and leveraging up-to-date
relevant information, which provides a cost-effective solution to the challenge
of LLMs producing plausible but potentially inaccurate responses. Recently, as
RAG evolves and incorporates multiple components that influence its
performance, QO has emerged as a critical element, playing a pivotal role in
determining the effectiveness of RAG's retrieval stage in accurately sourcing
the necessary multiple pieces of evidence to answer queries correctly. In this
paper, we trace the evolution of QO techniques by summarizing and analyzing
significant studies. Through an organized framework and categorization, we aim
to consolidate existing QO techniques in RAG, elucidate their technological
foundations, and highlight their potential to enhance the versatility and
applications of LLMs.",Mingyang Song
2024-12-23T16:16:30Z,http://arxiv.org/abs/2412.17690v2,"RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF
  for Conversational QA over KGs with RAG","Conversational question answering (ConvQA) is a convenient means of searching
over RDF knowledge graphs (KGs), where a prevalent approach is to translate
natural language questions to SPARQL queries. However, SPARQL has certain
shortcomings: (i) it is brittle for complex intents and conversational
questions, and (ii) it is not suitable for more abstract needs. Instead, we
propose a novel two-pronged system where we fuse: (i) SQL-query results over a
database automatically derived from the KG, and (ii) text-search results over
verbalizations of KG facts. Our pipeline supports iterative retrieval: when the
results of any branch are found to be unsatisfactory, the system can
automatically opt for further rounds. We put everything together in a retrieval
augmented generation (RAG) setup, where an LLM generates a coherent response
from accumulated search results. We demonstrate the superiority of our proposed
system over several baselines on a knowledge graph of BMW automobiles.",Rishiraj Saha Roy
2024-12-23T19:54:28Z,http://arxiv.org/abs/2412.17942v1,"Contrato360 2.0: A Document and Database-Driven Question-Answer System
  using Large Language Models and Agents","We present a question-and-answer (Q\&A) application designed to support the
contract management process by leveraging combined information from contract
documents (PDFs) and data retrieved from contract management systems
(database). This data is processed by a large language model (LLM) to provide
precise and relevant answers. The accuracy of these responses is further
enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL
techniques, and agents that dynamically orchestrate the workflow. These
techniques eliminate the need to retrain the language model. Additionally, we
employed Prompt Engineering to fine-tune the focus of responses. Our findings
demonstrate that this multi-agent orchestration and combination of techniques
significantly improve the relevance and accuracy of the answers, offering a
promising direction for future information systems.",Antony Seabra
2024-12-23T20:28:20Z,http://arxiv.org/abs/2412.17964v1,"Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source
  Question-Answer Systems using Large Language Models","We propose a methodology that combines several advanced techniques in Large
Language Model (LLM) retrieval to support the development of robust,
multi-source question-answer systems. This methodology is designed to integrate
information from diverse data sources, including unstructured documents (PDFs)
and structured databases, through a coordinated multi-agent orchestration and
dynamic retrieval approach. Our methodology leverages specialized agents-such
as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents -
that dynamically select the most appropriate retrieval strategy based on the
nature of each query. To further improve accuracy and contextual relevance, we
employ dynamic prompt engineering, which adapts in real time to query-specific
contexts. The methodology's effectiveness is demonstrated within the domain of
Contract Management, where complex queries often require seamless interaction
between unstructured and structured data. Our results indicate that this
approach enhances response accuracy and relevance, offering a versatile and
scalable framework for developing question-answer systems that can operate
across various domains and data sources.",Antony Seabra
2024-12-24T00:55:59Z,http://arxiv.org/abs/2412.18069v1,Improving Factuality with Explicit Working Memory,"Large language models can generate factually inaccurate content, a problem
known as hallucination. Recent works have built upon retrieved-augmented
generation to improve factuality through iterative prompting but these methods
are limited by the traditional RAG design. To address these challenges, we
introduce EWE (Explicit Working Memory), a novel approach that enhances
factuality in long-form text generation by integrating a working memory that
receives real-time feedback from external resources. The memory is refreshed
based on online fact-checking and retrieval feedback, allowing EWE to rectify
false claims during the generation process and ensure more accurate and
reliable outputs. Our experiments demonstrate that Ewe outperforms strong
baselines on four fact-seeking long-form generation datasets, increasing the
factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing
the helpfulness of the responses. Further analysis reveals that the design of
rules for memory updates, configurations of memory units, and the quality of
the retrieval datastore are crucial factors for influencing model performance.",Mingda Chen
2024-12-24T02:08:38Z,http://arxiv.org/abs/2412.18093v1,"Molly: Making Large Language Model Agents Solve Python Problem More
  Logically","Applying large language models (LLMs) as teaching assists has attracted much
attention as an integral part of intelligent education, particularly in
computing courses. To reduce the gap between the LLMs and the computer
programming education expert, fine-tuning and retrieval augmented generation
(RAG) are the two mainstream methods in existing researches. However,
fine-tuning for specific tasks is resource-intensive and may diminish the
model`s generalization capabilities. RAG can perform well on reducing the
illusion of LLMs, but the generation of irrelevant factual content during
reasoning can cause significant confusion for learners. To address these
problems, we introduce the Molly agent, focusing on solving the proposed
problem encountered by learners when learning Python programming language. Our
agent automatically parse the learners' questioning intent through a
scenario-based interaction, enabling precise retrieval of relevant documents
from the constructed knowledge base. At generation stage, the agent reflect on
the generated responses to ensure that they not only align with factual content
but also effectively answer the user's queries. Extensive experimentation on a
constructed Chinese Python QA dataset shows the effectiveness of the Molly
agent, indicating an enhancement in its performance for providing useful
responses to Python questions.",Rui Xiao
2024-12-24T09:03:57Z,http://arxiv.org/abs/2412.18295v1,Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases,"The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in
several real-world services triggers severe concerns about their security. A
RAG system improves the generative capabilities of a Large Language Models
(LLM) by a retrieval mechanism which operates on a private knowledge base,
whose unintended exposure could lead to severe consequences, including breaches
of private and sensitive information. This paper presents a black-box attack to
force a RAG system to leak its private knowledge base which, differently from
existing approaches, is adaptive and automatic. A relevance-based mechanism and
an attacker-side open-source LLM favor the generation of effective queries to
leak most of the (hidden) knowledge base. Extensive experimentation proves the
quality of the proposed algorithm in different RAG pipelines and domains,
comparing to very recent related approaches, which turn out to be either not
fully black-box, not adaptive, or not based on open-source models. The findings
from our study remark the urgent need for more robust privacy safeguards in the
design and deployment of RAG systems.",Christian Di Maio
2020-06-09T17:09:29Z,http://arxiv.org/abs/2006.05405v5,Retrieval-Augmented Generation for Code Summarization via Hybrid GNN,"Source code summarization aims to generate natural language summaries from
structured code snippets for better understanding code functionalities.
However, automatic code summarization is challenging due to the complexity of
the source code and the language gap between the source code and natural
language summaries. Most previous approaches either rely on retrieval-based
(which can take advantage of similar examples seen from the retrieval database,
but have low generalization performance) or generation-based methods (which
have better generalization performance, but cannot take advantage of similar
examples). This paper proposes a novel retrieval-augmented mechanism to combine
the benefits of both worlds. Furthermore, to mitigate the limitation of Graph
Neural Networks (GNNs) on capturing global graph structure information of
source code, we propose a novel attention-based dynamic graph to complement the
static graph representation of the source code, and design a hybrid message
passing GNN for capturing both the local and global structural information. To
evaluate the proposed approach, we release a new challenging benchmark, crawled
from diversified large-scale open-source C projects (total 95k+ unique
functions in the dataset). Our method achieves the state-of-the-art
performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of
BLEU-4, ROUGE-L and METEOR.",Shangqing Liu
2021-04-17T18:24:51Z,http://arxiv.org/abs/2104.08610v1,Zero-shot Slot Filling with DPR and RAG,"The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.",Michael Glass
2022-10-06T13:58:03Z,http://arxiv.org/abs/2210.02928v2,"MuRAG: Multimodal Retrieval-Augmented Generator for Open Question
  Answering over Images and Text","While language Models store a massive amount of world knowledge implicitly in
their parameters, even very large models often fail to encode information about
rare entities and events, while incurring huge computational costs. Recently,
retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated
world knowledge into language generation by leveraging an external
non-parametric index and have demonstrated impressive performance with
constrained model sizes. However, these methods are restricted to retrieving
only textual knowledge, neglecting the ubiquitous amount of knowledge in other
modalities like images -- much of which contains information not covered by any
text. To address this limitation, we propose the first Multimodal
Retrieval-Augmented Transformer (MuRAG), which accesses an external
non-parametric multimodal memory to augment language generation. MuRAG is
pre-trained with a mixture of large-scale image-text and text-only corpora
using a joint contrastive and generative loss. We perform experiments on two
different datasets that require retrieving and reasoning over both images and
text to answer a given query: WebQA, and MultimodalQA. Our results show that
MuRAG achieves state-of-the-art accuracy, outperforming existing models by
10-20\% absolute on both datasets and under both distractor and full-wiki
settings.",Wenhu Chen
2022-10-23T16:34:39Z,http://arxiv.org/abs/2210.12777v4,"Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful
  Clinical Medicine","Language models (LMs), including large language models (such as ChatGPT),
have the potential to assist clinicians in generating various clinical notes.
However, LMs are prone to produce ``hallucinations'', i.e., generated content
that is not aligned with facts and knowledge. In this paper, we propose the
Re$^3$Writer method with retrieval-augmented generation and knowledge-grounded
reasoning to enable LMs to generate faithful clinical texts. We demonstrate the
effectiveness of our method in generating patient discharge instructions. It
requires the LMs not to only understand the patients' long clinical documents,
i.e., the health records during hospitalization, but also to generate critical
instructional information provided both to carers and to the patient at the
time of discharge. The proposed Re$^3$Writer imitates the working patterns of
physicians to first \textbf{re}trieve related working experience from
historical instructions written by physicians, then \textbf{re}ason related
medical knowledge. Finally, it \textbf{re}fines the retrieved working
experience and reasoned medical knowledge to extract useful information, which
is used to generate the discharge instructions for previously-unseen patients.
Our experiments show that, using our method, the performance of five
representative LMs can be substantially boosted across all metrics. Meanwhile,
we show results from human evaluations to measure the effectiveness in terms of
fluency, faithfulness, and comprehensiveness.",Fenglin Liu
2023-01-06T06:47:21Z,http://arxiv.org/abs/2301.02401v1,"You Truly Understand What I Need: Intellectual and Friendly Dialogue
  Agents grounding Knowledge and Persona","To build a conversational agent that interacts fluently with humans, previous
studies blend knowledge or personal profile into the pre-trained language
model. However, the model that considers knowledge and persona at the same time
is still limited, leading to hallucination and a passive way of using personas.
We propose an effective dialogue agent that grounds external knowledge and
persona simultaneously. The agent selects the proper knowledge and persona to
use for generating the answers with our candidate scoring implemented with a
poly-encoder. Then, our model generates the utterance with lesser hallucination
and more engagingness utilizing retrieval augmented generation with
knowledge-persona enhanced query. We conduct experiments on the
persona-knowledge chat and achieve state-of-the-art performance in grounding
and generation tasks on the automatic metrics. Moreover, we validate the
answers from the models regarding hallucination and engagingness through human
evaluation and qualitative results. We show our retriever's effectiveness in
extracting relevant documents compared to the other previous retrievers, along
with the comparison of multiple candidate scoring methods. Code is available at
https://github.com/dlawjddn803/INFO",Jungwoo Lim
2023-08-09T02:02:46Z,http://arxiv.org/abs/2308.04662v3,"VulLibGen: Generating Names of Vulnerability-Affected Packages via a
  Large Language Model","Security practitioners maintain vulnerability reports (e.g., GitHub Advisory)
to help developers mitigate security risks. An important task for these
databases is automatically extracting structured information mentioned in the
report, e.g., the affected software packages, to accelerate the defense of the
vulnerability ecosystem.
  However, it is challenging for existing work on affected package
identification to achieve a high accuracy. One reason is that all existing work
focuses on relatively smaller models, thus they cannot harness the knowledge
and semantic capabilities of large language models.
  To address this limitation, we propose VulLibGen, the first method to use LLM
for affected package identification. In contrast to existing work, VulLibGen
proposes the novel idea to directly generate the affected package. To improve
the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval
augmented generation (RAG) and a local search algorithm. The local search
algorithm is a novel postprocessing algorithm we introduce for reducing the
hallucination of the generated packages. Our evaluation results show that
VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages
in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go)
while the best average accuracy in previous work is 0.721. Additionally,
VulLibGen has high value to security practice: we submitted 60 <vulnerability,
affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them
have been accepted and merged and 20 are pending approval. Our code and dataset
can be found in the attachments.",Tianyu Chen
2023-09-03T07:03:17Z,http://arxiv.org/abs/2309.01105v2,"A Study on the Implementation of Generative AI Services Using an
  Enterprise Data-Based LLM Application Architecture","This study presents a method for implementing generative AI services by
utilizing the Large Language Models (LLM) application architecture. With recent
advancements in generative AI technology, LLMs have gained prominence across
various domains. In this context, the research addresses the challenge of
information scarcity and proposes specific remedies by harnessing LLM
capabilities. The investigation delves into strategies for mitigating the issue
of inadequate data, offering tailored solutions. The study delves into the
efficacy of employing fine-tuning techniques and direct document integration to
alleviate data insufficiency. A significant contribution of this work is the
development of a Retrieval-Augmented Generation (RAG) model, which tackles the
aforementioned challenges. The RAG model is carefully designed to enhance
information storage and retrieval processes, ensuring improved content
generation. The research elucidates the key phases of the information storage
and retrieval methodology underpinned by the RAG model. A comprehensive
analysis of these steps is undertaken, emphasizing their significance in
addressing the scarcity of data. The study highlights the efficacy of the
proposed method, showcasing its applicability through illustrative instances.
By implementing the RAG model for information storage and retrieval, the
research not only contributes to a deeper comprehension of generative AI
technology but also facilitates its practical usability within enterprises
utilizing LLMs. This work holds substantial value in advancing the field of
generative AI, offering insights into enhancing data-driven content generation
and fostering active utilization of LLM-based services within corporate
settings.",Cheonsu Jeong
2023-09-15T22:12:44Z,http://arxiv.org/abs/2309.08788v2,"BioinspiredLLM: Conversational Large Language Model for the Mechanics of
  Biological and Bio-inspired Materials","The study of biological materials and bio-inspired materials science is well
established; however, surprisingly little knowledge has been systematically
translated to engineering solutions. To accelerate discovery and guide
insights, an open-source autoregressive transformer large language model (LLM),
BioinspiredLLM, is reported. The model was finetuned with a corpus of over a
thousand peer-reviewed articles in the field of structural biological and
bio-inspired materials and can be prompted to recall information, assist with
research tasks, and function as an engine for creativity. The model has proven
that it is able to accurately recall information about biological materials and
is further enhanced with enhanced reasoning ability, as well as with
retrieval-augmented generation to incorporate new data during generation that
can also help to traceback sources, update the knowledge base, and connect
knowledge domains. BioinspiredLLM also has been shown to develop sound
hypotheses regarding biological materials design and remarkably so for
materials that have never been explicitly studied before. Lastly, the model
showed impressive promise in collaborating with other generative artificial
intelligence models in a workflow that can reshape the traditional materials
design process. This collaborative generative artificial intelligence method
can stimulate and enhance bio-inspired materials design workflows. Biological
materials are at a critical intersection of multiple scientific fields and
models like BioinspiredLLM help to connect knowledge domains.",Rachel K. Luu
2023-09-29T17:26:03Z,http://arxiv.org/abs/2309.17415v3,"Intuitive or Dependent? Investigating LLMs' Behavior Style to
  Conflicting Prompts","This study investigates the behaviors of Large Language Models (LLMs) when
faced with conflicting prompts versus their internal memory. This will not only
help to understand LLMs' decision mechanism but also benefit real-world
applications, such as retrieval-augmented generation (RAG). Drawing on
cognitive theory, we target the first scenario of decision-making styles where
there is no superiority in the conflict and categorize LLMs' preference into
dependent, intuitive, and rational/irrational styles. Another scenario of
factual robustness considers the correctness of prompt and memory in
knowledge-intensive tasks, which can also distinguish if LLMs behave rationally
or irrationally in the first scenario. To quantify them, we establish a
complete benchmarking framework including a dataset, a robustness evaluation
pipeline, and corresponding metrics. Extensive experiments with seven LLMs
reveal their varying behaviors. And, with role play intervention, we can change
the styles, but different models present distinct adaptivity and upper-bound.
One of our key takeaways is to optimize models or the prompts according to the
identified style. For instance, RAG models with high role play adaptability may
dynamically adjust the interventions according to the quality of retrieval
results -- being dependent to better leverage informative context; and, being
intuitive when external prompt is noisy.",Jiahao Ying
2023-09-28T15:32:36Z,http://arxiv.org/abs/2310.01429v1,Chatmap : Large Language Model Interaction with Cartographic Data,"The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.",Eren Unlu
2023-10-08T01:43:39Z,http://arxiv.org/abs/2310.04963v3,LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,"Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. The goal of this work is to automatically generate tests
and use these tests to validate and verify compiler implementations of a
directive-based parallel programming paradigm, OpenACC. To do so, in this
paper, we explore the capabilities of state-of-the-art LLMs, including
open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama,
Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and
GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using
our own testsuite dataset along with using the OpenACC specification. We also
explored these LLMs using various prompt engineering techniques that include
code template, template with retrieval-augmented generation (RAG), one-shot
example, one-shot with RAG, expressive prompt with code template and RAG. This
paper highlights our findings from over 5000 tests generated via all the above
mentioned methods. Our contributions include: (a) exploring the capabilities of
the latest and relevant LLMs for code generation, (b) investigating fine-tuning
and prompt methods, and (c) analyzing the outcome of LLMs generated tests
including manually analysis of representative set of tests. We found the LLM
Deepseek-Coder-33b-Instruct produced the most passing tests followed by
GPT-4-Turbo.",Christian Munley
2023-10-09T11:34:41Z,http://arxiv.org/abs/2310.05628v3,"Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models","Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Publicly released information on sustainability
practices is often disclosed in diverse, unstructured, and multi-modal
documentation. This poses a challenge in efficiently gathering and aligning the
data into a unified framework to derive insights related to Corporate Social
Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes
an intuitive choice for delivering insightful and actionable data to
stakeholders. In this study, we employ Large Language Models (LLMs), In-Context
Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract
structured insights related to ESG aspects from companies' sustainability
reports. We then leverage graph-based representations to conduct statistical
analyses concerning the extracted insights. These analyses revealed that ESG
criteria cover a wide range of topics, exceeding 500, often beyond those
considered in existing categorizations, and are addressed by companies through
a variety of initiatives. Moreover, disclosure similarities emerged among
companies from the same region or sector, validating ongoing hypotheses in the
ESG literature. Lastly, by incorporating additional company attributes into our
analyses, we investigated which factors impact the most on companies' ESG
ratings, showing that ESG disclosure affects the obtained ratings more than
other financial or company data.",Marco Bronzini
2023-10-10T00:39:04Z,http://arxiv.org/abs/2310.06225v2,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",Bruno Silva
2023-10-11T18:27:12Z,http://arxiv.org/abs/2310.07793v5,"GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large
  Language Models","The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.",Ruotong Liao
2023-10-13T13:17:03Z,http://arxiv.org/abs/2310.09089v2,"Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large
  Language Model","Integrating large language models (LLMs) into healthcare holds great
potential but faces challenges. Pre-training LLMs from scratch for domains like
medicine is resource-heavy and often unfeasible. On the other hand, sole
reliance on Supervised Fine-tuning (SFT) can result in overconfident
predictions and may not tap into domain-specific insights. In response, we
present a multi-stage training method combining Domain-specific Continued
Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In
addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing
medical question answering, plain texts, knowledge graphs, and dialogues,
segmented into three training stages. The medical LLM trained with our
pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and
SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,
respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by
7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the
Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in
BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the
model's performance through the Retrieval Augmented Generation (RAG) approach.
Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%
on CMExam. These results highlight the contribution of our novel training
approach in building LLMs for medical applications.",Qichen Ye
2023-10-14T08:46:24Z,http://arxiv.org/abs/2310.09536v1,"CarExpert: Leveraging Large Language Models for In-Car Conversational
  Question Answering","Large language models (LLMs) have demonstrated remarkable performance by
following natural language instructions without fine-tuning them on
domain-specific tasks and data. However, leveraging LLMs for domain-specific
question answering suffers from severe limitations. The generated answer tends
to hallucinate due to the training data collection time (when using
off-the-shelf), complex user utterance and wrong retrieval (in
retrieval-augmented generation). Furthermore, due to the lack of awareness
about the domain and expected output, such LLMs may generate unexpected and
unsafe answers that are not tailored to the target domain. In this paper, we
propose CarExpert, an in-car retrieval-augmented conversational
question-answering system leveraging LLMs for different tasks. Specifically,
CarExpert employs LLMs to control the input, provide domain-specific documents
to the extractive and generative answering components, and controls the output
to ensure safe and domain-specific answers. A comprehensive empirical
evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in
generating natural, safe and car-specific answers.",Md Rashad Al Hasan Rony
2023-10-16T14:29:35Z,http://arxiv.org/abs/2310.10445v1,"MechGPT, a language-based strategy for mechanics and materials modeling
  that connects knowledge across scales, disciplines and modalities","For centuries, researchers have sought out ways to connect disparate areas of
knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across
fields, specialization has taken hold later. With the advent of Artificial
Intelligence, we can now explore relationships across areas (e.g.,
mechanics-biology) or disparate domains (e.g., failure mechanics-art). To
achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset
of knowledge in multiscale materials failure. The approach includes the use of
a general-purpose LLM to distill question-answer pairs from raw sources
followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used
in a series of computational experiments to explore its capacity for knowledge
retrieval, various language tasks, hypothesis generation, and connecting
knowledge across disparate areas. While the model has some ability to recall
knowledge from training, we find that LLMs are particularly useful to extract
structural insights through Ontological Knowledge Graphs. These interpretable
graph structures provide explanatory insights, frameworks for new research
questions, and visual representations of knowledge that also can be used in
retrieval-augmented generation. Three versions of MechGPT are discussed,
featuring different sizes from 13 billion to 70 billion parameters, and
reaching context lengths of more than 10,000 tokens. This provides ample
capacity for sophisticated retrieval augmented strategies, as well as
agent-based modeling where multiple LLMs interact collaboratively and/or
adversarially, the incorporation of new data from the literature or web
searches, as well as multimodality.",Markus J. Buehler
2023-10-20T22:47:18Z,http://arxiv.org/abs/2310.13848v2,"FABULA: Intelligence Report Generation Using Retrieval-Augmented
  Narrative Construction","Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.",Priyanka Ranade
2023-10-31T03:52:08Z,http://arxiv.org/abs/2310.20158v1,GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval,"Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.",Daman Arora
2023-11-05T08:34:26Z,http://arxiv.org/abs/2311.02597v1,"FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented
  Generation with an LLM","Fast disaster impact reporting is crucial in planning humanitarian
assistance. Large Language Models (LLMs) are well known for their ability to
write coherent text and fulfill a variety of tasks relevant to impact
reporting, such as question answering or text summarization. However, LLMs are
constrained by the knowledge within their training data and are prone to
generating inaccurate, or ""hallucinated"", information. To address this, we
introduce a sophisticated pipeline embodied in our tool FloodBrain
(floodbrain.com), specialized in generating flood disaster impact reports by
extracting and curating information from the web. Our pipeline assimilates
information from web search results to produce detailed and accurate reports on
flood events. We test different LLMs as backbones in our tool and compare their
generated reports to human-written reports on different metrics. Similar to
other studies, we find a notable correlation between the scores assigned by
GPT-4 and the scores given by human evaluators when comparing our generated
reports to human-authored ones. Additionally, we conduct an ablation study to
test our single pipeline components and their relevancy for the final reports.
With our tool, we aim to advance the use of LLMs for disaster impact reporting
and reduce the time for coordination of humanitarian efforts in the wake of
flood disasters.",Grace Colverd
2023-11-05T21:43:02Z,http://arxiv.org/abs/2311.02775v3,"AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using
  Open-Source LLMs","Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of AI-TA, an
intelligent QA assistant customizable for courses with an online QA platform",Yann Hicke
2023-11-13T18:22:32Z,http://arxiv.org/abs/2311.07536v3,"A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual
  Question Answering","The emergence of multimodal large models (MLMs) has significantly advanced
the field of visual understanding, offering remarkable capabilities in the
realm of visual question answering (VQA). Yet, the true challenge lies in the
domain of knowledge-intensive VQA tasks, which necessitate not just recognition
of visual elements, but also a deep comprehension of the visual information in
conjunction with a vast repository of learned knowledge. To uncover such
capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we
provide an in-depth evaluation from three perspectives: 1) Commonsense
Knowledge, which assesses how well models can understand visual cues and
connect to general knowledge; 2) Fine-grained World Knowledge, which tests the
model's skill in reasoning out specific knowledge from images, showcasing their
proficiency across various specialized fields; 3) Comprehensive Knowledge with
Decision-making Rationales, which examines model's capability to provide
logical explanations for its inference, facilitating a deeper analysis from the
interpretability perspective. Additionally, we utilize a visual
knowledge-enhanced training strategy and multimodal retrieval-augmented
generation approach to enhance MLMs, highlighting the future need for
advancements in this research direction. Extensive experiments indicate that:
a) GPT-4V demonstrates enhanced explanation generation when using composite
images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations
when dealing with world knowledge; c) Visual knowledge enhanced training and
prompting technicals present potential to improve performance. Codes:
https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper",Yunxin Li
2023-11-16T01:21:33Z,http://arxiv.org/abs/2311.10776v5,"Chemist-X: Large Language Model-empowered Agent for Reaction Condition
  Recommendation in Chemical Synthesis","Recent AI research plots a promising future of automatic chemical reactions
within the chemistry society. This study proposes Chemist-X, a transformative
AI agent that automates the reaction condition recommendation (RCR) task in
chemical synthesis with retrieval-augmented generation (RAG) technology. To
emulate expert chemists' strategies when solving RCR tasks, Chemist-X utilizes
advanced RAG schemes to interrogate online molecular databases and distill
critical data from the latest literature database. Further, the agent leverages
state-of-the-art computer-aided design (CAD) tools with a large language model
(LLM) supervised programming interface. With the ability to utilize updated
chemical knowledge and CAD tools, our agent significantly outperforms
conventional synthesis AIs confined to the fixed knowledge within its training
data. Chemist-X considerably reduces chemists' workload and allows them to
focus on more fundamental and creative problems, thereby bringing closer
computational techniques and chemical research and making a remarkable leap
toward harnessing AI's full capabilities in scientific discovery.",Kexin Chen
2023-11-22T17:24:21Z,http://arxiv.org/abs/2311.13538v5,"AlignedCoT: Prompting Large Language Models via Native-Speaking
  Demonstrations","Large Language Models prompting, such as using in-context demonstrations, is
a mainstream technique for invoking LLMs to perform high-performance and solid
complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and
has the potential for further human-machine collaborative scientific findings.
However, current LLMs are delicate and elusive in prompt words and styles. And
there is an unseen gap between LLM understanding and human-written prompts.
This paper introduces Alignedcot, an LLM-acquainted prompting technique that
includes proficient ``native-speaking'' in in-context learning for the LLMs.
Specifically, it achieves consistent and correct step-wise prompts in zero-shot
scenarios by progressively probing, refining, and formatting the LLM chain of
thoughts so that free from handcrafted few-shot demonstrations while
maintaining the prompt quality. We conduct experiments on mathematical
reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform
significantly superior to them with human-crafted demonstrations. We further
apply Alignedcot for rewriting the GSM8K training set, resulting in a
GSM8K-Align dataset. We observe its benefits for retrieval augmented
generation. The code and data can be found at
https://github.com/yangzhch6/AlignedCoT.",Zhicheng Yang
2023-11-29T15:21:35Z,http://arxiv.org/abs/2311.17722v1,SenTest: Evaluating Robustness of Sentence Encoders,"Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.",Tanmay Chavan
2023-11-30T09:48:51Z,http://arxiv.org/abs/2311.18397v1,"IAG: Induction-Augmented Generation Framework for Answering Reasoning
  Questions","Retrieval-Augmented Generation (RAG), by incorporating external knowledge
with parametric memory of language models, has become the state-of-the-art
architecture for open-domain QA tasks. However, common knowledge bases are
inherently constrained by limited coverage and noisy information, making
retrieval-based approaches inadequate to answer implicit reasoning questions.
In this paper, we propose an Induction-Augmented Generation (IAG) framework
that utilizes inductive knowledge along with the retrieved documents for
implicit reasoning. We leverage large language models (LLMs) for deriving such
knowledge via a novel prompting method based on inductive reasoning patterns.
On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,
respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for
answer prediction, while IAG-Student gets rid of dependencies on GPT service at
inference time by incorporating a student inductor model. The inductor is
firstly trained via knowledge distillation and further optimized by
back-propagating the generator feedback via differentiable beam scores.
Experimental results show that IAG outperforms RAG baselines as well as ChatGPT
on two Open-Domain QA tasks. Notably, our best models have won the first place
in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA
(since Jan 8, 2023).",Zhebin Zhang
2023-12-04T17:35:42Z,http://arxiv.org/abs/2312.02073v3,"A Glitch in the Matrix? Locating and Detecting Language Model Grounding
  with Fakepedia","Large language models (LLMs) have an impressive ability to draw on novel
information supplied in their context. Yet the mechanisms underlying this
contextual grounding remain unknown, especially in situations where contextual
information contradicts factual knowledge stored in the parameters, which LLMs
also excel at recalling. Favoring the contextual information is critical for
retrieval-augmented generation methods, which enrich the context with
up-to-date information, hoping that grounding can rectify outdated or noisy
stored knowledge. We present a novel method to study grounding abilities using
Fakepedia, a novel dataset of counterfactual texts constructed to clash with a
model's internal parametric knowledge. In this study, we introduce Fakepedia, a
counterfactual dataset designed to evaluate grounding abilities when the
internal parametric knowledge clashes with the contextual information. We
benchmark various LLMs with Fakepedia and conduct a causal mediation analysis
of LLM components when answering Fakepedia queries, based on our Masked Grouped
Causal Tracing (MGCT) method. Through this analysis, we identify distinct
computational patterns between grounded and ungrounded responses. We finally
demonstrate that distinguishing grounded from ungrounded responses is
achievable through computational analysis alone. Our results, together with
existing findings about factual recall mechanisms, provide a coherent narrative
of how grounding and factual recall mechanisms interact within LLMs.",Giovanni Monea
2023-12-05T21:21:01Z,http://arxiv.org/abs/2312.03141v2,"NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest
  Neighbor Search through Near Data Processing","Approximate nearest neighbor search (ANNS) is a key retrieval technique for
vector database and many data center applications, such as person
re-identification and recommendation systems. It is also fundamental to
retrieval augmented generation (RAG) for large language models (LLM) now. Among
all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall
rate. However, as the size of dataset increases, the graph may require hundreds
of gigabytes of memory, exceeding the main memory capacity of a single
workstation node. Although we can do partitioning and use solid-state drive
(SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades
the performance of the system. To address this challenge, we present NDSEARCH,
a hardware-software co-designed near-data processing (NDP) solution for ANNS
processing. NDSEARCH consists of a novel in-storage computing architecture,
namely, SEARSSD, that supports the ANNS kernels and leverages logic unit
(LUN)-level parallelism inside the NAND flash chips. NDSEARCH also includes a
processing model that is customized for NDP and cooperates with SEARSSD. The
processing model enables us to apply a two-level scheduling to improve the data
locality and exploit the internal bandwidth in NDSEARCH, and a speculative
searching mechanism to further accelerate the ANNS workload. Our results show
that NDSEARCH improves the throughput by up to 31.7x, 14.6x, 7.4x 2.9x over
CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively.
NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than
CPU and GPU.",Yitu Wang
2023-12-21T10:19:58Z,http://arxiv.org/abs/2312.14211v1,"Experimenting with Large Language Models and vector embeddings in NASA
  SciX","Open-source Large Language Models enable projects such as NASA SciX (i.e.,
NASA ADS) to think out of the box and try alternative approaches for
information retrieval and data augmentation, while respecting data copyright
and users' privacy. However, when large language models are directly prompted
with questions without any context, they are prone to hallucination. At NASA
SciX we have developed an experiment where we created semantic vectors for our
large collection of abstracts and full-text content, and we designed a prompt
system to ask questions using contextual chunks from our system. Based on a
non-systematic human evaluation, the experiment shows a lower degree of
hallucination and better responses when using Retrieval Augmented Generation.
Further exploration is required to design new features and data augmentation
processes at NASA SciX that leverages this technology while respecting the high
level of trust and quality that the project holds.",Sergi Blanco-Cuaresma
2023-12-21T23:42:13Z,http://arxiv.org/abs/2312.14335v2,"Context-aware Decoding Reduces Hallucination in Query-focused
  Summarization","Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",Zhichao Xu
2023-12-26T04:49:56Z,http://arxiv.org/abs/2312.15883v2,"HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and
  Reliable Medical LLMs Responses","In this paper, we investigate the retrieval-augmented generation (RAG) based
on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large
Language Models (LLMs). Recent approaches suffer from insufficient and
repetitive knowledge retrieval, tedious and time-consuming query parsing, and
monotonous knowledge utilization. To this end, we develop a Hypothesis
Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful
reasoning capacity to compensate for the incompleteness of user queries,
optimizes the interaction process with LLMs, and provides diverse retrieved
knowledge. Specifically, HyKGE explores the zero-shot capability and the rich
knowledge of LLMs with Hypothesis Outputs to extend feasible exploration
directions in the KGs, as well as the carefully curated prompt to enhance the
density and efficiency of LLMs' responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise while ensuring the
balance between diversity and relevance in retrieved knowledge. Experiments on
two Chinese medical multiple-choice question datasets and one Chinese
open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority
of HyKGE in terms of accuracy and explainability.",Xinke Jiang
2023-12-26T08:24:24Z,http://arxiv.org/abs/2312.16262v1,Dynamic In-Context Learning from Nearest Neighbors for Bundle Generation,"Product bundling has evolved into a crucial marketing strategy in e-commerce.
However, current studies are limited to generating (1) fixed-size or single
bundles, and most importantly, (2) bundles that do not reflect consistent user
intents, thus being less intelligible or useful to users. This paper explores
two interrelated tasks, i.e., personalized bundle generation and the underlying
intent inference based on users' interactions in a session, leveraging the
logical reasoning capability of large language models. We introduce a dynamic
in-context learning paradigm, which enables ChatGPT to seek tailored and
dynamic lessons from closely related sessions as demonstrations while
performing tasks in the target session. Specifically, it first harnesses
retrieval augmented generation to identify nearest neighbor sessions for each
target session. Then, proper prompts are designed to guide ChatGPT to perform
the two tasks on neighbor sessions. To enhance reliability and mitigate the
hallucination issue, we develop (1) a self-correction strategy to foster mutual
improvement in both tasks without supervision signals; and (2) an auto-feedback
mechanism to recurrently offer dynamic supervision based on the distinct
mistakes made by ChatGPT on various neighbor sessions. Thus, the target session
can receive customized and dynamic lessons for improved performance by
observing the demonstrations of its neighbor sessions. Finally, experimental
results on three real-world datasets verify the effectiveness of our methods on
both tasks. Additionally, the inferred intents can prove beneficial for other
intriguing downstream tasks, such as crafting appealing bundle names.",Zhu Sun
2023-12-25T06:44:32Z,http://arxiv.org/abs/2312.17264v1,"ESGReveal: An LLM-based approach for extracting structured data from ESG
  reports","ESGReveal is an innovative method proposed for efficiently extracting and
analyzing Environmental, Social, and Governance (ESG) data from corporate
reports, catering to the critical need for reliable ESG information retrieval.
This approach utilizes Large Language Models (LLM) enhanced with Retrieval
Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG
metadata module for targeted queries, a preprocessing module for assembling
databases, and an LLM agent for data extraction. Its efficacy was appraised
using ESG reports from 166 companies across various sectors listed on the Hong
Kong Stock Exchange in 2022, ensuring comprehensive industry and market
capitalization representation. Utilizing ESGReveal unearthed significant
insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in
data extraction and 83.7% in disclosure analysis, which is an improvement over
baseline models. This highlights the framework's capacity to refine ESG data
analysis precision. Moreover, it revealed a demand for reinforced ESG
disclosures, with environmental and social data disclosures standing at 69.5%
and 57.2%, respectively, suggesting a pursuit for more corporate transparency.
While current iterations of ESGReveal do not process pictorial information, a
functionality intended for future enhancement, the study calls for continued
research to further develop and compare the analytical capabilities of various
LLMs. In summary, ESGReveal is a stride forward in ESG data processing,
offering stakeholders a sophisticated tool to better evaluate and advance
corporate sustainability efforts. Its evolution is promising in promoting
transparency in corporate reporting and aligning with broader sustainable
development aims.",Yi Zou
2023-12-31T04:43:45Z,http://arxiv.org/abs/2401.00396v2,"RAGTruth: A Hallucination Corpus for Developing Trustworthy
  Retrieval-Augmented Language Models","Retrieval-augmented generation (RAG) has become a main technique for
alleviating hallucinations in large language models (LLMs). Despite the
integration of RAG, LLMs may still present unsupported or contradictory claims
to the retrieved contents. In order to develop effective hallucination
prevention strategies under RAG, it is important to create benchmark datasets
that can measure the extent of hallucination. This paper presents RAGTruth, a
corpus tailored for analyzing word-level hallucinations in various domains and
tasks within the standard RAG frameworks for LLM applications. RAGTruth
comprises nearly 18,000 naturally generated responses from diverse LLMs using
RAG. These responses have undergone meticulous manual annotations at both the
individual cases and word levels, incorporating evaluations of hallucination
intensity. We not only benchmark hallucination frequencies across different
LLMs, but also critically assess the effectiveness of several existing
hallucination detection methodologies. Furthermore, we show that using a
high-quality dataset such as RAGTruth, it is possible to finetune a relatively
small LLM and achieve a competitive level of performance in hallucination
detection when compared to the existing prompt-based approaches using
state-of-the-art large language models such as GPT-4.",Cheng Niu
2023-12-31T17:15:25Z,http://arxiv.org/abs/2401.00544v2,"A Reliable Knowledge Processing Framework for Combustion Science using
  Foundation Models","This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.",Vansh Sharma
2024-01-03T12:09:43Z,http://arxiv.org/abs/2401.01701v3,"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks
  via Iterative Grounding","Large language models (LLMs) trained on datasets of publicly available source
code have established a new state of the art in code generation tasks. However,
these models are mostly unaware of the code that exists within a specific
project, preventing the models from making good use of existing APIs. Instead,
LLMs often invent, or ""hallucinate"", non-existent APIs or produce variants of
already existing code. This paper presents De-Hallucinator, a technique that
grounds the predictions of an LLM through a novel combination of retrieving
suitable API references and iteratively querying the model with increasingly
suitable context information in the prompt. The approach exploits the
observation that predictions by LLMs often resemble the desired code, but they
fail to correctly refer to already existing APIs. De-Hallucinator automatically
identifies project-specific API references related to the model's initial
predictions and adds these references into the prompt. Unlike
retrieval-augmented generation (RAG), our approach uses the initial
prediction(s) by the model to iteratively retrieve increasingly suitable API
references. Our evaluation applies the approach to two tasks: predicting API
usages in Python and generating tests in JavaScript. We show that
De-Hallucinator consistently improves the generated code across five LLMs. In
particular, the approach improves the edit distance by 23.3-50.6% and the
recall of correctly predicted API usages by 23.9-61.0% for code completion, and
improves the number of fixed tests that initially failed because of
hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage
for test generation.",Aryaz Eghbali
2024-01-15T05:38:37Z,http://arxiv.org/abs/2401.07483v1,"Graph database while computationally efficient filters out quickly the
  ESG integrated equities in investment management","Design/methodology/approach This research evaluated the databases of SQL,
No-SQL and graph databases to compare and contrast efficiency and performance.
To perform this experiment the data were collected from multiple sources
including stock price and financial news. Python is used as an interface to
connect and query databases (to create database structures according to the
feed file structure, to load data into tables, objects, to read data , to
connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM
(Large language model) including RAG (Retrieval Augmented Generation) with
Machine Learning, deep learning, NLP (natural language processing) or Decision
Analytics are computationally expensive. Finding a better option to consume
less resources and time to get the result. Findings The Graph database of ESG
(Environmental, Social and Governance) is comparatively better and can be
considered for extended analytics to integrate ESG in business and investment.
Practical implications A graph ML with a RAG architecture model can be
introduced as a new framework with less computationally expensive LLM
application in the equity filtering process for portfolio management.
Originality/value Filtering out selective stocks out of two thousand or more
listed companies in any stock exchange for active investment, consuming less
resource consumption especially memory and energy to integrate artificial
intelligence and ESG in business and investment.",Partha Sen
2024-01-15T16:00:50Z,http://arxiv.org/abs/2401.07793v1,"Flexibly Scaling Large Language Models Contexts Through Extensible
  Tokenization","Large language models (LLMs) are in need of sufficient contexts to handle
many critical applications, such as retrieval augmented generation and few-shot
learning. However, due to the constrained window size, the LLMs can only access
to the information within a limited context. Although the size of context
window can be extended by fine-tuning, it will result in a substantial cost in
both training and inference stage. In this paper, we present Extensible
Tokenization as an alternative method which realizes the flexible scaling of
LLMs' context. Extensible Tokenization stands as a midware in between of the
tokenized context and the LLM, which transforms the raw token embeddings into
the extensible embeddings. Such embeddings provide a more compact
representation for the long context, on top of which the LLM is able to
perceive more information with the same context window. Extensible Tokenization
is also featured by its flexibility: the scaling factor can be flexibly
determined within a feasible scope, leading to the extension of an arbitrary
context length at the inference time. Besides, Extensible Tokenization is
introduced as a drop-in component, which can be seamlessly plugged into not
only the LLM itself and but also its fine-tuned derivatives, bringing in the
extended contextual information while fully preserving the LLM's existing
capabilities. We perform comprehensive experiments on long-context language
modeling and understanding tasks, which verify Extensible Tokenization as an
effective, efficient, flexible, and compatible method to extend LLM's context.
Our model and source code will be made publicly available.",Ninglu Shao
2024-01-15T18:25:18Z,http://arxiv.org/abs/2401.07883v1,"The Chronicles of RAG: The Retriever, the Chunk and the Generator","Retrieval Augmented Generation (RAG) has become one of the most popular
paradigms for enabling LLMs to access external data, and also as a mechanism
for grounding to mitigate against hallucinations. When implementing RAG you can
face several challenges like effective integration of retrieval models,
efficient representation learning, data diversity, computational efficiency
optimization, evaluation, and quality of text generation. Given all these
challenges, every day a new technique to improve RAG appears, making it
unfeasible to experiment with all combinations for your problem. In this
context, this paper presents good practices to implement, optimize, and
evaluate RAG for the Brazilian Portuguese language, focusing on the
establishment of a simple pipeline for inference and experiments. We explored a
diverse set of methods to answer questions about the first Harry Potter book.
To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,
gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the
retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to
the baseline. When optimizing the input size in the application, we observed
that it is possible to further enhance it by 2.4%. Finally, we present the
complete architecture of the RAG with our recommendations. As result, we moved
from a baseline of 57.88% to a maximum relative score of 98.61%.",Paulo Finardi
2024-01-17T09:53:50Z,http://arxiv.org/abs/2401.09092v1,"BibSonomy Meets ChatLLMs for Publication Management: From Chat to
  Publication Management: Organizing your related work using BibSonomy & LLMs","The ever-growing corpus of scientific literature presents significant
challenges for researchers with respect to discovery, management, and
annotation of relevant publications. Traditional platforms like Semantic
Scholar, BibSonomy, and Zotero offer tools for literature management, but
largely require manual laborious and error-prone input of tags and metadata.
Here, we introduce a novel retrieval augmented generation system that leverages
chat-based large language models (LLMs) to streamline and enhance the process
of publication management. It provides a unified chat-based interface, enabling
intuitive interactions with various backends, including Semantic Scholar,
BibSonomy, and the Zotero Webscraper. It supports two main use-cases: (1)
Explorative Search & Retrieval - leveraging LLMs to search for and retrieve
both specific and general scientific publications, while addressing the
challenges of content hallucination and data obsolescence; and (2) Cataloguing
& Management - aiding in the organization of personal publication libraries, in
this case BibSonomy, by automating the addition of metadata and tags, while
facilitating manual edits and updates. We compare our system to different LLM
models in three different settings, including a user study, and we can show its
advantages in different metrics.",Tom Völker
2024-01-18T18:59:11Z,http://arxiv.org/abs/2401.10225v5,ChatQA: Surpassing GPT-4 on Conversational QA and RAG,"In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on
retrieval-augmented generation (RAG) and conversational question answering
(QA). To enhance generation, we propose a two-stage instruction tuning method
that significantly boosts the performance of RAG. For effective retrieval, we
introduce a dense retriever optimized for conversational QA, which yields
results comparable to the alternative state-of-the-art query rewriting models,
while substantially reducing deployment costs. We also present the ChatRAG
Bench, which encompasses ten datasets covering comprehensive evaluations on
RAG, table-related QA, arithmetic calculations, and scenarios involving
unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a
weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:
53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without
relying on any synthetic data from OpenAI GPT models. Notably, the
Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,
achieving a 4.4% improvement. To advance research in this field, we
open-sourced the model weights, instruction tuning data, ChatRAG Bench, and
retriever for the community: https://chatqa-project.github.io/.",Zihan Liu
2024-01-20T03:41:23Z,http://arxiv.org/abs/2401.12998v1,"Evaluating and Enhancing Large Language Models Performance in
  Domain-specific Medicine: Osteoarthritis Management with DocOA","The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.",Xi Chen
2024-01-26T14:14:59Z,http://arxiv.org/abs/2401.14887v4,The Power of Noise: Redefining Retrieval for RAG Systems,"Retrieval-Augmented Generation (RAG) has recently emerged as a method to
extend beyond the pre-trained knowledge of Large Language Models by augmenting
the original prompt with relevant passages or documents retrieved by an
Information Retrieval (IR) system. RAG has become increasingly important for
Generative AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized in the LLM. We
argue here that the retrieval component of RAG systems, be it dense or sparse,
deserves increased attention from the research community, and accordingly, we
conduct the first comprehensive and systematic examination of the retrieval
strategy of RAG systems. We focus, in particular, on the type of passages IR
systems within a RAG solution should retrieve. Our analysis considers multiple
factors, such as the relevance of the passages included in the prompt context,
their position, and their number. One counter-intuitive finding of this work is
that the retriever's highest-scoring documents that are not directly relevant
to the query (e.g., do not contain the answer) negatively impact the
effectiveness of the LLM. Even more surprising, we discovered that adding
random documents in the prompt improves the LLM accuracy by up to 35%. These
results highlight the need to investigate the appropriate strategies when
integrating retrieval with LLMs, thereby laying the groundwork for future
research in this area.",Florin Cuconasu
2024-01-30T18:37:45Z,http://arxiv.org/abs/2401.17244v3,"LLaMP: Large Language Model Made Powerful for High-fidelity Materials
  Knowledge Retrieval and Distillation","Reducing hallucination of Large Language Models (LLMs) is imperative for use
in the sciences, where reliability and reproducibility are crucial. However,
LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and
inevitably biased task to fine-tune them on domain-specific literature and
data. Here we introduce LLaMP, a multimodal retrieval-augmented generation
(RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can
dynamically and recursively interact with computational and experimental data
on Materials Project (MP) and run atomistic simulations via high-throughput
workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage
ability to comprehend and integrate various modalities of materials science
concepts, fetch relevant data stores on the fly, process higher-order data
(such as crystal structure and elastic tensor), and streamline complex tasks in
computational materials and chemistry. We propose a simple metric combining
uncertainty and confidence estimates to evaluate the self-consistency of
responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively
mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli,
electronic bandgaps, and formation energies that seem to derive from mixed data
sources. We also demonstrate LLaMP's capability to edit crystal structures and
run annealing molecular dynamics simulations using pre-trained machine-learning
force fields. The framework offers an intuitive and nearly hallucination-free
approach to exploring and scaling materials informatics, and establishes a
pathway for knowledge distillation and fine-tuning other language models. Code
and live demo are available at https://github.com/chiang-yuan/llamp",Yuan Chiang
2024-01-31T07:58:54Z,http://arxiv.org/abs/2401.17645v1,"ReSLLM: Large Language Models are Strong Resource Selectors for
  Federated Search","Federated search, which involves integrating results from multiple
independent search engines, will become increasingly pivotal in the context of
Retrieval-Augmented Generation pipelines empowering LLM-based applications such
as chatbots. These systems often distribute queries among various search
engines, ranging from specialized (e.g., PubMed) to general (e.g., Google),
based on the nature of user utterances. A critical aspect of federated search
is resource selection - the selection of appropriate resources prior to issuing
the query to ensure high-quality and rapid responses, and contain costs
associated with calling the external search engines. However, current SOTA
resource selection methodologies primarily rely on feature-based learning
approaches. These methods often involve the labour intensive and expensive
creation of training labels for each resource. In contrast, LLMs have exhibited
strong effectiveness as zero-shot methods across NLP and IR tasks. We
hypothesise that in the context of federated search LLMs can assess the
relevance of resources without the need for extensive predefined labels or
features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to
drive the selection of resources in federated search in a zero-shot setting. In
addition, we devise an unsupervised fine tuning protocol, the Synthetic Label
Augmentation Tuning (SLAT), where the relevance of previously logged queries
and snippets from resources is predicted using an off-the-shelf LLM and then in
turn used to fine-tune ReSLLM with respect to resource selection. Our empirical
evaluation and analysis details the factors influencing the effectiveness of
LLMs in this context. The results showcase the merits of ReSLLM for resource
selection: not only competitive effectiveness in the zero-shot setting, but
also obtaining large when fine-tuned using SLAT-protocol.",Shuai Wang
2024-02-01T16:40:32Z,http://arxiv.org/abs/2402.00746v7,Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,"Recent advancements in artificial intelligence (AI), especially large
language models (LLMs), have significantly advanced healthcare applications and
demonstrated potentials in intelligent medical treatment. However, there are
conspicuous challenges such as vast data volumes and inconsistent symptom
characterization standards, preventing full integration of healthcare AI
systems with individual patients' needs. To promote professional and
personalized healthcare, we propose an innovative framework, Heath-LLM, which
combines large-scale feature extraction and medical knowledge trade-off
scoring. Compared to traditional health management applications, our system has
three main advantages: (1) It integrates health reports and medical knowledge
into a large model to ask relevant questions to large language model for
disease prediction; (2) It leverages a retrieval augmented generation (RAG)
mechanism to enhance feature extraction; (3) It incorporates a semi-automated
feature updating framework that can merge and delete features to improve
accuracy of disease prediction. We experiment on a large number of health
reports to assess the effectiveness of Health-LLM system. The results indicate
that the proposed system surpasses the existing ones and has the potential to
significantly advance disease prediction and personalized health management.",Mingyu Jin
2024-02-02T06:44:22Z,http://arxiv.org/abs/2402.01176v2,"CorpusLM: Towards a Unified Language Model on Corpus for
  Knowledge-Intensive Tasks","Large language models (LLMs) have gained significant attention in various
fields but prone to hallucination, especially in knowledge-intensive (KI)
tasks. To address this, retrieval-augmented generation (RAG) has emerged as a
popular solution to enhance factual accuracy. However, traditional retrieval
modules often rely on large document index and disconnect with generative
tasks. With the advent of generative retrieval (GR), language models can
retrieve by directly generating document identifiers (DocIDs), offering
superior performance in retrieval tasks. However, the potential relationship
between GR and downstream tasks remains unexplored. In this paper, we propose
\textbf{CorpusLM}, a unified language model that leverages external corpus to
tackle various knowledge-intensive tasks by integrating generative retrieval,
closed-book generation, and RAG through a unified greedy decoding process. We
design the following mechanisms to facilitate effective retrieval and
generation, and improve the end-to-end effectiveness of KI tasks: (1) We
develop a ranking-oriented DocID list generation strategy, which refines GR by
directly learning from a DocID ranking list, to improve retrieval quality. (2)
We design a continuous DocIDs-References-Answer generation strategy, which
facilitates effective and efficient RAG. (3) We employ well-designed
unsupervised DocID understanding tasks, to comprehend DocID semantics and their
relevance to downstream tasks. We evaluate our approach on the widely used KILT
benchmark with two variants of backbone models, i.e., T5 and Llama2.
Experimental results demonstrate the superior performance of our models in both
retrieval and downstream tasks.",Xiaoxi Li
2024-02-02T02:41:28Z,http://arxiv.org/abs/2402.01788v1,LitLLM: A Toolkit for Scientific Literature Review,"Conducting literature reviews for scientific papers is essential for
understanding research, its limitations, and building on existing work. It is a
tedious task which makes an automatic literature review generator appealing.
Unfortunately, many existing works that generate such reviews using Large
Language Models (LLMs) have significant limitations. They tend to
hallucinate-generate non-actual information-and ignore the latest research they
have not been trained on. To address these limitations, we propose a toolkit
that operates on Retrieval Augmented Generation (RAG) principles, specialized
prompting and instructing techniques with the help of LLMs. Our system first
initiates a web search to retrieve relevant papers by summarizing user-provided
abstracts into keywords using an off-the-shelf LLM. Authors can enhance the
search by supplementing it with relevant papers or keywords, contributing to a
tailored retrieval process. Second, the system re-ranks the retrieved papers
based on the user-provided abstract. Finally, the related work section is
generated based on the re-ranked results and the abstract. There is a
substantial reduction in time and effort for literature review compared to
traditional methods, establishing our toolkit as an efficient alternative. Our
open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM
and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)
with the video demo at https://youtu.be/E2ggOZBAFw0.",Shubham Agarwal
2024-02-02T18:23:09Z,http://arxiv.org/abs/2402.01828v1,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed SLM, a joint speech and language model, which fuses a
pretrained foundational speech model and a large language model (LLM), while
preserving the in-context learning capability intrinsic to the pretrained LLM.
In this paper, we apply SLM to speech dialog applications where the dialog
states are inferred directly from the audio signal.
  Task-oriented dialogs often contain domain-specific entities, i.e.,
restaurants, hotels, train stations, and city names, which are difficult to
recognize, however, critical for the downstream applications. Inspired by the
RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented
SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to
retrieve text entities mentioned in the audio. The retrieved entities are then
added as text inputs to the underlying SLM to bias model predictions. We
evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that
this retrieval augmentation boosts model performance, achieving joint goal
accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error
rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach
is broadly applicable to other speech tasks requiring contextual information or
domain-specific entities, such as contextual ASR with biasing capability.",Mingqiu Wang
2024-02-03T03:44:57Z,http://arxiv.org/abs/2402.02008v1,"How well do LLMs cite relevant medical references? An evaluation
  framework and analyses","Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.",Kevin Wu
2024-02-03T05:43:39Z,http://arxiv.org/abs/2402.02044v1,Locally-Adaptive Quantization for Streaming Vector Search,"Retrieving the most similar vector embeddings to a given query among a
massive collection of vectors has long been a key component of countless
real-world applications. The recently introduced Retrieval-Augmented Generation
is one of the most prominent examples. For many of these applications, the
database evolves over time by inserting new data and removing outdated data. In
these cases, the retrieval problem is known as streaming similarity search.
While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector
compression method, yields state-of-the-art search performance for non-evolving
databases, its usefulness in the streaming setting has not been yet
established. In this work, we study LVQ in streaming similarity search. In
support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and
multi-means LVQ that boost its search performance by up to 28% and 27%,
respectively. Our studies show that LVQ and its new variants enable blazing
fast vector search, outperforming its closest competitor by up to 9.4x for
identically distributed data and by up to 8.8x under the challenging scenario
of data distribution shifts (i.e., where the statistical distribution of the
data changes over time). We release our contributions as part of Scalable
Vector Search, an open-source library for high-performance similarity search.",Cecilia Aguerrebere
2024-02-09T19:53:29Z,http://arxiv.org/abs/2402.06764v3,"GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph
  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding","Integrating large language models (LLMs) with knowledge graphs derived from
domain-specific data represents an important advancement towards more powerful
and factual reasoning. As these models grow more capable, it is crucial to
enable them to perform multi-step inferences over real-world knowledge graphs
while minimizing hallucination. While large language models excel at
conversation and text generation, their ability to reason over
domain-specialized graphs of interconnected entities remains limited. For
example, can we query a LLM to identify the optimal contact in a professional
network for a specific goal, based on relationships and attributes in a private
database? The answer is no--such capabilities lie beyond current methods.
However, this question underscores a critical technical gap that must be
addressed. Many high-value applications in areas such as science, security, and
e-commerce rely on proprietary knowledge graphs encoding unique structures,
relationships, and logical constraints. We introduce a fine-tuning framework
for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge
graph into an alternate text representation with labeled question-answer pairs.
We demonstrate that grounding the models in specific graph-based knowledge
expands the models' capacity for structure-based reasoning. Our methodology
leverages the large-language model's generative capabilities to create the
dataset and proposes an efficient alternate to retrieval-augmented generation
styled methods.",Stefan Dernbach
2024-02-10T18:27:28Z,http://arxiv.org/abs/2402.07016v1,"REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models","The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.",Yinghao Zhu
2024-02-12T08:45:08Z,http://arxiv.org/abs/2402.07483v2,T-RAG: Lessons from the LLM Trenches,"Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations, including a Needle in a Haystack test, show that this combination
performs better than a simple RAG or finetuning implementation. Finally, we
share some lessons learned based on our experiences building an LLM application
for real-world use.",Masoomali Fatehkia
2024-02-12T17:17:50Z,http://arxiv.org/abs/2402.07812v2,"Retrieval Augmented Thought Process for Private Data Handling in
  Healthcare","Large Language Models (LLMs) have demonstrated the strong potential to assist
both clinicians and the general public with their extensive medical knowledge.
However, their application in healthcare is constrained due to concerns about
the privacy of data used in training, which prevents the integration of private
and personal information because of security and ethical issues. Moreover, if
their capabilities can be enhanced with information retrieval to access
up-to-date knowledge, the current integration of LLMs with Information
retrieval lacks robustness to imperfect retrieval, which can hinder their
effectiveness and even reduce overall performance. In this work, we address
this challenge by introducing the Retrieval-Augmented Thought Process (RATP).
Given access to external knowledge, RATP formulates the thought generation of
LLMs as a multiple-step decision process. To optimise such a thought process,
RATP leverages Monte-Carlo Tree Search and learns a proxy reward function that
permits cost-efficient inference. On a private dataset of electronic medical
records, deliberately excluded from any LLM training set, RATP achieves 35%
additional accuracy compared to in-context retrieval-augmented generation for
the question-answering task.",Thomas Pouplin
2024-02-15T07:22:04Z,http://arxiv.org/abs/2402.09760v1,Grounding Language Model with Chunking-Free In-Context Retrieval,"This paper presents a novel Chunking-Free In-Context (CFIC) retrieval
approach, specifically tailored for Retrieval-Augmented Generation (RAG)
systems. Traditional RAG systems often struggle with grounding responses using
precise evidence text due to the challenges of processing lengthy documents and
filtering out irrelevant content. Commonly employed solutions, such as document
chunking and adapting language models to handle longer contexts, have their
limitations. These methods either disrupt the semantic coherence of the text or
fail to effectively address the issues of noise and inaccuracy in evidence
retrieval.
  CFIC addresses these challenges by circumventing the conventional chunking
process. It utilizes the encoded hidden states of documents for in-context
retrieval, employing auto-aggressive decoding to accurately identify the
specific evidence text required for user queries, eliminating the need for
chunking. CFIC is further enhanced by incorporating two decoding strategies,
namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies
not only improve the efficiency of the retrieval process but also ensure that
the fidelity of the generated grounding text evidence is maintained. Our
evaluations of CFIC on a range of open QA datasets demonstrate its superiority
in retrieving relevant and accurate evidence, offering a significant
improvement over traditional methods. By doing away with the need for document
chunking, CFIC presents a more streamlined, effective, and efficient retrieval
solution, making it a valuable advancement in the field of RAG systems.",Hongjin Qian
2024-02-15T13:39:55Z,http://arxiv.org/abs/2402.09939v1,Generative AI in the Construction Industry: A State-of-the-art Analysis,"The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.",Ridwan Taiwo
2024-02-16T19:26:09Z,http://arxiv.org/abs/2402.11034v2,"PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal
  Question-Answering","Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. ""Who was the US
president in 1970?""). Little work has studied questions whose temporal context
is relative to the present time (e.g. ""Who was the previous US president?""). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.",Jannat Ara Meem
2024-02-19T17:37:28Z,http://arxiv.org/abs/2402.12317v2,EVOR: Evolving Retrieval for Code Generation,"Recently the retrieval-augmented generation (RAG) has been successfully
applied in code generation. However, existing pipelines for retrieval-augmented
code generation (RACG) employ static knowledge bases with a single source,
limiting the adaptation capabilities of Large Language Models (LLMs) to domains
they have insufficient knowledge of. In this work, we develop a novel pipeline,
EVOR, that employs the synchronous evolution of both queries and diverse
knowledge bases. On two realistic settings where the external knowledge is
required to solve code generation tasks, we compile four new datasets
associated with frequently updated libraries and long-tail programming
languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR
achieves two to four times of execution accuracy compared to other methods such
as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We
demonstrate that EVOR is flexible and can be easily combined with them to
achieve further improvement. Further analysis reveals that EVOR benefits from
the synchronous evolution of queries and documents and the diverse information
sources in the knowledge base. We hope that our studies will inspire more
insights into the design of advanced RACG pipelines in future research. Our
model, code, and data are available at https://arks-codegen.github.io.",Hongjin Su
2024-02-19T18:31:11Z,http://arxiv.org/abs/2402.12352v1,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,"Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.",Julien Delile
2024-02-22T12:13:35Z,http://arxiv.org/abs/2402.14480v1,"MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems
  in LLM Augmented Generation","Augmented generation techniques such as Retrieval-Augmented Generation (RAG)
and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing
large language model (LLM) outputs with external knowledge and cached
information. However, the integration of vector databases, which serve as a
backbone for these augmentations, introduces critical challenges, particularly
in ensuring accurate vector matching. False vector matching in these databases
can significantly compromise the integrity and reliability of LLM outputs,
leading to misinformation or erroneous responses. Despite the crucial impact of
these issues, there is a notable research gap in methods to effectively detect
and address false vector matches in LLM-augmented generation. This paper
presents MeTMaP, a metamorphic testing framework developed to identify false
vector matching in LLM-augmented generation systems. We derive eight
metamorphic relations (MRs) from six NLP datasets, which form our method's
core, based on the idea that semantically similar texts should match and
dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets
for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over
203 vector matching configurations, involving 29 embedding models and 7
distance metrics, uncovers significant inaccuracies. The results, showing a
maximum accuracy of only 41.51\% on our tests compared to the original
datasets, emphasize the widespread issue of false matches in vector matching
methods and the critical need for effective detection and mitigation in
LLM-augmented applications.",Guanyu Wang
2024-02-25T11:24:41Z,http://arxiv.org/abs/2402.16063v3,Citation-Enhanced Generation for LLM-based Chatbots,"Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc Citation-Enhanced Generation (CEG) approach combined with
retrieval argumentation. Unlike previous studies that focus on preventing
hallucinations during generation, our method addresses this issue in a post-hoc
way. It incorporates a retrieval module to search for supporting documents
relevant to the generated content, and employs a natural language
inference-based citation generation module. Once the statements in the
generated content lack of reference, our model can regenerate responses until
all statements are supported by citations. Note that our method is a
training-free plug-and-play plugin that is capable of various LLMs. Experiments
on various hallucination-related datasets show our framework outperforms
state-of-the-art methods in both hallucination detection and response
regeneration on three benchmarks. Our codes and dataset will be publicly
available.",Weitao Li
2024-02-26T23:37:59Z,http://arxiv.org/abs/2402.17081v1,"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI
  Judge","This study presents an innovative enhancement to retrieval-augmented
generation (RAG) systems by seamlessly integrating fine-tuned large language
models (LLMs) with vector databases. This integration capitalizes on the
combined strengths of structured data retrieval and the nuanced comprehension
provided by advanced LLMs. Central to our approach are the LoRA and QLoRA
methodologies, which stand at the forefront of model refinement through
parameter-efficient fine-tuning and memory optimization. A novel feature of our
research is the incorporation of user feedback directly into the training
process, ensuring the model's continuous adaptation to user expectations and
thus, improving its performance and applicability. Additionally, we introduce a
Quantized Influence Measure (QIM) as an innovative ""AI Judge"" mechanism to
enhance the precision of result selection, further refining the system's
accuracy. Accompanied by an executive diagram and a detailed algorithm for
fine-tuning QLoRA, our work provides a comprehensive framework for implementing
these advancements within chatbot technologies. This research contributes
significant insights into LLM optimization for specific uses and heralds new
directions for further development in retrieval-augmented models. Through
extensive experimentation and analysis, our findings lay a robust foundation
for future advancements in chatbot technology and retrieval systems, marking a
significant step forward in the creation of more sophisticated, precise, and
user-centric conversational AI systems.",Keshav Rangan
2024-02-27T13:22:51Z,http://arxiv.org/abs/2402.17497v2,"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering","Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (eg., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness regarding the reliability of external knowledge for LLMs, so
as to adaptively utilize external knowledge in RAG systems. Specially, we
develop a novel architecture for LLM-based RAG systems, by incorporating a
specially designed assessment module that precisely assesses the relevance of
retrieved documents. Furthermore, we propose an improved training method based
on bi-granularity relevance fusion and noise-resistant training. By combining
the improvements in both architecture and training, our proposed REAR can
better utilize external knowledge by effectively perceiving the relevance of
retrieved documents. Experiments on four open-domain QA tasks show that REAR
significantly outperforms previous a number of competitive RAG approaches. Our
codes can be accessed at https://github.com/RUCAIBox/REAR.",Yuhao Wang
2024-02-27T18:42:31Z,http://arxiv.org/abs/2402.17753v1,Evaluating Very Long-Term Conversational Memory of LLM Agents,"Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.",Adyasha Maharana
2024-02-27T21:01:41Z,http://arxiv.org/abs/2402.17887v4,"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability","Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.",Junda Wang
2024-02-29T18:20:37Z,http://arxiv.org/abs/2402.19421v1,"Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based
  Search Engines","In the domain of digital information dissemination, search engines act as
pivotal conduits linking information seekers with providers. The advent of
chat-based search engines utilizing Large Language Models (LLMs) and Retrieval
Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary
leap in the search ecosystem. They demonstrate metacognitive abilities in
interpreting web information and crafting responses with human-like
understanding and creativity. Nonetheless, the intricate nature of LLMs renders
their ""cognitive"" processes opaque, challenging even their designers'
understanding. This research aims to dissect the mechanisms through which an
LLM-powered chat-based search engine, specifically Bing Chat, selects
information sources for its responses. To this end, an extensive dataset has
been compiled through engagements with New Bing, documenting the websites it
cites alongside those listed by the conventional search engine. Employing
natural language processing (NLP) techniques, the research reveals that Bing
Chat exhibits a preference for content that is not only readable and formally
structured, but also demonstrates lower perplexity levels, indicating a unique
inclination towards text that is predictable by the underlying LLM. Further
enriching our analysis, we procure an additional dataset through interactions
with the GPT-4 based knowledge retrieval API, unveiling a congruent text
preference between the RAG API and Bing Chat. This consensus suggests that
these text preferences intrinsically emerge from the underlying language
models, rather than being explicitly crafted by Bing Chat's developers.
Moreover, our investigation documents a greater similarity among websites cited
by RAG technologies compared to those ranked highest by conventional search
engines.",Lijia Ma
2024-02-23T18:45:35Z,http://arxiv.org/abs/2403.00801v2,"Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model","The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.",Qiaoyu Tang
2024-02-29T09:35:41Z,http://arxiv.org/abs/2403.00840v1,EyeGPT: Ophthalmic Assistant with Large Language Models,"Artificial intelligence (AI) has gained significant attention in healthcare
consultation due to its potential to improve clinical workflow and enhance
medical communication. However, owing to the complex nature of medical
information, large language models (LLM) trained with general world knowledge
might not possess the capability to tackle medical-related tasks at an expert
level. Here, we introduce EyeGPT, a specialized LLM designed specifically for
ophthalmology, using three optimization strategies including role-playing,
finetuning, and retrieval-augmented generation. In particular, we proposed a
comprehensive evaluation framework that encompasses a diverse dataset, covering
various subspecialties of ophthalmology, different users, and diverse inquiry
intents. Moreover, we considered multiple evaluation metrics, including
accuracy, understandability, trustworthiness, empathy, and the proportion of
hallucinations. By assessing the performance of different EyeGPT variants, we
identify the most effective one, which exhibits comparable levels of
understandability, trustworthiness, and empathy to human ophthalmologists (all
Ps>0.05). Overall, ur study provides valuable insights for future research,
facilitating comprehensive comparisons and evaluations of different strategies
for developing specialized LLMs in ophthalmology. The potential benefits
include enhancing the patient experience in eye care and optimizing
ophthalmologists' services.",Xiaolan Chen
2024-03-01T07:14:45Z,http://arxiv.org/abs/2403.00872v1,"DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy
  in Large-Scale Databases","The task of converting natural language queries into SQL queries is
intricate, necessitating a blend of precise techniques for an accurate
translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a
significant development in this domain. This paper introduces DFIN (Decomposed
Focused-In-Context), an innovative extension of DIN-SQL that enhances
Text-to-SQL conversion by addressing schema linking errors, which are a major
source of inaccuracies. DFIN uniquely alternates between prompting techniques
and Retrieval-Augmented Generation (RAG), adapting to the size and complexity
of the database schema. A preprocessing phase embeds database definitions and
leverages annotated files, akin to those in the BIRD dataset, facilitating the
runtime retrieval of pertinent schema information. This strategy significantly
reduces the token count for schema linking prompts, enabling the use of a
standard GPT-4 model over its larger context variant, thus handling large-scale
databases more effectively and economically. Our evaluation on the BIRD
dataset, a challenging real-world benchmark, demonstrates that DFIN not only
scales efficiently but also improves accuracy, achieving a score of 51.69. This
improvement surpasses DIN-SQL method (the current third-place), which is the
highest-ranked model employing in-context learning rather than fine-tuning,
previously scoring 50.72. The advancement of DFIN underscores the evolving
capabilities of in-context learning methodologies combined with advanced
language models, offering a promising avenue for future research in complex
Text-to-SQL conversion tasks.",Shai Volvovsky
2024-03-07T08:34:57Z,http://arxiv.org/abs/2403.04317v2,Online Adaptation of Language Models with a Memory of Amortized Contexts,"Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.",Jihoon Tack
2024-03-13T08:50:15Z,http://arxiv.org/abs/2403.08345v1,"From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction","The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.",Vamsi Krishna Kommineni
2024-03-15T06:59:43Z,http://arxiv.org/abs/2403.10059v2,Repoformer: Selective Retrieval for Repository-Level Code Completion,"Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). In this paper, we propose a selective RAG
framework to avoid retrieval when unnecessary. To power this framework, we
design a self-supervised learning approach to enable a code LM to accurately
self-evaluate whether retrieval can improve its output quality and robustly
leverage the potentially noisy retrieved contexts. Using this LM as both the
selective RAG policy and the generation model, our framework achieves
state-of-the-art repository-level code completion performance on diverse
benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new
long-form code completion benchmark. Meanwhile, our analyses show that
selectively retrieving brings as much as 70% inference speedup in the online
serving setting without harming the performance. We further demonstrate that
our framework is able to accommodate different generation models, retrievers,
and programming languages. These advancements position our framework as an
important step towards more accurate and efficient repository-level code
completion.",Di Wu
2024-03-15T15:43:02Z,http://arxiv.org/abs/2403.10408v1,"SocialGenPod: Privacy-Friendly Generative AI Social Web Applications
  with Decentralised Personal Data Stores","We present SocialGenPod, a decentralised and privacy-friendly way of
deploying generative AI Web applications. Unlike centralised Web and data
architectures that keep user data tied to application and service providers, we
show how one can use Solid -- a decentralised Web specification -- to decouple
user data from generative AI applications. We demonstrate SocialGenPod using a
prototype that allows users to converse with different Large Language Models,
optionally leveraging Retrieval Augmented Generation to generate answers
grounded in private documents stored in any Solid Pod that the user is allowed
to access, directly or indirectly. SocialGenPod makes use of Solid access
control mechanisms to give users full control of determining who has access to
data stored in their Pods. SocialGenPod keeps all user data (chat history, app
configuration, personal documents, etc) securely in the user's personal Pod;
separate from specific model or application providers. Besides better privacy
controls, this approach also enables portability across different services and
applications. Finally, we discuss challenges, posed by the large compute
requirements of state-of-the-art models, that future research in this area
should address. Our prototype is open-source and available at:
https://github.com/Vidminas/socialgenpod/.",Vidminas Vizgirda
2024-03-15T17:04:27Z,http://arxiv.org/abs/2403.10588v1,"S3LLM: Large-Scale Scientific Software Understanding with LLMs using
  Source, Metadata, and Document","The understanding of large-scale scientific software poses significant
challenges due to its diverse codebase, extensive code length, and target
computing architectures. The emergence of generative AI, specifically large
language models (LLMs), provides novel pathways for understanding such complex
scientific codes. This paper presents S3LLM, an LLM-based framework designed to
enable the examination of source code, code metadata, and summarized
information in conjunction with textual technical reports in an interactive,
conversational manner through a user-friendly interface. S3LLM leverages
open-source LLaMA-2 models to enhance code analysis through the automatic
transformation of natural language queries into domain-specific language (DSL)
queries. Specifically, it translates these queries into Feature Query Language
(FQL), enabling efficient scanning and parsing of entire code repositories. In
addition, S3LLM is equipped to handle diverse metadata types, including DOT,
SQL, and customized formats. Furthermore, S3LLM incorporates retrieval
augmented generation (RAG) and LangChain technologies to directly query
extensive documents. S3LLM demonstrates the potential of using locally deployed
open-source LLMs for the rapid understanding of large-scale scientific
computing software, eliminating the need for extensive coding expertise, and
thereby making the process more efficient and effective. S3LLM is available at
https://github.com/ResponsibleAILab/s3llm.",Kareem Shaik
2024-03-18T11:19:37Z,http://arxiv.org/abs/2403.11671v1,HDLdebugger: Streamlining HDL debugging with Large Language Models,"In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.",Xufeng Yao
2024-03-19T09:45:33Z,http://arxiv.org/abs/2403.12582v1,"AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented
  Stock-Chain Framework","The task of financial analysis primarily encompasses two key areas: stock
trend prediction and the corresponding financial question answering. Currently,
machine learning and deep learning algorithms (ML&DL) have been widely applied
for stock trend predictions, leading to significant progress. However, these
methods fail to provide reasons for predictions, lacking interpretability and
reasoning processes. Also, they can not integrate textual information such as
financial news or reports. Meanwhile, large language models (LLMs) have
remarkable textual understanding and generation ability. But due to the
scarcity of financial training datasets and limited integration with real-time
knowledge, LLMs still suffer from hallucinations and are unable to keep up with
the latest information. To tackle these challenges, we first release AlphaFin
datasets, combining traditional research datasets, real-time financial data,
and handwritten chain-of-thought (CoT) data. It has a positive impact on
training LLMs for completing financial analysis. We then use AlphaFin datasets
to benchmark a state-of-the-art method, called Stock-Chain, for effectively
tackling the financial analysis task, which integrates retrieval-augmented
generation (RAG) techniques. Extensive experiments are conducted to demonstrate
the effectiveness of our framework on financial analysis.",Xiang Li
2024-03-21T14:17:28Z,http://arxiv.org/abs/2403.14421v3,DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning,"Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.",Jonathan Lebensold
2024-03-22T15:06:45Z,http://arxiv.org/abs/2403.15268v5,"Awakening Augmented Generation: Learning to Awaken Internal Knowledge of
  Large Language Models for Question Answering","Retrieval-Augmented-Generation and Generation-Augmented-Generation have been
proposed to enhance the knowledge required for question answering with Large
Language Models (LLMs) by leveraging richer context. However, the former relies
on external resources, and both require incorporating explicit documents into
the context, which increases execution costs and susceptibility to noise data
during inference. Recent works indicate that LLMs model rich knowledge, but it
is often not effectively activated and awakened. Inspired by this, we propose a
novel knowledge-augmented framework, $\textbf{Awakening-Augmented-Generation}$
(AAG), which mimics the human ability to answer questions using only thinking
and recalling to compensate for knowledge gaps, thereby awaking relevant
knowledge in LLMs without relying on external resources. AAG consists of two
key components for awakening richer context. Explicit awakening fine-tunes a
context generator to create a synthetic, compressed document that functions as
symbolic context. Implicit awakening utilizes a hypernetwork to generate
adapters based on the question and synthetic document, which are inserted into
LLMs to serve as parameter context. Experimental results on three datasets
demonstrate that AAG exhibits significant advantages in both open-domain and
closed-book settings, as well as in out-of-distribution generalization. Our
code will be available at \url{https://github.com/Xnhyacinth/IAG}.",Huanxuan Liao
2024-03-23T05:32:46Z,http://arxiv.org/abs/2403.15729v3,Towards a RAG-based Summarization Agent for the Electron-Ion Collider,"The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.",Karthik Suresh
2024-03-23T06:03:36Z,http://arxiv.org/abs/2403.15736v2,"General LLMs as Instructors for Domain-Specific LLMs: A Sequential
  Fusion Method to Integrate Extraction and Editing","The substantial interest in updating Large Language Models (LLMs) without
retraining from scratch is accompanied by several challenges. This is
particularly true when updating LLMs with datasets that necessitate
domain-expert reasoning across extensive texts, despite limited samples. We
termed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs
(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval
Augmented Generation (RAG) are inadequate for addressing this critical issue,
particularly evident in our exploration of a specific medical dataset that
epitomizes the distinct needs of FDoR-UL. To tackle this challenge, we
introduce a Sequential Fusion method to integrate knowledge from complex
contexts into LLMs. This method employs a two-stage framework: initially
leveraging general LLMs to perform relation extraction for knowledge
acquisition from complex texts, followed by updating domain-specific LLMs
through Knowledge Editing (KE). Employing our method, domain-specific LLMs
achieved a 71.7% accuracy (an average gain of 39.1%) in question-answering
tasks. Furthermore, we expanded our evaluation to a novel economics-management
dataset we developed, where our method achieved a 75.0% accuracy (an average
gain of 45.0%). These findings underscore the effectiveness and flexibility of
our approach in FDoR-UL across various domains.",Xin Zhang
2024-03-28T03:14:18Z,http://arxiv.org/abs/2403.19116v1,MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,"In today's fast-paced industry, professionals face the challenge of
summarizing a large number of documents and extracting vital information from
them on a daily basis. These metrics are frequently hidden away in tables
and/or their nested hyperlinks. To address this challenge, the approach of
Table Question Answering (QA) has been developed to extract the relevant
information. However, traditional Table QA training tasks that provide a table
and an answer(s) from a gold cell coordinate(s) for a question may not always
ensure extracting the accurate answer(s). Recent advancements in Large Language
Models (LLMs) have opened up new possibilities for extracting information from
tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot
Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The
first step involves Few-Shot Learning (FSL), where relevant tables and
associated contexts of hyperlinks are retrieved based on a given question. The
retrieved content is then used to construct few-shot prompts as inputs to an
LLM, such as ChatGPT. To tackle the challenge of answering complex questions,
the second step leverages Chain-of-thought (CoT) prompting to decompose the
complex question into a sequential chain of questions and reasoning thoughts in
a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process
by retrieving relevant tables and contexts of hyperlinks that are relevant to
the resulting reasoning thoughts and questions. These additional contexts are
then used to supplement the prompt used in the first step, resulting in more
accurate answers from an LLM. Empirical results from OTT-QA demonstrate that
our abstractive QA approach significantly improves the accuracy of extractive
Table QA methods.",Che Guan
2024-03-28T08:27:44Z,http://arxiv.org/abs/2403.19216v2,Are Large Language Models Good at Utility Judgments?,"Retrieval-augmented generation (RAG) is considered to be a promising approach
to alleviate the hallucination issue of large language models (LLMs), and it
has received widespread attention from researchers recently. Due to the
limitation in the semantic understanding of retrieval models, the success of
RAG heavily lies on the ability of LLMs to identify passages with utility.
Recent efforts have explored the ability of LLMs to assess the relevance of
passages in retrieval, but there has been limited work on evaluating the
utility of passages in supporting question answering. In this work, we conduct
a comprehensive study about the capabilities of LLMs in utility evaluation for
open-domain QA. Specifically, we introduce a benchmarking procedure and
collection of candidate passages with different characteristics, facilitating a
series of experiments with five representative LLMs. Our experiments reveal
that: (i) well-instructed LLMs can distinguish between relevance and utility,
and that LLMs are highly receptive to newly generated counterfactual passages.
Moreover, (ii) we scrutinize key factors that affect utility judgments in the
instruction design. And finally, (iii) to verify the efficacy of utility
judgments in practical retrieval augmentation applications, we delve into LLMs'
QA capabilities using the evidence judged with utility and direct dense
retrieval results. (iv) We propose a k-sampling, listwise approach to reduce
the dependency of LLMs on the sequence of input passages, thereby facilitating
subsequent answer generation. We believe that the way we formalize and study
the problem along with our findings contributes to a critical assessment of
retrieval-augmented LLMs. Our code and benchmark can be found at
\url{https://github.com/ict-bigdatalab/utility_judgments}.",Hengran Zhang
2024-03-30T22:41:05Z,http://arxiv.org/abs/2404.00486v1,"Dialectical Alignment: Resolving the Tension of 3H and Security Threats
  of LLMs","With the rise of large language models (LLMs), ensuring they embody the
principles of being helpful, honest, and harmless (3H), known as Human
Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,
etc., effectively fine-tune LLMs to match preferences in the preference
dataset, they often lead LLMs to highly receptive human input and external
evidence, even when this information is poisoned. This leads to a tendency for
LLMs to be Adaptive Chameleons when external evidence conflicts with their
parametric memory. This exacerbates the risk of LLM being attacked by external
poisoned data, which poses a significant security risk to LLM system
applications such as Retrieval-augmented generation (RAG). To address the
challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)
utilizes AI feedback to identify optimal strategies for LLMs to navigate
inter-context conflicts and context-memory conflicts with different external
evidence in context window (i.e., different ratios of poisoned factual
contexts); (2) constructs the SFT dataset as well as the preference dataset
based on the AI feedback and strategies above; (3) uses the above datasets for
LLM alignment to defense poisoned context attack while preserving the
effectiveness of in-context knowledge editing. Our experiments show that the
dialectical alignment model improves poisoned data attack defense by 20 and
does not require any additional prompt engineering or prior declaration of
``you may be attacked`` to the LLMs' context window.",Shu Yang
2024-04-05T02:53:51Z,http://arxiv.org/abs/2404.03868v2,"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge
  Graph Construction","In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that, in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schemas
easily exceed the LLMs' context window length. Furthermore, there are scenarios
where a fixed pre-defined schema is not available and we would like the method
to construct a high-quality KG with a succinct self-generated schema. To
address these problems, we propose a three-phase framework named
Extract-Define-Canonicalize (EDC): open information extraction followed by
schema definition and post-hoc canonicalization. EDC is flexible in that it can
be applied to settings where a pre-defined target schema is available and when
it is not; in the latter case, it constructs a schema automatically and applies
self-canonicalization. To further improve performance, we introduce a trained
component that retrieves schema elements relevant to the input text; this
improves the LLMs' extraction performance in a retrieval-augmented
generation-like manner. We demonstrate on three KGC benchmarks that EDC is able
to extract high-quality triplets without any parameter tuning and with
significantly larger schemas compared to prior works. Code for EDC is available
at https://github.com/clear-nus/edc.",Bowen Zhang
2024-04-05T11:55:52Z,http://arxiv.org/abs/2404.04044v2,A Comparison of Methods for Evaluating Generative IR,"Information retrieval systems increasingly incorporate generative components.
For example, in a retrieval augmented generation (RAG) system, a retrieval
component might provide a source of ground truth, while a generative component
summarizes and augments its responses. In other systems, a large language model
(LLM) might directly generate responses without consulting a retrieval
component. While there are multiple definitions of generative information
retrieval (Gen-IR) systems, in this paper we focus on those systems where the
system's response is not drawn from a fixed collection of documents or
passages. The response to a query may be entirely new text. Since traditional
IR evaluation methods break down under this model, we explore various methods
that extend traditional offline evaluation approaches to the Gen-IR context.
Offline IR evaluation traditionally employs paid human assessors, but
increasingly LLMs are replacing human assessment, demonstrating capabilities
similar or superior to crowdsourced labels. Given that Gen-IR systems do not
generate responses from a fixed set, we assume that methods for Gen-IR
evaluation must largely depend on LLM-generated labels. Along with methods
based on binary and graded relevance, we explore methods based on explicit
subtopics, pairwise preferences, and embeddings. We first validate these
methods against human assessments on several TREC Deep Learning Track tasks; we
then apply these methods to evaluate the output of several purely generative
systems. For each method we consider both its ability to act autonomously,
without the need for human labels or other input, and its ability to support
human auditing. To trust these methods, we must be assured that their results
align with human assessments. In order to do so, evaluation criteria must be
transparent, so that outcomes can be audited by human assessors.",Negar Arabzadeh
2024-04-04T02:58:21Z,http://arxiv.org/abs/2404.04287v1,CONFLARE: CONFormal LArge language model REtrieval,"Retrieval-augmented generation (RAG) frameworks enable large language models
(LLMs) to retrieve relevant information from a knowledge base and incorporate
it into the context for generating responses. This mitigates hallucinations and
allows for the updating of knowledge without retraining the LLM. However, RAG
does not guarantee valid responses if retrieval fails to identify the necessary
information as the context for response generation. Also, if there is
contradictory content, the RAG response will likely reflect only one of the two
possible responses. Therefore, quantifying uncertainty in the retrieval process
is crucial for ensuring RAG trustworthiness. In this report, we introduce a
four-step framework for applying conformal prediction to quantify retrieval
uncertainty in RAG frameworks. First, a calibration set of questions answerable
from the knowledge base is constructed. Each question's embedding is compared
against document embeddings to identify the most relevant document chunks
containing the answer and record their similarity scores. Given a
user-specified error rate ({\alpha}), these similarity scores are then analyzed
to determine a similarity score cutoff threshold. During inference, all chunks
with similarity exceeding this threshold are retrieved to provide context to
the LLM, ensuring the true answer is captured in the context with a
(1-{\alpha}) confidence level. We provide a Python package that enables users
to implement the entire workflow proposed in our work, only using LLMs and
without human intervention.",Pouria Rouzrokh
2024-04-08T15:03:57Z,http://arxiv.org/abs/2404.05590v2,"MedExpQA: Multilingual Benchmarking of Large Language Models for Medical
  Question Answering","Large Language Models (LLMs) have the potential of facilitating the
development of Artificial Intelligence technology to assist medical experts for
interactive decision support, which has been demonstrated by their competitive
performances in Medical QA. However, while impressive, the required quality bar
for medical applications remains far from being achieved. Currently, LLMs
remain challenged by outdated knowledge and by their tendency to generate
hallucinated content. Furthermore, most benchmarks to assess medical knowledge
lack reference gold explanations which means that it is not possible to
evaluate the reasoning of LLMs predictions. Finally, the situation is
particularly grim if we consider benchmarking LLMs for languages other than
English which remains, as far as we know, a totally neglected topic. In order
to address these shortcomings, in this paper we present MedExpQA, the first
multilingual benchmark based on medical exams to evaluate LLMs in Medical
Question Answering. To the best of our knowledge, MedExpQA includes for the
first time reference gold explanations written by medical doctors which can be
leveraged to establish various gold-based upper-bounds for comparison with LLMs
performance. Comprehensive multilingual experimentation using both the gold
reference explanations and Retrieval Augmented Generation (RAG) approaches show
that performance of LLMs still has large room for improvement, especially for
languages other than English. Furthermore, and despite using state-of-the-art
RAG methods, our results also demonstrate the difficulty of obtaining and
integrating readily available medical knowledge that may positively impact
results on downstream evaluations for Medical Question Answering. So far the
benchmark is available in four languages, but we hope that this work may
encourage further development to other languages.",Iñigo Alonso
2024-04-09T14:34:48Z,http://arxiv.org/abs/2404.06347v2,RAR-b: Reasoning as Retrieval Benchmark,"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks
have been the two major avenues to record the progress of embedding models in
the past few years. Under the emerging Retrieval-augmented Generation (RAG)
paradigm, we envision the need to evaluate next-level language understanding
abilities of embedding models, and take a conscious look at the reasoning
abilities stored in them. Addressing this, we pose the question: Can retrievers
solve reasoning problems? By transforming reasoning tasks into retrieval tasks,
we find that without specifically trained for reasoning-level language
understanding, current state-of-the-art retriever models may still be far from
being competent for playing the role of assisting LLMs, especially in
reasoning-intensive tasks. Moreover, albeit trained to be aware of
instructions, instruction-aware IR models are often better off without
instructions in inference time for reasoning tasks, posing an overlooked
retriever-LLM behavioral gap for the research community to align. However,
recent decoder-based embedding models show great promise in narrowing the gap,
highlighting the pathway for embedding models to achieve reasoning-level
language understanding. We also show that, although current off-the-shelf
re-ranker models fail on these tasks, injecting reasoning abilities into them
through fine-tuning still appears easier than doing so to bi-encoders, and we
are able to achieve state-of-the-art performance across all tasks by
fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark
(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning
abilities stored in retriever models. RAR-b is available at
https://github.com/gowitheflow-1998/RAR-b.",Chenghao Xiao
2024-04-10T07:56:26Z,http://arxiv.org/abs/2404.06809v3,Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation,"The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.",Ruotong Pan
2024-04-10T16:12:50Z,http://arxiv.org/abs/2404.07135v2,"Towards Robustness of Text-to-Visualization Translation against Lexical
  and Phrasal Variability","Text-to-Vis is an emerging task in the natural language processing (NLP) area
that aims to automatically generate data visualizations from natural language
questions (NLQs). Despite their progress, existing text-to-vis models often
heavily rely on lexical matching between words in the questions and tokens in
data schemas. This overreliance on lexical matching may lead to a diminished
level of model robustness against input variations. In this study, we
thoroughly examine the robustness of current text-to-vis models, an area that
has not previously been explored. In particular, we construct the first
robustness dataset nvBench-Rob, which contains diverse lexical and phrasal
variations based on the original text-to-vis benchmark nvBench. Then, we found
that the performance of existing text-to-vis models on this new dataset
dramatically drops, implying that these methods exhibit inadequate robustness
overall. Finally, we propose a novel framework based on Retrieval-Augmented
Generation (RAG) technique, named GRED, specifically designed to address input
perturbations in these two variants. The framework consists of three parts:
NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and
Annotation-based Debugger, which are used to tackle the challenges posed by
natural language variants, programming style differences and data schema
variants, respectively. Extensive experimental evaluations show that, compared
to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs
better in terms of model robustness, with a 32% increase in accuracy on the
proposed nvBench-Rob dataset.",Jinwei Lu
2024-04-11T21:48:54Z,http://arxiv.org/abs/2404.08137v2,Generative Information Retrieval Evaluation,"This paper is a draft of a chapter intended to appear in a forthcoming book
on generative information retrieval, co-edited by Chirag Shah and Ryen White.
In this chapter, we consider generative information retrieval evaluation from
two distinct but interrelated perspectives. First, large language models (LLMs)
themselves are rapidly becoming tools for evaluation, with current research
indicating that LLMs may be superior to crowdsource workers and other paid
assessors on basic relevance judgement tasks. We review past and ongoing
related research, including speculation on the future of shared task
initiatives, such as TREC, and a discussion on the continuing need for human
assessments. Second, we consider the evaluation of emerging LLM-based
generative information retrieval (GenIR) systems, including retrieval augmented
generation (RAG) systems. We consider approaches that focus both on the
end-to-end evaluation of GenIR systems and on the evaluation of a retrieval
component as an element in a RAG system. Going forward, we expect the
evaluation of GenIR systems to be at least partially based on LLM-based
assessment, creating an apparent circularity, with a system seemingly
evaluating its own output. We resolve this apparent circularity in two ways: 1)
by viewing LLM-based assessment as a form of ""slow search"", where a slower IR
system is used for evaluation and training of a faster production IR system;
and 2) by recognizing a continuing need to ground evaluation in human
assessment, even if the characteristics of that human assessment must change.",Marwah Alaofi
2024-04-13T02:39:36Z,http://arxiv.org/abs/2404.08878v1,"Generative AI Agent for Next-Generation MIMO Design: Fundamentals,
  Challenges, and Vision","Next-generation multiple input multiple output (MIMO) is expected to be
intelligent and scalable. In this paper, we study generative artificial
intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we
provide an overview of the development, fundamentals, and challenges of the
next-generation MIMO. Then, we propose the concept of the generative AI agent,
which is capable of generating tailored and specialized contents with the aid
of large language model (LLM) and retrieval augmented generation (RAG). Next,
we comprehensively discuss the features and advantages of the generative AI
agent framework. More importantly, to tackle existing challenges of
next-generation MIMO, we discuss generative AI agent-enabled next-generation
MIMO design, from the perspective of performance analysis, signal processing,
and resource allocation. Furthermore, we present two compelling case studies
that demonstrate the effectiveness of leveraging the generative AI agent for
performance analysis in complex configuration scenarios. These examples
highlight how the integration of generative AI agents can significantly enhance
the analysis and design of next-generation MIMO systems. Finally, we discuss
important potential research future directions.",Zhe Wang
2024-04-14T03:44:54Z,http://arxiv.org/abs/2404.09134v2,"Generative AI Agents with Large Language Model for Satellite Networks
  via a Mixture of Experts Transmission","In response to the needs of 6G global communications, satellite communication
networks have emerged as a key solution. However, the large-scale development
of satellite communication networks is constrained by the complex system
models, whose modeling is challenging for massive users. Moreover, transmission
interference between satellites and users seriously affects communication
performance. To solve these problems, this paper develops generative artificial
intelligence (AI) agents for model formulation and then applies a mixture of
experts (MoE) approach to design transmission strategies. Specifically, we
leverage large language models (LLMs) to build an interactive modeling paradigm
and utilize retrieval-augmented generation (RAG) to extract satellite expert
knowledge that supports mathematical modeling. Afterward, by integrating the
expertise of multiple specialized components, we propose an MoE-proximal policy
optimization (PPO) approach to solve the formulated problem. Each expert can
optimize the optimization variables at which it excels through specialized
training through its own network and then aggregates them through the gating
network to perform joint optimization. The simulation results validate the
accuracy and effectiveness of employing a generative agent for problem
formulation. Furthermore, the superiority of the proposed MoE-ppo approach over
other benchmarks is confirmed in solving the formulated problem. The
adaptability of MoE-PPO to various customized modeling problems has also been
demonstrated.",Ruichen Zhang
2024-04-14T16:34:31Z,http://arxiv.org/abs/2404.09296v2,"Cross-Data Knowledge Graph Construction for LLM-enabled Educational
  Question-Answering System: A Case Study at HCMUT","In today's rapidly evolving landscape of Artificial Intelligence, large
language models (LLMs) have emerged as a vibrant research topic. LLMs find
applications in various fields and contribute significantly. Despite their
powerful language capabilities, similar to pre-trained language models (PLMs),
LLMs still face challenges in remembering events, incorporating new
information, and addressing domain-specific issues or hallucinations. To
overcome these limitations, researchers have proposed Retrieval-Augmented
Generation (RAG) techniques, some others have proposed the integration of LLMs
with Knowledge Graphs (KGs) to provide factual context, thereby improving
performance and delivering more accurate feedback to user queries.
  Education plays a crucial role in human development and progress. With the
technology transformation, traditional education is being replaced by digital
or blended education. Therefore, educational data in the digital environment is
increasing day by day. Data in higher education institutions are diverse,
comprising various sources such as unstructured/structured text, relational
databases, web/app-based API access, etc. Constructing a Knowledge Graph from
these cross-data sources is not a simple task. This article proposes a method
for automatically constructing a Knowledge Graph from multiple data sources and
discusses some initial applications (experimental trials) of KG in conjunction
with LLMs for question-answering tasks.",Tuan Bui
2024-04-16T12:10:01Z,http://arxiv.org/abs/2404.10496v4,"Spiral of Silence: How is Large Language Model Killing Information
  Retrieval? -- A Case Study on Open Domain Question Answering","The practice of Retrieval-Augmented Generation (RAG), which integrates Large
Language Models (LLMs) with retrieval systems, has become increasingly
prevalent. However, the repercussions of LLM-derived content infiltrating the
web and influencing the retrieval-generation feedback loop are largely
uncharted territories. In this study, we construct and iteratively run a
simulation pipeline to deeply investigate the short-term and long-term effects
of LLM text on RAG systems. Taking the trending Open Domain Question Answering
(ODQA) task as a point of entry, our findings reveal a potential digital
""Spiral of Silence"" effect, with LLM-generated text consistently outperforming
human-authored content in search rankings, thereby diminishing the presence and
impact of human contributions online. This trend risks creating an imbalanced
information ecosystem, where the unchecked proliferation of erroneous
LLM-generated content may result in the marginalization of accurate
information. We urge the academic community to take heed of this potential
issue, ensuring a diverse and authentic digital information landscape.",Xiaoyang Chen
2024-03-23T13:25:01Z,http://arxiv.org/abs/2404.10779v1,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",Mathav Raj J
2024-04-17T23:00:03Z,http://arxiv.org/abs/2404.11792v2,"Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning:
  A Comparative Study","This paper investigates the impact of domain-specific model fine-tuning and
of reasoning mechanisms on the performance of question-answering (Q&A) systems
powered by large language models (LLMs) and Retrieval-Augmented Generation
(RAG). Using the FinanceBench SEC financial filings dataset, we observe that,
for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves
better accuracy than generic models, with relatively greater gains attributable
to fine-tuned embedding models. Additionally, employing reasoning iterations on
top of RAG delivers an even bigger jump in performance, enabling the Q&A
systems to get closer to human-expert quality. We discuss the implications of
such findings, propose a structured technical design space capturing major
technical components of Q&A AI, and provide recommendations for making
high-impact technical choices for such components. We plan to follow up on this
work with actionable guides for AI teams and further investigations into the
impact of domain-specific augmentation in RAG and into agentic AI capabilities
such as advanced planning and reasoning.",Zooey Nguyen
2024-04-19T10:27:40Z,http://arxiv.org/abs/2404.12772v1,"Generating Test Scenarios from NL Requirements using Retrieval-Augmented
  LLMs: An Industrial Study","Test scenarios are specific instances of test cases that describe actions to
validate a particular software functionality. By outlining the conditions under
which the software operates and the expected outcomes, test scenarios ensure
that the software functionality is tested in an integrated manner. Test
scenarios are crucial for systematically testing an application under various
conditions, including edge cases, to identify potential issues and guarantee
overall performance and reliability. Specifying test scenarios is tedious and
requires a deep understanding of software functionality and the underlying
domain. It further demands substantial effort and investment from already time-
and budget-constrained requirements engineers and testing teams. This paper
presents an automated approach (RAGTAG) for test scenario generation using
Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG
allows the integration of specific domain knowledge with LLMs' generation
capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post
with bilingual requirements in German and English. Our results from an
interview survey conducted with four experts on five dimensions -- relevance,
coverage, correctness, coherence and feasibility, affirm the potential of
RAGTAG in automating test scenario generation. Specifically, our results
indicate that, despite the difficult task of analyzing bilingual requirements,
RAGTAG is able to produce scenarios that are well-aligned with the underlying
requirements and provide coverage of different aspects of the intended
functionality. The generated scenarios are easily understandable to experts and
feasible for testing in the project environment. The overall correctness is
deemed satisfactory; however, gaps in capturing exact action sequences and
domain nuances remain, underscoring the need for domain expertise when applying
LLMs.",Chetan Arora
2024-04-22T07:44:20Z,http://arxiv.org/abs/2404.13947v3,"Self-Bootstrapped Visual-Language Model for Knowledge Selection and
  Question Answering","While large visual-language models (LVLM) have shown promising results on
traditional visual question answering benchmarks, it is still challenging for
them to answer complex VQA problems which requires diverse world knowledge.
Motivated by the research of retrieval-augmented generation in the field of
natural language processing, we use Dense Passage Retrieval (DPR) to retrieve
related knowledge to help the model answer questions. However, DPR conduct
retrieving in natural language space, which may not ensure comprehensive
acquisition of image information. Thus, the retrieved knowledge is not truly
conducive to helping answer the question, affecting the performance of the
overall system. To address this issue, we propose a novel framework that
leverages the visual-language model to select the key knowledge retrieved by
DPR and answer questions. The framework consists of two modules: Selector and
Answerer, where both are initialized by the LVLM and parameter-efficiently
finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge
documents using the Selector, and then use them to finetune the Answerer to
predict answers; obtain the pseudo-labels of key knowledge documents based on
the predictions of the Answerer and weak supervision labels, and then finetune
the Selector to select key knowledge; repeat. Our framework significantly
enhances the performance of the baseline on the challenging open-domain
Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of
62.83%. Our code is publicly available at
https://github.com/haodongze/Self-KSel-QAns.",Dongze Hao
2024-04-22T07:49:36Z,http://arxiv.org/abs/2404.13948v2,"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by
  Simulating Documents in the Wild via Low-level Perturbations","The robustness of recent Large Language Models (LLMs) has become increasingly
crucial as their applicability expands across various domains and real-world
applications. Retrieval-Augmented Generation (RAG) is a promising solution for
addressing the limitations of LLMs, yet existing studies on the robustness of
RAG often overlook the interconnected relationships between RAG components or
the potential threats prevalent in real-world databases, such as minor textual
errors. In this work, we investigate two underexplored aspects when assessing
the robustness of RAG: 1) vulnerability to noisy documents through low-level
perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we
introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}),
which targets these aspects. Specifically, GARAG is designed to reveal
vulnerabilities within each component and test the overall system functionality
against noisy documents. We validate RAG robustness by applying our
\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and
LLMs. The experimental results show that GARAG consistently achieves high
attack success rates. Also, it significantly devastates the performance of each
component and their synergy, highlighting the substantial risk that minor
textual inaccuracies pose in disrupting RAG systems in the real world.",Sukmin Cho
2024-04-03T11:37:01Z,http://arxiv.org/abs/2404.15317v1,Concept-Guided LLM Agents for Human-AI Safety Codesign,"Generative AI is increasingly important in software engineering, including
safety engineering, where its use ensures that software does not cause harm to
people. This also leads to high quality requirements for generative AI.
Therefore, the simplistic use of Large Language Models (LLMs) alone will not
meet these quality demands. It is crucial to develop more advanced and
sophisticated approaches that can effectively address the complexities and
safety concerns of software systems. Ultimately, humans must understand and
take responsibility for the suggestions provided by generative AI to ensure
system safety. To this end, we present an efficient, hybrid strategy to
leverage LLMs for safety analysis and Human-AI codesign. In particular, we
develop a customized LLM agent that uses elements of prompt engineering,
heuristic reasoning, and retrieval-augmented generation to solve tasks
associated with predefined safety concepts, in interaction with a system model
graph. The reasoning is guided by a cascade of micro-decisions that help
preserve structured information. We further suggest a graph verbalization which
acts as an intermediate representation of the system model to facilitate
LLM-graph interactions. Selected pairs of prompts and responses relevant for
safety analytics illustrate our method for the use case of a simplified
automated driving system.",Florian Geissler
2024-04-24T18:38:11Z,http://arxiv.org/abs/2404.16130v1,"From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization","The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as ""What are the main themes in the dataset?"", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities
of text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose a Graph RAG approach to question answering over
private text corpora that scales with both the generality of user questions and
the quantity of source text to be indexed. Our approach uses an LLM to build a
graph-based text index in two stages: first to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely-related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that Graph RAG
leads to substantial improvements over a na\""ive RAG baseline for both the
comprehensiveness and diversity of generated answers. An open-source,
Python-based implementation of both global and local Graph RAG approaches is
forthcoming at https://aka.ms/graphrag.",Darren Edge
2024-04-27T13:11:42Z,http://arxiv.org/abs/2404.17897v1,"Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented
  Large Language Models","Large-scale language models (LLMs) have achieved remarkable success across
various language tasks but suffer from hallucinations and temporal
misalignment. To mitigate these shortcomings, Retrieval-augmented generation
(RAG) has been utilized to provide external knowledge to facilitate the answer
generation. However, applying such models to the medical domain faces several
challenges due to the lack of domain-specific knowledge and the intricacy of
real-world scenarios. In this study, we explore LLMs with RAG framework for
knowledge-intensive tasks in the medical field. To evaluate the capabilities of
LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates
the real-world medication consultation scenario and requires LLMs to answer
with retrieved evidence from the medicine database. MedicineQA contains 300
multi-round question-answering pairs, each embedded within a detailed dialogue
history, highlighting the challenge posed by this knowledge-intensive task to
current LLMs. We further propose a new \textit{Distill-Retrieve-Read} framework
instead of the previous \textit{Retrieve-then-Read}. Specifically, the
distillation and retrieval process utilizes a tool calling mechanism to
formulate search queries that emulate the keyword-based inquiries used by
search engines. With experimental results, we show that our framework brings
notable performance improvements and surpasses the previous counterparts in the
evidence retrieval process in terms of evidence retrieval accuracy. This
advancement sheds light on applying RAG to the medical domain.",Zhongzhen Huang
2024-04-28T05:46:28Z,http://arxiv.org/abs/2404.18077v2,"Generative AI for Low-Carbon Artificial Intelligence of Things with
  Large Language Models","By integrating Artificial Intelligence (AI) with the Internet of Things
(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.
However, AIoT is facing the challenges of energy consumption and carbon
emissions due to the continuous advancement of mobile technology. Fortunately,
Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT
due to its excellent reasoning and generation capabilities. In this article, we
explore the potential of GAI for carbon emissions reduction and propose a novel
GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main
impacts that cause carbon emissions in AIoT, and then introduce GAI techniques
and their relations to carbon emissions. We then explore the application
prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon
emissions of network components. Subsequently, we propose a Large Language
Model (LLM)-enabled carbon emission optimization framework, in which we design
pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more
accurate and reliable optimization problems. Furthermore, we utilize Generative
Diffusion Models (GDMs) to identify optimal strategies for carbon emission
reduction. Numerical results demonstrate the effectiveness of the proposed
framework. Finally, we insightfully provide open research directions for
low-carbon AIoT.",Jinbo Wen
2024-04-29T07:11:39Z,http://arxiv.org/abs/2404.18470v2,"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls
  using Large Language Model for Stock Performance Prediction","In the realm of financial analytics, leveraging unstructured data, such as
earnings conference calls (ECCs), to forecast stock volatility is a critical
challenge that has attracted both academics and investors. While previous
studies have used multimodal deep learning-based models to obtain a general
view of ECCs for volatility predicting, they often fail to capture detailed,
complex information. Our research introduces a novel framework: \textbf{ECC
Analyzer}, which utilizes large language models (LLMs) to extract richer, more
predictive content from ECCs to aid the model's prediction performance. We use
the pre-trained large models to extract textual and audio features from ECCs
and implement a hierarchical information extraction strategy to extract more
fine-grained information. This strategy first extracts paragraph-level general
information by summarizing the text and then extracts fine-grained focus
sentences using Retrieval-Augmented Generation (RAG). These features are then
fused through multimodal feature fusion to perform volatility prediction.
Experimental results demonstrate that our model outperforms traditional
analytical benchmarks, confirming the effectiveness of advanced LLM techniques
in financial analysis.",Yupeng Cao
2024-04-30T03:29:30Z,http://arxiv.org/abs/2404.19232v7,"GRAMMAR: Grounded and Modular Methodology for Assessment of
  Closed-Domain Retrieval-Augmented Language Model","Retrieval-Augmented Generation (RAG) systems are widely used across various
industries for querying closed-domain and in-house knowledge bases. However,
evaluating these systems presents significant challenges due to the private
nature of closed-domain data and a scarcity of queries with verifiable ground
truths. Moreover, there is a lack of analytical methods to diagnose problematic
modules and identify types of failure, such as those caused by knowledge
deficits or issues with robustness. To address these challenges, we introduce
GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation
framework comprising a grounded data generation process and an evaluation
protocol that effectively pinpoints defective modules. Our validation
experiments reveal that GRAMMAR provides a reliable approach for identifying
vulnerable modules and supports hypothesis testing for textual form
vulnerabilities. An open-source tool accompanying this framework is available
in our GitHub repository (see https://github.com/xinzhel/grammar), allowing for
easy reproduction of our results and enabling reliable and modular evaluation
in closed-domain settings.",Xinzhe Li
2024-04-30T13:14:51Z,http://arxiv.org/abs/2404.19543v1,"RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural
  Language Processing","Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.",Yucheng Hu
2024-04-30T17:44:44Z,http://arxiv.org/abs/2404.19744v1,"PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for
  Privacy Policy Compliance Verification","Data protection and privacy is becoming increasingly crucial in the digital
era. Numerous companies depend on third-party vendors and service providers to
carry out critical functions within their operations, encompassing tasks such
as data handling and storage. However, this reliance introduces potential
vulnerabilities, as these vendors' security measures and practices may not
always align with the standards expected by regulatory bodies. Businesses are
required, often under the penalty of law, to ensure compliance with the
evolving regulatory rules. Interpreting and implementing these regulations pose
challenges due to their complexity. Regulatory documents are extensive,
demanding significant effort for interpretation, while vendor-drafted privacy
policies often lack the detail required for full legal compliance, leading to
ambiguity. To ensure a concise interpretation of the regulatory requirements
and compliance of organizational privacy policy with said regulations, we
propose a Large Language Model (LLM) and Semantic Web based approach for
privacy compliance. In this paper, we develop the novel Privacy Policy
Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to
efficiently store and retrieve comprehensive information concerning privacy
policies, regulatory frameworks, and domain-specific knowledge pertaining to
the legal landscape of privacy. Using Retrieval Augmented Generation, we
identify the relevant sections in a privacy policy with corresponding
regulatory rules. This information about individual privacy policies is
populated into the PrivComp-KG. Combining this with the domain context and
rules, the PrivComp-KG can be queried to check for compliance with privacy
policies by each vendor against relevant policy regulations. We demonstrate the
relevance of the PrivComp-KG, by verifying compliance of privacy policy
documents for various organizations.",Leon Garza
2024-05-01T12:01:39Z,http://arxiv.org/abs/2405.00465v3,BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine,"Large Language Models (LLMs) have swiftly emerged as vital resources for
different applications in the biomedical and healthcare domains; however, these
models encounter issues such as generating inaccurate information or
hallucinations. Retrieval-augmented generation provided a solution for these
models to update knowledge and enhance their performance. In contrast to
previous retrieval-augmented LMs, which utilize specialized cross-attention
mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler
approach by directly inputting the retrieved chunk-based documents into the
LLM. This straightforward design is easily applicable to existing retrieval and
language models, effectively bypassing noise information in retrieved
documents, particularly in noise-intensive tasks. Moreover, we demonstrate the
potential for utilizing the LLM to supervise the retrieval model in the
biomedical domain, enabling it to retrieve the document that assists the LM in
improving its predictions. Our experiments reveal that with the tuned
scorer,\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP
tasks, encompassing information extraction (triple extraction, relation
extraction), text classification, link prediction, and question-answering,
leveraging over 9 datasets. For instance, in the triple extraction task,
\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1
scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.",Mingchen Li
2024-05-02T14:19:25Z,http://arxiv.org/abs/2405.01310v1,"Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf
  Disease Remediation","This research introduces an innovative AI-driven precision agriculture
system, leveraging YOLOv8 for disease identification and Retrieval Augmented
Generation (RAG) for context-aware diagnosis. Focused on addressing the
challenges of diseases affecting the coffee production sector in Karnataka, The
system integrates sophisticated object detection techniques with language
models to address the inherent constraints associated with Large Language
Models (LLMs). Our methodology not only tackles the issue of hallucinations in
LLMs, but also introduces dynamic disease identification and remediation
strategies. Real-time monitoring, collaborative dataset expansion, and
organizational involvement ensure the system's adaptability in diverse
agricultural settings. The effect of the suggested system extends beyond
automation, aiming to secure food supplies, protect livelihoods, and promote
eco-friendly farming practices. By facilitating precise disease identification,
the system contributes to sustainable and environmentally conscious
agriculture, reducing reliance on pesticides. Looking to the future, the
project envisions continuous development in RAG-integrated object detection
systems, emphasizing scalability, reliability, and usability. This research
strives to be a beacon for positive change in agriculture, aligning with global
efforts toward sustainable and technologically enhanced food production.",Selva Kumar S
2024-05-06T00:18:43Z,http://arxiv.org/abs/2405.03085v1,"Compressing Long Context for Enhancing RAG with AMR-based Concept
  Distillation","Large Language Models (LLMs) have made significant strides in information
acquisition. However, their overreliance on potentially flawed parametric
knowledge leads to hallucinations and inaccuracies, particularly when handling
long-tail, domain-specific queries. Retrieval Augmented Generation (RAG)
addresses this limitation by incorporating external, non-parametric knowledge.
Nevertheless, the retrieved long-context documents often contain noisy,
irrelevant information alongside vital knowledge, negatively diluting LLMs'
attention. Inspired by the supportive role of essential concepts in
individuals' reading comprehension, we propose a novel concept-based RAG
framework with the Abstract Meaning Representation (AMR)-based concept
distillation algorithm. The proposed algorithm compresses the cluttered raw
retrieved documents into a compact set of crucial concepts distilled from the
informative nodes of AMR by referring to reliable linguistic features. The
concepts explicitly constrain LLMs to focus solely on vital information in the
inference process. We conduct extensive experiments on open-domain
question-answering datasets to empirically evaluate the proposed method's
effectiveness. The results indicate that the concept-based RAG framework
outperforms other baseline methods, particularly as the number of supporting
documents increases, while also exhibiting robustness across various backbone
LLMs. This emphasizes the distilled concepts are informative for augmenting the
RAG process by filtering out interference information. To the best of our
knowledge, this is the first work introducing AMR to enhance the RAG,
presenting a potential solution to augment inference performance with
semantic-based context compression.",Kaize Shi
2024-05-06T02:35:10Z,http://arxiv.org/abs/2405.03122v2,"Automatic Retrieval-augmented Generation of 6G Network Specifications
  for Use Cases","6G Open Radio Access Networks (O-RAN) promises to open data interfaces to
enable plug-and-play service Apps, many of which are consumer and
business-facing. Opening up 6G access lowers the barrier to innovation but
raises the challenge that the required communication specifications are not
fully known to all service designers. As such, business innovators must either
be familiar with 6G standards or consult with experts. Enabling consistent,
unbiased, rapid, and low-cost requirement assessment and specification
generation is crucial to the O-RAN innovation ecosystem.
  Here, we discuss our initiative to bridge service specification gaps between
network service providers and business innovators leveraging Large Language
Models (LLMs). We first review the state-of-the-art and motivation in 6G
plug-and-play services, capabilities, potential use cases and LLMs. We identify
an ample innovation space for hybrid use cases that may require diverse and
variational wireless functionalities across its operating time. We show that
the network specification can be automated and present the first automatic
retrieval-augmented network service specification framework for 6G use cases.
To enable public acceptance and feedback, a website interface is published for
the research and industrial community to experiment with the framework. We hope
this review highlights the need for emerging foundation models for this area
and motivates researcher engagement and contribution to the community through
our framework.",Yun Tang
2024-05-06T20:50:17Z,http://arxiv.org/abs/2405.03845v1,Self-Improving Customer Review Response Generation Based on LLMs,"Previous studies have demonstrated that proactive interaction with user
reviews has a positive impact on the perception of app users and encourages
them to submit revised ratings. Nevertheless, developers encounter challenges
in managing a high volume of reviews, particularly in the case of popular apps
with a substantial influx of daily reviews. Consequently, there is a demand for
automated solutions aimed at streamlining the process of responding to user
reviews. To address this, we have developed a new system for generating
automatic responses by leveraging user-contributed documents with the help of
retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
Our solution, named SCRABLE, represents an adaptive customer review response
automation that enhances itself with self-optimizing prompts and a judging
mechanism based on LLMs. Additionally, we introduce an automatic scoring
mechanism that mimics the role of a human evaluator to assess the quality of
responses generated in customer review domains. Extensive experiments and
analyses conducted on real-world datasets reveal that our method is effective
in producing high-quality responses, yielding improvement of more than 8.5%
compared to the baseline. Further validation through manual examination of the
generated responses underscores the efficacy our proposed system.",Guy Azov
2024-05-07T02:49:59Z,http://arxiv.org/abs/2405.03963v4,ERATTA: Extreme RAG for Table To Answers with Large Language Models,"Large language models (LLMs) with retrieval augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. Although RAG implemented with AI agents (agentic-RAG) has been recently
popularized, its suffers from unstable cost and unreliable performances for
Enterprise-level data-practices. Most existing use-cases that incorporate RAG
with LLMs have been either generic or extremely domain specific, thereby
questioning the scalability and generalizability of RAG-LLM approaches. In this
work, we propose a unique LLM-based system where multiple LLMs can be invoked
to enable data authentication, user-query routing, data-retrieval and custom
prompting for question-answering capabilities from Enterprise-data tables. The
source tables here are highly fluctuating and large in size and the proposed
framework enables structured responses in under 10 seconds per query.
Additionally, we propose a five metric scoring module that detects and reports
hallucinations in the LLM responses. Our proposed system and scoring metrics
achieve >90% confidence scores across hundreds of user queries in the
sustainability, financial health and social media domains. Extensions to the
proposed extreme RAG architectures can enable heterogeneous source querying
using LLMs.",Sohini Roychowdhury
2024-05-07T17:59:31Z,http://arxiv.org/abs/2405.04533v1,"ChatHuman: Language-driven 3D Human Understanding with
  Retrieval-Augmented Tool Reasoning","Numerous methods have been proposed to detect, estimate, and analyze
properties of people in images, including the estimation of 3D pose, shape,
contact, human-object interaction, emotion, and more. Each of these methods
works in isolation instead of synergistically. Here we address this problem and
build a language-driven human understanding system -- ChatHuman, which combines
and integrates the skills of many different methods. To do so, we finetune a
Large Language Model (LLM) to select and use a wide variety of existing tools
in response to user inputs. In doing so, ChatHuman is able to combine
information from multiple tools to solve problems more accurately than the
individual tools themselves and to leverage tool output to improve its ability
to reason about humans. The novel features of ChatHuman include leveraging
academic publications to guide the application of 3D human-related tools,
employing a retrieval-augmented generation model to generate
in-context-learning examples for handling new tools, and discriminating and
integrating tool results to enhance 3D human understanding. Our experiments
show that ChatHuman outperforms existing models in both tool selection accuracy
and performance across multiple 3D human-related tasks. ChatHuman is a step
towards consolidating diverse methods for human analysis into a single,
powerful, system for 3D human reasoning.",Jing Lin
2024-05-07T21:14:38Z,http://arxiv.org/abs/2405.04674v1,"Towards Accurate and Efficient Document Analytics with Large Language
  Models","Unstructured data formats account for over 80% of the data currently stored,
and extracting value from such formats remains a considerable challenge. In
particular, current approaches for managing unstructured documents do not
support ad-hoc analytical queries on document collections. Moreover, Large
Language Models (LLMs) directly applied to the documents themselves, or on
portions of documents through a process of Retrieval-Augmented Generation
(RAG), fail to provide high accuracy query results, and in the LLM-only case,
additionally incur high costs. Since many unstructured documents in a
collection often follow similar templates that impart a common semantic
structure, we introduce ZenDB, a document analytics system that leverages this
semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document
collections. ZenDB efficiently extracts semantic hierarchical structures from
such templatized documents, and introduces a novel query engine that leverages
these structures for accurate and cost-effective query execution. Users can
impose a schema on their documents, and query it, all via SQL. Extensive
experiments on three real-world document collections demonstrate ZenDB's
benefits, achieving up to 30% cost savings compared to LLM-based baselines,
while maintaining or improving accuracy, and surpassing RAG-based baselines by
up to 61% in precision and 80% in recall, at a marginally higher cost.",Yiming Lin
2024-05-08T22:23:58Z,http://arxiv.org/abs/2405.05444v1,"Evaluating Students' Open-ended Written Responses with LLMs: Using the
  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large","Evaluating open-ended written examination responses from students is an
essential yet time-intensive task for educators, requiring a high degree of
effort, consistency, and precision. Recent developments in Large Language
Models (LLMs) present a promising opportunity to balance the need for thorough
evaluation with efficient use of educators' time. In our study, we explore the
effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in
assessing university students' open-ended answers to questions made about
reference material they have studied. Each model was instructed to evaluate 54
answers repeatedly under two conditions: 10 times (10-shot) with a temperature
setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of
1,080 evaluations per model and 4,320 evaluations across all models. The RAG
(Retrieval Augmented Generation) framework was used as the framework to make
the LLMs to process the evaluation of the answers. As of spring 2024, our
analysis revealed notable variations in consistency and the grading outcomes
provided by studied LLMs. There is a need to comprehend strengths and
weaknesses of LLMs in educational settings for evaluating open-ended written
responses. Further comparative research is essential to determine the accuracy
and cost-effectiveness of using LLMs for educational assessments.",Jussi S. Jauhiainen
2024-05-09T12:58:22Z,http://arxiv.org/abs/2405.05741v1,Can large language models understand uncommon meanings of common words?,"Large language models (LLMs) like ChatGPT have shown significant advancements
across diverse natural language understanding (NLU) tasks, including
intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged
testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely
comprehend the world' remains unclear, fostering numerous studies and sparking
heated debates. Prevailing research mainly focuses on surface-level NLU,
neglecting fine-grained explorations. However, such explorations are crucial
for understanding their unique comprehension mechanisms, aligning with human
cognition, and finally enhancing LLMs' general NLU capacities. To address this
gap, our study delves into LLMs' nuanced semantic comprehension capabilities,
particularly regarding common words with uncommon meanings. The idea stems from
foundational principles of human communication within psychology, which
underscore accurate shared understandings of word semantics. Specifically, this
paper presents the innovative construction of a Lexical Semantic Comprehension
(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing
both fine-grained and cross-lingual dimensions. Introducing models of both
open-source and closed-source, varied scales and architectures, our extensive
empirical experiments demonstrate the inferior performance of existing models
in this basic lexical-meaning understanding task. Notably, even the
state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%
and 22.3%, respectively. Additionally, multiple advanced prompting techniques
and retrieval-augmented generation are also introduced to help alleviate this
trouble, yet limitations persist. By highlighting the above critical
shortcomings, this research motivates further investigation and offers novel
insights for developing more intelligent LLMs.",Jinyang Wu
2024-05-06T04:42:18Z,http://arxiv.org/abs/2405.06683v1,"ERAGent: Enhancing Retrieval-Augmented Language Models with Improved
  Accuracy, Efficiency, and Personalization","Retrieval-augmented generation (RAG) for language models significantly
improves language understanding systems. The basic retrieval-then-read pipeline
of response generation has evolved into a more extended process due to the
integration of various components, sometimes even forming loop structures.
Despite its advancements in improving response accuracy, challenges like poor
retrieval quality for complex questions that require the search of multifaceted
semantic information, inefficiencies in knowledge re-retrieval during long-term
serving, and lack of personalized responses persist. Motivated by transcending
these limitations, we introduce ERAGent, a cutting-edge framework that embodies
an advancement in the RAG area. Our contribution is the introduction of the
synergistically operated module: Enhanced Question Rewriter and Knowledge
Filter, for better retrieval quality. Retrieval Trigger is incorporated to
curtail extraneous external knowledge retrieval without sacrificing response
quality. ERAGent also personalizes responses by incorporating a learned user
profile. The efficiency and personalization characteristics of ERAGent are
supported by the Experiential Learner module which makes the AI assistant being
capable of expanding its knowledge and modeling user profile incrementally.
Rigorous evaluations across six datasets and three question-answering tasks
prove ERAGent's superior accuracy, efficiency, and personalization, emphasizing
its potential to advance the RAG field and its applicability in practical
systems.",Yunxiao Shi
2024-05-21T13:02:27Z,http://arxiv.org/abs/2405.12750v1,"Generative AI and Large Language Models for Cyber Security: All Insights
  You Need","This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.",Mohamed Amine Ferrag
2024-05-20T11:05:56Z,http://arxiv.org/abs/2405.13057v1,Can Github issues be solved with Tree Of Thoughts?,"While there have been extensive studies in code generation by large language
models (LLM), where benchmarks like HumanEval have been surpassed with an
impressive 96.3% success rate, these benchmarks predominantly judge a model's
performance on basic function-level code generation and lack the critical
thinking and concept of scope required of real-world scenarios such as solving
GitHub issues. This research introduces the application of the Tree of Thoughts
(ToT) language model reasoning framework for enhancing the decision-making and
problem-solving abilities of LLMs for this complex task. Compared to
traditional input-output (IO) prompting and Retrieval Augmented Generation
(RAG) techniques, ToT is designed to improve performance by facilitating a
structured exploration of multiple reasoning trajectories and enabling
self-assessment of potential solutions. We experimentally deploy ToT in
tackling a Github issue contained within an instance of the SWE-bench. However,
our results reveal that the ToT framework alone is not enough to give LLMs the
critical reasoning capabilities to outperform existing methods. In this paper
we analyze the potential causes of these shortcomings and identify key areas
for improvement such as deepening the thought process and introducing agentic
capabilities. The insights of this research are aimed at informing future
directions for refining the application of ToT and better harnessing the
potential of LLMs in real-world problem-solving scenarios.",Ricardo La Rosa
2024-05-23T15:37:06Z,http://arxiv.org/abs/2405.14702v2,"G3: An Effective and Adaptive Framework for Worldwide Geolocalization
  Using Large Multi-Modality Models","Worldwide geolocalization aims to locate the precise location at the
coordinate level of photos taken anywhere on the Earth. It is very challenging
due to 1) the difficulty of capturing subtle location-aware visual semantics,
and 2) the heterogeneous geographical distribution of image data. As a result,
existing studies have clear limitations when scaled to a worldwide context.
They may easily confuse distant images with similar visual contents, or cannot
adapt to various locations worldwide with different amounts of relevant data.
To resolve these limitations, we propose G3, a novel framework based on
Retrieval-Augmented Generation (RAG). In particular, G3 consists of three
steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to
optimize both retrieval and generation phases of worldwide geolocalization.
During Geo-alignment, our solution jointly learns expressive multi-modal
representations for images, GPS and textual descriptions, which allows us to
capture location-aware semantics for retrieving nearby images for a given
query. During Geo-diversification, we leverage a prompt ensembling method that
is robust to inconsistent retrieval performance for different image queries.
Finally, we combine both retrieved and generated GPS candidates in
Geo-verification for location prediction. Experiments on two well-established
datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other
state-of-the-art methods. Our code and data are available online for
reproduction.",Pengyue Jia
2024-05-23T17:47:55Z,http://arxiv.org/abs/2405.14831v2,"HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language
  Models","In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",Bernal Jiménez Gutiérrez
2024-05-25T11:10:04Z,http://arxiv.org/abs/2405.16178v1,"Accelerating Inference of Retrieval-Augmented Generation via Sparse
  Context Selection","Large language models (LLMs) augmented with retrieval exhibit robust
performance and extensive versatility by incorporating external contexts.
However, the input length grows linearly in the number of retrieved documents,
causing a dramatic increase in latency. In this paper, we propose a novel
paradigm named Sparse RAG, which seeks to cut computation costs through
sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel,
which eliminates latency introduced by long-range attention of retrieved
documents. Then, LLMs selectively decode the output by only attending to highly
relevant caches auto-regressively, which are chosen via prompting LLMs with
special control tokens. It is notable that Sparse RAG combines the assessment
of each individual document and the generation of the response into a single
process. The designed sparse mechanism in a RAG system can facilitate the
reduction of the number of documents loaded during decoding for accelerating
the inference of the RAG system. Additionally, filtering out undesirable
contexts enhances the model's focus on relevant context, inherently improving
its generation quality. Evaluation results of two datasets show that Sparse RAG
can strike an optimal balance between generation quality and computational
efficiency, demonstrating its generalizability across both short- and long-form
generation tasks.",Yun Zhu
2024-05-28T16:56:42Z,http://arxiv.org/abs/2405.18359v1,"Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual
  Performance in LLMs","Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs without
extensive training or fine-tuning. Through systematic investigation and
evaluation of diverse languages using popular question-answering (QA) datasets,
we present novel techniques that unlock the true potential of LLMs in a
polyglot landscape. Our approach encompasses three key strategies that yield
significant improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval
Augmented Generation (RAG) with multilingual embeddings and achieves improved
multilingual task performance. Finally, we introduce a novel learning approach
that dynamically selects the optimal prompt strategy, LLM model, and embedding
model per query at run-time. This dynamic adaptation maximizes the efficacy of
LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online
settings, and can seamlessly adapt to new languages and datasets, leading to
substantial advancements in multilingual understanding and generation across
diverse languages.",Somnath Kumar
2024-05-30T15:14:24Z,http://arxiv.org/abs/2405.20139v1,GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning,"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form
of triplets (head, relation, tail), which collectively form a graph. Question
Answering over KGs (KGQA) is the task of answering natural questions grounding
the reasoning to the information provided by the KG. Large Language Models
(LLMs) are the state-of-the-art models for QA tasks due to their remarkable
ability to understand natural language. On the other hand, Graph Neural
Networks (GNNs) have been widely used for KGQA as they can handle the complex
graph information stored in the KG. In this work, we introduce GNN-RAG, a novel
method for combining language understanding abilities of LLMs with the
reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for
a given question. Second, the shortest paths in the KG that connect question
entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with
RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to
extract useful graph information, while the LLM leverages its natural language
processing ability for ultimate KGQA. Furthermore, we develop a retrieval
augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in
two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching
GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop
and multi-entity questions outperforming competing approaches by 8.9--15.5%
points at answer F1.",Costas Mavromatis
2024-05-30T18:00:21Z,http://arxiv.org/abs/2405.20389v1,"Designing an Evaluation Framework for Large Language Models in Astronomy
  Research","Large Language Models (LLMs) are shifting how scientific research is done. It
is imperative to understand how researchers interact with these models and how
scientific sub-communities like astronomy might benefit from them. However,
there is currently no standard for evaluating the use of LLMs in astronomy.
Therefore, we present the experimental design for an evaluation study on how
astronomy researchers interact with LLMs. We deploy a Slack chatbot that can
answer queries from users via Retrieval-Augmented Generation (RAG); these
responses are grounded in astronomy papers from arXiv. We record and anonymize
user questions and chatbot answers, user upvotes and downvotes to LLM
responses, user feedback to the LLM, and retrieved documents and similarity
scores with the query. Our data collection method will enable future dynamic
evaluations of LLM tools for astronomy.",John F. Wu
2024-05-30T20:05:44Z,http://arxiv.org/abs/2405.20455v5,"DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency
  Management","In the era of Large Language Models (LLMs) with their advanced capabilities,
a unique opportunity arises to develop LLM-based digital assistant tools that
can support software developers by facilitating comprehensive reasoning about
software dependencies and open-source libraries before importing them. This
reasoning process is daunting, mandating multiple specialized tools and
dedicated expertise, each focusing on distinct aspects (e.g., security analysis
tools may overlook design flaws such as circular dependencies, which hinder
software maintainability). Creating a significant bottleneck in the software
development lifecycle. In this paper, we introduce DepsRAG, a multi-agent
framework designed to assist developers in reasoning about software
dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG)
that includes both direct and transitive dependencies. Developers can interact
with DepsRAG through a conversational interface, posing queries about the
dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance
these queries by retrieving relevant information from the KG as well as
external sources, such as the Web and vulnerability databases, thus
demonstrating its adaptability to novel scenarios. DepsRAG incorporates a
Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated
responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three
multi-step reasoning tasks, observing a threefold increase in accuracy with the
integration of the Critic-Agent mechanism. DepsRAG demo and implementation are
available: https://github.com/Mohannadcse/DepsRAG.",Mohannad Alhanahnah
2024-05-30T21:19:24Z,http://arxiv.org/abs/2405.20485v2,"Phantom: General Trigger Attacks on Retrieval Augmented Language
  Generation","Retrieval Augmented Generation (RAG) expands the capabilities of modern large
language models (LLMs), by anchoring, adapting, and personalizing their
responses to the most relevant knowledge sources. It is particularly useful in
chatbot applications, allowing developers to customize LLM output without
expensive retraining. Despite their significant utility in various
applications, RAG systems present new security risks. In this work, we propose
new attack vectors that allow an adversary to inject a single malicious
document into a RAG system's knowledge base, and mount a backdoor poisoning
attack. We design Phantom, a general two-stage optimization framework against
RAG systems, that crafts a malicious poisoned document leading to an integrity
violation in the model's output. First, the document is constructed to be
retrieved only when a specific trigger sequence of tokens appears in the
victim's queries. Second, the document is further optimized with crafted
adversarial text that induces various adversarial objectives on the LLM output,
including refusal to answer, reputation damage, privacy violations, and harmful
behaviors. We demonstrate our attacks on multiple LLM architectures, including
Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and
GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's
black-box production RAG system, ""Chat with RTX"".",Harsh Chaudhari
2024-05-31T14:23:49Z,http://arxiv.org/abs/2405.20834v1,"Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits
  Multimodal Reasoning","Large language models equipped with retrieval-augmented generation (RAG)
represent a burgeoning field aimed at enhancing answering capabilities by
leveraging external knowledge bases. Although the application of RAG with
language-only models has been extensively explored, its adaptation into
multimodal vision-language models remains nascent. Going beyond mere answer
generation, the primary goal of multimodal RAG is to cultivate the models'
ability to reason in response to relevant queries. To this end, we introduce a
novel multimodal RAG framework named RMR (Retrieval Meets Reasoning). The RMR
framework employs a bi-modal retrieval module to identify the most relevant
question-answer pairs, which then serve as scaffolds for the multimodal
reasoning process. This training-free approach not only encourages the model to
engage deeply with the reasoning processes inherent in the retrieved content
but also facilitates the generation of answers that are precise and richly
interpretable. Surprisingly, utilizing solely the ScienceQA dataset, collected
from elementary and high school science curricula, RMR significantly boosts the
performance of various vision-language models across a spectrum of benchmark
datasets, including A-OKVQA, MMBench, and SEED. These outcomes highlight the
substantial potential of our multimodal retrieval and reasoning mechanism to
improve the reasoning capabilities of vision-language models.",Cheng Tan
2024-05-31T16:24:53Z,http://arxiv.org/abs/2405.20978v1,"Enhancing Noise Robustness of Retrieval-Augmented Language Models with
  Adaptive Adversarial Training","Large Language Models (LLMs) exhibit substantial capabilities yet encounter
challenges, including hallucination, outdated knowledge, and untraceable
reasoning processes. Retrieval-augmented generation (RAG) has emerged as a
promising solution, integrating knowledge from external databases to mitigate
these challenges. However, inappropriate retrieved passages can potentially
hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine
themselves to a limited set of noise types, deviating from real-world retrieval
environments and limiting practical applicability. In this study, we initially
investigate retrieval noises and categorize them into three distinct types,
reflecting real-world environments. We analyze the impact of these various
retrieval noises on the robustness of LLMs. Subsequently, we propose a novel
RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's
training process in response to retrieval noises. Concurrently, it employs
multi-task learning to ensure the model's capacity to internally recognize
noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model
trained using RAAT exhibits significant improvements in F1 and EM scores under
diverse noise conditions. For reproducibility, we release our code and data at:
https://github.com/calubkk/RAAT.",Feiteng Fang
2024-05-24T20:03:32Z,http://arxiv.org/abs/2406.00031v1,"AMGPT: a Large Language Model for Contextual Querying in Additive
  Manufacturing","Generalized large language models (LLMs) such as GPT-4 may not provide
specific answers to queries formulated by materials science researchers. These
models may produce a high-level outline but lack the capacity to return
detailed instructions on manufacturing and material properties of novel alloys.
Enhancing a smaller model with specialized domain knowledge may provide an
advantage over large language models which cannot be retrained quickly enough
to keep up with the rapid pace of research in metal additive manufacturing
(AM). We introduce ""AMGPT,"" a specialized LLM text generator designed for metal
AM queries. The goal of AMGPT is to assist researchers and users in navigating
the extensive corpus of literature in AM. Instead of training from scratch, we
employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented
Generation (RAG) setup, utilizing it to dynamically incorporate information
from $\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert
these PDF documents into TeX format, facilitating their integration into the
RAG pipeline managed by LlamaIndex. Expert evaluations of this project
highlight that specific embeddings from the RAG setup accelerate response times
and maintain coherence in the generated text.",Achuth Chandrasekhar
2024-05-27T10:53:15Z,http://arxiv.org/abs/2406.00036v1,EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling,"The integration of multimodal Electronic Health Records (EHR) data has
notably advanced clinical predictive capabilities. However, current models that
utilize clinical notes and multivariate time-series EHR data often lack the
necessary medical context for precise clinical tasks. Previous methods using
knowledge graphs (KGs) primarily focus on structured knowledge extraction. To
address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven
framework aimed at enhancing multimodal EHR predictive modeling. Our approach
extracts entities from both time-series data and clinical notes by prompting
Large Language Models (LLMs) and aligns them with professional PrimeKG to
ensure consistency. Beyond triplet relationships, we include entities'
definitions and descriptions to provide richer semantics. The extracted
knowledge is then used to generate task-relevant summaries of patients' health
statuses. These summaries are fused with other modalities utilizing an adaptive
multimodal fusion network with cross-attention. Extensive experiments on the
MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day
readmission tasks demonstrate the superior performance of the EMERGE framework
compared to baseline models. Comprehensive ablation studies and analyses
underscore the efficacy of each designed module and the framework's robustness
to data sparsity. EMERGE significantly enhances the use of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts crucial for
informed clinical predictions.",Yinghao Zhu
2024-05-29T18:19:46Z,http://arxiv.org/abs/2406.00057v2,"Toward Conversational Agents with Context and Time Sensitive Long-term
  Memory","There has recently been growing interest in conversational agents with
long-term memory which has led to the rapid development of language models that
use retrieval-augmented generation (RAG). Until recently, most work on RAG has
focused on information retrieval from large databases of texts, like Wikipedia,
rather than information from long-form conversations. In this paper, we argue
that effective retrieval from long-form conversational data faces two unique
problems compared to static database retrieval: 1) time/event-based queries,
which requires the model to retrieve information about previous conversations
based on time or the order of a conversational event (e.g., the third
conversation on Tuesday), and 2) ambiguous queries that require surrounding
conversational context to understand. To better develop RAG-based agents that
can deal with these challenges, we generate a new dataset of ambiguous and
time-based questions that build upon a recent dataset of long-form, simulated
conversations, and demonstrate that standard RAG based approaches handle such
questions poorly. We then develop a novel retrieval model which combines
chained-of-table search methods, standard vector-database retrieval, and a
prompting method to disambiguate queries, and demonstrate that this approach
substantially improves over current methods at solving these tasks. We believe
that this new dataset and more advanced RAG agent can act as a key benchmark
and stepping stone towards effective memory augmented conversational agents
that can be used in a wide variety of AI applications.",Nick Alonso
2024-06-03T15:26:06Z,http://arxiv.org/abs/2406.01428v2,"Superhuman performance in urology board questions by an explainable
  large language model enabled for context integration of the European
  Association of Urology guidelines: the UroBot study","Large Language Models (LLMs) are revolutionizing medical Question-Answering
(medQA) through extensive use of medical literature. However, their performance
is often hampered by outdated training data and a lack of explainability, which
limits clinical applicability. This study aimed to create and assess UroBot, a
urology-specialized chatbot, by comparing it with state-of-the-art models and
the performance of urologists on urological board questions, ensuring full
clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,
and GPT-4o models, employing retrieval-augmented generation (RAG) and the
latest 2023 guidelines from the European Association of Urology (EAU). The
evaluation included ten runs of 200 European Board of Urology (EBU) In-Service
Assessment (ISA) questions, with performance assessed by the mean Rate of
Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing
GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and
exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).
By comparison, the average performance of urologists on board questions, as
reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and
superior accuracy compared to both existing models and urologists on board
questions highlight its potential for clinical integration. The study also
provides the necessary code and instructions for further development of UroBot.",Martin J. Hetz
2024-06-04T08:36:39Z,http://arxiv.org/abs/2406.02110v1,"UniOQA: A Unified Framework for Knowledge Graph Question Answering with
  Large Language Models","OwnThink stands as the most extensive Chinese open-domain knowledge graph
introduced in recent times. Despite prior attempts in question answering over
OwnThink (OQA), existing studies have faced limitations in model representation
capabilities, posing challenges in further enhancing overall accuracy in
question answering. In this paper, we introduce UniOQA, a unified framework
that integrates two complementary parallel workflows. Unlike conventional
approaches, UniOQA harnesses large language models (LLMs) for precise question
answering and incorporates a direct-answer-prediction process as a
cost-effective complement. Initially, to bolster representation capacity, we
fine-tune an LLM to translate questions into the Cypher query language (CQL),
tackling issues associated with restricted semantic understanding and
hallucinations. Subsequently, we introduce the Entity and Relation Replacement
algorithm to ensure the executability of the generated CQL. Concurrently, to
augment overall accuracy in question answering, we further adapt the
Retrieval-Augmented Generation (RAG) process to the knowledge graph.
Ultimately, we optimize answer accuracy through a dynamic decision algorithm.
Experimental findings illustrate that UniOQA notably advances SpCQL Logical
Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new
state-of-the-art results on this benchmark. Through ablation experiments, we
delve into the superior representation capacity of UniOQA and quantify its
performance breakthrough.",Zhuoyang Li
2024-06-04T20:02:52Z,http://arxiv.org/abs/2406.02746v5,RATT: A Thought Structure for Coherent and Correct LLM Reasoning,"Large Language Models (LLMs) gain substantial reasoning and decision-making
capabilities from thought structures. However, existing methods such as Tree of
Thought and Retrieval Augmented Thoughts often fall short in complex tasks due
to the limitations of insufficient local retrieval of factual knowledge and
inadequate global selection of strategies. These limitations make it
challenging for these methods to balance factual accuracy and comprehensive
logical optimization effectively. To address these limitations, we introduce
the Retrieval Augmented Thought Tree (RATT), a novel thought structure that
considers both overall logical soundness and factual correctness at each step
of the thinking process. Specifically, at every point of a thought branch, RATT
performs planning and lookahead to explore and evaluate multiple potential
reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented
Generation (RAG) with LLM's ability to assess overall strategy. Through this
combination of factual knowledge and strategic feasibility, the RATT adjusts
and integrates the thought tree structure to search for the most promising
branches within the search space. This thought structure significantly enhances
the model's coherence in logical inference and efficiency in decision-making,
and thus increases the limit of the capacity of LLM to generate reliable
inferences and decisions based on thought structures. A broad range of
experiments on different types of tasks showcases that the RATT structure
significantly outperforms existing methods in factual correctness and logical
coherence.",Jinghan Zhang
2024-06-04T23:36:08Z,http://arxiv.org/abs/2406.02818v1,"Chain of Agents: Large Language Models Collaborating on Long-Context
  Tasks","Addressing the challenge of effectively processing long contexts has become a
critical issue for Large Language Models (LLMs). Two common strategies have
emerged: 1) reducing the input length, such as retrieving relevant chunks by
Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit
of LLMs. However, both strategies have drawbacks: input reduction has no
guarantee of covering the part with needed information, while window extension
struggles with focusing on the pertinent information for solving the task. To
mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework
that harnesses multi-agent collaboration through natural language to enable
information aggregation and context reasoning across various LLMs over
long-context tasks. CoA consists of multiple worker agents who sequentially
communicate to handle different segmented portions of the text, followed by a
manager agent who synthesizes these contributions into a coherent final output.
CoA processes the entire input by interleaving reading and reasoning, and it
mitigates long context focus issues by assigning each agent a short context. We
perform comprehensive evaluation of CoA on a wide range of long-context tasks
in question answering, summarization, and code completion, demonstrating
significant improvements by up to 10% over strong baselines of RAG,
Full-Context, and multi-agent LLMs.",Yusen Zhang
2024-06-06T11:14:27Z,http://arxiv.org/abs/2406.03963v1,"A + B: A General Generator-Reader Framework for Optimizing LLMs to
  Unleash Synergy Potential","Retrieval-Augmented Generation (RAG) is an effective solution to supplement
necessary knowledge to large language models (LLMs). Targeting its bottleneck
of retriever performance, ""generate-then-read"" pipeline is proposed to replace
the retrieval stage with generation from the LLM itself. Although promising,
this research direction is underexplored and still cannot work in the scenario
when source knowledge is given. In this paper, we formalize a general ""A + B""
framework with varying combinations of foundation models and types for
systematic investigation. We explore the efficacy of the base and chat versions
of LLMs and found their different functionalities suitable for generator A and
reader B, respectively. Their combinations consistently outperform single
models, especially in complex scenarios. Furthermore, we extend the application
of the ""A + B"" framework to scenarios involving source documents through
continuous learning, enabling the direct integration of external knowledge into
LLMs. This approach not only facilitates effective acquisition of new knowledge
but also addresses the challenges of safety and helpfulness post-adaptation.
The paper underscores the versatility of the ""A + B"" framework, demonstrating
its potential to enhance the practical application of LLMs across various
domains.",Wei Tang
2024-06-07T16:59:38Z,http://arxiv.org/abs/2406.05085v2,Multi-Head RAG: Solving Multi-Aspect Problems with LLMs,"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving motivation is that different attention
heads can learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, multi-aspect datasets that we release
online, and real-world use cases to demonstrate MRAG's effectiveness, showing
improvements of up to 20% in relevance over standard RAG baselines. MRAG can be
seamlessly integrated with existing RAG frameworks and benchmarking tools like
RAGAS as well as different classes of data stores.",Maciej Besta
2024-06-07T17:02:35Z,http://arxiv.org/abs/2406.05087v2,Corpus Poisoning via Approximate Greedy Gradient Descent,"Dense retrievers are widely used in information retrieval and have also been
successfully extended to other knowledge intensive areas such as language
models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they
have recently been shown to be vulnerable to corpus poisoning attacks in which
a malicious user injects a small fraction of adversarial passages into the
retrieval corpus to trick the system into returning these passages among the
top-ranked results for a broad set of user queries. Further study is needed to
understand the extent to which these attacks could limit the deployment of
dense retrievers in real-world applications. In this work, we propose
Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval
systems based on the widely used HotFlip method for efficiently generating
adversarial passages. We demonstrate that AGGD can select a higher quality set
of token-level perturbations than HotFlip by replacing its random token
sampling with a more structured search. Experimentally, we show that our method
achieves a high attack success rate on several datasets and using several
retrievers, and can generalize to unseen queries and new domains. Notably, our
method is extremely effective in attacking the ANCE retrieval model, achieving
attack success rates that are 15.24\% and 17.44\% higher on the NQ and MS MARCO
datasets, respectively, compared to HotFlip. Additionally, we demonstrate
AGGD's potential to replace HotFlip in other adversarial attacks, such as
knowledge poisoning of RAG systems.",Jinyan Su
2024-06-10T15:52:49Z,http://arxiv.org/abs/2406.06399v3,"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt
  LLMs for Dialogue","We study the limitations of Large Language Models (LLMs) for the task of
response generation in human-machine dialogue. Several techniques have been
proposed in the literature for different dialogue types (e.g., Open-Domain).
However, the evaluations of these techniques have been limited in terms of base
LLMs, dialogue types and evaluation metrics. In this work, we extensively
analyze different LLM adaptation techniques when applied to different dialogue
types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue
types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.
We evaluate the performance of in-context learning and fine-tuning techniques
across datasets selected for each dialogue type. We assess the impact of
incorporating external knowledge to ground the generation in both scenarios of
Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent
evaluation and explainability criteria for automatic metrics and human
evaluation protocols. Our analysis shows that there is no universal
best-technique for adapting large language models as the efficacy of each
technique depends on both the base LLM and the specific type of dialogue. Last
but not least, the assessment of the best adaptation technique should include
human evaluation to avoid false expectations and outcomes derived from
automatic metrics.",Simone Alghisi
2024-06-03T07:44:32Z,http://arxiv.org/abs/2406.06566v4,"Natural Language Interaction with a Household Electricity
  Knowledge-based Digital Twin","Domain specific digital twins, representing a digital replica of various
segments of the smart grid, are foreseen as able to model, simulate, and
control the respective segments. At the same time, knowledge-based digital
twins, coupled with AI, may also empower humans to understand aspects of the
system through natural language interaction in view of planning and policy
making. This paper is the first to assess and report on the potential of
Retrieval Augmented Generation (RAG) question answers related to household
electrical energy measurement aspects leveraging a knowledge-based energy
digital twin. Relying on the recently published electricity consumption
knowledge graph that actually represents a knowledge-based digital twin, we
study the capabilities of ChatGPT, Gemini and Llama in answering electricity
related questions. Furthermore, we compare the answers with the ones generated
through a RAG techniques that leverages an existing electricity knowledge-based
digital twin. Our findings illustrate that the RAG approach not only reduces
the incidence of incorrect information typically generated by LLMs but also
significantly improves the quality of the output by grounding responses in
verifiable data. This paper details our methodology, presents a comparative
analysis of responses with and without RAG, and discusses the implications of
our findings for future applications of AI in specialized sectors like energy
data analysis.",Carolina Fortuna
2024-06-03T17:07:46Z,http://arxiv.org/abs/2406.06572v2,Graph Neural Network Enhanced Retrieval for Question Answering of LLMs,"Retrieval augmented generation has revolutionized large language model (LLM)
outputs by providing factual supports. Nevertheless, it struggles to capture
all the necessary knowledge for complex reasoning questions. Existing retrieval
methods typically divide reference documents into passages, treating them in
isolation. These passages, however, are often interrelated, such as passages
that are contiguous or share the same keywords. Therefore, it is crucial to
recognize such relatedness for enhancing the retrieval process. In this paper,
we propose a novel retrieval method, called GNN-Ret, which leverages graph
neural networks (GNNs) to enhance retrieval by exploiting the relatedness
between passages. Specifically, we first construct a graph of passages by
connecting passages that are structure-related or keyword-related. A graph
neural network (GNN) is then leveraged to exploit the relationships between
passages and improve the retrieval of supporting passages. Furthermore, we
extend our method to handle multi-hop reasoning questions using a recurrent
graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates
the graphs of passages from previous steps, thereby enhancing the retrieval of
supporting passages. Extensive experiments on benchmark datasets demonstrate
that GNN-Ret achieves higher accuracy for question answering with a single
query of LLMs than strong baselines that require multiple queries, and RGNN-Ret
further improves accuracy and achieves state-of-the-art performance, with up to
10.4% accuracy improvement on the 2WikiMQA dataset.",Zijian Li
2024-06-04T08:34:19Z,http://arxiv.org/abs/2406.06577v1,"RAG-based Crowdsourcing Task Decomposition via Masked Contrastive
  Learning with Prompts","Crowdsourcing is a critical technology in social manufacturing, which
leverages an extensive and boundless reservoir of human resources to handle a
wide array of complex tasks. The successful execution of these complex tasks
relies on task decomposition (TD) and allocation, with the former being a
prerequisite for the latter. Recently, pre-trained language models (PLMs)-based
methods have garnered significant attention. However, they are constrained to
handling straightforward common-sense tasks due to their inherent restrictions
involving limited and difficult-to-update knowledge as well as the presence of
hallucinations. To address these issues, we propose a retrieval-augmented
generation-based crowdsourcing framework that reimagines TD as event detection
from the perspective of natural language understanding. However, the existing
detection methods fail to distinguish differences between event types and
always depend on heuristic rules and external semantic analyzing tools.
Therefore, we present a Prompt-Based Contrastive learning framework for TD
(PBCT), which incorporates a prompt-based trigger detector to overcome
dependence. Additionally, trigger-attentive sentinel and masked contrastive
learning are introduced to provide varying attention to trigger and contextual
features according to different event types. Experiment results demonstrate the
competitiveness of our method in both supervised and zero-shot detection. A
case study on printed circuit board manufacturing is showcased to validate its
adaptability to unknown professional domains.",Jing Yang
2024-05-09T18:15:12Z,http://arxiv.org/abs/2406.07561v1,"Artificial Intelligence as the New Hacker: Developing Agents for
  Offensive Security","In the vast domain of cybersecurity, the transition from reactive defense to
offensive has become critical in protecting digital infrastructures. This paper
explores the integration of Artificial Intelligence (AI) into offensive
cybersecurity, particularly through the development of an autonomous AI agent,
ReaperAI, designed to simulate and execute cyberattacks. Leveraging the
capabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI
demonstrates the potential to identify, exploit, and analyze security
vulnerabilities autonomously.
  This research outlines the core methodologies that can be utilized to
increase consistency and performance, including task-driven penetration testing
frameworks, AI-driven command generation, and advanced prompting techniques.
The AI agent operates within a structured environment using Python, enhanced by
Retrieval Augmented Generation (RAG) for contextual understanding and memory
retention. ReaperAI was tested on platforms including, Hack The Box, where it
successfully exploited known vulnerabilities, demonstrating its potential
power.
  However, the deployment of AI in offensive security presents significant
ethical and operational challenges. The agent's development process revealed
complexities in command execution, error handling, and maintaining ethical
constraints, highlighting areas for future enhancement.
  This study contributes to the discussion on AI's role in cybersecurity by
showcasing how AI can augment offensive security strategies. It also proposes
future research directions, including the refinement of AI interactions with
cybersecurity tools, enhancement of learning mechanisms, and the discussion of
ethical guidelines for AI in offensive roles. The findings advocate for a
unique approach to AI implementation in cybersecurity, emphasizing innovation.",Leroy Jacob Valencia
2024-06-13T16:10:19Z,http://arxiv.org/abs/2406.09272v3,"Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric
  Videos","Generating realistic audio for human actions is important for many
applications, such as creating sound effects for films or virtual reality
games. Existing approaches implicitly assume total correspondence between the
video and audio during training, yet many sounds happen off-screen and have
weak to no correspondence with the visuals -- resulting in uncontrolled ambient
sounds or hallucinations at test time. We propose a novel ambient-aware audio
generation model, AV-LDM. We devise a novel audio-conditioning mechanism to
learn to disentangle foreground action sounds from the ambient background
sounds in in-the-wild training videos. Given a novel silent video, our model
uses retrieval-augmented generation to create audio that matches the visual
content both semantically and temporally. We train and evaluate our model on
two in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we
introduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.
Our model outperforms an array of existing methods, allows controllable
generation of the ambient sound, and even shows promise for generalizing to
computer graphics game clips. Overall, our approach is the first to focus
video-to-audio generation faithfully on the observed visual content despite
training from uncurated clips with natural background sounds.",Changan Chen
2024-06-14T13:28:31Z,http://arxiv.org/abs/2406.10018v1,"STALL+: Boosting LLM-based Repository-level Code Completion with Static
  Analysis","Repository-level code completion is challenging as it involves complicated
contexts from multiple files in the repository. To date, researchers have
proposed two technical categories to enhance LLM-based repository-level code
completion, i.e., retrieval-augmented generation (RAG) and static analysis
integration. This work performs the first study on the static analysis
integration in LLM-based repository-level code completion by investigating both
the effectiveness and efficiency of static analysis integration strategies
across different phases of code completion. We first implement a framework
STALL+, which supports an extendable and customizable integration of multiple
static analysis strategies into the complete pipeline of LLM-based
repository-level code completion; and based on STALL+, we perform extensive
experiments by including different code LLMs on the latest repository-level
code completion benchmark CrossCodeEval. Our findings show that integrating
file-level dependencies in prompting phase performs the best while the
integration in post-processing phase performs the worse. Additionally, we
observe different improvements from static analysis between dynamic languages
and static languages, i.e., the best combination is prompting-phase with
decoding-phase integration for Java while the best combination is
prompting-phase with post-processing-phase integration for Python given the
limitations of statically analyzing dynamic languages. Additionally, we find
the complementarity between RAG and static analysis integration as well as
their cost-effectiveness after combination.",Junwei Liu
2024-06-14T16:00:29Z,http://arxiv.org/abs/2406.10149v2,"BABILong: Testing the Limits of LLMs with Long Context
  Reasoning-in-a-Haystack","In recent years, the input context sizes of large language models (LLMs) have
increased dramatically. However, existing evaluation methods have not kept
pace, failing to comprehensively assess the efficiency of models in handling
long contexts. To bridge this gap, we introduce the BABILong benchmark,
designed to test language models' ability to reason across facts distributed in
extremely long documents. BABILong includes a diverse set of 20 reasoning
tasks, including fact chaining, simple induction, deduction, counting, and
handling lists/sets. These tasks are challenging on their own, and even more
demanding when the required facts are scattered across long natural text. Our
evaluations show that popular LLMs effectively utilize only 10-20\% of the
context and their performance declines sharply with increased reasoning
complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
Generation methods achieve a modest 60\% accuracy on single-fact question
answering, independent of context length. Among context extension methods, the
highest performance is demonstrated by recurrent memory transformers after
fine-tuning, enabling the processing of lengths up to 50 million tokens. The
BABILong benchmark is extendable to any length to support the evaluation of new
upcoming models with increased capabilities, and we provide splits up to 10
million token lengths.",Yuri Kuratov
2024-06-11T01:27:00Z,http://arxiv.org/abs/2406.10261v1,"FoodSky: A Food-oriented Large Language Model that Passes the Chef and
  Dietetic Examination","Food is foundational to human life, serving not only as a source of
nourishment but also as a cornerstone of cultural identity and social
interaction. As the complexity of global dietary needs and preferences grows,
food intelligence is needed to enable food perception and reasoning for various
tasks, ranging from recipe generation and dietary recommendation to
diet-disease correlation discovery and understanding. Towards this goal, for
powerful capabilities across various domains and tasks in Large Language Models
(LLMs), we introduce Food-oriented LLM FoodSky to comprehend food data through
perception and reasoning. Considering the complexity and typicality of Chinese
cuisine, we first construct one comprehensive Chinese food corpus FoodEarth
from various authoritative sources, which can be leveraged by FoodSky to
achieve deep understanding of food-related data. We then propose Topic-based
Selective State Space Model (TS3M) and the Hierarchical Topic Retrieval
Augmented Generation (HTRAG) mechanism to enhance FoodSky in capturing
fine-grained food semantics and generating context-aware food-relevant text,
respectively. Our extensive evaluations demonstrate that FoodSky significantly
outperforms general-purpose LLMs in both chef and dietetic examinations, with
an accuracy of 67.2% and 66.4% on the Chinese National Chef Exam and the
National Dietetic Exam, respectively. FoodSky not only promises to enhance
culinary creativity and promote healthier eating patterns, but also sets a new
standard for domain-specific LLMs that address complex real-world issues in the
food domain. An online demonstration of FoodSky is available at
http://222.92.101.211:8200.",Pengfei Zhou
2024-06-15T17:07:31Z,http://arxiv.org/abs/2406.10690v3,"Automating Pharmacovigilance Evidence Generation: Using Large Language
  Models to Produce Context-Aware SQL","Objective: To enhance the efficiency and accuracy of information retrieval
from pharmacovigilance (PV) databases by employing Large Language Models (LLMs)
to convert natural language queries (NLQs) into Structured Query Language (SQL)
queries, leveraging a business context document.
  Materials and Methods: We utilized OpenAI's GPT-4 model within a
retrieval-augmented generation (RAG) framework, enriched with a business
context document, to transform NLQs into syntactically precise SQL queries.
Each NLQ was presented to the LLM randomly and independently to prevent
memorization. The study was conducted in three phases, varying query
complexity, and assessing the LLM's performance both with and without the
business context document.
  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing
from 8.3\% with the database schema alone to 78.3\% with the business context
document. This enhancement was consistent across low, medium, and high
complexity queries, indicating the critical role of contextual knowledge in
query generation.
  Discussion: The integration of a business context document markedly improved
the LLM's ability to generate accurate and contextually relevant SQL queries.
Performance achieved a maximum of 85\% when high complexity queries are
excluded, suggesting promise for routine deployment.
  Conclusion: This study presents a novel approach to employing LLMs for safety
data retrieval and analysis, demonstrating significant advancements in query
generation accuracy. The methodology offers a framework applicable to various
data-intensive domains, enhancing the accessibility and efficiency of
information retrieval for non-technical users.",Jeffery L. Painter
2024-06-17T02:25:45Z,http://arxiv.org/abs/2406.11147v2,"Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level
  RAG","Vulnerability detection is essential for software quality assurance. In
recent years, deep learning models (especially large language models) have
shown promise in vulnerability detection. In this work, we propose a novel
LLM-based vulnerability detection technique Vul-RAG, which leverages
knowledge-level retrieval-augmented generation (RAG) framework to detect
vulnerability for the given code in three phases. First, Vul-RAG constructs a
vulnerability knowledge base by extracting multi-dimension knowledge via LLMs
from existing CVE instances; second, for a given code snippet, Vul-RAG}
retrieves the relevant vulnerability knowledge from the constructed knowledge
base based on functional semantics; third, Vul-RAG leverages LLMs to check the
vulnerability of the given code snippet by reasoning the presence of
vulnerability causes and fixing solutions of the retrieved vulnerability
knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows
that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative
improvement in accuracy/pairwise-accuracy. In addition, our user study shows
that the vulnerability knowledge generated by Vul-RAG can serve as high-quality
explanations which can improve the manual detection accuracy from 0.60 to 0.77.",Xueying Du
2024-06-17T04:35:17Z,http://arxiv.org/abs/2406.11201v2,"Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large
  Language Models","Large Language Models (LLMs) have the unique capability to understand and
generate human-like text from input queries. When fine-tuned, these models show
enhanced performance on domain-specific queries. OpenAI highlights the process
of fine-tuning, stating: ""To fine-tune a model, you are required to provide at
least 10 examples. We typically see clear improvements from fine-tuning on 50
to 100 training examples, but the right number varies greatly based on the
exact use case."" This study extends this concept to the integration of LLMs
within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve
accuracy and relevance by leveraging external corpus data for information
retrieval. However, RAG's promise of delivering optimal responses often falls
short in complex query scenarios. This study aims to specifically examine the
effects of fine-tuning LLMs on their ability to extract and integrate
contextual data to enhance the performance of RAG systems across multiple
domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data
extraction and contextual understanding by comparing the accuracy and
completeness of fine-tuned models against baseline performances across datasets
from multiple domains. Our findings indicate that fine-tuning resulted in a
decline in performance compared to the baseline models, contrary to the
improvements observed in standalone LLM applications as suggested by OpenAI.
This study highlights the need for vigorous investigation and validation of
fine-tuned models for domain-specific tasks.",Scott Barnett
2024-06-17T09:25:10Z,http://arxiv.org/abs/2406.11357v2,"Refiner: Restructure Retrieval Content Efficiently to Advance
  Question-Answering Capabilities","Large Language Models (LLMs) are limited by their parametric knowledge,
leading to hallucinations in knowledge-extensive tasks. To address this,
Retrieval-Augmented Generation (RAG) incorporates external document chunks to
expand LLM knowledge. Furthermore, compressing information from document chunks
through extraction or summarization can improve LLM performance. Nonetheless,
LLMs still struggle to notice and utilize scattered key information, a problem
known as the ""lost-in-the-middle"" syndrome. Therefore, we typically need to
restructure the content for LLM to recognize the key information. We propose
$\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that
operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a
single decoder-only LLM to adaptively extract query-relevant contents verbatim
along with the necessary context, and section them based on their
interconnectedness, thereby highlights information distinction, and aligns
downstream LLMs with the original context effectively. Experiments show that a
trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to
downstream LLM in improving answer accuracy, and outperforms other
state-of-the-art advanced RAG and concurrent compressing approaches in various
single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5%
tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared
to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that
can be seamlessly integrated with RAG systems, facilitating its application
across diverse open-source frameworks.",Zhonghao Li
2024-05-25T13:38:15Z,http://arxiv.org/abs/2406.12881v1,Towards Unlocking Insights from Logbooks Using AI,"Electronic logbooks contain valuable information about activities and events
concerning their associated particle accelerator facilities. However, the
highly technical nature of logbook entries can hinder their usability and
automation. As natural language processing (NLP) continues advancing, it offers
opportunities to address various challenges that logbooks present. This work
explores jointly testing a tailored Retrieval Augmented Generation (RAG) model
for enhancing the usability of particle accelerator logbooks at institutes like
DESY, BESSY, Fermilab, BNL, SLAC, LBNL, and CERN. The RAG model uses a corpus
built on logbook contributions and aims to unlock insights from these logbooks
by leveraging retrieval over facility datasets, including discussion about
potential multimodal sources. Our goals are to increase the FAIR-ness
(findability, accessibility, interoperability, and reusability) of logbooks by
exploiting their information content to streamline everyday use, enable
macro-analysis for root cause analysis, and facilitate problem-solving
automation.",Antonin Sulc
2024-06-16T22:04:10Z,http://arxiv.org/abs/2406.12934v1,Current state of LLM Risks and AI Guardrails,"Large language models (LLMs) have become increasingly sophisticated, leading
to widespread deployment in sensitive applications where safety and reliability
are paramount. However, LLMs have inherent risks accompanying them, including
bias, potential for unsafe actions, dataset poisoning, lack of explainability,
hallucinations, and non-reproducibility. These risks necessitate the
development of ""guardrails"" to align LLMs with desired behaviors and mitigate
potential harm.
  This work explores the risks associated with deploying LLMs and evaluates
current approaches to implementing guardrails and model alignment techniques.
We examine intrinsic and extrinsic bias evaluation methods and discuss the
importance of fairness metrics for responsible AI development. The safety and
reliability of agentic LLMs (those capable of real-world actions) are explored,
emphasizing the need for testability, fail-safes, and situational awareness.
  Technical strategies for securing LLMs are presented, including a layered
protection model operating at external, secondary, and internal levels. System
prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to
minimize bias and protect privacy are highlighted.
  Effective guardrail design requires a deep understanding of the LLM's
intended use case, relevant regulations, and ethical considerations. Striking a
balance between competing requirements, such as accuracy and privacy, remains
an ongoing challenge. This work underscores the importance of continuous
research and development to ensure the safe and responsible use of LLMs in
real-world applications.",Suriya Ganesh Ayyamperumal
2024-06-19T20:13:42Z,http://arxiv.org/abs/2406.13805v1,"WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge
  Conflicts from Wikipedia","Retrieval-augmented generation (RAG) has emerged as a promising solution to
mitigate the limitations of large language models (LLMs), such as
hallucinations and outdated information. However, it remains unclear how LLMs
handle knowledge conflicts arising from different augmented retrieved passages,
especially when these passages originate from the same source and have equal
trustworthiness. In this work, we conduct a comprehensive evaluation of
LLM-generated answers to questions that have varying answers based on
contradictory passages from Wikipedia, a dataset widely regarded as a
high-quality pre-training resource for most LLMs. Specifically, we introduce
WikiContradict, a benchmark consisting of 253 high-quality, human-annotated
instances designed to assess LLM performance when augmented with retrieved
passages containing real-world knowledge conflicts. We benchmark a diverse
range of both closed and open-source LLMs under different QA scenarios,
including RAG with a single passage, and RAG with 2 contradictory passages.
Through rigorous human evaluations on a subset of WikiContradict instances
involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and
limitations of these models. For instance, when provided with two passages
containing contradictory facts, all models struggle to generate answers that
accurately reflect the conflicting nature of the context, especially for
implicit conflicts requiring reasoning. Since human evaluation is costly, we
also introduce an automated model that estimates LLM performance using a strong
open-source language model, achieving an F-score of 0.8. Using this automated
metric, we evaluate more than 1,500 answers from seven LLMs across all
WikiContradict instances. To facilitate future work, we release WikiContradict
on: https://ibm.biz/wikicontradict.",Yufang Hou
2024-06-20T16:59:52Z,http://arxiv.org/abs/2406.14497v1,CodeRAG-Bench: Can Retrieval Augment Code Generation?,"While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.",Zora Zhiruo Wang
2024-06-20T21:27:57Z,http://arxiv.org/abs/2406.14745v2,"Relation Extraction with Fine-Tuned Large Language Models in Retrieval
  Augmented Generation Frameworks","Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.",Sefika Efeoglu
2024-06-21T08:45:52Z,http://arxiv.org/abs/2406.14979v2,"Retrieve-Plan-Generation: An Iterative Planning and Answering Framework
  for Knowledge-Intensive LLM Generation","Despite the significant progress of large language models (LLMs) in various
tasks, they often produce factual errors due to their limited internal
knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with
external knowledge sources, offers a promising solution. However, these methods
can be misled by irrelevant paragraphs in retrieved documents. Due to the
inherent uncertainty in LLM generation, inputting the entire document may
introduce off-topic information, causing the model to deviate from the central
topic and affecting the relevance of the generated content. To address these
issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates
plan tokens to guide subsequent generation in the plan stage. In the answer
stage, the model selects relevant fine-grained paragraphs based on the plan and
uses them for further answer generation. This plan-answer process is repeated
iteratively until completion, enhancing generation relevance by focusing on
specific topics. To implement this framework efficiently, we utilize a simple
but effective multi-task prompt-tuning method, enabling the existing LLMs to
handle both planning and answering. We comprehensively compare RPG with
baselines across 5 knowledge-intensive generation tasks, demonstrating the
effectiveness of our approach.",Yuanjie Lyu
2024-06-21T17:23:21Z,http://arxiv.org/abs/2406.15319v3,LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,"In traditional RAG framework, the basic retrieval units are normally short.
The common retrievers like DPR normally work with 100-word Wikipedia
paragraphs. Such a design forces the retriever to search over a large corpus to
find the `needle' unit. In contrast, the readers only need to generate answers
from the short retrieved units. The imbalanced `heavy' retriever and `light'
reader design can lead to sub-optimal performance. The loss of contextual
information in the short, chunked units may increase the likelihood of
introducing hard negatives during the retrieval stage. Additionally, the reader
might not fully leverage the capabilities of recent advancements in LLMs. In
order to alleviate the imbalance, we propose a new framework LongRAG,
consisting of a `long retriever' and a `long reader'. In the two
Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire
Wikipedia corpus into 4K-token units by grouping related documents. By
increasing the unit size, we significantly reduce the total number of units.
This greatly reduces the burden on the retriever, resulting in strong retrieval
performance with only a few (less than 8) top units. Without requiring any
training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which
are on par with the (fully-trained) SoTA model. Furthermore, we test on two
non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes
each individual document as a single (long) unit rather than chunking them into
smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%
on MultiFieldQA-en. Our study offers insights into the future roadmap for
combining RAG with long-context LLMs.",Ziyan Jiang
2024-06-23T04:35:42Z,http://arxiv.org/abs/2406.16008v2,"Found in the Middle: Calibrating Positional Attention Bias Improves Long
  Context Utilization","Large language models (LLMs), even when specifically trained to process long
input contexts, struggle to capture relevant information located in the middle
of their input. This phenomenon has been known as the lost-in-the-middle
problem. In this work, we make three contributions. First, we set out to
understand the factors that cause this phenomenon. In doing so, we establish a
connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs
exhibit a U-shaped attention bias where the tokens at the beginning and at the
end of its input receive higher attention, regardless of their relevance.
Second, we mitigate this positional bias through a calibration mechanism,
found-in-the-middle, that allows the model to attend to contexts faithfully
according to their relevance, even though when they are in the middle. Third,
we show found-in-the-middle not only achieves better performance in locating
relevant information within a long context, but also eventually leads to
improved retrieval-augmented generation (RAG) performance across various tasks,
outperforming existing methods by up to 15 percentage points. These findings
open up future directions in understanding LLM attention bias and its potential
consequences.",Cheng-Yu Hsieh
2024-06-24T01:22:54Z,http://arxiv.org/abs/2406.16252v2,"Graph-Augmented LLMs for Personalized Health Insights: A Case Study in
  Sleep Analysis","Health monitoring systems have revolutionized modern healthcare by enabling
the continuous capture of physiological and behavioral data, essential for
preventive measures and early health intervention. While integrating this data
with Large Language Models (LLMs) has shown promise in delivering interactive
health advice, traditional methods like Retrieval-Augmented Generation (RAG)
and fine-tuning often fail to fully utilize the complex, multi-dimensional, and
temporally relevant data from wearable devices. These conventional approaches
typically provide limited actionable and personalized health insights due to
their inadequate capacity to dynamically integrate and interpret diverse health
data streams. In response, this paper introduces a graph-augmented LLM
framework designed to significantly enhance the personalization and clarity of
health insights. Utilizing a hierarchical graph structure, the framework
captures inter and intra-patient relationships, enriching LLM prompts with
dynamic feature importance scores derived from a Random Forest Model. The
effectiveness of this approach is demonstrated through a sleep analysis case
study involving 20 college students during the COVID-19 lockdown, highlighting
the potential of our model to generate actionable and personalized health
insights efficiently. We leverage another LLM to evaluate the insights for
relevance, comprehensiveness, actionability, and personalization, addressing
the critical need for models that process and interpret complex health data
effectively. Our findings show that augmenting prompts with our framework
yields significant improvements in all 4 criteria. Through our framework, we
can elicit well-crafted, more thoughtful responses tailored to a specific
patient.",Ajan Subramanian
2024-06-24T23:57:57Z,http://arxiv.org/abs/2406.17186v2,"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented
  Analysis Generation","Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.",Abe Bohan Hou
2024-06-25T09:42:56Z,http://arxiv.org/abs/2406.17419v2,"Leave No Document Behind: Benchmarking Long-Context LLMs with Extended
  Multi-Doc QA","Long-context modeling capabilities have garnered widespread attention,
leading to the emergence of Large Language Models (LLMs) with ultra-context
windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually
catching up. However, existing benchmarks employ irrelevant noise texts to
artificially extend the length of test cases, diverging from the real-world
scenarios of long-context applications. To bridge this gap, we propose a novel
long-context benchmark, Loong, aligning with realistic scenarios through
extended multi-document question answering (QA). Unlike typical document QA, in
Loong's test cases, each document is relevant to the final answer, ignoring any
document will lead to the failure of the answer. Furthermore, Loong introduces
four types of tasks with a range of context lengths: Spotlight Locating,
Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic
and comprehensive evaluation of long-context understanding. Extensive
experiments indicate that existing long-context language models still exhibit
considerable potential for enhancement. Retrieval augmented generation (RAG)
achieves poor performance, demonstrating that Loong can reliably assess the
model's long-context modeling capabilities.",Minzheng Wang
2024-06-25T15:43:20Z,http://arxiv.org/abs/2406.17651v5,"Software Model Evolution with Large Language Models: Experiments on
  Simulated, Public, and Industrial Datasets","Modeling structure and behavior of software systems plays a crucial role in
the industrial practice of software engineering. As with other software
engineering artifacts, software models are subject to evolution. Supporting
modelers in evolving software models with recommendations for model completions
is still an open problem, though. In this paper, we explore the potential of
large language models for this task. In particular, we propose an approach,
RAMC, leveraging large language models, model histories, and
retrieval-augmented generation for model completion. Through experiments on
three datasets, including an industrial application, one public open-source
community dataset, and one controlled collection of simulated model
repositories, we evaluate the potential of large language models for model
completion with RAMC. We found that large language models are indeed a
promising technology for supporting software model evolution (62.30%
semantically correct completions on real-world industrial data and up to 86.19%
type-correct completions). The general inference capabilities of large language
models are particularly useful when dealing with concepts for which there are
few, noisy, or no examples at all.",Christof Tinnes
2024-06-26T00:00:45Z,http://arxiv.org/abs/2406.17987v4,Multi-step Inference over Unstructured Data,"The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.",Aditya Kalyanpur
2024-06-27T07:56:44Z,http://arxiv.org/abs/2406.18966v3,"UniGen: A Unified Framework for Textual Dataset Generation Using Large
  Language Models","Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.",Siyuan Wu
2024-06-28T01:14:43Z,http://arxiv.org/abs/2406.19593v1,"SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs","Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.",Xin Su
2024-06-21T08:52:11Z,http://arxiv.org/abs/2407.00072v5,Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback,"RAG systems face limitations when semantic relevance alone does not guarantee
improved generation quality. This issue becomes particularly evident due to the
sensitivity of large language models (LLMs) to the ordering of few-shot
prompts, which can affect model performance. To address this challenge,
aligning LLM outputs with human preferences using structured feedback, such as
options to copy, regenerate, or dislike, offers a promising method for
improvement. This feedback is applied to the entire list of inputs rather than
giving specific ratings for individual documents, making it a Listwide Labels
Learning-to-Rank task.
  To address this task, we propose Pistis-RAG, a new RAG framework designed
with a content-centric approach to better align LLMs with human preferences.
Pistis-RAG effectively utilizes human feedback, enhancing content ranking and
generation quality. To validate our framework, we use public datasets to
simulate human feedback, allowing us to evaluate and refine our method
effectively. Experimental results indicate that Pistis-RAG improves alignment
with human preferences relative to the baseline RAG system, showing a 6.06%
increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy
metrics. These results highlight Pistis-RAG's effectiveness in overcoming the
limitations associated with traditional RAG approaches.",Yu Bai
2024-06-30T15:38:48Z,http://arxiv.org/abs/2407.00731v2,"Large Language Models Struggle in Token-Level Clinical Named Entity
  Recognition","Large Language Models (LLMs) have revolutionized various sectors, including
healthcare where they are employed in diverse applications. Their utility is
particularly significant in the context of rare diseases, where data scarcity,
complexity, and specificity pose considerable challenges. In the clinical
domain, Named Entity Recognition (NER) stands out as an essential task and it
plays a crucial role in extracting relevant information from clinical texts.
Despite the promise of LLMs, current research mostly concentrates on
document-level NER, identifying entities in a more general context across
entire documents, without extracting their precise location. Additionally,
efforts have been directed towards adapting ChatGPT for token-level NER.
However, there is a significant research gap when it comes to employing
token-level NER for clinical texts, especially with the use of local
open-source LLMs. This study aims to bridge this gap by investigating the
effectiveness of both proprietary and local LLMs in token-level clinical NER.
Essentially, we delve into the capabilities of these models through a series of
experiments involving zero-shot prompting, few-shot prompting,
retrieval-augmented generation (RAG), and instruction-fine-tuning. Our
exploration reveals the inherent challenges LLMs face in token-level NER,
particularly in the context of rare diseases, and suggests possible
improvements for their application in healthcare. This research contributes to
narrowing a significant gap in healthcare informatics and offers insights that
could lead to a more refined application of LLMs in the healthcare sector.",Qiuhao Lu
2024-07-01T05:28:40Z,http://arxiv.org/abs/2407.00978v2,"Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in
  Internet of Medical Things: A Diffusion-based Contract Approach","Secure data management and effective data sharing have become paramount in
the rapidly evolving healthcare landscape, especially with the growing
integration of the Internet of Medical Things (IoMT). The rise of generative
artificial intelligence has further elevated Multi-modal Large Language Models
(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.
MLLMs can support multi-modal inputs and generate diverse types of content by
leveraging large-scale training on vast amounts of multi-modal data. However,
critical challenges persist in developing medical MLLMs, including security and
freshness issues of healthcare data, affecting the output quality of MLLMs. To
this end, in this paper, we propose a hybrid Retrieval-Augmented Generation
(RAG)-empowered medical MLLM framework for healthcare data management. This
framework leverages a hierarchical cross-chain architecture to facilitate
secure data training. Moreover, it enhances the output quality of MLLMs through
hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG
results and incorporates these retrieval results as additional inputs to MLLMs.
Additionally, we employ age of information to indirectly evaluate the data
freshness impact of MLLMs and utilize contract theory to incentivize healthcare
data holders to share their fresh data, mitigating information asymmetry during
data sharing. Finally, we utilize a generative diffusion model-based deep
reinforcement learning algorithm to identify the optimal contract for efficient
data sharing. Numerical results demonstrate the effectiveness of the proposed
schemes, which achieve secure and efficient healthcare data management.",Cheng Su
2024-07-01T09:19:50Z,http://arxiv.org/abs/2407.01110v1,"SecGenAI: Enhancing Security of Cloud-based Generative AI Applications
  within Australian Critical Technologies of National Interest","The rapid advancement of Generative AI (GenAI) technologies offers
transformative opportunities within Australia's critical technologies of
national interest while introducing unique security challenges. This paper
presents SecGenAI, a comprehensive security framework for cloud-based GenAI
applications, with a focus on Retrieval-Augmented Generation (RAG) systems.
SecGenAI addresses functional, infrastructure, and governance requirements,
integrating end-to-end security analysis to generate specifications emphasizing
data privacy, secure deployment, and shared responsibility models. Aligned with
Australian Privacy Principles, AI Ethics Principles, and guidelines from the
Australian Cyber Security Centre and Digital Transformation Agency, SecGenAI
mitigates threats such as data leakage, adversarial attacks, and model
inversion. The framework's novel approach combines advanced machine learning
techniques with robust security measures, ensuring compliance with Australian
regulations while enhancing the reliability and trustworthiness of GenAI
systems. This research contributes to the field of intelligent systems by
providing actionable strategies for secure GenAI implementation in industry,
fostering innovation in AI applications, and safeguarding national interests.",Christoforus Yoga Haryanto
2024-07-01T11:07:23Z,http://arxiv.org/abs/2407.01178v1,$\text{Memory}^3$: Language Modeling with Explicit Memory,"The training and inference of large language models (LLMs) are together a
costly process that transports knowledge from raw data to meaningful
computation. Inspired by the memory hierarchy of the human brain, we reduce
this cost by equipping LLMs with explicit memory, a memory format cheaper than
model parameters and text retrieval-augmented generation (RAG). Conceptually,
with most of its knowledge externalized to explicit memories, the LLM can enjoy
a smaller parameter size, training cost, and inference cost, all proportional
to the amount of remaining ""abstract knowledge"". As a preliminary proof of
concept, we train from scratch a 2.4B LLM, which achieves better performance
than much larger LLMs as well as RAG models, and maintains higher decoding
speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is
the third form of memory in LLMs after implicit memory (model parameters) and
working memory (context key-values). We introduce a memory circuitry theory to
support the externalization of knowledge, and present novel techniques
including a memory sparsification mechanism that makes storage tractable and a
two-stage pretraining scheme that facilitates memory formation.",Hongkang Yang
2024-07-02T07:52:30Z,http://arxiv.org/abs/2407.02028v1,"Why does in-context learning fail sometimes? Evaluating in-context
  learning on open and closed questions","We measure the performance of in-context learning as a function of task
novelty and difficulty for open and closed questions. For that purpose, we
created a novel benchmark consisting of hard scientific questions, each paired
with a context of various relevancy. We show that counter-intuitively, a
context that is more aligned with the topic does not always help more than a
less relevant context. This effect is especially visible for open questions and
questions of high difficulty or novelty. This result reveals a fundamental
difference between the treatment of close-form and open-form questions by
large-language models and shows a need for a more robust evaluation of
in-context learning on the variety of different types of questions. It also
poses a new question of how to optimally select a context for large language
models, especially in the context of Retrieval Augmented Generation (RAG)
systems. Our results suggest that the answer to this question can be highly
application-dependent and might be contingent on factors including the format
of the question, the perceived difficulty level of the questions, and the
novelty or popularity of the information we seek.",Xiang Li
2024-07-03T01:28:51Z,http://arxiv.org/abs/2407.02742v1,"A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized
  Retrieval Augmentation","Natural Language to Code Generation has made significant progress in recent
years with the advent of Large Language Models(LLMs). While generation for
general-purpose languages like C, C++, and Python has improved significantly,
LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs
having a high number of custom function names. Additionally, constant updates
to function names add to the challenge as LLMs need to stay up-to-date. In this
paper, we present optimizations for using Retrieval Augmented Generation (or
RAG) with LLMs for DSL generation along with an ablation study comparing these
strategies. We generated a train as well as test dataset with a DSL to
represent automation tasks across roughly 700 APIs in public domain. We used
the training dataset to fine-tune a Codex model for this DSL. Our results
showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The
compilation rate, however, showed that both the models still got the syntax
wrong many times, with RAG-based method being 2 pts better. Conversely,
hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for
API parameter keys. We conclude that an optimized RAG model can match the
quality of fine-tuned models and offer advantages for new, unseen APIs.",Nastaran Bassamzadeh
2024-07-04T12:29:06Z,http://arxiv.org/abs/2407.03889v1,"Automated C/C++ Program Repair for High-Level Synthesis via Large
  Language Models","In High-Level Synthesis (HLS), converting a regular C/C++ program into its
HLS-compatible counterpart (HLS-C) still requires tremendous manual effort.
Various program scripts have been introduced to automate this process. But the
resulting codes usually contain many issues that should be manually repaired by
developers. Since Large Language Models (LLMs) have the ability to automate
code generation, they can also be used for automated program repair in HLS.
However, due to the limited training of LLMs considering hardware and software
simultaneously, hallucinations may occur during program repair using LLMs,
leading to compilation failures. Besides, using LLMs for iterative repair also
incurs a high cost. To address these challenges, we propose an LLM-driven
program repair framework that takes regular C/C++ code as input and
automatically generates its corresponding HLS-C code for synthesis while
minimizing human repair effort. To mitigate the hallucinations in LLMs and
enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is
introduced to guide the LLMs toward correct repair. In addition, we use LLMs to
create a static bit width optimization program to identify the optimized bit
widths for variables. Moreover, LLM-driven HLS optimization strategies are
introduced to add/tune pragmas in HLS-C programs for circuit optimization.
Experimental results demonstrate that the proposed LLM-driven automated
framework can achieve much higher repair pass rates in 24 real-world
applications compared with the traditional scripts and the direct application
of LLMs for program repair.",Kangwei Xu
2024-07-05T17:43:30Z,http://arxiv.org/abs/2407.04681v1,"Rethinking Visual Prompting for Multimodal Large Language Models with
  External Knowledge","In recent years, multimodal large language models (MLLMs) have made
significant strides by training on vast high-quality image-text datasets,
enabling them to generally understand images well. However, the inherent
difficulty in explicitly conveying fine-grained or spatially dense information
in text, such as masks, poses a challenge for MLLMs, limiting their ability to
answer questions requiring an understanding of detailed or localized visual
elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)
concept, this paper proposes a new visual prompt approach to integrate
fine-grained external knowledge, gleaned from specialized vision models (e.g.,
instance segmentation/OCR models), into MLLMs. This is a promising yet
underexplored direction for enhancing MLLMs' performance. Our approach diverges
from concurrent works, which transform external knowledge into additional text
prompts, necessitating the model to indirectly learn the correspondence between
visual content and text coordinates. Instead, we propose embedding fine-grained
knowledge information directly into a spatial embedding map as a visual prompt.
This design can be effortlessly incorporated into various MLLMs, such as LLaVA
and Mipha, considerably improving their visual understanding performance.
Through rigorous experiments, we demonstrate that our method can enhance MLLM
performance across nine benchmarks, amplifying their fine-grained context-aware
capabilities.",Yuanze Lin
2024-07-06T09:10:05Z,http://arxiv.org/abs/2407.05015v1,"How do you know that? Teaching Generative Language Models to Reference
  Answers to Biomedical Questions","Large language models (LLMs) have recently become the leading source of
answers for users' questions online. Despite their ability to offer eloquent
answers, their accuracy and reliability can pose a significant challenge. This
is especially true for sensitive domains such as biomedicine, where there is a
higher need for factually correct answers. This paper introduces a biomedical
retrieval-augmented generation (RAG) system designed to enhance the reliability
of generated responses. The system is based on a fine-tuned LLM for the
referenced question-answering, where retrieved relevant abstracts from PubMed
are passed to LLM's context as input through a prompt. Its output is an answer
based on PubMed abstracts, where each statement is referenced accordingly,
allowing the users to verify the answer. Our retrieval system achieves an
absolute improvement of 23% compared to the PubMed search engine. Based on the
manual evaluation on a small sample, our fine-tuned LLM component achieves
comparable results to GPT-4 Turbo in referencing relevant abstracts. We make
the dataset used to fine-tune the models and the fine-tuned models based on
Mistral-7B-instruct-v0.1 and v0.2 publicly available.",Bojana Bašaragin
2024-07-06T16:45:07Z,http://arxiv.org/abs/2407.05131v2,"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language
  Models","The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on medical VQA and report generation tasks across three
datasets, achieving an average improvement of 47.4% in factual accuracy. We
publicly release our benchmark and code in
https://github.com/richard-peng-xia/RULE.",Peng Xia
2024-07-08T13:07:50Z,http://arxiv.org/abs/2407.06245v2,"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open
  Radio Access Networks","Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.",Pranshav Gajjar
2024-07-09T15:59:28Z,http://arxiv.org/abs/2407.06985v4,"PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and
  Tuning Methods","In domain-specific applications, GPT-4, augmented with precise prompts or
Retrieval-Augmented Generation (RAG), shows notable potential but faces the
critical tri-lemma of performance, cost, and data privacy. High performance
requires sophisticated processing techniques, yet managing multiple agents
within a complex workflow often proves costly and challenging. To address this,
we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.
This systematizes domain-specific tasks by integrating precise question
decomposition, advanced information retrieval, comprehensive summarization, and
rigorous self-assessment. Given the concerns of cost and data privacy,
enterprises are shifting from proprietary models like GPT-4 to custom models,
striking a balance between cost, security, and performance. We developed
industrial practices leveraging online data and user feedback for efficient
model tuning. This study provides best practice guidelines for applying
multi-agent systems in domain-specific problem-solving and implementing
effective agent tuning strategies. Our empirical studies, particularly in the
financial question-answering domain, demonstrate that our approach achieves
95.0% of GPT-4's performance, while effectively managing costs and ensuring
data privacy.",Yiying Wang
2024-07-09T17:33:24Z,http://arxiv.org/abs/2407.07061v2,"Internet of Agents: Weaving a Web of Heterogeneous Agents for
  Collaborative Intelligence","The rapid advancement of large language models (LLMs) has paved the way for
the development of highly capable autonomous agents. However, existing
multi-agent frameworks often struggle with integrating diverse capable
third-party agents due to reliance on agents defined within their own
ecosystems. They also face challenges in simulating distributed environments,
as most frameworks are limited to single-device setups. Furthermore, these
frameworks often rely on hard-coded communication pipelines, limiting their
adaptability to dynamic task requirements. Inspired by the concept of the
Internet, we propose the Internet of Agents (IoA), a novel framework that
addresses these limitations by providing a flexible and scalable platform for
LLM-based multi-agent collaboration. IoA introduces an agent integration
protocol, an instant-messaging-like architecture design, and dynamic mechanisms
for agent teaming and conversation flow control. Through extensive experiments
on general assistant tasks, embodied AI tasks, and retrieval-augmented
generation benchmarks, we demonstrate that IoA consistently outperforms
state-of-the-art baselines, showcasing its ability to facilitate effective
collaboration among heterogeneous agents. IoA represents a step towards linking
diverse agents in an Internet-like environment, where agents can seamlessly
collaborate to achieve greater intelligence and capabilities. Our codebase has
been released at \url{https://github.com/OpenBMB/IoA}.",Weize Chen
2024-07-10T16:08:46Z,http://arxiv.org/abs/2407.07791v2,"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent
  Communities","The rapid adoption of large language models (LLMs) in multi-agent systems has
highlighted their impressive capabilities in various applications, such as
collaborative problem-solving and autonomous negotiation. However, the security
implications of these LLM-based multi-agent systems have not been thoroughly
investigated, particularly concerning the spread of manipulated knowledge. In
this paper, we investigate this critical issue by constructing a detailed
threat model and a comprehensive simulation environment that mirrors real-world
multi-agent deployments in a trusted platform. Subsequently, we propose a novel
two-stage attack method involving Persuasiveness Injection and Manipulated
Knowledge Injection to systematically explore the potential for manipulated
knowledge (i.e., counterfactual and toxic knowledge) spread without explicit
prompt manipulation.
  Our method leverages the inherent vulnerabilities of LLMs in handling world
knowledge, which can be exploited by attackers to unconsciously spread
fabricated information. Through extensive experiments, we demonstrate that our
attack method can successfully induce LLM-based agents to spread both
counterfactual and toxic knowledge without degrading their foundational
capabilities during agent communication. Furthermore, we show that these
manipulations can persist through popular retrieval-augmented generation
frameworks, where several benign agents store and retrieve manipulated chat
histories for future interactions. This persistence indicates that even after
the interaction has ended, the benign agents may continue to be influenced by
manipulated knowledge. Our findings reveal significant security risks in
LLM-based multi-agent systems, emphasizing the imperative need for robust
defenses against manipulated knowledge spread, such as introducing ``guardian''
agents and advanced fact-checking tools.",Tianjie Ju
2024-07-13T21:13:55Z,http://arxiv.org/abs/2407.10005v1,"Fine-grained Analysis of In-context Linear Estimation: Data,
  Architecture, and Beyond","Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through
gradient descent steps. However, the existing results on the optimization
landscape apply under stylized settings where task and feature vectors are
assumed to be IID and the attention weights are fully parameterized. In this
work, we develop a stronger characterization of the optimization and
generalization landscape of ICL through contributions on architectures,
low-rank parameterization, and correlated designs: (1) We study the landscape
of 1-layer linear attention and 1-layer H3, a state-space model. Under a
suitable correlated design assumption, we prove that both implement 1-step
preconditioned gradient descent. We show that thanks to its native convolution
filters, H3 also has the advantage of implementing sample weighting and
outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank
parameterized attention weights in terms of covariance spectrum. Through this,
we also shed light on how LoRA can adapt to a new distribution by capturing the
shift between task covariances. Experimental results corroborate our
theoretical findings. Overall, this work explores the optimization and risk
landscape of ICL in practically meaningful settings and contributes to a more
thorough understanding of its mechanics.",Yingcong Li
2024-07-13T22:45:46Z,http://arxiv.org/abs/2407.10021v1,"Document-level Clinical Entity and Relation Extraction via Knowledge
  Base-Guided Generation","Generative pre-trained transformer (GPT) models have shown promise in
clinical entity and relation extraction tasks because of their precise
extraction and contextual understanding capability. In this work, we further
leverage the Unified Medical Language System (UMLS) knowledge base to
accurately identify medical concepts and improve clinical entity and relation
extraction at the document level. Our framework selects UMLS concepts relevant
to the text and combines them with prompts to guide language models in
extracting entities. Our experiments demonstrate that this initial concept
mapping and the inclusion of these mapped concepts in the prompts improves
extraction results compared to few-shot extraction tasks on generic language
models that do not leverage UMLS. Further, our results show that this approach
is more effective than the standard Retrieval Augmented Generation (RAG)
technique, where retrieved data is compared with prompt embeddings to generate
results. Overall, we find that integrating UMLS concepts with GPT models
significantly improves entity and relation identification, outperforming the
baseline and RAG models. By combining the precise concept mapping capability of
knowledge-based approaches like UMLS with the contextual understanding
capability of GPT, our method highlights the potential of these approaches in
specialized domains like healthcare.",Kriti Bhattarai
2024-05-01T20:43:06Z,http://arxiv.org/abs/2407.10246v3,"CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science
  Education","The growing enrollments in computer science courses and increase in class
sizes necessitate scalable, automated tutoring solutions to adequately support
student learning. While Large Language Models (LLMs) like GPT-4 have
demonstrated potential in assisting students through question-answering,
educators express concerns over student overreliance, miscomprehension of
generated code, and the risk of inaccurate answers. Rather than banning these
tools outright, we advocate for a constructive approach that harnesses the
capabilities of AI while mitigating potential risks. This poster introduces
CourseAssist, a novel LLM-based tutoring system tailored for computer science
education. Unlike generic LLM systems, CourseAssist uses retrieval-augmented
generation, user intent classification, and question decomposition to align AI
responses with specific course materials and learning objectives, thereby
ensuring pedagogical appropriateness of LLMs in educational settings. We
evaluated CourseAssist against a baseline of GPT-4 using a dataset of 50
question-answer pairs from a programming languages course, focusing on the
criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation
results show that CourseAssist significantly outperforms the baseline,
demonstrating its potential to serve as an effective learning assistant. We
have also deployed CourseAssist in 6 computer science courses at a large public
R1 research university reaching over 500 students. Interviews with 20 student
users show that CourseAssist improves computer science instruction by
increasing the accessibility of course-specific tutoring help and shortening
the feedback loop on their programming assignments. Future work will include
extensive pilot testing at more universities and exploring better collaborative
relationships between students, educators, and AI that improve computer science
learning experiences.",Ty Feng
2024-07-15T12:35:00Z,http://arxiv.org/abs/2407.10670v1,"Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for
  Improved Quality and Efficiency in RAG Systems","Retrieval-augmented generation (RAG) techniques leverage the in-context
learning capabilities of large language models (LLMs) to produce more accurate
and relevant responses. Originating from the simple 'retrieve-then-read'
approach, the RAG framework has evolved into a highly flexible and modular
paradigm. A critical component, the Query Rewriter module, enhances knowledge
retrieval by generating a search-friendly query. This method aligns input
questions more closely with the knowledge base. Our research identifies
opportunities to enhance the Query Rewriter module to Query Rewriter+ by
generating multiple queries to overcome the Information Plateaus associated
with a single query and by rewriting questions to eliminate Ambiguity, thereby
clarifying the underlying intent. We also find that current RAG systems exhibit
issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge
Filter. These two modules are both based on the instruction-tuned Gemma-2B
model, which together enhance response quality. The final identified issue is
Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the
Retriever Trigger to solve this. The former supports the dynamic expansion of
the RAG system's knowledge base in a parameter-free manner, while the latter
optimizes the cost for accessing external knowledge, thereby improving resource
utilization and response efficiency. These four RAG modules synergistically
improve the response quality and efficiency of the RAG system. The
effectiveness of these modules has been validated through experiments and
ablation studies across six common QA datasets. The source code can be accessed
at https://github.com/Ancientshi/ERM4.",Yunxiao Shi
2024-06-24T12:09:34Z,http://arxiv.org/abs/2407.10994v1,"Panza: A Personalized Text Writing Assistant via Data Playback and Local
  Fine-Tuning","The availability of powerful open-source large language models (LLMs) opens
exciting use-cases, such as automated personal assistants that adapt to the
user's unique data and demands. Two key desiderata for such assistants are
personalization-in the sense that the assistant should reflect the user's own
style-and privacy-in the sense that users may prefer to always store their
personal data locally, on their own computing device. We present a new design
for such an automated assistant, for the specific use case of personal
assistant for email generation, which we call Panza. Specifically, Panza can be
both trained and inferenced locally on commodity hardware, and is personalized
to the user's writing style. Panza's personalization features are based on a
new technique called data playback, which allows us to fine-tune an LLM to
better reflect a user's writing style using limited data. We show that, by
combining efficient fine-tuning and inference methods, Panza can be executed
entirely locally using limited resources-specifically, it can be executed
within the same resources as a free Google Colab instance. Finally, our key
methodological contribution is a careful study of evaluation metrics, and of
how different choices of system components (e.g. the use of Retrieval-Augmented
Generation or different fine-tuning approaches) impact the system's
performance.",Armand Nicolicioiu
2024-07-16T08:21:02Z,http://arxiv.org/abs/2407.11485v1,Scientific QA System with Verifiable Answers,"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.",Adela Ljajić
2024-07-16T11:58:54Z,http://arxiv.org/abs/2407.11638v1,"A Comprehensive Evaluation of Large Language Models on Temporal Event
  Forecasting","Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.",He Chang
2024-07-17T17:59:47Z,http://arxiv.org/abs/2407.12784v1,"AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
  Bases","LLM agents have demonstrated remarkable performance across various
applications, primarily due to their advanced capabilities in reasoning,
utilizing external knowledge and tools, calling APIs, and executing actions to
interact with environments. Current agents typically utilize a memory module or
a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and
instances with similar embeddings from knowledge bases to inform task planning
and execution. However, the reliance on unverified knowledge bases raises
significant concerns about their safety and trustworthiness. To uncover such
vulnerabilities, we propose a novel red teaming approach AgentPoison, the first
backdoor attack targeting generic and RAG-based LLM agents by poisoning their
long-term memory or RAG knowledge base. In particular, we form the trigger
generation process as a constrained optimization to optimize backdoor triggers
by mapping the triggered instances to a unique embedding space, so as to ensure
that whenever a user instruction contains the optimized backdoor trigger, the
malicious demonstrations are retrieved from the poisoned memory or knowledge
base with high probability. In the meantime, benign instructions without the
trigger will still maintain normal performance. Unlike conventional backdoor
attacks, AgentPoison requires no additional model training or fine-tuning, and
the optimized backdoor trigger exhibits superior transferability, in-context
coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's
effectiveness in attacking three types of real-world LLM agents: RAG-based
autonomous driving agent, knowledge-intensive QA agent, and healthcare
EHRAgent. On each agent, AgentPoison achieves an average attack success rate
higher than 80% with minimal impact on benign performance (less than 1%) with a
poison rate less than 0.1%.",Zhaorun Chen
2024-07-04T15:10:51Z,http://arxiv.org/abs/2407.12843v4,"NutriBench: A Dataset for Evaluating Large Language Models on Nutrition
  Estimation from Meal Descriptions","Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html",Andong Hua
2024-07-15T17:40:15Z,http://arxiv.org/abs/2407.12873v1,Evaluation of RAG Metrics for Question Answering in the Telecom Domain,"Retrieval Augmented Generation (RAG) is widely used to enable Large Language
Models (LLMs) perform Question Answering (QA) tasks in various domains.
However, RAG based on open-source LLM for specialized domains has challenges of
evaluating generated responses. A popular framework in the literature is the
RAG Assessment (RAGAS), a publicly available library which uses LLMs for
evaluation. One disadvantage of RAGAS is the lack of details of derivation of
numerical value of the evaluation metrics. One of the outcomes of this work is
a modified version of this package for few metrics (faithfulness, context
relevance, answer relevance, answer correctness, answer similarity and factual
correctness) through which we provide the intermediate outputs of the prompts
by using any LLMs. Next, we analyse the expert evaluations of the output of the
modified RAGAS package and observe the challenges of using it in the telecom
domain. We also study the effect of the metrics under correct vs. wrong
retrieval and observe that few of the metrics have higher values for correct
retrieval. We also study for differences in metrics between base embeddings and
those domain adapted via pre-training and fine-tuning. Finally, we comment on
the suitability and challenges of using these metrics for in-the-wild telecom
QA task.",Sujoy Roychowdhury
2024-07-18T13:43:01Z,http://arxiv.org/abs/2407.13511v1,"Can Open-Source LLMs Compete with Commercial Models? Exploring the
  Few-Shot Performance of Current GPT Models in Biomedical Tasks","Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT
and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)
benchmarks across different domains. New competing Open-Source alternatives
like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while
often offering higher throughput and being less costly to use. Open-Source LLMs
can also be self-hosted, which makes them interesting for enterprise and
clinical use cases where sensitive data should not be processed by third
parties. We participated in the 12th BioASQ challenge, which is a retrieval
augmented generation (RAG) setting, and explored the performance of current GPT
models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning
(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional
relevant knowledge from Wikipedia added to the context-window of the LLM might
improve their performance. Mixtral 8x7b was competitive in the 10-shot setting,
both with and without fine-tuning, but failed to produce usable results in the
zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to
measurable performance gains. Our results indicate that the performance gap
between commercial and open-source models in RAG setups exists mainly in the
zero-shot setting and can be closed by simply collecting few-shot examples for
domain-specific use cases. The code needed to rerun these experiments is
available through GitHub.",Samy Ateia
2024-07-18T17:59:30Z,http://arxiv.org/abs/2407.13766v2,Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark,"Large Multimodal Models (LMMs) have made significant strides in visual
question-answering for single images. Recent advancements like long-context
LMMs have allowed them to ingest larger, or even multiple, images. However, the
ability to process a large number of visual tokens does not guarantee effective
retrieval and reasoning for multi-image question answering (MIQA), especially
in real-world applications like photo album searches or satellite imagery
analysis. In this work, we first assess the limitations of current benchmarks
for long-context LMMs. We address these limitations by introducing a new
vision-centric, long-context benchmark, ""Visual Haystacks (VHs)"". We
comprehensively evaluate both open-source and proprietary models on VHs, and
demonstrate that these models struggle when reasoning across potentially
unrelated images, perform poorly on cross-image reasoning, as well as exhibit
biases based on the placement of key information within the context window.
Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented
Generation), an open-source, lightweight visual-RAG framework that processes up
to 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of
contemporary models. MIRAGE demonstrates up to 13% performance improvement over
existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA
multi-image QA benchmark, and achieves competitive performance on single-image
QA with state-of-the-art LMMs.",Tsung-Han Wu
2024-07-20T01:02:27Z,http://arxiv.org/abs/2407.14717v2,Differential Privacy of Cross-Attention with Provable Guarantee,"Cross-attention has become a fundamental module nowadays in many important
artificial intelligence applications, e.g., retrieval-augmented generation
(RAG), system prompt, guided stable diffusion, and many more. Ensuring
cross-attention privacy is crucial and urgently needed because its key and
value matrices may contain sensitive information about model providers and
their users. In this work, we design a novel differential privacy (DP) data
structure to address the privacy security of cross-attention with a theoretical
guarantee. In detail, let $n$ be the input token length of system prompt/RAG
data, $d$ be the feature dimension, $0 < \alpha \le 1$ be the relative error
parameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the
maximum value of the value matrix, and $r,s,\epsilon_s$ be parameters of
polynomial kernel methods. Then, our data structure requires
$\widetilde{O}(ndr^2)$ memory consumption with $\widetilde{O}(nr^2)$
initialization time complexity and $\widetilde{O}(\alpha^{-1} r^2)$ query time
complexity for a single token query. In addition, our data structure can
guarantee that the process of answering user query satisfies $(\epsilon,
\delta)$-DP with $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w
r^2)$ additive error and $n^{-1} (\alpha + \epsilon_s)$ relative error between
our output and the true answer. Furthermore, our result is robust to adaptive
queries in which users can intentionally attack the cross-attention system. To
our knowledge, this is the first work to provide DP for cross-attention and is
promising to inspire more privacy algorithm design in large generative models
(LGMs).",Yingyu Liang
2024-07-14T00:42:39Z,http://arxiv.org/abs/2407.15718v1,Integrating AI Tutors in a Programming Course,"RAGMan is an LLM-powered tutoring system that can support a variety of
course-specific and homework-specific AI tutors. RAGMan leverages Retrieval
Augmented Generation (RAG), as well as strict instructions, to ensure the
alignment of the AI tutors' responses. By using RAGMan's AI tutors, students
receive assistance with their specific homework assignments without directly
obtaining solutions, while also having the ability to ask general
programming-related questions.
  RAGMan was deployed as an optional resource in an introductory programming
course with an enrollment of 455 students. It was configured as a set of five
homework-specific AI tutors. This paper describes the interactions the students
had with the AI tutors, the students' feedback, and a comparative grade
analysis. Overall, about half of the students engaged with the AI tutors, and
the vast majority of the interactions were legitimate homework questions. When
students posed questions within the intended scope, the AI tutors delivered
accurate responses 98% of the time. Within the students used AI tutors, 78%
reported that the tutors helped their learning. Beyond AI tutors' ability to
provide valuable suggestions, students reported appreciating them for fostering
a safe learning environment free from judgment.",Iris Ma
2024-07-24T06:06:07Z,http://arxiv.org/abs/2407.17023v2,DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models,"Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.",Sara Vera Marjanović
2024-07-26T07:05:54Z,http://arxiv.org/abs/2407.18553v2,REAPER: Reasoning based Retrieval Planning for Complex RAG Systems,"Complex dialog systems often use retrieved evidence to facilitate factual
responses. Such RAG (Retrieval Augmented Generation) systems retrieve from
massive heterogeneous data stores that are usually architected as multiple
indexes or APIs instead of a single monolithic source. For a given query,
relevant evidence needs to be retrieved from one or a small subset of possible
retrieval sources. Complex queries can even require multi-step retrieval. For
example, a conversational agent on a retail site answering customer questions
about past orders will need to retrieve the appropriate customer order first
and then the evidence relevant to the customer's question in the context of the
ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by
interleaving reasoning and retrieval steps. However, each reasoning step
directly adds to the latency of the system. For large models this latency cost
is significant -- in the order of multiple seconds. Multi-agent systems may
classify the query to a single Agent associated with a retrieval source, though
this means that a (small) classification model dictates the performance of a
large language model. In this work we present REAPER (REAsoning-based PlannER)
- an LLM based planner to generate retrieval plans in conversational systems.
We show significant gains in latency over Agent-based systems and are able to
scale easily to new and unseen use cases as compared to classification-based
planning. Though our method can be applied to any RAG system, we show our
results in the context of a conversational shopping assistant.",Ashutosh Joshi
2024-07-28T12:47:20Z,http://arxiv.org/abs/2407.19487v1,RLCoder: Reinforcement Learning for Repository-Level Code Completion,"Repository-level code completion aims to generate code for unfinished code
snippets within the context of a specified repository. Existing approaches
mainly rely on retrieval-augmented generation strategies due to limitations in
input sequence length. However, traditional lexical-based retrieval methods
like BM25 struggle to capture code semantics, while model-based retrieval
methods face challenges due to the lack of labeled data for training.
Therefore, we propose RLCoder, a novel reinforcement learning framework, which
can enable the retriever to learn to retrieve useful content for code
completion without the need for labeled data. Specifically, we iteratively
evaluate the usefulness of retrieved content based on the perplexity of the
target code when provided with the retrieved content as additional context, and
provide feedback to update the retriever parameters. This iterative process
enables the retriever to learn from its successes and failures, gradually
improving its ability to retrieve relevant and high-quality content.
Considering that not all situations require information beyond code files and
not all retrieved context is helpful for generation, we also introduce a stop
signal mechanism, allowing the retriever to decide when to retrieve and which
candidates to retain autonomously. Extensive experimental results demonstrate
that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval
and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover,
experiments show that our framework can generalize across different programming
languages and further improve previous methods like RepoCoder. We provide the
code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.",Yanlin Wang
2024-07-30T09:04:45Z,http://arxiv.org/abs/2407.20668v1,"Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion
  Prediction for Social Media Influencers","Predicting influencers' views and public sentiment on social media is crucial
for anticipating societal trends and guiding strategic responses. This study
introduces a novel computational framework to predict opinion leaders'
perspectives and the emotive reactions of the populace, addressing the inherent
challenges posed by the unstructured, context-sensitive, and heterogeneous
nature of online communication. Our research introduces an innovative module
that starts with the automatic 5W1H (Where, Who, When, What, Why, and How)
questions formulation engine, tailored to emerging news stories and trending
topics. We then build a total of 60 anonymous opinion leader agents in six
domains and realize the views generation based on an enhanced large language
model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we
synthesize the potential views of opinion leaders and predicted the emotional
responses to different events. The efficacy of our automated 5W1H module is
corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity.
The influencer agents exhibit a consistent performance, achieving an average
GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the
'Russia-Ukraine War' as a case study, our methodology accurately foresees key
influencers' perspectives and aligns emotional predictions with real-world
sentiment trends in various domains.",Qinglan Wei
2024-07-31T04:01:08Z,http://arxiv.org/abs/2407.21320v2,MetaOpenFOAM: an LLM-based multi-agent framework for CFD,"Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and so on. MetaOpenFOAM harnesses the power of
MetaGPT's assembly line paradigm, which assigns diverse roles to various
agents, efficiently breaking down complex CFD tasks into manageable subtasks.
Langchain further complements MetaOpenFOAM by integrating Retrieval-Augmented
Generation (RAG) technology, which enhances the framework's ability by
integrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a
benchmark for natural language-based CFD solver, consisting of eight CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The eight CFD
simulation tasks encompass a range of multidimensional flow problems, covering
compressible and incompressible flows with different physical processes. This
demonstrates the capability to automate CFD simulations using only natural
language input, iteratively correcting errors to achieve the desired
simulations. An ablation study was conducted to verify the necessity of each
component in the multi-agent system and the RAG technology. A sensitivity study
on the randomness of LLM showed that LLM with low randomness can obtain more
stable and accurate results. Additionally, MetaOpenFOAM owns the ability to
identify and modify key parameters in user requirements, and excels in
correcting bugs when failure match occur,which demonstrates the generalization
of MetaOpenFOAM.",Yuxuan Chen
2024-07-28T14:55:22Z,http://arxiv.org/abs/2408.01462v1,"Faculty Perspectives on the Potential of RAG in Computer Science Higher
  Education","The emergence of Large Language Models (LLMs) has significantly impacted the
field of Natural Language Processing and has transformed conversational tasks
across various domains because of their widespread integration in applications
and public access. The discussion surrounding the application of LLMs in
education has raised ethical concerns, particularly concerning plagiarism and
policy compliance. Despite the prowess of LLMs in conversational tasks, the
limitations of reliability and hallucinations exacerbate the need to guardrail
conversations, motivating our investigation of RAG in computer science higher
education. We developed Retrieval Augmented Generation (RAG) applications for
the two tasks of virtual teaching assistants and teaching aids. In our study,
we collected the ratings and opinions of faculty members in undergraduate and
graduate computer science university courses at various levels, using our
personalized RAG systems for each course. This study is the first to gather
faculty feedback on the application of LLM-based RAG in education. The
investigation revealed that while faculty members acknowledge the potential of
RAG systems as virtual teaching assistants and teaching aids, certain barriers
and features are suggested for their full-scale deployment. These findings
contribute to the ongoing discussion on the integration of advanced language
models in educational settings, highlighting the need for careful consideration
of ethical implications and the development of appropriate safeguards to ensure
responsible and effective implementation.",Sagnik Dakshit
2024-08-06T03:44:06Z,http://arxiv.org/abs/2408.02937v2,"A Real-Time Adaptive Multi-Stream GPU System for Online Approximate
  Nearest Neighborhood Search","In recent years, Approximate Nearest Neighbor Search (ANNS) has played a
pivotal role in modern search and recommendation systems, especially in
emerging LLM applications like Retrieval-Augmented Generation. There is a
growing exploration into harnessing the parallel computing capabilities of GPUs
to meet the substantial demands of ANNS. However, existing systems primarily
focus on offline scenarios, overlooking the distinct requirements of online
applications that necessitate real-time insertion of new vectors. This
limitation renders such systems inefficient for real-world scenarios. Moreover,
previous architectures struggled to effectively support real-time insertion due
to their reliance on serial execution streams. In this paper, we introduce a
novel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our
architecture achieves its objectives through three key advancements: 1) We
initially examined the real-time insertion mechanisms in existing GPU ANNS
systems and discovered their reliance on repetitive copying and memory
allocation, which significantly hinders real-time effectiveness on GPUs. As a
solution, we introduce a dynamic vector insertion algorithm based on memory
blocks, which includes in-place rearrangement. 2) To enable real-time vector
insertion in parallel, we introduce a multi-stream parallel execution mode,
which differs from existing systems that operate serially within a single
stream. Our system utilizes a dynamic resource pool, allowing multiple streams
to execute concurrently without additional execution blocking. 3) Through
extensive experiments and comparisons, our approach effectively handles varying
QPS levels across different datasets, reducing latency by up to 40%-80%. The
proposed system has also been deployed in real-world industrial search and
recommendation systems, serving hundreds of millions of users daily, and has
achieved good results.",Yiping Sun
2024-08-06T09:02:53Z,http://arxiv.org/abs/2408.03047v2,"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready
  Multimodal Conversational Agents","Multimodal conversational agents are highly desirable because they offer
natural and human-like interaction. However, there is a lack of comprehensive
end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive
integration of audio, video, and text with response times of 200-250ms,
challenges remain in balancing latency, accuracy, cost, and data privacy. To
better understand and quantify these issues, we developed OpenOmni, an
open-source, end-to-end pipeline benchmarking tool that integrates advanced
technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented
Generation, Large Language Models, along with the ability to integrate
customized models. OpenOmni supports local and cloud deployment, ensuring data
privacy and supporting latency and accuracy benchmarking. This flexible
framework allows researchers to customize the pipeline, focusing on real
bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can
significantly enhance applications like indoor assistance for visually impaired
individuals, advancing human-computer interaction. Our demonstration video is
available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via
https://openomni.ai4wa.com, code is available via
https://github.com/AI4WA/OpenOmniFramework.",Qiang Sun
2024-08-06T16:55:54Z,http://arxiv.org/abs/2408.03297v2,"KnowPO: Knowledge-aware Preference Optimization for Controllable
  Knowledge Selection in Retrieval-Augmented Language Models","By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors such
as contextual ignorance and contextual overinclusion. To this end, we propose a
Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at
achieving adaptive knowledge selection based on contextual relevance in real
retrieval scenarios. Concretely, we proposed a general paradigm for
constructing knowledge conflict datasets, which comprehensively cover various
error types and learn how to avoid these negative signals through preference
optimization methods. Simultaneously, we proposed a rewriting strategy and data
ratio optimization strategy to address preference imbalances. Experimental
results show that KnowPO outperforms previous methods for handling knowledge
conflicts by over 37\%, while also exhibiting robust generalization across
various out-of-distribution datasets.",Ruizhe Zhang
2024-08-07T23:22:58Z,http://arxiv.org/abs/2408.04125v2,Exploring RAG-based Vulnerability Augmentation with LLMs,"Detecting vulnerabilities is vital for software security, yet deep
learning-based vulnerability detectors (DLVD) face a data shortage, which
limits their effectiveness. Data augmentation can potentially alleviate the
data shortage, but augmenting vulnerable code is challenging and requires a
generative solution that maintains vulnerability. Previous works have only
focused on generating samples that contain single statements or specific types
of vulnerabilities. Recently, large language models (LLMs) have been used to
solve various code generation and comprehension tasks with inspiring results,
especially when fused with retrieval augmented generation (RAG). Therefore, we
propose VulScribeR, a novel LLM-based solution that leverages carefully curated
prompt templates to augment vulnerable datasets. More specifically, we explore
three strategies to augment both single and multi-statement vulnerabilities,
with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation
across three vulnerability datasets and DLVD models, using two LLMs, show that
our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling
(ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable
samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated
vulnerable samples. Our approach demonstrates its feasibility for large-scale
data augmentation by generating 1K samples at as cheap as US$ 1.88.",Seyed Shayan Daneshvar
2024-08-02T19:49:19Z,http://arxiv.org/abs/2408.04645v1,"Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors
  for a Robotics Course","This study evaluates the performance of Large Language Models (LLMs) as an
Artificial Intelligence-based tutor for a university course. In particular,
different advanced techniques are utilized, such as prompt engineering,
Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the
different models and applied techniques using common similarity metrics like
BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of
helpfulness and trustworthiness. Our findings indicate that RAG combined with
prompt engineering significantly enhances model responses and produces better
factual answers. In the context of education, RAG appears as an ideal technique
as it is based on enriching the input of the model with additional information
and material which usually is already present for a university course.
Fine-tuning, on the other hand, can produce quite small, still strong expert
models, but poses the danger of overfitting. Our study further asks how we
measure performance of LLMs and how well current measurements represent
correctness or relevance? We find high correlation on similarity metrics and a
bias of most of these metrics towards shorter responses. Overall, our research
points to both the potential and challenges of integrating LLMs in educational
settings, suggesting a need for balanced training approaches and advanced
evaluation frameworks.",Sebastian Kahl
2024-08-08T22:18:01Z,http://arxiv.org/abs/2408.04775v1,"Hybrid Student-Teacher Large Language Model Refinement for Cancer
  Toxicity Symptom Extraction","Large Language Models (LLMs) offer significant potential for clinical symptom
extraction, but their deployment in healthcare settings is constrained by
privacy concerns, computational limitations, and operational costs. This study
investigates the optimization of compact LLMs for cancer toxicity symptom
extraction using a novel iterative refinement approach. We employ a
student-teacher architecture, utilizing Zephyr-7b-beta and Phi3-mini-128 as
student models and GPT-4o as the teacher, to dynamically select between prompt
refinement, Retrieval-Augmented Generation (RAG), and fine-tuning strategies.
Our experiments on 294 clinical notes covering 12 post-radiotherapy toxicity
symptoms demonstrate the effectiveness of this approach. The RAG method proved
most efficient, improving average accuracy scores from 0.32 to 0.73 for
Zephyr-7b-beta and from 0.40 to 0.87 for Phi3-mini-128 during refinement. In
the test set, both models showed an approximate 0.20 increase in accuracy
across symptoms. Notably, this improvement was achieved at a cost 45 times
lower than GPT-4o for Zephyr and 79 times lower for Phi-3. These results
highlight the potential of iterative refinement techniques in enhancing the
capabilities of compact LLMs for clinical applications, offering a balance
between performance, cost-effectiveness, and privacy preservation in healthcare
settings.",Reza Khanmohammadi
2024-08-09T05:20:05Z,http://arxiv.org/abs/2408.04870v5,ConfusedPilot: Confused Deputy Risks in RAG-based LLMs,"Retrieval augmented generation (RAG) is a process where a large language
model (LLM) retrieves useful information from a database and then generates the
responses. It is becoming popular in enterprise settings for daily business
operations. For example, Copilot for Microsoft 365 has accumulated millions of
businesses. However, the security implications of adopting such RAG-based
systems are unclear.
  In this paper, we introduce ConfusedPilot, a class of security
vulnerabilities of RAG systems that confuse Copilot and cause integrity and
confidentiality violations in its responses. First, we investigate a
vulnerability that embeds malicious text in the modified prompt in RAG,
corrupting the responses generated by the LLM. Second, we demonstrate a
vulnerability that leaks secret data, which leverages the caching mechanism
during retrieval. Third, we investigate how both vulnerabilities can be
exploited to propagate misinformation within the enterprise and ultimately
impact its operations, such as sales and manufacturing. We also discuss the
root cause of these attacks by investigating the architecture of a RAG-based
system. This study highlights the security vulnerabilities in today's RAG-based
systems and proposes design guidelines to secure future RAG-based systems.",Ayush RoyChowdhury
2024-08-09T12:26:05Z,http://arxiv.org/abs/2408.05025v2,"Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations
  in LLM-based Application Frameworks","Retrieval Augmented Generation (RAG) is a technique commonly used to equip
models with out of distribution knowledge. This process involves collecting,
indexing, retrieving, and providing information to an LLM for generating
responses. Despite its growing popularity due to its flexibility and low cost,
the security implications of RAG have not been extensively studied. The data
for such systems are often collected from public sources, providing an attacker
a gateway for indirect prompt injections to manipulate the responses of the
model. In this paper, we investigate the security of RAG systems against
end-to-end indirect prompt manipulations. First, we review existing RAG
framework pipelines, deriving a prototypical architecture and identifying
critical parameters. We then examine prior works searching for techniques that
attackers can use to perform indirect prompt manipulations. Finally, we
implemented Rag 'n Roll, a framework to determine the effectiveness of attacks
against end-to-end RAG applications. Our results show that existing attacks are
mostly optimized to boost the ranking of malicious documents during the
retrieval phase. However, a higher rank does not immediately translate into a
reliable attack. Most attacks, against various configurations, settle around a
40% success rate, which could rise to 60% when considering ambiguous answers as
successful attacks (those that include the expected benign one as well).
Additionally, when using unoptimized documents, attackers deploying two of them
(or more) for a target query can achieve similar results as those using
optimized ones. Finally, exploration of the configuration space of a RAG showed
limited impact in thwarting the attacks, where the most successful combination
severely undermines functionality.",Gianluca De Stefano
2024-08-09T15:53:55Z,http://arxiv.org/abs/2408.05141v3,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",Ye Yuan
2024-08-12T03:52:11Z,http://arxiv.org/abs/2408.05911v1,"A New Pipeline For Generating Instruction Dataset via RAG and Self
  Fine-Tuning","With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.",Chih-Wei Song
2024-08-12T06:16:37Z,http://arxiv.org/abs/2408.05933v1,"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case
  Study with Locally Deployed Ollama Models","With the growing demand for offline PDF chatbots in automotive industrial
production environments, optimizing the deployment of large language models
(LLMs) in local, low-performance settings has become increasingly important.
This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques
for processing complex automotive industry documents using locally deployed
Ollama models. Based on the Langchain framework, we propose a multi-dimensional
optimization approach for Ollama's local RAG implementation. Our method
addresses key challenges in automotive document processing, including
multi-column layouts and technical specifications. We introduce improvements in
PDF processing, retrieval mechanisms, and context compression, tailored to the
unique characteristics of automotive industry documents. Additionally, we
design custom classes supporting embedding pipelines and an agent supporting
self-RAG based on LangGraph best practices. To evaluate our approach, we
constructed a proprietary dataset comprising typical automotive industry
documents, including technical reports and corporate regulations. We compared
our optimized RAG model and self-RAG agent against a naive RAG baseline across
three datasets: our automotive industry dataset, QReCC, and CoQA. Results
demonstrate significant improvements in context precision, context recall,
answer relevancy, and faithfulness, with particularly notable performance on
the automotive industry dataset. Our optimization scheme provides an effective
solution for deploying local RAG systems in the automotive sector, addressing
the specific needs of PDF chatbots in industrial production environments. This
research has important implications for advancing information processing and
intelligent production in the automotive industry.",Fei Liu
2024-08-12T16:33:51Z,http://arxiv.org/abs/2408.06272v1,"A RAG-Based Question-Answering Solution for Cyber-Attack Investigation
  and Attribution","In the constantly evolving field of cybersecurity, it is imperative for
analysts to stay abreast of the latest attack trends and pertinent information
that aids in the investigation and attribution of cyber-attacks. In this work,
we introduce the first question-answering (QA) model and its application that
provides information to the cybersecurity experts about cyber-attacks
investigations and attribution. Our QA model is based on Retrieval Augmented
Generation (RAG) techniques together with a Large Language Model (LLM) and
provides answers to the users' queries based on either our knowledge base (KB)
that contains curated information about cyber-attacks investigations and
attribution or on outside resources provided by the users. We have tested and
evaluated our QA model with various types of questions, including KB-based,
metadata-based, specific documents from the KB, and external sources-based
questions. We compared the answers for KB-based questions with those from
OpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms
OpenAI's GPT models by providing the source of the answers and overcoming the
hallucination limitations of the GPT models, which is critical for cyber-attack
investigation and attribution. Additionally, our analysis showed that when the
RAG QA model is given few-shot examples rather than zero-shot instructions, it
generates better answers compared to cases where no examples are supplied in
addition to the query.",Sampath Rajapaksha
2024-08-15T04:29:33Z,http://arxiv.org/abs/2408.08335v1,Plan with Code: Comparing approaches for robust NL to DSL generation,"Planning in code is considered a more reliable approach for many
orchestration tasks. This is because code is more tractable than steps
generated via Natural Language and make it easy to support more complex
sequences by abstracting deterministic logic into functions. It also allows
spotting issues with incorrect function names with the help of parsing checks
that can be run on code. Progress in Code Generation methodologies, however,
remains limited to general-purpose languages like C, C++, and Python. LLMs
continue to face challenges with custom function names in Domain Specific
Languages or DSLs, leading to higher hallucination rates and syntax errors.
This is more common for custom function names, that are typically part of the
plan. Moreover, keeping LLMs up-to-date with newer function names is an issue.
This poses a challenge for scenarios like task planning over a large number of
APIs, since the plan is represented as a DSL having custom API names. In this
paper, we focus on workflow automation in RPA (Robotic Process Automation)
domain as a special case of task planning. We present optimizations for using
Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with
an ablation study comparing these strategies with a fine-tuned model. Our
results showed that the fine-tuned model scored the best on code similarity
metric. However, with our optimizations, RAG approach is able to match the
quality for in-domain API names in the test set. Additionally, it offers
significant advantage for out-of-domain or unseen API names, outperforming
Fine-Tuned model on similarity metric by 7 pts.",Nastaran Bassamzadeh
2024-08-16T20:55:21Z,http://arxiv.org/abs/2408.09017v1,Meta Knowledge for Retrieval Augmented Large Language Models,"Retrieval Augmented Generation (RAG) is a technique used to augment Large
Language Models (LLMs) with contextually relevant, time-critical, or
domain-specific information without altering the underlying model parameters.
However, constructing RAG systems that can effectively synthesize information
from large and diverse set of documents remains a significant challenge. We
introduce a novel data-centric RAG workflow for LLMs, transforming the
traditional retrieve-then-read system into a more advanced
prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher
domain expert-level understanding of the knowledge base. Our methodology relies
on generating metadata and synthetic Questions and Answers (QA) for each
document, as well as introducing the new concept of Meta Knowledge Summary (MK
Summary) for metadata-based clusters of documents. The proposed innovations
enable personalized user-query augmentation and in-depth information retrieval
across the knowledge base. Our research makes two significant contributions:
using LLMs as evaluators and employing new comparative performance metrics, we
demonstrate that (1) using augmented queries with synthetic question matching
significantly outperforms traditional RAG pipelines that rely on document
chunking (p < 0.01), and (2) meta knowledge-augmented queries additionally
significantly improve retrieval precision and recall, as well as the final
answers breadth, depth, relevancy, and specificity. Our methodology is
cost-effective, costing less than $20 per 2000 research papers using Claude 3
Haiku, and can be adapted with any fine-tuning of either the language or
embedding models to further enhance the performance of end-to-end RAG
pipelines.",Laurent Mombaerts
