Published Date,Link,Title,Summary,First Author
2024-08-20T17:49:51Z,http://arxiv.org/abs/2408.11043v1,"Reconciling Methodological Paradigms: Employing Large Language Models as
  Novice Qualitative Research Assistants in Talent Management Research","Qualitative data collection and analysis approaches, such as those employing
interviews and focus groups, provide rich insights into customer attitudes,
sentiment, and behavior. However, manually analyzing qualitative data requires
extensive time and effort to identify relevant topics and thematic insights.
This study proposes a novel approach to address this challenge by leveraging
Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for
analyzing interview transcripts. The novelty of this work lies in strategizing
the research inquiry as one that is augmented by an LLM that serves as a novice
research assistant. This research explores the mental model of LLMs to serve as
novice qualitative research assistants for researchers in the talent management
space. A RAG-based LLM approach is extended to enable topic modeling of
semi-structured interview data, showcasing the versatility of these models
beyond their traditional use in information retrieval and search. Our findings
demonstrate that the LLM-augmented RAG approach can successfully extract topics
of interest, with significant coverage compared to manually generated topics
from the same dataset. This establishes the viability of employing LLMs as
novice qualitative research assistants. Additionally, the study recommends that
researchers leveraging such models lean heavily on quality criteria used in
traditional qualitative research to ensure rigor and trustworthiness of their
approach. Finally, the paper presents key recommendations for industry
practitioners seeking to reconcile the use of LLMs with established qualitative
research paradigms, providing a roadmap for the effective integration of these
powerful, albeit novice, AI tools in the analysis of qualitative datasets
within talent",Sreyoshi Bhaduri
2024-08-20T20:47:27Z,http://arxiv.org/abs/2408.11189v1,Reading with Intent,"Retrieval augmented generation (RAG) systems augment how knowledge language
models are by integrating external information sources such as Wikipedia,
internal documents, scientific papers, or the open internet. RAG systems that
rely on the open internet as their knowledge source have to contend with the
complexities of human-generated content. Human communication extends much
deeper than just the words rendered as text. Intent, tonality, and connotation
can all change the meaning of what is being conveyed. Recent real-world
deployments of RAG systems have shown some difficulty in understanding these
nuances of human communication. One significant challenge for these systems
lies in processing sarcasm. Though the Large Language Models (LLMs) that make
up the backbone of these RAG systems are able to detect sarcasm, they currently
do not always use these detections for the subsequent processing of text. To
address these issues, in this paper, we synthetically generate sarcastic
passages from Natural Question's Wikipedia retrieval corpus. We then test the
impact of these passages on the performance of both the retriever and reader
portion of the RAG pipeline. We introduce a prompting system designed to
enhance the model's ability to interpret and generate responses in the presence
of sarcasm, thus improving overall system performance. Finally, we conduct
ablation studies to validate the effectiveness of our approach, demonstrating
improvements in handling sarcastic content within RAG systems.",Benjamin Reichman
2024-08-21T12:09:37Z,http://arxiv.org/abs/2408.11557v4,"A Quick, trustworthy spectral knowledge Q&A system leveraging
  retrieval-augmented generation on LLM","Large Language Model (LLM) has demonstrated significant success in a range of
natural language processing (NLP) tasks within general domain. The emergence of
LLM has introduced innovative methodologies across diverse fields, including
the natural sciences. Researchers aim to implement automated, concurrent
process driven by LLM to supplant conventional manual, repetitive and
labor-intensive work. In the domain of spectral analysis and detection, it is
imperative for researchers to autonomously acquire pertinent knowledge across
various research objects, which encompasses the spectroscopic techniques and
the chemometric methods that are employed in experiments and analysis.
Paradoxically, despite the recognition of spectroscopic detection as an
effective analytical method, the fundamental process of knowledge retrieval
remains both time-intensive and repetitive. In response to this challenge, we
first introduced the Spectral Detection and Analysis Based Paper(SDAAP)
dataset, which is the first open-source textual knowledge dataset for spectral
analysis and detection and contains annotated literature data as well as
corresponding knowledge instruction data. Subsequently, we also designed an
automated Q\&A framework based on the SDAAP dataset, which can retrieve
relevant knowledge and generate high-quality responses by extracting entities
in the input as retrieval parameters. It is worth noting that: within this
framework, LLM is only used as a tool to provide generalizability, while RAG
technique is used to accurately capture the source of the knowledge.This
approach not only improves the quality of the generated responses, but also
ensures the traceability of the knowledge. Experimental results show that our
framework generates responses with more reliable expertise compared to the
baseline.",Jiheng Liang
2024-08-22T09:37:40Z,http://arxiv.org/abs/2408.12249v1,LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction,"Large Language Models (LLMs) are increasingly adopted for applications in
healthcare, reaching the performance of domain experts on tasks such as
question answering and document summarisation. Despite their success on these
tasks, it is unclear how well LLMs perform on tasks that are traditionally
pursued in the biomedical domain, such as structured information extration. To
breach this gap, in this paper, we systematically benchmark LLM performance in
Medical Classification and Named Entity Recognition (NER) tasks. We aim to
disentangle the contribution of different factors to the performance,
particularly the impact of LLMs' task knowledge and reasoning capabilities,
their (parametric) domain knowledge, and addition of external knowledge. To
this end we evaluate various open LLMs -- including BioMistral and Llama-2
models -- on a diverse set of biomedical datasets, using standard prompting,
Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as
Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
Counter-intuitively, our results reveal that standard prompting consistently
outperforms more complex techniques across both tasks, laying bare the
limitations in the current application of CoT, self-consistency and RAG in the
biomedical domain. Our findings suggest that advanced prompting methods
developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are
not easily portable to biomedical tasks where precise structured outputs are
required. This highlights the need for more effective integration of external
knowledge and reasoning mechanisms in LLMs to enhance their performance in
real-world biomedical applications.",Aishik Nagar
2024-08-27T00:50:14Z,http://arxiv.org/abs/2408.14717v1,Text2SQL is Not Enough: Unifying AI and Databases with TAG,"AI systems that serve natural language questions over databases promise to
unlock tremendous value. Such systems would allow users to leverage the
powerful reasoning and knowledge capabilities of language models (LMs)
alongside the scalable computational power of data management systems. These
combined capabilities would empower users to ask arbitrary natural language
questions over custom data sources. However, existing methods and benchmarks
insufficiently explore this setting. Text2SQL methods focus solely on natural
language questions that can be expressed in relational algebra, representing a
small subset of the questions real users wish to ask. Likewise,
Retrieval-Augmented Generation (RAG) considers the limited subset of queries
that can be answered with point lookups to one or a few data records within the
database. We propose Table-Augmented Generation (TAG), a unified and
general-purpose paradigm for answering natural language questions over
databases. The TAG model represents a wide range of interactions between the LM
and database that have been previously unexplored and creates exciting research
opportunities for leveraging the world knowledge and reasoning capabilities of
LMs over data. We systematically develop benchmarks to study the TAG problem
and find that standard methods answer no more than 20% of queries correctly,
confirming the need for further research in this area. We release code for the
benchmark at https://github.com/TAG-Research/TAG-Bench.",Asim Biswal
2024-08-28T06:28:01Z,http://arxiv.org/abs/2408.15562v1,"Boosting Lossless Speculative Decoding via Feature Sampling and Partial
  Alignment Distillation","Lossless speculative decoding accelerates target large language model (LLM)
inference by employing a lightweight draft model for generating tree-structured
candidates, which are subsequently verified in parallel by the target LLM.
Currently, effective approaches leverage feature-level rather than token-level
autoregression within the draft model to facilitate more straightforward
predictions and enhanced knowledge distillation. In this paper, we reassess
these approaches and propose FSPAD (Feature Sampling and Partial Alignment
Distillation for Lossless Speculative Decoding), which introduces two
straightforward and effective components within the existing framework to boost
lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to
sample features of the target LLM in high-dimensional space before feeding them
into the draft model, due to the inherent uncertainty of the features
preventing the draft model from obtaining the specific token output by the
target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken
the draft model's connection between features and logits, aiming to reduce the
conflict between feature alignment and logit confidence during training. Our
experiments include both greedy and non-greedy decoding on the largest and
smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in
multi-turn conversation, translation, summarization, question answering,
mathematical reasoning, and retrieval-augmented generation. The results show
that FSPAD outperforms the state-of-the-art method across all the
aforementioned tasks and target LLMs.",Lujun Gui
2024-08-24T19:34:04Z,http://arxiv.org/abs/2409.00082v1,"Towards Human-Level Understanding of Complex Process Engineering
  Schematics: A Pedagogical, Introspective Multi-Agent Framework for
  Open-Domain Question Answering","In the chemical and process industries, Process Flow Diagrams (PFDs) and
Piping and Instrumentation Diagrams (P&IDs) are critical for design,
construction, and maintenance. Recent advancements in Generative AI, such as
Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in
understanding and interpreting process diagrams for Visual Question Answering
(VQA). However, proprietary models pose data privacy risks, and their
computational complexity prevents knowledge editing for domain-specific
customization on consumer hardware. To overcome these challenges, we propose a
secure, on-premises enterprise solution using a hierarchical, multi-agent
Retrieval Augmented Generation (RAG) framework for open-domain question
answering (ODQA) tasks, offering enhanced data privacy, explainability, and
cost-effectiveness. Our novel multi-agent framework employs introspective and
specialized sub-agents using open-source, small-scale multimodal models with
the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis,
integrating multiple information sources to provide accurate and contextually
relevant answers. Our approach, supported by iterative self-correction, aims to
deliver superior performance in ODQA tasks. We conducted rigorous experimental
studies, and the empirical results validated the proposed approach
effectiveness.",Sagar Srinivas Sakhinana
2024-08-26T08:17:42Z,http://arxiv.org/abs/2409.00090v1,Evaluating ChatGPT on Nuclear Domain-Specific Data,"This paper examines the application of ChatGPT, a large language model (LLM),
for question-and-answer (Q&A) tasks in the highly specialized field of nuclear
data. The primary focus is on evaluating ChatGPT's performance on a curated
test dataset, comparing the outcomes of a standalone LLM with those generated
through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their
recent advancements, are prone to generating incorrect or 'hallucinated'
information, which is a significant limitation in applications requiring high
accuracy and reliability. This study explores the potential of utilizing RAG in
LLMs, a method that integrates external knowledge bases and sophisticated
retrieval techniques to enhance the accuracy and relevance of generated
outputs. In this context, the paper evaluates ChatGPT's ability to answer
domain-specific questions, employing two methodologies: A) direct response from
the LLM, and B) response from the LLM within a RAG framework. The effectiveness
of these methods is assessed through a dual mechanism of human and LLM
evaluation, scoring the responses for correctness and other metrics. The
findings underscore the improvement in performance when incorporating a RAG
pipeline in an LLM, particularly in generating more accurate and contextually
appropriate responses for nuclear domain-specific queries. Additionally, the
paper highlights alternative approaches to further refine and improve the
quality of answers in such specialized domains.",Muhammad Anwar
2024-09-01T07:01:22Z,http://arxiv.org/abs/2409.00636v1,"A Learnable Agent Collaboration Network Framework for Personalized
  Multimodal AI Search Engine","Large language models (LLMs) and retrieval-augmented generation (RAG)
techniques have revolutionized traditional information access, enabling AI
agent to search and summarize information on behalf of users during dynamic
dialogues. Despite their potential, current AI search engines exhibit
considerable room for improvement in several critical areas. These areas
include the support for multimodal information, the delivery of personalized
responses, the capability to logically answer complex questions, and the
facilitation of more flexible interactions. This paper proposes a novel AI
Search Engine framework called the Agent Collaboration Network (ACN). The ACN
framework consists of multiple specialized agents working collaboratively, each
with distinct roles such as Account Manager, Solution Strategist, Information
Manager, and Content Creator. This framework integrates mechanisms for picture
content understanding, user profile tracking, and online evolution, enhancing
the AI search engine's response quality, personalization, and interactivity. A
highlight of the ACN is the introduction of a Reflective Forward Optimization
method (RFO), which supports the online synergistic adjustment among agents.
This feature endows the ACN with online learning capabilities, ensuring that
the system has strong interactive flexibility and can promptly adapt to user
feedback. This learning method may also serve as an optimization approach for
agent-based systems, potentially influencing other domains of agent
applications.",Yunxiao Shi
2024-09-02T23:28:15Z,http://arxiv.org/abs/2409.01495v1,The Compressor-Retriever Architecture for Language Model OS,"Recent advancements in large language models (LLMs) have significantly
enhanced their capacity to aggregate and process information across multiple
modalities, enabling them to perform a wide range of tasks such as multimodal
data querying, tool usage, web interactions, and handling long documents. These
capabilities pave the way for transforming LLMs from mere chatbots into
general-purpose agents capable of interacting with the real world. This paper
explores the concept of using a language model as the core component of an
operating system (OS), effectively acting as a CPU that processes data stored
in a context window, which functions as RAM. A key challenge in realizing such
an LM OS is managing the life-long context and ensuring statefulness across
sessions, a feature limited by the current session-based interaction paradigm
due to context window size limit. To address this, we introduce
compressor-retriever, a model-agnostic architecture designed for life-long
context management. Unlike other long-context solutions such as
retrieval-augmented generation, our approach exclusively uses the base model's
forward function to compress and retrieve context, ensuring end-to-end
differentiability. Preliminary experiments demonstrate the effectiveness of
this architecture in in-context learning tasks, marking a step towards the
development of a fully stateful LLM OS. Project repo available at:
https://github.com/gblackout/LM-OS",Yuan Yang
2024-09-03T02:50:04Z,http://arxiv.org/abs/2409.01556v2,"Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka
  Culture","This study introduces a comprehensive benchmark designed to evaluate the
performance of large language models (LLMs) in understanding and processing
cultural knowledge, with a specific focus on Hakka culture as a case study.
Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework
that systematically assesses LLMs across six cognitive domains: Remembering,
Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark
extends beyond traditional single-dimensional evaluations by providing a deeper
analysis of LLMs' abilities to handle culturally specific content, ranging from
basic recall of facts to higher-order cognitive tasks such as creative
synthesis. Additionally, the study integrates Retrieval-Augmented Generation
(RAG) technology to address the challenges of minority cultural knowledge
representation in LLMs, demonstrating how RAG enhances the models' performance
by dynamically incorporating relevant external information. The results
highlight the effectiveness of RAG in improving accuracy across all cognitive
domains, particularly in tasks requiring precise retrieval and application of
cultural knowledge. However, the findings also reveal the limitations of RAG in
creative tasks, underscoring the need for further optimization. This benchmark
provides a robust tool for evaluating and comparing LLMs in culturally diverse
contexts, offering valuable insights for future research and development in
AI-driven cultural knowledge preservation and dissemination.",Chen-Chi Chang
2024-09-04T00:10:36Z,http://arxiv.org/abs/2409.02343v1,"NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
  Retrieval","$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of $k$-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE's non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.",Sepanta Zeighami
2024-09-04T09:46:33Z,http://arxiv.org/abs/2409.02572v3,"Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models","Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within
the field of Digital Forensics (DF). It focuses on examining and analyzing
time-based digital artefacts, such as timestamps derived from event logs, file
metadata, and other relevant data, to correlate events linked to cyber
incidents and reconstruct their chronological sequence. Traditional tools often
struggle to efficiently handle the large volume and variety of data generated
during DF investigations and Incident Response (IR) processes. This paper
introduces a novel framework, GenDFIR, which combines Rule-Based Artificial
Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance
and automate the TA process. The proposed approach consists of two key stages:
(1) R-BAI is used to identify and select anomalous digital artefacts based on
predefined rules. (2) The selected artefacts are then transformed into
embeddings for processing by an LLM with the assistance of a
Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to
perform automated TA on the artefacts and predict potential incident outcomes.
To validate the framework, we evaluated its performance, efficiency, and
reliability. Several metrics were applied to simulated cyber incident
scenarios, which were presented as forensic case documents. Our findings
demonstrate the significant potential of integrating R-BAI and LLMs for TA.
This innovative approach underscores the power of Generative AI (GenAI),
particularly LLMs, and opens up new possibilities for advanced threat detection
and incident reconstruction, marking a significant advancement in the field.",Fatma Yasmine Loumachi
2024-09-04T13:49:19Z,http://arxiv.org/abs/2409.02711v1,"Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for
  PostNL","The developments in the field of generative AI has brought a lot of
opportunities for companies, for instance to improve efficiency in customer
service and automating tasks. PostNL, the biggest parcel and E-commerce
corporation of the Netherlands wants to use generative AI to enhance the
communication around track and trace of parcels. During the internship a
Minimal Viable Product (MVP) is created to showcase the value of using
generative AI technologies, to enhance parcel tracking, analyzing the parcel's
journey and being able to communicate about it in an easy to understand manner.
The primary goal was to develop an in-house LLM-based system, reducing
dependency on external platforms and establishing the feasibility of a
dedicated generative AI team within the company. This multi-agent LLM based
system aimed to construct parcel journey stories and identify logistical
disruptions with heightened efficiency and accuracy. The research involved
deploying a sophisticated AI-driven communication system, employing
Retrieval-Augmented Generation (RAG) for enhanced response precision, and
optimizing large language models (LLMs) tailored to domain specific tasks.
  The MVP successfully implemented a multi-agent open-source LLM system, called
SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of
user inquiries and improving internal knowledge handling. Results and
evaluation demonstrated technological innovation and feasibility, notably in
communication about the track and trace of a parcel, which exceeded initial
expectations. These advancements highlight the potential of AI-driven solutions
in logistics, suggesting many opportunities for further refinement and broader
implementation within PostNL operational framework.",Mohammad Reshadati
2024-09-04T16:43:14Z,http://arxiv.org/abs/2409.02864v3,Language Model Powered Digital Biology with BRAD,"Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, engineering, and every day life. However, integrating the
wide array of computational tools, databases, and scientific literature
continues to pose a challenge to biological research. LLMs are well-suited for
unstructured integration, efficient information retrieval, and automating
standard workflows and actions from these diverse resources. To harness these
capabilities in bioinformatics, we present a prototype Bioinformatics Retrieval
Augmented Digital assistant (BRAD). BRAD is a chatbot and agentic system that
integrates a variety of bioinformatics tools. The Python package implements an
AI \texttt{Agent} that is powered by LLMs and connects to a local file system,
online databases, and a user's software. The \texttt{Agent} is highly
configurable, enabling tasks such as Retrieval-Augmented Generation, searches
across bioinformatics databases, and the execution of software pipelines.
BRAD's coordinated integration of bioinformatics tools delivers a context-aware
and semi-autonomous system that extends beyond the capabilities of conventional
LLM-based chatbots. A graphical user interface (GUI) provides an intuitive
interface to the system.",Joshua Pickard
2024-09-05T17:14:23Z,http://arxiv.org/abs/2409.03708v2,RAG based Question-Answering for Contextual Response Prediction System,"Large Language Models (LLMs) have shown versatility in various Natural
Language Processing (NLP) tasks, including their potential as effective
question-answering systems. However, to provide precise and relevant
information in response to specific customer queries in industry settings, LLMs
require access to a comprehensive knowledge base to avoid hallucinations.
Retrieval Augmented Generation (RAG) emerges as a promising technique to
address this challenge. Yet, developing an accurate question-answering
framework for real-world applications using RAG entails several challenges: 1)
data availability issues, 2) evaluating the quality of generated content, and
3) the costly nature of human evaluation. In this paper, we introduce an
end-to-end framework that employs LLMs with RAG capabilities for industry use
cases. Given a customer query, the proposed system retrieves relevant knowledge
documents and leverages them, along with previous chat history, to generate
response suggestions for customer service agents in the contact centers of a
major retail company. Through comprehensive automated and human evaluations, we
show that this solution outperforms the current BERT-based algorithms in
accuracy and relevance. Our findings suggest that RAG-based LLMs can be an
excellent support to human customer service representatives by lightening their
workload.",Sriram Veturi
2024-08-16T21:59:59Z,http://arxiv.org/abs/2409.03759v1,VERA: Validation and Evaluation of Retrieval-Augmented Systems,"The increasing use of Retrieval-Augmented Generation (RAG) systems in various
applications necessitates stringent protocols to ensure RAG systems accuracy,
safety, and alignment with user intentions. In this paper, we introduce VERA
(Validation and Evaluation of Retrieval-Augmented Systems), a framework
designed to enhance the transparency and reliability of outputs from large
language models (LLMs) that utilize retrieved information. VERA improves the
way we evaluate RAG systems in two important ways: (1) it introduces a
cross-encoder based mechanism that encompasses a set of multidimensional
metrics into a single comprehensive ranking score, addressing the challenge of
prioritizing individual metrics, and (2) it employs Bootstrap statistics on
LLM-based metrics across the document repository to establish confidence
bounds, ensuring the repositorys topical coverage and improving the overall
reliability of retrieval systems. Through several use cases, we demonstrate how
VERA can strengthen decision-making processes and trust in AI applications. Our
findings not only contribute to the theoretical understanding of LLM-based RAG
evaluation metric but also promote the practical implementation of responsible
AI systems, marking a significant advancement in the development of reliable
and transparent generative AI technologies.",Tianyu Ding
2024-09-05T13:45:42Z,http://arxiv.org/abs/2409.04475v2,"Revolutionizing Database Q&A with Large Language Models: Comprehensive
  Benchmark and Evaluation","The development of Large Language Models (LLMs) has revolutionized QA across
various industries, including the database domain. However, there is still a
lack of a comprehensive benchmark to evaluate the capabilities of different
LLMs and their modular components in database QA. To this end, we introduce
DQABench, the first comprehensive database QA benchmark for LLMs. DQABench
features an innovative LLM-based method to automate the generation, cleaning,
and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in
English and Chinese, separately. These QA pairs cover a wide range of
database-related knowledge extracted from manuals, online communities, and
database instances. This inclusion allows for an additional assessment of LLMs'
Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)
capabilities in the database QA task. Furthermore, we propose a comprehensive
LLM-based database QA testbed DQATestbed. This testbed is highly modular and
scalable, with basic and advanced components such as Question Classification
Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,
DQABench provides a comprehensive evaluation pipeline that computes various
metrics throughout a standardized evaluation process to ensure the accuracy and
fairness of the evaluation. We use DQABench to evaluate the database QA
capabilities under the proposed testbed comprehensively. The evaluation reveals
findings like (i) the strengths and limitations of nine LLM-based QA bots and
(ii) the performance impact and potential improvements of various service
components (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the
future development of LLM-based database QA research.",Yihang Zheng
2024-09-09T20:52:25Z,http://arxiv.org/abs/2409.06062v1,Retrieval Augmented Correction of Named Entity Speech Recognition Errors,"In recent years, end-to-end automatic speech recognition (ASR) systems have
proven themselves remarkably accurate and performant, but these systems still
have a significant error rate for entity names which appear infrequently in
their training data. In parallel to the rise of end-to-end ASR systems, large
language models (LLMs) have proven to be a versatile tool for various natural
language processing (NLP) tasks. In NLP tasks where a database of relevant
knowledge is available, retrieval augmented generation (RAG) has achieved
impressive results when used with LLMs. In this work, we propose a RAG-like
technique for correcting speech recognition entity name errors. Our approach
uses a vector database to index a set of relevant entities. At runtime,
database queries are generated from possibly errorful textual ASR hypotheses,
and the entities retrieved using these queries are fed, along with the ASR
hypotheses, to an LLM which has been adapted to correct ASR errors. Overall,
our best system achieves 33%-39% relative word error rate reductions on
synthetic test sets focused on voice assistant queries of rare music entities
without regressing on the STOP test set, a publicly available voice assistant
test set covering many domains.",Ernest Pusateri
2024-09-10T12:12:09Z,http://arxiv.org/abs/2409.06450v1,"Multimodal Large Language Model Driven Scenario Testing for Autonomous
  Vehicles","The generation of corner cases has become increasingly crucial for
efficiently testing autonomous vehicles prior to road deployment. However,
existing methods struggle to accommodate diverse testing requirements and often
lack the ability to generalize to unseen situations, thereby reducing the
convenience and usability of the generated scenarios. A method that facilitates
easily controllable scenario generation for efficient autonomous vehicles (AV)
testing with realistic and challenging situations is greatly needed. To address
this, we proposed OmniTester: a multimodal Large Language Model (LLM) based
framework that fully leverages the extensive world knowledge and reasoning
capabilities of LLMs. OmniTester is designed to generate realistic and diverse
scenarios within a simulation environment, offering a robust solution for
testing and evaluating AVs. In addition to prompt engineering, we employ tools
from Simulation of Urban Mobility to simplify the complexity of codes generated
by LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a
self-improvement mechanism to enhance the LLM's understanding of scenarios,
thereby increasing its ability to produce more realistic scenes. In the
experiments, we demonstrated the controllability and realism of our approaches
in generating three types of challenging and complex scenarios. Additionally,
we showcased its effectiveness in reconstructing new scenarios described in
crash report, driven by the generalization capability of LLMs.",Qiujing Lu
2024-09-11T17:21:59Z,http://arxiv.org/abs/2409.07431v2,Synthetic continued pretraining,"Pretraining on large-scale, unstructured internet text enables language
models to acquire a significant amount of world knowledge. However, this
knowledge acquisition is data-inefficient--to learn a given fact, models must
be trained on hundreds to thousands of diverse representations of it. This
poses a challenge when adapting a pretrained model to a small corpus of
domain-specific documents, where each fact may appear rarely or only once. We
propose to bridge this gap with synthetic continued pretraining: using the
small domain-specific corpus to synthesize a large corpus more amenable to
learning, and then performing continued pretraining on the synthesized corpus.
We instantiate this proposal with EntiGraph, a synthetic data augmentation
algorithm that extracts salient entities from the source documents and then
generates diverse text by drawing connections between the sampled entities.
Synthetic continued pretraining with EntiGraph enables a language model to
answer questions and follow generic instructions related to the source
documents without access to them. If, instead, the source documents are
available at inference time, we show that the knowledge acquired through our
approach compounds with retrieval-augmented generation. To better understand
these results, we build a simple mathematical model of EntiGraph, and show how
synthetic data augmentation can ""rearrange"" knowledge to enable more
data-efficient learning.",Zitong Yang
2024-09-08T00:23:37Z,http://arxiv.org/abs/2409.07489v2,RAGent: Retrieval-based Access Control Policy Generation,"Manually generating access control policies from an organization's high-level
requirement specifications poses significant challenges. It requires laborious
efforts to sift through multiple documents containing such specifications and
translate their access requirements into access control policies. Also, the
complexities and ambiguities of these specifications often result in errors by
system administrators during the translation process, leading to data breaches.
However, the automated policy generation frameworks designed to help
administrators in this process are unreliable due to limitations, such as the
lack of domain adaptation. Therefore, to improve the reliability of access
control policy generation, we propose RAGent, a novel retrieval-based access
control policy generation framework based on language models. RAGent identifies
access requirements from high-level requirement specifications with an average
state-of-the-art F1 score of 87.9%. Through retrieval augmented generation,
RAGent then translates the identified access requirements into access control
policies with an F1 score of 77.9%. Unlike existing frameworks, RAGent
generates policies with complex components like purposes and conditions, in
addition to subjects, actions, and resources. Moreover, RAGent automatically
verifies the generated policies and iteratively refines them through a novel
verification-refinement mechanism, further improving the reliability of the
process by 3%, reaching the F1 score of 80.6%. We also introduce three
annotated datasets for developing access control policy generation frameworks
in the future, addressing the data scarcity of the domain.",Sakuna Harinda Jayasundara
2024-09-12T01:51:06Z,http://arxiv.org/abs/2409.07691v1,"Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,
  fine-tuning and deploying Rerankers for RAG","Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.",Gabriel de Souza P. Moreira
2024-09-12T08:25:33Z,http://arxiv.org/abs/2409.07829v1,"Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:
  A Case Study in WeChat","UI automation tests play a crucial role in ensuring the quality of mobile
applications. Despite the growing popularity of machine learning techniques to
generate these tests, they still face several challenges, such as the mismatch
of UI elements. The recent advances in Large Language Models (LLMs) have
addressed these issues by leveraging their semantic understanding capabilities.
However, a significant gap remains in applying these models to industrial-level
app testing, particularly in terms of cost optimization and knowledge
limitation. To address this, we introduce CAT to create cost-effective UI
automation tests for industry apps by combining machine learning and LLMs with
best practices. Given the task description, CAT employs Retrieval Augmented
Generation (RAG) to source examples of industrial app usage as the few-shot
learning context, assisting LLMs in generating the specific sequence of
actions. CAT then employs machine learning techniques, with LLMs serving as a
complementary optimizer, to map the target element on the UI screen. Our
evaluations on the WeChat testing dataset demonstrate the CAT's performance and
cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming
the state-of-the-art. We have also integrated our approach into the real-world
WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and
enhancing the developers' testing process.",Sidong Feng
2024-08-30T13:31:32Z,http://arxiv.org/abs/2409.09052v1,"OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in
  Computed Tomography","Multimodal large language models (MLLMs) have achieved significant success in
the general field of image processing. Their emerging task generalization and
freeform conversational capabilities can greatly facilitate medical diagnostic
assistance, helping patients better understand their conditions and enhancing
doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging
technique used to capture the internal mechanisms of a patient's condition and
is widely utilized. However, in past research, the complex textural features of
this imaging data have made accurate interpretation by algorithms challenging,
impeding the performance of general LLMs in diagnostic assistance. To address
this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is
trained on 120,000 CT images and diagnostic reports and includes a
Retrieval-Augmented Generation (RAG) module capable of effectively mitigating
model hallucinations. This module is informed by extensive medical literature,
textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT
images but also stores, understands, and reasons over medical knowledge and
language. In extensive experiments, OrthoDoc outperforms commercial models led
by GPT-4, demonstrating superior diagnostic capabilities and accuracy.
Specifically, OrthoDoc significantly surpasses existing models in the diagnosis
of common orthopedic conditions such as fractures, arthritis, and tumors.
Additionally, OrthoDoc exhibits robust generalization and stability when
handling rare and complex cases.",Youzhu Jin
2024-09-14T07:19:18Z,http://arxiv.org/abs/2409.09343v1,"Generative AI in Data Center Networking: Fundamentals, Perspectives, and
  Case Study","Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as
OpenAI's ChatGPT, is revolutionizing various fields. Central to this
transformation is Data Center Networking (DCN), which not only provides the
computational power necessary for GenAI training and inference but also
delivers GenAI-driven services to users. This article examines an interplay
between GenAI and DCNs, highlighting their symbiotic relationship and mutual
advancements. We begin by reviewing current challenges within DCNs and discuss
how GenAI contributes to enhancing DCN capabilities through innovations, such
as data augmentation, process automation, and domain transfer. We then focus on
analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining
insights that catalyze the evolution of DCNs to more effectively support GenAI
and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs,
we present a case study on full-lifecycle DCN digital twins. In this study, we
employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate
optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning
(DRL) for optimizing the RAG knowledge placement strategy. This approach not
only demonstrates the application of advanced GenAI methods within DCNs but
also positions the digital twin as a pivotal GenAI service operating on DCNs.
We anticipate that this article can promote further research into enhancing the
virtuous interaction between GenAI and DCNs.",Yinqiu Liu
2024-09-16T01:08:18Z,http://arxiv.org/abs/2409.09916v1,SFR-RAG: Towards Contextually Faithful LLMs,"Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.",Xuan-Phi Nguyen
2024-09-17T16:55:25Z,http://arxiv.org/abs/2409.11353v3,"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models","Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.",Mengfei Liang
2024-09-19T08:26:45Z,http://arxiv.org/abs/2409.12558v1,"RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval
  Augmented Dialogues","In real-world applications with Large Language Models (LLMs), external
retrieval mechanisms - such as Search-Augmented Generation (SAG), tool
utilization, and Retrieval-Augmented Generation (RAG) - are often employed to
enhance the quality of augmented generations in dialogues. These approaches
often come with multi-turn dialogue, where each interaction is enriched by
relevant information retrieved from external sources. Existing benchmarks
either assess LLMs' chat abilities in multi-turn dialogues or their use of
retrieval for augmented responses in single-turn settings. However, there is a
gap in evaluating LLMs' ability to leverage retrieval for more precise
responses across multiple turns. To address this limitation, we introduce
RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate
LLMs' capabilities in multi-turn dialogues following retrievals, essential for
their deployment in context-rich applications. RAD-Bench evaluates two key
abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are
measured using discriminative questions and retrieved contexts, and
corresponding reference answers, assessing how effectively LLMs integrate and
reason with context to maintain and enhance conversation quality over multiple
turns. Our evaluation results on commonly used LLMs reveal that model
performance deteriorates as additional layers of conditions or constraints are
applied across conversation turns, even when accurate retrieved contexts are
provided.",Tzu-Lin Kuo
2024-09-19T11:48:29Z,http://arxiv.org/abs/2409.12682v1,Retrieval-Augmented Test Generation: How Far Are We?,"Retrieval Augmented Generation (RAG) has shown notable advancements in
software engineering tasks. Despite its potential, RAG's application in unit
test generation remains under-explored. To bridge this gap, we take the
initiative to investigate the efficacy of RAG-based LLMs in test generation. As
RAGs can leverage various knowledge sources to enhance their performance, we
also explore the impact of different sources of RAGs' knowledge bases on unit
test generation to provide insights into their practical benefits and
limitations. Specifically, we examine RAG built upon three types of domain
knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.
Each source offers essential knowledge for creating tests from different
perspectives, i.e., API documentations provide official API usage guidelines,
GitHub issues offer resolutions of issues related to the APIs from the library
developers, and StackOverflow Q&As present community-driven solutions and best
practices. For our experiment, we focus on five widely used and typical
Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,
Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex
neural networks efficiently. We conducted experiments using the top 10% most
widely used APIs across these projects, involving a total of 188 APIs. We
investigate the effectiveness of four state-of-the-art LLMs (open and
closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1
405B. Additionally, we compare three prompting strategies in generating unit
test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an
API-level RAG on the three external sources. Finally, we compare the cost of
different sources of knowledge used for the RAG.",Jiho Shin
2024-09-19T14:36:00Z,http://arxiv.org/abs/2409.12812v2,"Towards Interactive and Learnable Cooperative Driving Automation: a
  Large Language Model-Driven Decision-Making Framework","At present, Connected Autonomous Vehicles (CAVs) have begun to open road
testing around the world, but their safety and efficiency performance in
complex scenarios is still not satisfactory. Cooperative driving leverages the
connectivity ability of CAVs to achieve synergies greater than the sum of their
parts, making it a promising approach to improving CAV performance in complex
scenarios. However, the lack of interaction and continuous learning ability
limits current cooperative driving to single-scenario applications and specific
Cooperative Driving Automation (CDA). To address these challenges, this paper
proposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative
driving framework, to achieve all-scenario and all-CDA. First, since Large
Language Models(LLMs) are not adept at handling mathematical calculations, an
environment module is introduced to update vehicle positions based on semantic
decisions, thus avoiding potential errors from direct LLM control of vehicle
positions. Second, based on the four levels of CDA defined by the SAE J3216
standard, we propose a Chain-of-Thought (COT) based reasoning module that
includes state perception, intent sharing, negotiation, and decision-making,
enhancing the stability of LLMs in multi-step reasoning tasks. Centralized
conflict resolution is then managed through a conflict coordinator in the
reasoning process. Finally, by introducing a memory module and employing
retrieval-augmented generation, CAVs are endowed with the ability to learn from
their past experiences. We validate the proposed CoDrivingLLM through ablation
experiments on the negotiation module, reasoning with different shots
experience, and comparison with other cooperative driving methods.",Shiyu Fang
2024-09-03T15:30:57Z,http://arxiv.org/abs/2409.13695v1,You Only Use Reactive Attention Slice For Long Context Retrieval,"Supporting longer context for Large Language Models (LLM) is a promising
direction to advance LLMs. As training a model for a longer context window is
computationally expensive, many alternative solutions, such as Retrieval
Augmented Generation (RAG), have been used. However, most existing RAG methods
adopt embedding-based retrieval that falls short on long contexts.
  To address such challenges, we propose an attention-based retrieval
technique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a
novel retrieval heuristic called reaction score to rank the relevance of each
sentence in the input context with the query sentence. Intuitively, we measure
how the per-token attention score ""reacts"" to the query and greedily retrieves
the most reactive sentences. Internally, YOURA generates a token-indexed vector
(called reaction vector) for the whole input context. To map each sentence to
the token-indexed vector, we propose an Embedding-Agnostic Sentence Yield
(EASY), a best-effort token wiggling algorithm.
  We evaluate our retrieval technique on three open-source pre-trained LLM
models across six LongBench QA datasets. Our technique achieves up to 30% vLLM
inference throughput improvement for serving long-context queries with a nearly
identical quality score to the simple yet effective truncate-middle approach.",Yun Joon Soh
2024-09-05T02:34:05Z,http://arxiv.org/abs/2409.13699v1,Vietnamese Legal Information Retrieval in Question-Answering System,"In the modern era of rapidly increasing data volumes, accurately retrieving
and recommending relevant documents has become crucial in enhancing the
reliability of Question Answering (QA) systems. Recently, Retrieval Augmented
Generation (RAG) has gained significant recognition for enhancing the
capabilities of large language models (LLMs) by mitigating hallucination issues
in QA systems, which is particularly beneficial in the legal domain. Various
methods, such as semantic search using dense vector embeddings or a combination
of multiple techniques to improve results before feeding them to LLMs, have
been proposed. However, these methods often fall short when applied to the
Vietnamese language due to several challenges, namely inefficient Vietnamese
data processing leading to excessive token length or overly simplistic ensemble
techniques that lead to instability and limited improvement. Moreover, a
critical issue often overlooked is the ordering of final relevant documents
which are used as reference to ensure the accuracy of the answers provided by
LLMs. In this report, we introduce our three main modifications taken to
address these challenges. First, we explore various practical approaches to
data processing to overcome the limitations of the embedding model.
Additionally, we enhance Reciprocal Rank Fusion by normalizing order to combine
results from keyword and vector searches effectively. We also meticulously
re-rank the source pieces of information used by LLMs with Active Retrieval to
improve user experience when refining the information generated. In our
opinion, this technique can also be considered as a new re-ranking method that
might be used in place of the traditional cross encoder. Finally, we integrate
these techniques into a comprehensive QA system, significantly improving its
performance and reliability",Thiem Nguyen Ba
2024-09-22T16:20:00Z,http://arxiv.org/abs/2409.14516v1,"Beyond Words: Evaluating Large Language Models in Transportation
  Planning","The resurgence and rapid advancement of Generative Artificial Intelligence
(GenAI) in 2023 has catalyzed transformative shifts across numerous industry
sectors, including urban transportation and logistics. This study investigates
the evaluation of Large Language Models (LLMs), specifically GPT-4 and
Phi-3-mini, to enhance transportation planning. The study assesses the
performance and spatial comprehension of these models through a
transportation-informed evaluation framework that includes general geospatial
skills, general transportation domain skills, and real-world transportation
problem-solving. Utilizing a mixed-methods approach, the research encompasses
an evaluation of the LLMs' general Geographic Information System (GIS) skills,
general transportation domain knowledge as well as abilities to support human
decision-making in the real-world transportation planning scenarios of
congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy
and reliability across various GIS and transportation-specific tasks compared
to Phi-3-mini, highlighting its potential as a robust tool for transportation
planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical
scenarios, suggesting its utility in resource-constrained environments. The
findings underscore the transformative potential of GenAI technologies in urban
transportation planning. Future work could explore the application of newer
LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a
broader set of real-world transportation planning and operations challenges, to
deepen the integration of advanced AI models in transportation management
practices.",Shaowei Ying
2024-09-23T16:16:08Z,http://arxiv.org/abs/2409.15163v1,"Lessons Learned on Information Retrieval in Electronic Health Records: A
  Comparison of Embedding Models and Pooling Strategies","Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.",Skatje Myers
2024-09-14T02:34:26Z,http://arxiv.org/abs/2409.15355v4,Block-Attention for Efficient RAG,"We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.",East Sun
2024-09-18T16:10:47Z,http://arxiv.org/abs/2409.15364v1,VERA: Validation and Enhancement for Retrieval Augmented systems,"Large language models (LLMs) exhibit remarkable capabilities but often
produce inaccurate responses, as they rely solely on their embedded knowledge.
Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external
information retrieval system, supplying additional context along with the query
to mitigate inaccuracies for a particular context. However, accuracy issues
still remain, as the model may rely on irrelevant documents or extrapolate
incorrectly from its training knowledge. To assess and improve the performance
of both the retrieval system and the LLM in a RAG framework, we propose
\textbf{VERA} (\textbf{V}alidation and \textbf{E}nhancement for
\textbf{R}etrieval \textbf{A}ugmented systems), a system designed to: 1)
Evaluate and enhance the retrieved context before response generation, and 2)
Evaluate and refine the LLM-generated response to ensure precision and minimize
errors. VERA employs an evaluator-cum-enhancer LLM that first checks if
external retrieval is necessary, evaluates the relevance and redundancy of the
retrieved context, and refines it to eliminate non-essential information.
Post-response generation, VERA splits the response into atomic statements,
assesses their relevance to the query, and ensures adherence to the context.
Our experiments demonstrate VERA's remarkable efficacy not only in improving
the performance of smaller open-source models, but also larger state-of-the art
models. These enhancements underscore VERA's potential to produce accurate and
relevant responses, advancing the state-of-the-art in retrieval-augmented
language modeling. VERA's robust methodology, combining multiple evaluation and
refinement steps, effectively mitigates hallucinations and improves retrieval
and response processes, making it a valuable tool for applications demanding
high accuracy and reliability in information generation. .",Nitin Aravind Birur
2024-09-26T21:44:11Z,http://arxiv.org/abs/2409.18313v4,"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and
  Generation","There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhouse of large-scale
non-parametric knowledge, however existing techniques do not directly transfer
to the embodied domain, which is multimodal, data is highly correlated, and
perception requires abstraction.
  To address these challenges, we introduce Embodied-RAG, a framework that
enhances the foundational model of an embodied agent with a non-parametric
memory system capable of autonomously constructing hierarchical knowledge for
both navigation and language generation. Embodied-RAG handles a full range of
spatial and semantic resolutions across diverse environments and query types,
whether for a specific object or a holistic description of ambiance. At its
core, Embodied-RAG's memory is structured as a semantic forest, storing
language descriptions at varying levels of detail. This hierarchical
organization allows the system to efficiently generate context-sensitive
outputs across different robotic platforms. We demonstrate that Embodied-RAG
effectively bridges RAG to the robotics domain, successfully handling over 200
explanation and navigation queries across 19 environments, highlighting its
promise for general-purpose non-parametric system for embodied agents.",Quanting Xie
2024-09-21T13:44:34Z,http://arxiv.org/abs/2409.19006v2,"Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent
  Framework for Intellectual Property Management and Analysis","Patents are the currency of innovation, and like any currency, they need to
be managed and protected (Gavin Potenza). Patents, as legal documents that
secure intellectual property rights, play a critical role in technological
innovation. The growing complexity of patent documents and the surge in patent
applications have created a need for automated solutions in patent analysis. In
this work, we present PatExpert, an autonomous multi-agent conversational
framework designed to streamline and optimize patent-related tasks. The
framework consists of a metaagent that coordinates task-specific expert agents
for various patent-related tasks and a critique agent for error handling and
feedback provision. The meta-agent orchestrates specialized expert agents, each
fine-tuned for specific tasks such as patent classification, acceptance, claim
generation, abstractive summarization, multi-patent analysis, and scientific
hypothesis generation. For multi-patent analysis, the framework incorporates
advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance
response accuracy and relevance by combining semantic similarity with knowledge
graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and
Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and
provide iterative feedback. The framework also prioritizes explainability,
ensuring transparent justifications for decisions made during patent analysis.
Its comprehensive capabilities make it a valuable tool for automating complex
patent workflows, enhancing efficiency, accuracy, and compliance in
patent-related tasks. Empirical evidence demonstrates significant improvements
in patent processing tasks, concluding that the framework offers a robust
solution for automating and optimizing patent analysis.",Sakhinana Sagar Srinivas
2024-09-24T23:33:07Z,http://arxiv.org/abs/2409.19019v1,RAGProbe: An Automated Approach for Evaluating RAG Applications,"Retrieval Augmented Generation (RAG) is increasingly being used when building
Generative AI applications. Evaluating these applications and RAG pipelines is
mostly done manually, via a trial and error process. Automating evaluation of
RAG pipelines requires overcoming challenges such as context misunderstanding,
wrong format, incorrect specificity, and missing content. Prior works therefore
focused on improving evaluation metrics as well as enhancing components within
the pipeline using available question and answer datasets. However, they have
not focused on 1) providing a schema for capturing different types of
question-answer pairs or 2) creating a set of templates for generating
question-answer pairs that can support automation of RAG pipeline evaluation.
In this paper, we present a technique for generating variations in
question-answer pairs to trigger failures in RAG pipelines. We validate 5
open-source RAG pipelines using 3 datasets. Our approach revealed the highest
failure rates when prompts combine multiple questions: 91% for questions when
spanning multiple documents and 78% for questions from a single document;
indicating a need for developers to prioritise handling these combined
questions. 60% failure rate was observed in academic domain dataset and 53% and
62% failure rates were observed in open-domain datasets. Our automated approach
outperforms the existing state-of-the-art methods, by increasing the failure
rate by 51% on average per dataset. Our work presents an automated approach for
continuously monitoring the health of RAG pipelines, which can be integrated
into existing CI/CD pipelines, allowing for improved quality.",Shangeetha Sivasothy
2024-09-30T10:48:20Z,http://arxiv.org/abs/2409.20181v2,"Reference Trustable Decoding: A Training-Free Augmentation Paradigm for
  Large Language Models","Large language models (LLMs) have rapidly advanced and demonstrated
impressive capabilities. In-Context Learning (ICL) and Parameter-Efficient
Fine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to
downstream tasks. ICL typically constructs a few-shot learning scenario, either
manually or by setting up a Retrieval-Augmented Generation (RAG) system,
helping models quickly grasp domain knowledge or question-answering patterns
without changing model parameters. However, this approach involves trade-offs,
such as slower inference speed and increased space occupancy. PEFT assists the
model in adapting to tasks through minimal parameter modifications, but the
training process still demands high hardware requirements, even with a small
number of parameters involved. To address these challenges, we propose
Reference Trustable Decoding (RTD), a paradigm that allows models to quickly
adapt to new tasks without fine-tuning, maintaining low inference costs. RTD
constructs a reference datastore from the provided training examples and
optimizes the LLM's final vocabulary distribution by flexibly selecting
suitable references based on the input, resulting in more trustable responses
and enabling the model to adapt to downstream tasks at a low cost. Experimental
evaluations on various LLMs using different benchmarks demonstrate that RTD
establishes a new paradigm for augmenting models to downstream tasks.
Furthermore, our method exhibits strong orthogonality with traditional methods,
allowing for concurrent usage. Our code can be found at
https://github.com/ShiLuohe/ReferenceTrustableDecoding",Luohe Shi
2024-10-02T04:29:08Z,http://arxiv.org/abs/2410.01231v1,"Revisiting the Index Construction of Proximity Graph-Based Approximate
  Nearest Neighbor Search","Proximity graphs (PG) have gained increasing popularity as the
state-of-the-art (SOTA) solutions to $k$-approximate nearest neighbor ($k$-ANN)
search on high-dimensional data, which serves as a fundamental function in
various fields, e.g. information retrieval and retrieval-augmented
generation~(RAG). Although PG-based approaches have the best $k$-ANN search
performance, their index construction cost is superlinear to the number of
points, since they have to identify close neighbors for each point to establish
the edges. Such superlinear cost substantially limits their scalability in the
era of big data. Hence, the goal of this paper is to accelerate the
construction of PG-based methods without compromising their $k$-ANN search
performance.
  To achieve this goal, two mainstream categories of PG are revisited: relative
neighborhood graph (RNG) and navigable small world graph (NSWG). By revisiting
their construction process, we find the issues of construction efficiency. To
address these issues, we propose a new construction framework with a novel
pruning strategy for edge selection, which accelerates RNG construction while
keeping its $k$-ANN search performance. Then, we integrate this framework into
NSWG construction to enhance both the construction efficiency and $k$-ANN
search performance. Moreover, extensive experiments are conducted to validate
our construction framework for both RNG and NSWG. The results demonstrate that
it significantly reduces the PG construction cost, achieving up to 5.6x speedup
while not compromising the $k$-ANN search performance.",Shuo Yang
2024-10-02T11:26:02Z,http://arxiv.org/abs/2410.01428v1,"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with
  Retrieval-Augmentation for Solving Challenging Tasks","State-of-the-art large language models (LLMs) exhibit impressive
problem-solving capabilities but may struggle with complex reasoning and
factual correctness. Existing methods harness the strengths of chain-of-thought
and retrieval-augmented generation (RAG) to decompose a complex problem into
simpler steps and apply retrieval to improve factual correctness. These methods
work well on straightforward reasoning tasks but often falter on challenging
tasks such as competitive programming and mathematics, due to frequent
reasoning errors and irrelevant knowledge retrieval. To address this, we
introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a
novel framework that leverages fine-tuned critic models to guide both reasoning
and retrieval processes through planning. CR-Planner solves a problem by
iteratively selecting and executing sub-goals. Initially, it identifies the
most promising sub-goal from reasoning, query generation, and retrieval, guided
by rewards given by a critic model named sub-goal critic. It then executes this
sub-goal through sampling and selecting the optimal output based on evaluations
from another critic model named execution critic. This iterative process,
informed by retrieved information and critic models, enables CR-Planner to
effectively navigate the solution space towards the final answer. We employ
Monte Carlo Tree Search to collect the data for training the critic models,
allowing for a systematic exploration of action sequences and their long-term
impacts. We validate CR-Planner on challenging domain-knowledge-intensive and
reasoning-heavy tasks, including competitive programming, theorem-driven math
reasoning, and complex domain retrieval problems. Our experiments demonstrate
that CR-Planner significantly outperforms baselines, highlighting its
effectiveness in addressing challenging problems by improving both reasoning
and retrieval.",Xingxuan Li
2024-10-02T17:37:18Z,http://arxiv.org/abs/2410.01782v1,"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large
  Language Models","Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/",Shayekh Bin Islam
2024-09-27T23:05:02Z,http://arxiv.org/abs/2410.01841v1,A GEN AI Framework for Medical Note Generation,"The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.",Hui Yi Leong
2024-10-03T12:24:18Z,http://arxiv.org/abs/2410.02429v2,"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language
  Models","Large Language Models (LLMs) have demonstrated remarkable capabilities across
textual and visual domains but often generate outputs that violate physical
laws, revealing a gap in their understanding of the physical world. Inspired by
human cognition, where perception is fundamental to reasoning, we explore
augmenting LLMs with enhanced perception abilities using Internet of Things
(IoT) sensor data and pertinent knowledge for IoT task reasoning in the
physical world. In this work, we systematically study LLMs capability to
address real-world IoT tasks by augmenting their perception and knowledge base,
and then propose a unified framework, IoT-LLM, to enhance such capability. In
IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats
amenable to LLMs, activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions, and expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning. To evaluate the performance, We design a new benchmark
with five real-world IoT tasks with different data types and reasoning
difficulties and provide the benchmarking results on six open-source and
close-source LLMs. Experimental results demonstrate the limitations of existing
LLMs with naive textual inputs that cannot perform these tasks effectively. We
show that IoT-LLM significantly enhances the performance of IoT tasks reasoning
of LLM, such as GPT-4, achieving an average improvement of 65% across various
tasks against previous methods. The results also showcase LLMs ability to
comprehend IoT data and the physical law behind data by providing a reasoning
process. Limitations of our work are claimed to inspire future research in this
new era.",Tuo An
2024-10-04T00:08:46Z,http://arxiv.org/abs/2410.03049v1,"Scalable Frame-based Construction of Sociocultural NormBases for
  Socially-Aware Dialogues","Sociocultural norms serve as guiding principles for personal conduct in
social interactions, emphasizing respect, cooperation, and appropriate
behavior, which is able to benefit tasks including conversational information
retrieval, contextual information retrieval and retrieval-enhanced machine
learning. We propose a scalable approach for constructing a Sociocultural Norm
(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We
construct a comprehensive and publicly accessible Chinese Sociocultural
NormBase. Our approach utilizes socially aware dialogues, enriched with
contextual frames, as the primary data source to constrain the generating
process and reduce the hallucinations. This enables extracting of high-quality
and nuanced natural-language norm statements, leveraging the pragmatic
implications of utterances with respect to the situation. As real dialogue
annotated with gold frames are not readily available, we propose using
synthetic data. Our empirical results show: (i) the quality of the SCNs derived
from synthetic data is comparable to that from real dialogues annotated with
gold frames, and (ii) the quality of the SCNs extracted from real data,
annotated with either silver (predicted) or gold frames, surpasses that without
the frame annotations. We further show the effectiveness of the extracted SCNs
in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple
downstream dialogue tasks.",Shilin Qu
2024-10-04T18:00:28Z,http://arxiv.org/abs/2410.03829v1,"Misinformation with Legal Consequences (MisLC): A New Task Towards
  Harnessing Societal Harm of Misinformation","Misinformation, defined as false or inaccurate information, can result in
significant societal harm when it is spread with malicious or even innocuous
intent. The rapid online information exchange necessitates advanced detection
mechanisms to mitigate misinformation-induced harm. Existing research, however,
has predominantly focused on assessing veracity, overlooking the legal
implications and social consequences of misinformation. In this work, we take a
novel angle to consolidate the definition of misinformation detection using
legal issues as a measurement of societal ramifications, aiming to bring
interdisciplinary efforts to tackle misinformation and its consequence. We
introduce a new task: Misinformation with Legal Consequence (MisLC), which
leverages definitions from a wide range of legal domains covering 4 broader
legal topics and 11 fine-grained legal issues, including hate speech, election
laws, and privacy regulations. For this task, we advocate a two-step dataset
curation approach that utilizes crowd-sourced checkworthiness and expert
evaluations of misinformation. We provide insights about the MisLC task through
empirical evidence, from the problem definition to experiments and expert
involvement. While the latest large language models and retrieval-augmented
generation are effective baselines for the task, we find they are still far
from replicating expert performance.",Chu Fei Luo
2024-10-06T18:46:28Z,http://arxiv.org/abs/2410.04585v1,"Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community
  Retrieval","Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.",Pengcheng Jiang
2024-10-07T07:02:09Z,http://arxiv.org/abs/2410.04790v1,"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA","In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.",Xinyu Wang
2024-10-08T07:28:17Z,http://arxiv.org/abs/2410.05752v1,"Exploring the Meaningfulness of Nearest Neighbor Search in
  High-Dimensional Space","Dense high dimensional vectors are becoming increasingly vital in fields such
as computer vision, machine learning, and large language models (LLMs), serving
as standard representations for multimodal data. Now the dimensionality of
these vector can exceed several thousands easily. Despite the nearest neighbor
search (NNS) over these dense high dimensional vectors have been widely used
for retrieval augmented generation (RAG) and many other applications, the
effectiveness of NNS in such a high-dimensional space remains uncertain, given
the possible challenge caused by the ""curse of dimensionality."" To address
above question, in this paper, we conduct extensive NNS studies with different
distance functions, such as $L_1$ distance, $L_2$ distance and
angular-distance, across diverse embedding datasets, of varied types,
dimensionality and modality. Our aim is to investigate factors influencing the
meaningfulness of NNS. Our experiments reveal that high-dimensional text
embeddings exhibit increased resilience as dimensionality rises to higher
levels when compared to random vectors. This resilience suggests that text
embeddings are less affected to the ""curse of dimensionality,"" resulting in
more meaningful NNS outcomes for practical use. Additionally, the choice of
distance function has minimal impact on the relevance of NNS. Our study shows
the effectiveness of the embedding-based data representation method and can
offer opportunity for further optimization of dense vector-related
applications.",Zhonghan Chen
2024-10-08T12:30:07Z,http://arxiv.org/abs/2410.05983v1,Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,"Retrieval-augmented generation (RAG) empowers large language models (LLMs) to
utilize external knowledge sources. The increasing capacity of LLMs to process
longer input sequences opens up avenues for providing more retrieved
information, to potentially enhance the quality of generated outputs. It is
plausible to assume that a larger retrieval set would contain more relevant
information (higher recall), that might result in improved performance.
However, our empirical findings demonstrate that for many long-context LLMs,
the quality of generated output initially improves first, but then subsequently
declines as the number of retrieved passages increases. This paper investigates
this phenomenon, identifying the detrimental impact of retrieved ""hard
negatives"" as a key contributor. To mitigate this and enhance the robustness of
long-context LLM-based RAG, we propose both training-free and training-based
approaches. We first showcase the effectiveness of retrieval reordering as a
simple yet powerful training-free optimization. Furthermore, we explore
training-based methods, specifically RAG-specific implicit LLM fine-tuning and
RAG-oriented fine-tuning with intermediate reasoning, demonstrating their
capacity for substantial performance gains. Finally, we conduct a systematic
analysis of design choices for these training-based methods, including data
distribution, retriever selection, and training context length.",Bowen Jin
2024-10-09T17:59:58Z,http://arxiv.org/abs/2410.07176v1,"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for Large Language Models","Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.",Fei Wang
2024-10-10T17:55:02Z,http://arxiv.org/abs/2410.08182v1,"MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal
  Models","Existing multimodal retrieval benchmarks primarily focus on evaluating
whether models can retrieve and utilize external textual knowledge for question
answering. However, there are scenarios where retrieving visual information is
either more beneficial or easier to access than textual data. In this paper, we
introduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in
which we systematically identify and categorize scenarios where visually
augmented knowledge is better than textual knowledge, for instance, more images
from varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353
human-annotated multiple-choice questions across 9 distinct scenarios. With
MRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large
vision-language models (LVLMs). Our results show that all LVLMs exhibit greater
improvements when augmented with images compared to textual knowledge,
confirming that MRAG-Bench is vision-centric. Additionally, we conduct
extensive analysis with MRAG-Bench, which offers valuable insights into
retrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces
challenges in effectively leveraging retrieved knowledge, achieving only a
5.82% improvement with ground-truth information, in contrast to a 33.16%
improvement observed in human participants. These findings highlight the
importance of MRAG-Bench in encouraging the community to enhance LVLMs' ability
to utilize retrieved visual knowledge more effectively.",Wenbo Hu
2024-10-10T18:21:00Z,http://arxiv.org/abs/2410.08289v1,"Increasing the Difficulty of Automatically Generated Questions via
  Reinforcement Learning with Synthetic Preference","As the cultural heritage sector increasingly adopts technologies like
Retrieval-Augmented Generation (RAG) to provide more personalised search
experiences and enable conversations with collections data, the demand for
specialised evaluation datasets has grown. While end-to-end system testing is
essential, it's equally important to assess individual components. We target
the final, answering task, which is well-suited to Machine Reading
Comprehension (MRC). Although existing MRC datasets address general domains,
they lack the specificity needed for cultural heritage information.
Unfortunately, the manual creation of such datasets is prohibitively expensive
for most heritage institutions. This paper presents a cost-effective approach
for generating domain-specific MRC datasets with increased difficulty using
Reinforcement Learning from Human Feedback (RLHF) from synthetic preference
data. Our method leverages the performance of existing question-answering
models on a subset of SQuAD to create a difficulty metric, assuming that more
challenging questions are answered correctly less frequently. This research
contributes: (1) A methodology for increasing question difficulty using PPO and
synthetic data; (2) Empirical evidence of the method's effectiveness, including
human evaluation; (3) An in-depth error analysis and study of emergent
phenomena; and (4) An open-source codebase and set of three llama-2-chat
adapters for reproducibility and adaptation.",William Thorne
2024-10-11T13:36:13Z,http://arxiv.org/abs/2410.08801v1,"A Methodology for Evaluating RAG Systems: A Case Study On Configuration
  Dependency Validation","Retrieval-augmented generation (RAG) is an umbrella of different components,
design decisions, and domain-specific adaptations to enhance the capabilities
of large language models and counter their limitations regarding hallucination
and outdated and missing knowledge. Since it is unclear which design decisions
lead to a satisfactory performance, developing RAG systems is often
experimental and needs to follow a systematic and sound methodology to gain
sound and reliable results. However, there is currently no generally accepted
methodology for RAG evaluation despite a growing interest in this technology.
In this paper, we propose a first blueprint of a methodology for a sound and
reliable evaluation of RAG systems and demonstrate its applicability on a
real-world software engineering research task: the validation of configuration
dependencies across software technologies. In summary, we make two novel
contributions: (i) A novel, reusable methodological design for evaluating RAG
systems, including a demonstration that represents a guideline, and (ii) a RAG
system, which has been developed following this methodology, that achieves the
highest accuracy in the field of dependency validation. For the blueprint's
demonstration, the key insights are the crucial role of choosing appropriate
baselines and metrics, the necessity for systematic RAG refinements derived
from qualitative failure analysis, as well as the reporting practices of key
design decision to foster replication and evaluation.",Sebastian Simon
2024-10-11T17:57:06Z,http://arxiv.org/abs/2410.09141v1,ACER: Automatic Language Model Context Extension via Retrieval,"Long-context modeling is one of the critical capabilities of language AI for
digesting and reasoning over complex information pieces. In practice,
long-context capabilities are typically built into a pre-trained language
model~(LM) through a carefully designed context extension stage, with the goal
of producing generalist long-context capabilities. In our preliminary
experiments, however, we discovered that the current open-weight generalist
long-context models are still lacking in practical long-context processing
tasks. While this means perfectly effective long-context modeling demands
task-specific data, the cost can be prohibitive. In this paper, we draw
inspiration from how humans process a large body of information: a lossy
\textbf{retrieval} stage ranks a large set of documents while the reader ends
up reading deeply only the top candidates. We build an \textbf{automatic} data
synthesis pipeline that mimics this process using short-context LMs. The
short-context LMs are further tuned using these self-generated data to obtain
task-specific long-context capabilities. Similar to how pre-training learns
from imperfect data, we hypothesize and further demonstrate that the
short-context model can bootstrap over the synthetic data, outperforming not
only long-context generalist models but also the retrieval and read pipeline
used to synthesize the training data in real-world tasks such as long-context
retrieval augmented generation.",Luyu Gao
2024-10-12T22:31:01Z,http://arxiv.org/abs/2410.09662v1,"Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and
  Nays!","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge bases, achieving state-of-the-art results in
various coding tasks. The core of RAG is retrieving demonstration examples,
which is essential to balance effectiveness (generation quality) and efficiency
(retrieval time) for optimal performance. However, the high-dimensional nature
of code representations and large knowledge bases often create efficiency
bottlenecks, which are overlooked in previous research. This paper
systematically evaluates the efficiency-effectiveness trade-off of retrievers
across three coding tasks: Program Synthesis, Commit Message Generation, and
Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)
and four dense retrievers, including one exhaustive dense retriever (SBERT's
Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).
Our findings show that while BM25 excels in effectiveness, it suffers in
efficiency as the knowledge base grows beyond 1000 entries. In large-scale
retrieval, efficiency differences become more pronounced, with approximate
dense retrievers offering the greatest gains. For instance, in Commit
Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in
RougeL compared with BM25. Our results also show that increasing the number of
demonstrations in the prompt doesn't always improve the effectiveness and can
increase latency and lead to incorrect outputs. Our findings provide valuable
insights for practitioners aiming to build efficient and effective RAG systems
for coding tasks.",Pengfei He
2024-10-13T16:28:38Z,http://arxiv.org/abs/2410.09908v1,"Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble
  for Zero-shot Learning","Foundation models have become a cornerstone in deep learning, with techniques
like Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models.
Similarly, methods such as Retrieval-Augmented Generation (RAG), which leverage
vectorized databases, have further improved model performance by grounding
outputs in external information. While these approaches have demonstrated
notable success, they often require extensive training or labeled data, which
can limit their adaptability in resource-constrained environments. To address
these challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new
method that creates a vectorized database of LoRAs, enabling efficient
retrieval and application of model adaptations to new tasks. RPE minimizes the
need for extensive training and eliminates the requirement for labeled data,
making it particularly effective for zero-shot learning. Additionally, RPE is
well-suited for privacy-sensitive domains like healthcare, as it modifies model
parameters without accessing raw data. When applied to tasks such as medical
report generation and image segmentation, RPE not only proved effective but
also surpassed supervised fine-tuning methods in certain cases, highlighting
its potential to enhance both computational efficiency and privacy in deep
learning applications.",Pengfei Jin
2024-10-14T17:59:58Z,http://arxiv.org/abs/2410.10817v1,When Does Perceptual Alignment Benefit Vision Representations?,"Humans judge perceptual similarity according to diverse visual attributes,
including scene layout, subject location, and camera pose. Existing vision
models understand a wide range of semantic abstractions but improperly weigh
these attributes and thus make inferences misaligned with human perception.
While vision representations have previously benefited from alignment in
contexts like image generation, the utility of perceptually aligned
representations in more general-purpose settings remains unclear. Here, we
investigate how aligning vision model representations to human perceptual
judgments impacts their usability across diverse computer vision tasks. We
finetune state-of-the-art models on human similarity judgments for image
triplets and evaluate them across standard vision benchmarks. We find that
aligning models to perceptual judgments yields representations that improve
upon the original backbones across many downstream tasks, including counting,
segmentation, depth estimation, instance retrieval, and retrieval-augmented
generation. In addition, we find that performance is widely preserved on other
tasks, including specialized out-of-distribution domains such as in medical
imaging and 3D environment frames. Our results suggest that injecting an
inductive bias about human perceptual knowledge into vision models can
contribute to better representations.",Shobhita Sundaram
2024-10-15T19:04:13Z,http://arxiv.org/abs/2410.11996v1,"Holistic Reasoning with Long-Context LMs: A Benchmark for Database
  Operations on Massive Textual Data","The rapid increase in textual information means we need more efficient
methods to sift through, organize, and understand it all. While
retrieval-augmented generation (RAG) models excel in accessing information from
large document collections, they struggle with complex tasks that require
aggregation and reasoning over information spanning across multiple
documents--what we call holistic reasoning. Long-context language models
(LCLMs) have great potential for managing large-scale documents, but their
holistic reasoning capabilities remain unclear. In this work, we introduce
HoloBench, a novel framework that brings database reasoning operations into
text-based contexts, making it easier to systematically evaluate how LCLMs
handle holistic reasoning across large documents. Our approach adjusts key
factors such as context length, information density, distribution of
information, and query complexity to evaluate LCLMs comprehensively. Our
experiments show that the amount of information in the context has a bigger
influence on LCLM performance than the actual context length. Furthermore, the
complexity of queries affects performance more than the amount of information,
particularly for different types of queries. Interestingly, queries that
involve finding maximum or minimum values are easier for LCLMs and are less
affected by context length, even though they pose challenges for RAG systems.
However, tasks requiring the aggregation of multiple pieces of information show
a noticeable drop in accuracy as context length increases. Additionally, we
find that while grouping relevant information generally improves performance,
the optimal positioning varies across models. Our findings surface both the
advancements and the ongoing challenges in achieving a holistic understanding
of long contexts.",Seiji Maekawa
2024-10-16T08:55:49Z,http://arxiv.org/abs/2410.12380v1,"Evaluation of Attribution Bias in Retrieval-Augmented Large Language
  Models","Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.",Amin Abolghasemi
2024-10-16T17:59:32Z,http://arxiv.org/abs/2410.12788v2,"Meta-Chunking: Learning Efficient Text Segmentation via Logical
  Perception","Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances
performance and speed, and precisely identifies the boundaries of text chunks
by analyzing the characteristics of context perplexity distribution.
Additionally, considering the inherent complexity of different texts, we
propose a strategy that combines PPL Chunking with dynamic merging to achieve a
balance between fine-grained and coarse-grained text chunking. Experiments
conducted on eleven datasets demonstrate that Meta-Chunking can more
efficiently improve the performance of single-hop and multi-hop question
answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it
outperforms similarity chunking by 1.32 while only consuming 45.8% of the time.
Furthermore, through the analysis of models of various scales and types, we
observed that PPL Chunking exhibits notable flexibility and adaptability. Our
code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",Jihao Zhao
2024-10-11T19:16:03Z,http://arxiv.org/abs/2410.12858v1,"Large Language Models for Medical OSCE Assessment: A Novel Approach to
  Transcript Analysis","Grading Objective Structured Clinical Examinations (OSCEs) is a
time-consuming and expensive process, traditionally requiring extensive manual
effort from human experts. In this study, we explore the potential of Large
Language Models (LLMs) to assess skills related to medical student
communication. We analyzed 2,027 video-recorded OSCE examinations from the
University of Texas Southwestern Medical Center (UTSW), spanning four years
(2019-2022), and several different medical cases or ""stations."" Specifically,
our focus was on evaluating students' ability to summarize patients' medical
history: we targeted the rubric item 'did the student summarize the patients'
medical history?' from the communication skills rubric. After transcribing
speech audio captured by OSCE videos using Whisper-v3, we studied the
performance of various LLM-based approaches for grading students on this
summarization task based on their examination transcripts. Using various
frontier-level open-source and proprietary LLMs, we evaluated different
techniques such as zero-shot chain-of-thought prompting, retrieval augmented
generation, and multi-model ensemble methods. Our results show that frontier
LLM models like GPT-4 achieved remarkable alignment with human graders,
demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential
for LLM-based OSCE grading to augment the current grading process. Open-source
models also showed promising results, suggesting potential for widespread,
cost-effective deployment. Further, we present a failure analysis identifying
conditions where LLM grading may be less reliable in this context and recommend
best practices for deploying LLMs in medical education settings.",Ameer Hamza Shakur
2024-10-16T23:03:27Z,http://arxiv.org/abs/2410.13085v1,"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language
  Models","Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.",Peng Xia
2024-10-17T04:00:29Z,http://arxiv.org/abs/2410.13198v1,"Failing Forward: Improving Generative Error Correction for ASR with
  Synthetic Data and Retrieval Augmentation","Generative Error Correction (GEC) has emerged as a powerful post-processing
method to enhance the performance of Automatic Speech Recognition (ASR)
systems. However, we show that GEC models struggle to generalize beyond the
specific types of errors encountered during training, limiting their ability to
correct new, unseen errors at test time, particularly in out-of-domain (OOD)
scenarios. This phenomenon amplifies with named entities (NEs), where, in
addition to insufficient contextual information or knowledge about the NEs,
novel NEs keep emerging. To address these issues, we propose DARAG (Data- and
Retrieval-Augmented Generative Error Correction), a novel approach designed to
improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC
training dataset with synthetic data generated by prompting LLMs and
text-to-speech models, thereby simulating additional errors from which the
model can learn. For OOD scenarios, we simulate test-time errors from new
domains similarly and in an unsupervised fashion. Additionally, to better
handle named entities, we introduce retrieval-augmented correction by
augmenting the input with entities retrieved from a database. Our approach is
simple, scalable, and both domain- and language-agnostic. We experiment on
multiple datasets and settings, showing that DARAG outperforms all our
baselines, achieving 8\% -- 30\% relative WER improvements in ID and 10\% --
33\% improvements in OOD settings.",Sreyan Ghosh
2024-10-17T08:37:25Z,http://arxiv.org/abs/2410.13326v1,"Comparing the Utility, Preference, and Performance of Course Material
  Search Functionality and Retrieval-Augmented Generation Large Language Model
  (RAG-LLM) AI Chatbots in Information-Seeking Tasks","Providing sufficient support for students requires substantial resources,
especially considering the growing enrollment numbers. Students need help in a
variety of tasks, ranging from information-seeking to requiring support with
course assignments. To explore the utility of recent large language models
(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that
augments the answers that are produced with information from the course
materials. To study the effect of the LLM-powered AI chatbot, we conducted a
lab-based user study (N=14), in which the participants worked on tasks from a
web software development course. The participants were divided into two groups,
where one of the groups first had access to the chatbot and then to a more
traditional search functionality, while another group started with the search
functionality and was then given the chatbot. We assessed the participants'
performance and perceptions towards the chatbot and the search functionality
and explored their preferences towards the support functionalities. Our
findings highlight that both support mechanisms are seen as useful and that
support mechanisms work well for specific tasks, while less so for other tasks.
We also observe that students tended to prefer the second support mechanism
more, where students who were first given the chatbot tended to prefer the
search functionality and vice versa.",Leonardo Pasquarelli
2024-10-17T13:33:12Z,http://arxiv.org/abs/2410.13542v1,LLM-based Unit Test Generation via Property Retrieval,"Automated unit test generation has been widely studied, with Large Language
Models (LLMs) recently showing significant potential. Moreover, in the context
of unit test generation, these tools prioritize high code coverage, often at
the expense of practical usability, correctness, and maintainability. In
response, we propose Property-Based Retrieval Augmentation, a novel mechanism
that extends LLM-based Retrieval-Augmented Generation (RAG) beyond basic
vector, text similarity, and graph-based methods. Our approach considers
task-specific context and introduces a tailored property retrieval mechanism.
Specifically, in the unit test generation task, we account for the unique
structure of unit tests by dividing the test generation process into Given,
When, and Then phases. When generating tests for a focal method, we not only
retrieve general context for the code under test but also consider
task-specific context such as pre-existing tests of other methods, which can
provide valuable insights for any of the Given, When, and Then phases. This
forms property relationships between focal method and other methods, thereby
expanding the scope of retrieval beyond traditional RAG. We implement this
approach in a tool called APT, which sequentially performs preprocessing,
property retrieval, and unit test generation, using an iterative strategy where
newly generated tests guide the creation of subsequent ones. We evaluated APT
on 12 open-source projects with 1515 methods, and the results demonstrate that
APT consistently outperforms existing tools in terms of correctness,
completeness, and maintainability of the generated tests. Moreover, we
introduce a novel code-context-aware retrieval mechanism for LLMs beyond
general context, offering valuable insights and potential applications for
other code-related tasks.",Zhe Zhang
2024-10-18T06:51:13Z,http://arxiv.org/abs/2410.14209v1,"Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents","In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.",Zihan Liu
2024-10-18T16:44:22Z,http://arxiv.org/abs/2410.14594v2,"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases","Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).",Elias Lumer
2024-10-19T07:08:40Z,http://arxiv.org/abs/2410.15016v1,"Transit Pulse: Utilizing Social Media as a Source for Customer Feedback
  and Information Extraction with Large Language Model","Users of the transit system flood social networks daily with messages that
contain valuable insights crucial for improving service quality. These posts
help transit agencies quickly identify emerging issues. Parsing topics and
sentiments is key to gaining comprehensive insights to foster service
excellence. However, the volume of messages makes manual analysis impractical,
and standard NLP techniques like Term Frequency-Inverse Document Frequency
(TF-IDF) fall short in nuanced interpretation. Traditional sentiment analysis
separates topics and sentiments before integrating them, often missing the
interaction between them. This incremental approach complicates classification
and reduces analytical productivity. To address these challenges, we propose a
novel approach to extracting and analyzing transit-related information,
including sentiment and sarcasm detection, identification of unusual system
problems, and location data from social media. Our method employs Large
Language Models (LLM), specifically Llama 3, for a streamlined analysis free
from pre-established topic labels. To enhance the model's domain-specific
knowledge, we utilize Retrieval-Augmented Generation (RAG), integrating
external knowledge sources into the information extraction pipeline. We
validated our method through extensive experiments comparing its performance
with traditional NLP approaches on user tweet data from the real world transit
system. Our results demonstrate the potential of LLMs to transform social media
data analysis in the public transit domain, providing actionable insights and
enhancing transit agencies' responsiveness by extracting a broader range of
information.",Jiahao Wang
2024-10-19T16:46:21Z,http://arxiv.org/abs/2410.15154v1,"MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation
  and Rigorous Verification","Large Language Models (LLMs) have shown considerable promise in code
generation. However, the automation sector, especially in motion control,
continues to rely heavily on manual programming due to the complexity of tasks
and critical safety considerations. In this domain, incorrect code execution
can pose risks to both machinery and personnel, necessitating specialized
expertise. To address these challenges, we introduce MCCoder, an LLM-powered
system designed to generate code that addresses complex motion control tasks,
with integrated soft-motion data verification. MCCoder enhances code generation
through multitask decomposition, hybrid retrieval-augmented generation (RAG),
and self-correction with a private motion library. Moreover, it supports data
verification by logging detailed trajectory data and providing simulations and
plots, allowing users to assess the accuracy of the generated code and
bolstering confidence in LLM-based programming. To ensure robust validation, we
propose MCEVAL, an evaluation dataset with metrics tailored to motion control
tasks of varying difficulties. Experiments indicate that MCCoder improves
performance by 11.61% overall and by 66.12% on complex tasks in MCEVAL dataset
compared with base models with naive RAG. This system and dataset aim to
facilitate the application of code generation in automation settings with
strict safety requirements. MCCoder is publicly available at
https://github.com/MCCodeAI/MCCoder.",Yin Li
2024-10-20T04:24:16Z,http://arxiv.org/abs/2410.15277v1,"BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via
  Compression","Retrieval-augmented generation (RAG) can supplement large language models
(LLMs) by integrating external knowledge. However, as the number of retrieved
documents increases, the input length to LLMs grows linearly, causing a
dramatic increase in latency and a degradation in long-context understanding.
This is particularly serious for multi-hop questions that require a chain of
reasoning across documents. To accelerate inference, reduce costs, and minimize
distractions, this paper presents BRIEF (Bridging Retrieval and Inference
through Evidence Fusion), a lightweight approach that performs query-aware
multi-hop reasoning by compressing retrieved documents into highly dense
textual summaries to integrate into in-context learning. To enable learning
compression for multi-hop reasoning, we curate synthetic data by extracting
atomic proposition expressions that encapsulate distinct factoids from the
source documents to compose synthetic summaries. Based on our synthetic data
built entirely by open-source models, BRIEF generates more concise summaries
and enables a range of LLMs to achieve exceptional open-domain question
answering (QA) performance. For example, on HotpotQA, BRIEF improves the
compression rate by 2 times compared to the state-of-the-art baseline, while
outperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader LM. It
also generates more concise summaries than proprietary GPT-3.5, while
demonstrating nearly identical QA performance.",Yuankai Li
2024-10-20T14:31:05Z,http://arxiv.org/abs/2410.15403v2,"MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis
  and Knowledge-based Departmental Consultation","We present MMDS, a system capable of recognizing medical images and patient
facial details, and providing professional medical diagnoses. The system
consists of two core components:The first component is the analysis of medical
images and videos. We trained a specialized multimodal medical model capable of
interpreting medical images and accurately analyzing patients' facial emotions
and facial paralysis conditions. The model achieved an accuracy of 72.59% on
the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in
recognizing the ""happy"" emotion. In facial paralysis recognition, the model
reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on
this model, we developed a parser for analyzing facial movement videos of
patients with facial paralysis, achieving precise grading of the paralysis
severity. In tests on 30 videos of facial paralysis patients, the system
demonstrated a grading accuracy of 83.3%.The second component is the generation
of professional medical responses. We employed a large language model,
integrated with a medical knowledge base, to generate professional diagnoses
based on the analysis of medical images or videos. The core innovation lies in
our development of a department-specific knowledge base routing management
mechanism, in which the large language model categorizes data by medical
departments and, during the retrieval process, determines the appropriate
knowledge base to query. This significantly improves retrieval accuracy in the
RAG (retrieval-augmented generation) process.",Yi Ren
2024-10-21T09:22:29Z,http://arxiv.org/abs/2410.15805v1,"RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for
  IT Operations and Maintenance","With the ever-increasing demands on Question Answering (QA) systems for IT
operations and maintenance, an efficient and supervised fine-tunable framework
is necessary to ensure the data security, private deployment and continuous
upgrading. Although Large Language Models (LLMs) have notably improved the
open-domain QA's performance, how to efficiently handle enterprise-exclusive
corpora and build domain-specific QA systems are still less-studied for
industrial applications. In this paper, we propose a general and comprehensive
framework based on Retrieval Augmented Generation (RAG) and facilitate the
whole business process of establishing QA systems for IT operations and
maintenance. In accordance with the prevailing RAG method, our proposed
framework, named with RAG4ITOps, composes of two major stages: (1) Models
Fine-tuning \& Data Vectorization, and (2) Online QA System Process. At the
Stage 1, we leverage a contrastive learning method with two negative sampling
strategies to fine-tune the embedding model, and design the instruction
templates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method.
At the Stage 2, an efficient process of QA system is built for serving. We
collect enterprise-exclusive corpora from the domain of cloud computing, and
the extensive experiments show that our method achieves superior results than
counterparts on two kinds of QA tasks. Our experiment also provide a case for
applying the RAG4ITOps to real-world enterprise-level applications.",Tianyang Zhang
2024-10-17T22:04:32Z,http://arxiv.org/abs/2410.16322v1,"SouLLMate: An Application Enhancing Diverse Mental Health Support with
  Adaptive LLMs, Prompt Engineering, and RAG Techniques","Mental health issues significantly impact individuals' daily lives, yet many
do not receive the help they need even with available online resources. This
study aims to provide diverse, accessible, stigma-free, personalized, and
real-time mental health support through cutting-edge AI technologies. It makes
the following contributions: (1) Conducting an extensive survey of recent
mental health support methods to identify prevalent functionalities and unmet
needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates
LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt
engineering, and domain knowledge. This system offers advanced features such as
Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for
personalized profile uploads and Conversational Information Extraction. (3)
Developing novel evaluation approaches for preliminary assessments and risk
detection via professionally annotated interview data and real-life suicide
tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive
Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to
enhance model performance and usability through context-sensitive response
adjustments, semantic coherence evaluations, and enhanced accuracy of
long-context reasoning in language models. This study contributes to advancing
mental health support technologies, potentially improving the accessibility and
effectiveness of mental health care globally.",Qiming Guo
2024-10-22T00:30:08Z,http://arxiv.org/abs/2410.16592v1,ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding,"The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.",Andrew Kan
2024-10-22T00:47:54Z,http://arxiv.org/abs/2410.16597v1,"Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency","Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.",Prafulla Kumar Choubey
2024-10-22T12:56:04Z,http://arxiv.org/abs/2410.16977v1,"IPL: Leveraging Multimodal Large Language Models for Intelligent Product
  Listing","Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,
Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are
mainly targeting individual sellers who usually lack sufficient experience in
e-commerce. Individual sellers often struggle to compose proper descriptions
for selling products. With the recent advancement of Multimodal Large Language
Models (MLLMs), we attempt to integrate such state-of-the-art generative AI
technologies into the product listing process. To this end, we develop IPL, an
Intelligent Product Listing tool tailored to generate descriptions using
various product attributes such as category, brand, color, condition, etc. IPL
enables users to compose product descriptions by merely uploading photos of the
selling product. More importantly, it can imitate the content style of our C2C
platform Xianyu. This is achieved by employing domain-specific instruction
tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation
(RAG) process. A comprehensive empirical evaluation demonstrates that the
underlying model of IPL significantly outperforms the base model in
domain-specific tasks while producing less hallucination. IPL has been
successfully deployed in our production system, where 72% of users have their
published product listings based on the generated content, and those product
listings are shown to have a quality score 5.6% higher than those without AI
assistance.",Kang Chen
2024-10-08T19:42:00Z,http://arxiv.org/abs/2410.18107v1,In-Context Code-Text Learning for Bimodal Software Engineering,"Bimodal software analysis initially appeared to be within reach with the
advent of large language models. Unfortunately, the complex interplay of
natural language text and code in software engineering, presents unique
challenges that prevent pretrained models to generalize to a variety of tasks.
We postulate that in-context learning for the code-text bimodality is a
promising avenue. This paper thus introduces a comprehensive study of
in-context code-text learning, focusing on leveraging pretrained CodeLLAMA
models.
  We consider a diverse dataset encompassing 23 software engineering tasks,
which we transform in an in-context learning format. To effectively extract
informative features, we propose a configurable prompt template. Our proposed
pipeline, InCTRL, then unifies prompt learning across various software
engineering tasks. Extensive evaluation on the study datasets demonstrates the
superiority of INCTRL-models in few-shot performance, surpassing
state-of-the-art models including the support model, CodeLLAMA. Typically, we
observe that applied to the CodeLLAMA model, INCTRL brings improvements in
terms of precision (at least about 12\%) and recall (up to 93.88\%) on various
tasks. For example, on the task of program repair, INCTRL improves the BLEU
score of CodeLLAMA by 85 points, while for clone detection, INCTRL achieves an
improvement of 69 percentage points. Moreover, INCTRL-models offer
state-of-the-art performance when using retrieval-augmented generation on
individual downstream tasks. Finally, we qualitatively analyze the benefits of
INCTRL over CodeLLAMA and open-source all models for broader impact.
  We make our code and dataset publicly available at: \begin{center}
  {\url{https://anonymous.4open.science/r/inctrl-B65B}} \end{center}",Xunzhu Tang
2024-10-24T14:47:25Z,http://arxiv.org/abs/2410.18792v2,An LLM Agent for Automatic Geospatial Data Analysis,"Large language models (LLMs) are being used in data science code generation
tasks, but they often struggle with complex sequential tasks, leading to
logical errors. Their application to geospatial data processing is particularly
challenging due to difficulties in incorporating complex data structures and
spatial constraints, effectively utilizing diverse function calls, and the
tendency to hallucinate less-used geospatial libraries. To tackle these
problems, we introduce GeoAgent, a new interactive framework designed to help
LLMs handle geospatial data processing more effectively. GeoAgent pioneers the
integration of a code interpreter, static analysis, and Retrieval-Augmented
Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,
offering a novel approach to geospatial data processing. In addition, we
contribute a new benchmark specifically designed to evaluate the LLM-based
approach in geospatial tasks. This benchmark leverages a variety of Python
libraries and includes both single-turn and multi-turn tasks such as data
acquisition, data analysis, and visualization. By offering a comprehensive
evaluation among diverse geospatial contexts, this benchmark sets a new
standard for developing LLM-based approaches in geospatial data analysis tasks.
Our findings suggest that relying solely on knowledge of LLM is insufficient
for accurate geospatial task programming, which requires coherent multi-step
processes and multiple function calls. Compared to the baseline LLMs, the
proposed GeoAgent has demonstrated superior performance, yielding notable
improvements in function calls and task completion. In addition, these results
offer valuable insights for the future development of LLM agents in automatic
geospatial data analysis task programming.",Yuxing Chen
2024-10-15T16:37:18Z,http://arxiv.org/abs/2410.19790v1,"Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP
  Technical Specifications","This paper proposes a Question-Answering (QA) system for the telecom domain
using 3rd Generation Partnership Project (3GPP) technical documents. Alongside,
a hybrid dataset, Telco-DPR, which consists of a curated 3GPP corpus in a
hybrid format, combining text and tables, is presented. Additionally, the
dataset includes a set of synthetic question/answer pairs designed to evaluate
the retrieval performance of QA systems on this type of data. The retrieval
models, including the sparse model, Best Matching 25 (BM25), as well as dense
models, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval
(DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank
(MRR). The results show that DHR, a retriever model utilising hierarchical
passage selection through fine-tuning at both the document and passage levels,
outperforms traditional methods in retrieving relevant technical information,
achieving a Top-10 accuracy of 86.2%. Additionally, the Retriever-Augmented
Generation (RAG) technique, used in the proposed QA system, is evaluated to
demonstrate the benefits of using the hybrid dataset and the DHR. The proposed
QA system, using the developed RAG model and the Generative Pretrained
Transformer (GPT)-4, achieves a 14% improvement in answer accuracy, when
compared to a previous benchmark on the same dataset.",Thaina Saraiva
2024-10-26T15:42:50Z,http://arxiv.org/abs/2410.20204v1,"Generative AI in Health Economics and Outcomes Research: A Taxonomy of
  Key Definitions and Emerging Applications, an ISPOR Working Group Report","Objective: This article offers a taxonomy of generative artificial
intelligence (AI) for health economics and outcomes research (HEOR), explores
its emerging applications, and outlines methods to enhance the accuracy and
reliability of AI-generated outputs. Methods: The review defines foundational
generative AI concepts and highlights current HEOR applications, including
systematic literature reviews, health economic modeling, real-world evidence
generation, and dossier development. Approaches such as prompt engineering
(zero-shot, few-shot, chain-of-thought, persona pattern prompting),
retrieval-augmented generation, model fine-tuning, and the use of
domain-specific models are introduced to improve AI accuracy and reliability.
Results: Generative AI shows significant potential in HEOR, enhancing
efficiency, productivity, and offering novel solutions to complex challenges.
Foundation models are promising in automating complex tasks, though challenges
remain in scientific reliability, bias, interpretability, and workflow
integration. The article discusses strategies to improve the accuracy of these
AI tools. Conclusion: Generative AI could transform HEOR by increasing
efficiency and accuracy across various applications. However, its full
potential can only be realized by building HEOR expertise and addressing the
limitations of current AI technologies. As AI evolves, ongoing research and
innovation will shape its future role in the field.",Rachael Fleurence
2024-10-26T19:48:47Z,http://arxiv.org/abs/2410.20263v1,"EfficientEQA: An Efficient Approach for Open Vocabulary Embodied
  Question Answering","Embodied Question Answering (EQA) is an essential yet challenging task for
robotic home assistants. Recent studies have shown that large vision-language
models (VLMs) can be effectively utilized for EQA, but existing works either
focus on video-based question answering without embodied exploration or rely on
closed-form choice sets. In real-world scenarios, a robotic agent must
efficiently explore and accurately answer questions in open-vocabulary
settings. To address these challenges, we propose a novel framework called
EfficientEQA for open-vocabulary EQA, which enables efficient exploration and
accurate answering. In EfficientEQA, the robot actively explores unknown
environments using Semantic-Value-Weighted Frontier Exploration, a strategy
that prioritizes exploration based on semantic importance provided by
calibrated confidence from black-box VLMs to quickly gather relevant
information. To generate accurate answers, we employ Retrieval-Augmented
Generation (RAG), which utilizes BLIP to retrieve useful images from
accumulated observations and VLM reasoning to produce responses without relying
on predefined answer choices. Additionally, we detect observations that are
highly relevant to the question as outliers, allowing the robot to determine
when it has sufficient information to stop exploring and provide an answer.
Experimental results demonstrate the effectiveness of our approach, showing an
improvement in answering accuracy by over 15% and efficiency, measured in
running steps, by over 20% compared to state-of-the-art methods.",Kai Cheng
2024-10-27T16:23:26Z,http://arxiv.org/abs/2410.21330v1,LLM Robustness Against Misinformation in Biomedical Question Answering,"The retrieval-augmented generation (RAG) approach is used to reduce the
confabulation of large language models (LLMs) for question answering by
retrieving and providing additional context coming from external knowledge
sources (e.g., by adding the context to the prompt). However, injecting
incorrect information can mislead the LLM to generate an incorrect answer.
  In this paper, we evaluate the effectiveness and robustness of four LLMs
against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in
answering biomedical questions. We assess the answer accuracy on yes-no and
free-form questions in three scenarios: vanilla LLM answers (no context is
provided), ""perfect"" augmented generation (correct context is provided), and
prompt-injection attacks (incorrect context is provided). Our results show that
Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla
(0.651) and ""perfect"" RAG (0.802) scenarios. However, the accuracy gap between
the models almost disappears with ""perfect"" RAG, suggesting its potential to
mitigate the LLM's size-related effectiveness differences.
  We further evaluate the ability of the LLMs to generate malicious context on
one hand and the LLM's robustness against prompt-injection attacks on the other
hand, using metrics such as attack success rate (ASR), accuracy under attack,
and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2,
GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is
injected in the target model's prompt. Interestingly, Llama is shown to be the
most effective adversary, causing accuracy drops of up to 0.48 for vanilla
answers and 0.63 for ""perfect"" RAG across target models. Our analysis reveals
that robustness rankings vary depending on the evaluation measure, highlighting
the complexity of assessing LLM resilience to adversarial attacks.",Alexander Bondarenko
2024-10-29T17:55:00Z,http://arxiv.org/abs/2410.22316v1,Understanding Synthetic Context Extension via Retrieval Heads,"Long-context LLMs are increasingly in demand for applications such as
retrieval-augmented generation. To defray the cost of pretraining LLMs over
long contexts, recent work takes an approach of synthetic context extension:
fine-tuning LLMs with synthetically generated long-context data in a
post-training stage. However, it remains unclear how and why this synthetic
context extension imparts abilities for downstream long-context tasks. In this
paper, we investigate fine-tuning on synthetic data for three long-context
tasks that require retrieval and reasoning. We vary the realism of ""needle""
concepts to be retrieved and diversity of the surrounding ""haystack"" context,
from using LLMs to construct synthetic documents to using templated relations
and creating symbolic datasets. We find that models trained on synthetic data
fall short of the real data, but surprisingly, the mismatch can be interpreted
and even predicted in terms of a special set of attention heads that are
responsible for retrieval over long context: retrieval heads (Wu et al., 2024).
The retrieval heads learned on synthetic data are mostly subsets of the
retrieval heads learned on real data, and there is a strong correlation between
the recall of heads learned and the downstream performance of a model.
Furthermore, with attention knockout and activation patching, we
mechanistically show that retrieval heads are necessary and explain model
performance, although they are not totally sufficient. Our results shed light
on how to interpret synthetic data fine-tuning performance and how to approach
creating better data for learning real-world capabilities over long contexts.",Xinyu Zhao
2024-10-30T13:22:22Z,http://arxiv.org/abs/2410.22996v1,"Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A
  Knowledge Graph Generation Approach","A well structured collection of the various Quantum Cascade Laser (QCL)
design and working properties data provides a platform to analyze and
understand the relationships between these properties. By analyzing these
relationships, we can gain insights into how different design features impact
laser performance properties such as the working temperature. Most of these QCL
properties are captured in scientific text. There is therefore need for
efficient methodologies that can be utilized to extract QCL properties from
text and generate a semantically enriched and interlinked platform where the
properties can be analyzed to uncover hidden relations. There is also the need
to maintain provenance and reference information on which these properties are
based. Semantic Web technologies such as Ontologies and Knowledge Graphs have
proven capability in providing interlinked data platforms for knowledge
representation in various domains. In this paper, we propose an approach for
generating a QCL properties Knowledge Graph (KG) from text for semantic
enrichment of the properties. The approach is based on the QCL ontology and a
Retrieval Augmented Generation (RAG) enabled information extraction pipeline
based on GPT 4-Turbo language model. The properties of interest include:
working temperature, laser design type, lasing frequency, laser optical power
and the heterostructure. The experimental results demonstrate the feasibility
and effectiveness of this approach for efficiently extracting QCL properties
from unstructured text and generating a QCL properties Knowledge Graph, which
has potential applications in semantic enrichment and analysis of QCL data.",Deperias Kerre
2024-10-30T14:08:50Z,http://arxiv.org/abs/2410.23041v1,Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval,"As LLMs exhibit a high degree of human-like capability, increasing attention
has been paid to role-playing research areas in which responses generated by
LLMs are expected to mimic human replies. This has promoted the exploration of
role-playing agents in various applications, such as chatbots that can engage
in natural conversations with users and virtual assistants that can provide
personalized support and guidance. The crucial factor in the role-playing task
is the effective utilization of character memory, which stores characters'
profiles, experiences, and historical dialogues. Retrieval Augmented Generation
(RAG) technology is used to access the related memory to enhance the response
generation of role-playing agents. Most existing studies retrieve related
information based on the semantic similarity of memory to maintain characters'
personalized traits, and few attempts have been made to incorporate the
emotional factor in the retrieval argument generation (RAG) of LLMs. Inspired
by the Mood-Dependent Memory theory, which indicates that people recall an
event better if they somehow reinstate during recall the original emotion they
experienced during learning, we propose a novel emotion-aware memory retrieval
framework, termed Emotional RAG, which recalls the related memory with
consideration of emotional state in role-playing agents. Specifically, we
design two kinds of retrieval strategies, i.e., combination strategy and
sequential strategy, to incorporate both memory semantic and emotional states
during the retrieval process. Extensive experiments on three representative
role-playing datasets demonstrate that our Emotional RAG framework outperforms
the method without considering the emotional factor in maintaining the
personalities of role-playing agents. This provides evidence to further
reinforce the Mood-Dependent Memory theory in psychology.",Le Huang
2024-10-31T14:22:20Z,http://arxiv.org/abs/2410.23968v1,"EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable
  Robot Task Planning","Recent advances in Large Language Models (LLMs) have helped facilitate
exciting progress for robotic planning in real, open-world environments. 3D
scene graphs (3DSGs) offer a promising environment representation for grounding
such LLM-based planners as they are compact and semantically rich. However, as
the robot's environment scales (e.g., number of entities tracked) and the
complexity of scene graph information increases (e.g., maintaining more
attributes), providing the 3DSG as-is to an LLM-based planner quickly becomes
infeasible due to input token count limits and attentional biases present in
LLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods
that retrieve query-relevant document chunks for LLM question and answering, we
adapt the paradigm for our embodied domain. Specifically, we propose a 3D scene
subgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based
planner with for executing natural language robotic tasks. Notably, our
retrieved subgraphs adapt to changes in the environment as well as changes in
task-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's
ability to significantly reduce input token counts (by an order of magnitude)
and planning time (up to 70% reduction in average time per planning step) while
improving success rates on AI2Thor simulated household tasks with a single-arm,
mobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with
a manipulator to highlight the performance benefits for robot deployment at the
edge in real environments.",Meghan Booker
2024-11-01T01:51:31Z,http://arxiv.org/abs/2411.00304v1,"Unified Generative and Discriminative Training for Multi-modal Large
  Language Models","In recent times, Vision-Language Models (VLMs) have been trained under two
predominant paradigms. Generative training has enabled Multimodal Large
Language Models (MLLMs) to tackle various complex tasks, yet issues such as
hallucinations and weak object discrimination persist. Discriminative training,
exemplified by models like CLIP, excels in zero-shot image-text classification
and retrieval, yet struggles with complex scenarios requiring fine-grained
semantic differentiation. This paper addresses these challenges by proposing a
unified approach that integrates the strengths of both paradigms. Considering
interleaved image-text sequences as the general format of input samples, we
introduce a structure-induced training strategy that imposes semantic
relationships between input samples and the MLLM's hidden state. This approach
enhances the MLLM's ability to capture global semantics and distinguish
fine-grained semantics. By leveraging dynamic sequence alignment within the
Dynamic Time Warping framework and integrating a novel kernel for fine-grained
semantic differentiation, our method effectively balances generative and
discriminative tasks. Extensive experiments demonstrate the effectiveness of
our approach, achieving state-of-the-art results in multiple generative tasks,
especially those requiring cognitive and discrimination abilities.
Additionally, our method surpasses discriminative benchmarks in interleaved and
fine-grained retrieval tasks. By employing a retrieval-augmented generation
strategy, our approach further enhances performance in some generative tasks
within one model, offering a promising direction for future research in
vision-language modeling.",Wei Chow
2024-11-03T15:25:47Z,http://arxiv.org/abs/2411.01606v2,"DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with
  Large Language Models","The rise of Large Language Models (LLMs) has streamlined frontend interface
creation through tools like Vercel's V0, yet surfaced challenges in design
quality (e.g., accessibility, and usability). Current solutions, often limited
by their focus, generalisability, or data dependency, fall short in addressing
these complexities. Moreover, none of them examine the quality of LLM-generated
UI design. In this work, we introduce DesignRepair, a novel dual-stream design
guideline-aware system to examine and repair the UI design quality issues from
both code aspect and rendered page aspect. We utilised the mature and popular
Material Design as our knowledge base to guide this process. Specifically, we
first constructed a comprehensive knowledge base encoding Google's Material
Design principles into low-level component knowledge base and high-level system
design knowledge base. After that, DesignRepair employs a LLM for the
extraction of key components and utilizes the Playwright tool for precise page
analysis, aligning these with the established knowledge bases. Finally, we
integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4
to holistically refine and repair frontend code through a strategic divide and
conquer approach. Our extensive evaluations validated the efficacy and utility
of our approach, demonstrating significant enhancements in adherence to design
guidelines, accessibility, and user experience metrics.",Mingyue Yuan
2024-11-04T05:25:39Z,http://arxiv.org/abs/2411.01807v1,Can Language Models Enable In-Context Database?,"Large language models (LLMs) are emerging as few-shot learners capable of
handling a variety of tasks, including comprehension, planning, reasoning,
question answering, arithmetic calculations, and more. At the core of these
capabilities is LLMs' proficiency in representing and understanding structural
or semi-structural data, such as tables and graphs. Numerous studies have
demonstrated that reasoning on tabular data or graphs is not only feasible for
LLMs but also gives a promising research direction which treats these data as
in-context data. The lightweight and human readable characteristics of
in-context database can potentially make it an alternative for the traditional
database in typical RAG (Retrieval Augmented Generation) settings. However,
almost all current work focuses on static in-context data, which does not allow
dynamic update. In this paper, to enable dynamic database update, delta
encoding of database is proposed. We explore how data stored in traditional
RDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD
(Create, Read, Update and Delete) operations on in-context databases. A
benchmark named InConDB is presented and extensive experiments are conducted to
show the performance of different language models in enabling in-context
database by varying the database encoding method, prompting method, operation
type and input data distribution, revealing both the proficiency and
limitations.",Yu Pan
2024-10-18T05:23:39Z,http://arxiv.org/abs/2411.02404v1,"Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative
  Mining","Ranking consistently emerges as a primary focus in information retrieval
research. Retrieval and ranking models serve as the foundation for numerous
applications, including web search, open domain QA, enterprise domain QA, and
text-based recommender systems. Typically, these models undergo training on
triplets consisting of binary relevance assignments, comprising one positive
and one negative passage. However, their utilization involves a context where a
significantly more nuanced understanding of relevance is necessary, especially
when re-ranking a large pool of potentially relevant passages. Although
collecting positive examples through user feedback like impressions or clicks
is straightforward, identifying suitable negative pairs from a vast pool of
possibly millions or even billions of documents possess a greater challenge.
Generating a substantial number of negative pairs is often necessary to
maintain the high quality of the model. Several approaches have been suggested
in literature to tackle the issue of selecting suitable negative pairs from an
extensive corpus. This study focuses on explaining the crucial role of hard
negatives in the training process of cross-encoder models, specifically aiming
to explain the performance gains observed with hard negative sampling compared
to random sampling. We have developed a robust hard negative mining technique
for efficient training of cross-encoder re-rank models on an enterprise dataset
which has domain specific context. We provide a novel perspective to enhance
retrieval models, ultimately influencing the performance of advanced LLM
systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action
Agents (ReAct). The proposed approach demonstrates that learning both
similarity and dissimilarity simultaneously with cross-encoders improves
performance of retrieval systems.",Hansa Meghwani
2024-11-04T22:45:52Z,http://arxiv.org/abs/2411.02657v1,"Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare
  Disease Knowledge","Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.",Karthik Soman
2024-11-05T09:58:36Z,http://arxiv.org/abs/2411.02959v1,"HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge
  in RAG Systems","Retrieval-Augmented Generation (RAG) has been shown to improve knowledge
capabilities and alleviate the hallucination problem of LLMs. The Web is a
major source of external knowledge used in RAG systems, and many commercial
systems such as ChatGPT and Perplexity have used Web search engines as their
major retrieval systems. Typically, such RAG systems retrieve search results,
download HTML sources of the results, and then extract plain texts from the
HTML sources. Plain text documents or chunks are fed into the LLMs to augment
the generation. However, much of the structural and semantic information
inherent in HTML, such as headings and table structures, is lost during this
plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,
which uses HTML instead of plain text as the format of retrieved knowledge in
RAG. We believe HTML is better than plain text in modeling knowledge in
external documents, and most LLMs possess robust capacities to understand HTML.
However, utilizing HTML presents new challenges. HTML contains additional
content such as tags, JavaScript, and CSS specifications, which bring extra
input tokens and noise to the RAG system. To address this issue, we propose
HTML cleaning, compression, and pruning strategies, to shorten the HTML while
minimizing the loss of information. Specifically, we design a two-step
block-tree-based pruning method that prunes useless HTML blocks and keeps only
the relevant part of the HTML. Experiments on six QA datasets confirm the
superiority of using HTML in RAG systems.",Jiejun Tan
2024-11-04T00:01:34Z,http://arxiv.org/abs/2411.03349v1,RuAG: Learned-rule-augmented Generation for Large Language Models,"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have
gained attention for their ability to enhance LLMs' reasoning by incorporating
external knowledge but suffer from limited contextual window size, leading to
insufficient information injection. To this end, we propose a novel framework,
RuAG, to automatically distill large volumes of offline data into interpretable
first-order logic rules, which are injected into LLMs to boost their reasoning
capabilities. Our method begins by formulating the search process relying on
LLMs' commonsense, where LLMs automatically define head and body predicates.
Then, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational
searching space and efficiently discover logic rules from data. The resulting
logic rules are translated into natural language, allowing targeted knowledge
injection and seamless integration into LLM prompts for LLM's downstream task
reasoning. We evaluate our framework on public and private industrial tasks,
including natural language processing, time-series, decision-making, and
industrial tasks, demonstrating its effectiveness in enhancing LLM's capability
over diverse tasks.",Yudi Zhang
2024-11-07T07:07:34Z,http://arxiv.org/abs/2411.04476v1,"LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation
  Combining Hierarchical Agents and RAG","The increasing use of smart devices has emphasized the critical role of
maintenance in production activities. Interactive Electronic Technical Manuals
(IETMs) are vital tools that support the maintenance of smart equipment.
However, traditional IETMs face challenges such as transitioning from Graphical
User Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing
complex logical relationships. Additionally, they must meet the current demands
for higher intelligence. This paper proposes a Maintenance Scheme Generation
Method based on Large Language Models (LLM-R). The proposed method includes
several key innovations: We propose the Low Rank Adaptation-Knowledge Retention
(LORA-KR) loss technology to proportionally adjust mixed maintenance data for
fine-tuning the LLM. This method prevents knowledge conflicts caused by mixed
data, improving the model's adaptability and reasoning ability in specific
maintenance domains, Besides, Hierarchical Task-Based Agent and
Instruction-level Retrieval-Augmented Generation (RAG) technologies are adopted
to optimize the generation steps and mitigate the phenomenon of hallucination
caused by the model's Inability to access contextual information. This
enhancement improves the model's flexibility and accuracy in handling known or
unknown maintenance objects and maintenance scheme scenarios. To validate the
proposed method's effectiveness in maintenance tasks, a maintenance scheme
dataset was constructed using objects from different fields. The experimental
results show that the accuracy of the maintenance schemes generated by the
proposed method reached 91.59%, indicating which improvement enhances the
intelligence of maintenance schemes and introduces novel technical approaches
for equipment maintenance.",Laifa Tao
2024-11-07T18:29:38Z,http://arxiv.org/abs/2411.04952v1,"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page
  Multi-document Understanding","Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.",Jaemin Cho
2024-11-07T21:10:39Z,http://arxiv.org/abs/2411.05185v1,PentestAgent: Incorporating LLM Agents to Automated Penetration Testing,"Penetration testing is a critical technique for identifying security
vulnerabilities, traditionally performed manually by skilled security
specialists. This complex process involves gathering information about the
target system, identifying entry points, exploiting the system, and reporting
findings. Despite its effectiveness, manual penetration testing is
time-consuming and expensive, often requiring significant expertise and
resources that many organizations cannot afford. While automated penetration
testing methods have been proposed, they often fall short in real-world
applications due to limitations in flexibility, adaptability, and
implementation.
  Recent advancements in large language models (LLMs) offer new opportunities
for enhancing penetration testing through increased intelligence and
automation. However, current LLM-based approaches still face significant
challenges, including limited penetration testing knowledge and a lack of
comprehensive automation capabilities. To address these gaps, we propose
PentestAgent, a novel LLM-based automated penetration testing framework that
leverages the power of LLMs and various LLM-based techniques like Retrieval
Augmented Generation (RAG) to enhance penetration testing knowledge and
automate various tasks. Our framework leverages multi-agent collaboration to
automate intelligence gathering, vulnerability analysis, and exploitation
stages, reducing manual intervention. We evaluate PentestAgent using a
comprehensive benchmark, demonstrating superior performance in task completion
and overall efficiency. This work significantly advances the practical
applicability of automated penetration testing systems.",Xiangmin Shen
2024-11-08T09:40:53Z,http://arxiv.org/abs/2411.05442v1,"IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge
  Delivery","In the rapidly evolving landscape of cyber security, intelligent chatbots are
gaining prominence. Artificial Intelligence, Machine Learning, and Natural
Language Processing empower these chatbots to handle user inquiries and deliver
threat intelligence. This helps cyber security knowledge readily available to
both professionals and the public. Traditional rule-based chatbots often lack
flexibility and struggle to adapt to user interactions. In contrast, Large
Language Model-based chatbots offer contextually relevant information across
multiple domains and adapt to evolving conversational contexts. In this work,
we develop IntellBot, an advanced cyber security Chatbot built on top of
cutting-edge technologies like Large Language Models and Langchain alongside a
Retrieval-Augmented Generation model to deliver superior capabilities. This
chatbot gathers information from diverse data sources to create a comprehensive
knowledge base covering known vulnerabilities, recent cyber attacks, and
emerging threats. It delivers tailored responses, serving as a primary hub for
cyber security insights. By providing instant access to relevant information
and resources, this IntellBot enhances threat intelligence, incident response,
and overall security posture, saving time and empowering users with knowledge
of cyber security best practices. Moreover, we analyzed the performance of our
copilot using a two-stage evaluation strategy. We achieved BERT score above 0.8
by indirect approach and a cosine similarity score ranging from 0.8 to 1, which
affirms the accuracy of our copilot. Additionally, we utilized RAGAS to
evaluate the RAG model, and all evaluation metrics consistently produced scores
above 0.77, highlighting the efficacy of our system.",Dincy R. Arikkat
2024-11-09T15:12:28Z,http://arxiv.org/abs/2411.06207v1,"Exploring Knowledge Boundaries in Large Language Models for Retrieval
  Judgment","Large Language Models (LLMs) are increasingly recognized for their practical
applications. However, these models often encounter challenges in dynamically
changing knowledge, as well as in managing unknown static knowledge.
Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a
significant impact on LLMs. Actually, we find that the impact of RAG on the
question answering capabilities of LLMs can be categorized into three groups:
beneficial, neutral, and harmful. By minimizing retrieval requests that yield
neutral or harmful results, we can effectively reduce both time and
computational costs, while also improving the overall performance of LLMs. This
insight motivates us to differentiate between types of questions using certain
metrics as indicators, to decrease the retrieval ratio without compromising
performance. In our work, we propose a method that is able to identify
different types of questions from this view by training a Knowledge Boundary
Model (KBM). Experiments conducted on 11 English and Chinese datasets
illustrate that the KBM effectively delineates the knowledge boundary,
significantly decreasing the proportion of retrievals required for optimal
end-to-end performance. Specifically, we evaluate the effectiveness of KBM in
three complex scenarios: dynamic knowledge, long-tail static knowledge, and
multi-hop problems, as well as its functionality as an external LLM plug-in.",Zhen Zhang
2024-11-11T14:25:37Z,http://arxiv.org/abs/2411.07021v2,Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation,"Retrieval-augmented generation (RAG) has shown impressive capability in
providing reliable answer predictions and addressing hallucination problems. A
typical RAG implementation uses powerful retrieval models to extract external
information and large language models (LLMs) to generate answers. In contrast,
recent LLM-based retrieval has gained attention for its substantial
improvements in information retrieval (IR) due to the LLMs' semantic
understanding capability. However, directly applying LLM to RAG systems
presents challenges. This may cause feature locality problems as massive
parametric knowledge can hinder effective usage of global information across
the corpus; for example, an LLM-based retriever often inputs document summaries
instead of full documents. Moreover, various pre-trained tasks in LLMs
introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning
architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever
is constructed by integrating LoRA-based representation learning to tackle
feature locality issues. To enhance retrieval performance, we develop two
patterns (invariant and variant patterns) and an invariance loss to reduce LLM
variance. In the generation stage, a refined fine-tuning method is employed to
improve LLM accuracy in generating answers based on retrieved information.
Experimental results show that Invar-RAG significantly outperforms existing
baselines across three open-domain question answering (ODQA) datasets. Code is
available in the Supplementary Material for reproducibility.",Ziwei Liu
2024-11-11T16:12:11Z,http://arxiv.org/abs/2411.07091v1,"Impact of LLM-based Review Comment Generation in Practice: A Mixed
  Open-/Closed-source User Study","We conduct a large-scale empirical user study in a live setup to evaluate the
acceptance of LLM-generated comments and their impact on the review process.
This user study was performed in two organizations, Mozilla (which has its
codebase available as open source) and Ubisoft (fully closed-source). Inside
their usual review environment, participants were given access to RevMate, an
LLM-based assistive tool suggesting generated review comments using an
off-the-shelf LLM with Retrieval Augmented Generation to provide extra code and
review context, combined with LLM-as-a-Judge, to auto-evaluate the generated
comments and discard irrelevant cases. Based on more than 587 patch reviews
provided by RevMate, we observed that 8.1% and 7.2%, respectively, of
LLM-generated comments were accepted by reviewers in each organization, while
14.6% and 20.5% other comments were still marked as valuable as review or
development tips. Refactoring-related comments are more likely to be accepted
than Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra
time spent by reviewers to inspect generated comments or edit accepted ones
(36/119), yielding an overall median of 43s per patch, is reasonable. The
accepted generated comments are as likely to yield future revisions of the
revised patch as human-written comments (74% vs 73% at chunk-level).",Doriane Olewicki
2024-11-12T10:12:12Z,http://arxiv.org/abs/2411.07688v1,"Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with
  ImageRAG","Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000
$\times$ 100,000 pixels or more) poses a significant challenge for current
Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize
the UHR image to standard input image size, the extensive spatial and
contextual information that UHR images contain will be neglected. Otherwise,
the original size of these images often exceeds the token limits of standard
RSMLLMs, making it difficult to process the entire image and capture long-range
dependencies to answer the query based on the abundant visual context. In this
paper, we introduce ImageRAG for RS, a training-free framework to address the
complexities of analyzing UHR remote sensing imagery. By transforming UHR
remote sensing image analysis task to image's long context selection task, we
design an innovative image contextual retrieval mechanism based on the
Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's
core innovation lies in its ability to selectively retrieve and focus on the
most relevant portions of the UHR image as visual contexts that pertain to a
given query. Fast path and slow path are proposed in this framework to handle
this task efficiently and effectively. ImageRAG allows RSMLLMs to manage
extensive context and spatial information from UHR RSI, ensuring the analysis
is both accurate and efficient.",Zilun Zhang
2024-10-29T07:25:30Z,http://arxiv.org/abs/2411.08041v1,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured
and unstructured data is a major challenge in many real-world applications.
Pattern matching and querying represent fundamental tasks in modern data
analytics that leverage this curated knowledge. The development of such
applications necessitates overcoming several research challenges, including
data extraction, named entity recognition, data modeling, and designing query
interfaces. Moreover, the explainability of these functionalities is critical
for their broader adoption.
  The emergence of Large Language Models (LLMs) has accelerated the development
lifecycle of new capabilities. Nonetheless, there is an ongoing need for
domain-specific tools tailored to user activities. The creation of digital
assistants has gained considerable traction in recent years, with LLMs offering
a promising avenue to develop such assistants utilizing domain-specific
knowledge and assumptions.
  In this context, we introduce an advanced query and reasoning system,
GraphAide, which constructs a knowledge graph (KG) from diverse sources and
allows to query and reason over the resulting KG. GraphAide harnesses both the
KG and LLMs to rapidly develop domain-specific digital assistants. It
integrates design patterns from retrieval augmented generation (RAG) and the
semantic web to create an agentic LLM application. GraphAide underscores the
potential for streamlined and efficient development of specialized digital
assistants, thereby enhancing their applicability across various domains.",Sumit Purohit
2024-11-12T19:55:07Z,http://arxiv.org/abs/2411.08148v1,"Adaptive Meta-Learning for Robust Deepfake Detection: A Multi-Agent
  Framework to Data Drift and Model Generalization","Pioneering advancements in artificial intelligence, especially in genAI, have
enabled significant possibilities for content creation, but also led to
widespread misinformation and false content. The growing sophistication and
realism of deepfakes is raising concerns about privacy invasion, identity
theft, and has societal, business impacts, including reputational damage and
financial loss. Many deepfake detectors have been developed to tackle this
problem. Nevertheless, as for every AI model, the deepfake detectors face the
wrath of lack of considerable generalization to unseen scenarios and
cross-domain deepfakes. Besides, adversarial robustness is another critical
challenge, as detectors drastically underperform to the slightest imperceptible
change. Most state-of-the-art detectors are trained on static datasets and lack
the ability to adapt to emerging deepfake attack trends. These three crucial
challenges though hold paramount importance for reliability in practise,
particularly in the deepfake domain, are also the problems with any other AI
application. This paper proposes an adversarial meta-learning algorithm using
task-specific adaptive sample synthesis and consistency regularization, in a
refinement phase. By focussing on the classifier's strengths and weaknesses, it
boosts both robustness and generalization of the model. Additionally, the paper
introduces a hierarchical multi-agent retrieval-augmented generation workflow
with a sample synthesis module to dynamically adapt the model to new data
trends by generating custom deepfake samples. The paper further presents a
framework integrating the meta-learning algorithm with the hierarchical
multi-agent workflow, offering a holistic solution for enhancing
generalization, robustness, and adaptability. Experimental results demonstrate
the model's consistent performance across various datasets, outperforming the
models in comparison.",Dinesh Srivasthav P
2024-11-14T08:12:36Z,http://arxiv.org/abs/2411.09269v1,"Harnessing multiple LLMs for Information Retrieval: A case study on Deep
  Learning methodologies in Biodiversity publications","Deep Learning (DL) techniques are increasingly applied in scientific studies
across various domains to address complex research questions. However, the
methodological details of these DL models are often hidden in the unstructured
text. As a result, critical information about how these models are designed,
trained, and evaluated is challenging to access and comprehend. To address this
issue, in this work, we use five different open-source Large Language Models
(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,
and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)
approach to extract and process DL methodological details from scientific
publications automatically. We built a voting classifier from the outputs of
five LLMs to accurately report DL methodological information. We tested our
approach using biodiversity publications, building upon our previous research.
To validate our pipeline, we employed two datasets of DL-related biodiversity
publications: a curated set of 100 publications from our prior work and a set
of 364 publications from the Ecological Informatics journal. Our results
demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of
DL methodological information, achieving an accuracy of 69.5% (417 out of 600
comparisons) based solely on textual content from publications. This
performance was assessed against human annotators who had access to code,
figures, tables, and other supplementary information. Although demonstrated in
biodiversity, our methodology is not limited to this field; it can be applied
across other scientific domains where detailed methodological reporting is
essential for advancing knowledge and ensuring reproducibility. This study
presents a scalable and reliable approach for automating information
extraction, facilitating better reproducibility and knowledge transfer across
studies.",Vamsi Krishna Kommineni
2024-11-14T17:25:43Z,http://arxiv.org/abs/2411.09607v1,"Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the
  AutoNuggetizer Framework","This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on ""refactoring"" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.",Ronak Pradeep
2024-11-16T20:18:57Z,http://arxiv.org/abs/2411.10878v1,"Empowering Meta-Analysis: Leveraging Large Language Models for
  Scientific Synthesis","This study investigates the automation of meta-analysis in scientific
documents using large language models (LLMs). Meta-analysis is a robust
statistical method that synthesizes the findings of multiple studies support
articles to provide a comprehensive understanding. We know that a meta-article
provides a structured analysis of several articles. However, conducting
meta-analysis by hand is labor-intensive, time-consuming, and susceptible to
human error, highlighting the need for automated pipelines to streamline the
process. Our research introduces a novel approach that fine-tunes the LLM on
extensive scientific datasets to address challenges in big data handling and
structured data extraction. We automate and optimize the meta-analysis process
by integrating Retrieval Augmented Generation (RAG). Tailored through prompt
engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for
fine-tuning on large contextual datasets, LLMs efficiently generate structured
meta-analysis content. Human evaluation then assesses relevance and provides
information on model performance in key metrics. This research demonstrates
that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs
generating 87.6% relevant meta-analysis abstracts. The relevance of the
context, based on human evaluation, shows a reduction in irrelevancy from 4.56%
to 1.9%. These experiments were conducted in a low-resource environment,
highlighting the study's contribution to enhancing the efficiency and
reliability of meta-analysis automation.",Jawad Ibn Ahad
2024-11-17T00:09:04Z,http://arxiv.org/abs/2411.10918v1,"LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems
  Anomaly Detection","Modern industrial infrastructures rely heavily on Cyber-Physical Systems
(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic
effects. To reduce these risks, anomaly detection methods based on physical
invariants have been developed. However, these methods often require
domain-specific expertise to manually define invariants, making them costly and
difficult to scale. To address this limitation, we propose a novel approach to
extract physical invariants from CPS testbeds for anomaly detection. Our
insight is that CPS design documentation often contains semantically rich
descriptions of physical procedures, which can profile inter-correlated
dynamics among system components. Leveraging the built-in physics and
engineering knowledge of recent generative AI models, we aim to automate this
traditionally manual process, improving scalability and reducing costs. This
work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)
workflow with a customized prompting system tailored for CPS documentation,
enabling accurate extraction of semantic information and inference of physical
invariants from complex, multimodal content. Then, rather than directly
applying the inferred invariants for anomaly detection, we introduce an
innovative statistics-based learning approach that integrates these invariants
into the training dataset. This method addresses limitations such as
hallucination and concept drift, enhancing the reliability of the model. We
evaluate our approach on real-world public CPS security dataset which contains
86 data points and 58 attacking cases. The results show that our approach
achieves a high precision of 0.923, accurately detecting anomalies while
minimizing false alarms.",Danial Abshari
2024-11-17T10:26:25Z,http://arxiv.org/abs/2411.11033v1,"REACCEPT: Automated Co-evolution of Production and Test Code Based on
  Dynamic Validation and Large Language Models","Synchronizing production and test code, known as PT co-evolution, is critical
for software quality in the software development lifecycle. Existing methods
for automatic PT co-evolution either utilize predefined heuristic rules or rely
on simple application of machine learning techniques. Due to the limitations of
underlying techniques, existing methods either only partially automate PT
co-evolution (e.g., only automate obsolete test code identification) or result
in low accuracy.
  In this paper, we propose REACCEPT, a novel approach that leverages large
language models and dynamic validation to fully automate PT co-evolution (i.e.,
capable of both identifying and updating obsolete test cases). REACCEPT relies
on experience-based prompt template generation, dynamic validation, and
retrieval-augmented generation techniques to accomplish automated PT
co-evolution. To evaluate REACCEPT's effectiveness, we extensive experiments
with a dataset of 537 Java projects and compared REACCEPT's performance with
several state-of-the-art methods. Results show that REACCEPT achieved an update
accuracy of 60.16% on correctly identified obsolete test code, surpassing the
state-of-the-art technique CEPROT by 90%. This confirms that REACCEPT can
effectively assist developers in maintaining test code, improving overall
software quality and reducing maintenance effort.",Jianlei Chi
2024-11-17T14:45:52Z,http://arxiv.org/abs/2411.11090v1,"ForPKG-1.0: A Framework for Constructing Forestry Policy Knowledge Graph
  and Application Analysis","A policy knowledge graph can provide decision support for tasks such as
project compliance, policy analysis, and intelligent question answering, and
can also serve as an external knowledge base to assist the reasoning process of
related large language models. Although there have been many related works on
knowledge graphs, there is currently a lack of research on the construction
methods of policy knowledge graphs. This paper, focusing on the forestry field,
designs a complete policy knowledge graph construction framework, including:
firstly, proposing a fine-grained forestry policy domain ontology; then,
proposing an unsupervised policy information extraction method, and finally,
constructing a complete forestry policy knowledge graph. The experimental
results show that the proposed ontology has good expressiveness and
extensibility, and the policy information extraction method proposed in this
paper achieves better results than other unsupervised methods. Furthermore, by
analyzing the application of the knowledge graph in the
retrieval-augmented-generation task of the large language models, the practical
application value of the knowledge graph in the era of large language models is
confirmed. The knowledge graph resource will be released on an open-source
platform and can serve as the basic knowledge base for forestry policy-related
intelligent systems. It can also be used for academic research. In addition,
this study can provide reference and guidance for the construction of policy
knowledge graphs in other fields.",Jingyun Sun
2024-11-18T06:33:05Z,http://arxiv.org/abs/2411.11323v1,"SayComply: Grounding Field Robotic Tasks in Operational Compliance
  through Retrieval-Based Language Models","This paper addresses the problem of task planning for robots that must comply
with operational manuals in real-world settings. Task planning under these
constraints is essential for enabling autonomous robot operation in domains
that require adherence to domain-specific knowledge. Current methods for
generating robot goals and plans rely on common sense knowledge encoded in
large language models. However, these models lack grounding of robot plans to
domain-specific knowledge and are not easily transferable between multiple
sites or customers with different compliance needs. In this work, we present
SayComply, which enables grounding robotic task planning with operational
compliance using retrieval-based language models. We design a hierarchical
database of operational, environment, and robot embodiment manuals and
procedures to enable efficient retrieval of the relevant context under the
limited context length of the LLMs. We then design a task planner using a
tree-based retrieval augmented generation (RAG) technique to generate robot
tasks that follow user instructions while simultaneously complying with the
domain knowledge in the database. We demonstrate the benefits of our approach
through simulations and hardware experiments in real-world scenarios that
require precise context retrieval across various types of context,
outperforming the standard RAG method. Our approach bridges the gap in
deploying robots that consistently adhere to operational protocols, offering a
scalable and edge-deployable solution for ensuring compliance across varied and
complex real-world environments. Project website: saycomply.github.io.",Muhammad Fadhil Ginting
2024-11-17T23:20:37Z,http://arxiv.org/abs/2411.11913v1,"On-Board Vision-Language Models for Personalized Autonomous Vehicle
  Motion Control: System Design and Real-World Validation","Personalized driving refers to an autonomous vehicle's ability to adapt its
driving behavior or control strategies to match individual users' preferences
and driving styles while maintaining safety and comfort standards. However,
existing works either fail to capture every individual preference precisely or
become computationally inefficient as the user base expands. Vision-Language
Models (VLMs) offer promising solutions to this front through their natural
language understanding and scene reasoning capabilities. In this work, we
propose a lightweight yet effective on-board VLM framework that provides
low-latency personalized driving performance while maintaining strong reasoning
capabilities. Our solution incorporates a Retrieval-Augmented Generation
(RAG)-based memory module that enables continuous learning of individual
driving preferences through human feedback. Through comprehensive real-world
vehicle deployment and experiments, our system has demonstrated the ability to
provide safe, comfortable, and personalized driving experiences across various
scenarios and significantly reduce takeover rates by up to 76.9%. To the best
of our knowledge, this work represents the first end-to-end VLM-based motion
control system in real-world autonomous vehicles.",Can Cui
2024-11-18T21:43:52Z,http://arxiv.org/abs/2411.12078v1,Molecule Generation with Fragment Retrieval Augmentation,"Fragment-based drug discovery, in which molecular fragments are assembled
into new molecules with desirable biochemical properties, has achieved great
success. However, many fragment-based molecule generation methods show limited
exploration beyond the existing fragments in the database as they only
reassemble or slightly modify the given ones. To tackle this problem, we
propose a new fragment-based molecule generation framework with retrieval
augmentation, namely Fragment Retrieval-Augmented Generation (f-RAG). f-RAG is
based on a pre-trained molecular generative model that proposes additional
fragments from input fragments to complete and generate a new molecule. Given a
fragment vocabulary, f-RAG retrieves two types of fragments: (1) hard
fragments, which serve as building blocks that will be explicitly included in
the newly generated molecule, and (2) soft fragments, which serve as reference
to guide the generation of new fragments through a trainable fragment injection
module. To extrapolate beyond the existing fragments, f-RAG updates the
fragment vocabulary with generated fragments via an iterative refinement
process which is further enhanced with post-hoc genetic fragment modification.
f-RAG can achieve an improved exploration-exploitation trade-off by maintaining
a pool of fragments and expanding it with novel and high-quality fragments
through a strong generative prior.",Seul Lee
2024-11-19T12:17:43Z,http://arxiv.org/abs/2411.12449v2,Neon: News Entity-Interaction Extraction for Enhanced Question Answering,"Capturing fresh information in near real-time and using it to augment
existing large language models (LLMs) is essential to generate up-to-date,
grounded, and reliable output. This problem becomes particularly challenging
when LLMs are used for informational tasks in rapidly evolving fields, such as
Web search related to recent or unfolding events involving entities, where
generating temporally relevant responses requires access to up-to-the-hour news
sources. However, the information modeled by the parametric memory of LLMs is
often outdated, and Web results from prototypical retrieval systems may fail to
capture the latest relevant information and struggle to handle conflicting
reports in evolving news. To address this challenge, we present the NEON
framework, designed to extract emerging entity interactions -- such as events
or activities -- as described in news articles. NEON constructs an
entity-centric timestamped knowledge graph that captures such interactions,
thereby facilitating enhanced QA capabilities related to news events. Our
framework innovates by integrating open Information Extraction (openIE) style
tuples into LLMs to enable in-context retrieval-augmented generation. This
integration demonstrates substantial improvements in QA performance when
tackling temporal, entity-centric search queries. Through NEON, LLMs can
deliver more accurate, reliable, and up-to-date responses.",Sneha Singhania
2024-11-20T07:44:34Z,http://arxiv.org/abs/2411.13093v3,Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension,"Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.",Yongdong Luo
2024-11-20T10:17:09Z,http://arxiv.org/abs/2411.13173v2,"Writing Style Matters: An Examination of Bias and Fairness in
  Information Retrieval Systems","The rapid advancement of Language Model technologies has opened new
opportunities, but also introduced new challenges related to bias and fairness.
This paper explores the uncharted territory of potential biases in
state-of-the-art universal text embedding models towards specific document and
query writing styles within Information Retrieval (IR) systems. Our
investigation reveals that different embedding models exhibit different
preferences of document writing style, while more informal and emotive styles
are less favored by most embedding models. In terms of query writing styles,
many embedding models tend to match the style of the query with the style of
the retrieved documents, but some show a consistent preference for specific
styles. Text embedding models fine-tuned on synthetic data generated by LLMs
display a consistent preference for certain style of generated data. These
biases in text embedding based IR systems can inadvertently silence or
marginalize certain communication styles, thereby posing a significant threat
to fairness in information retrieval. Finally, we also compare the answer
styles of Retrieval Augmented Generation (RAG) systems based on different LLMs
and find out that most text embedding models are biased towards LLM's answer
styles when used as evaluation metrics for answer correctness. This study sheds
light on the critical issue of writing style based bias in IR systems, offering
valuable insights for the development of more fair and robust models.",Hongliu Cao
2024-11-17T07:32:46Z,http://arxiv.org/abs/2411.13584v1,"AddrLLM: Address Rewriting via Large Language Model on Nationwide
  Logistics Data","Textual description of a physical location, commonly known as an address,
plays an important role in location-based services(LBS) such as on-demand
delivery and navigation. However, the prevalence of abnormal addresses, those
containing inaccuracies that fail to pinpoint a location, have led to
significant costs. Address rewriting has emerged as a solution to rectify these
abnormal addresses. Despite the critical need, existing address rewriting
methods are limited, typically tailored to correct specific error types, or
frequently require retraining to process new address data effectively. In this
study, we introduce AddrLLM, an innovative framework for address rewriting that
is built upon a retrieval augmented large language model. AddrLLM overcomes
aforementioned limitations through a meticulously designed Supervised
Fine-Tuning module, an Address-centric Retrieval Augmented Generation module
and a Bias-free Objective Alignment module. To the best of our knowledge, this
study pioneers the application of LLM-based address rewriting approach to solve
the issue of abnormal addresses. Through comprehensive offline testing with
real-world data on a national scale and subsequent online deployment, AddrLLM
has demonstrated superior performance in integration with existing logistics
system. It has significantly decreased the rate of parcel re-routing by
approximately 43\%, underscoring its exceptional efficacy in real-world
applications.",Qinchen Yang
2024-11-21T15:28:52Z,http://arxiv.org/abs/2411.14219v1,"Towards Context-Rich Automated Biodiversity Assessments: Deriving
  AI-Powered Insights from Camera Trap Data","Camera traps offer enormous new opportunities in ecological studies, but
current automated image analysis methods often lack the contextual richness
needed to support impactful conservation outcomes. Here we present an
integrated approach that combines deep learning-based vision and language
models to improve ecological reporting using data from camera traps. We
introduce a two-stage system: YOLOv10-X to localise and classify species
(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read
YOLOv10-X binding box labels to identify species, overcoming its limitation
with hard to classify objects in images. Additionally, Phi-3.5 detects broader
variables, such as vegetation type, and time of day, providing rich ecological
and environmental context to YOLO's species detection output. When combined,
this output is processed by the model's natural language system to answer
complex queries, and retrieval-augmented generation (RAG) is employed to enrich
responses with external information, like species weight and IUCN status
(information that cannot be obtained through direct visual analysis). This
information is used to automatically generate structured reports, providing
biodiversity stakeholders with deeper insights into, for example, species
abundance, distribution, animal behaviour, and habitat selection. Our approach
delivers contextually rich narratives that aid in wildlife management
decisions. By providing contextually rich insights, our approach not only
reduces manual effort but also supports timely decision-making in conservation,
potentially shifting efforts from reactive to proactive management.",Paul Fergus
2024-11-21T16:28:32Z,http://arxiv.org/abs/2411.14272v1,"Efficient Aspect-Based Summarization of Climate Change Reports with
  Small Language Models","The use of Natural Language Processing (NLP) for helping decision-makers with
Climate Change action has recently been highlighted as a use case aligning with
a broader drive towards NLP technologies for social good. In this context,
Aspect-Based Summarization (ABS) systems that extract and summarize relevant
information are particularly useful as they provide stakeholders with a
convenient way of finding relevant information in expert-curated reports. In
this work, we release a new dataset for ABS of Climate Change reports and we
employ different Large Language Models (LLMs) and so-called Small Language
Models (SLMs) to tackle this problem in an unsupervised way. Considering the
problem at hand, we also show how SLMs are not significantly worse for the
problem while leading to reduced carbon footprint; we do so by applying for the
first time an existing framework considering both energy efficiency and task
performance to the evaluation of zero-shot generative models for ABS. Overall,
our results show that modern language models, both big and small, can
effectively tackle ABS for Climate Change reports but more research is needed
when we frame the problem as a Retrieval Augmented Generation (RAG) problem and
our work and dataset will help foster efforts in this direction.",Iacopo Ghinassi
2024-11-22T08:21:03Z,http://arxiv.org/abs/2411.14790v3,KBAlign: Efficient Self Adaptation on Specific Knowledge Bases,"Humans can utilize techniques to quickly acquire knowledge from specific
materials in advance, such as creating self-assessment questions, enabling us
to achieving related tasks more efficiently. In contrast, large language models
(LLMs) usually relies on retrieval-augmented generation to exploit knowledge
materials in an instant manner, or requires external signals such as human
preference data and stronger LLM annotations to conduct knowledge adaptation.
To unleash the self-learning potential of LLMs, we propose KBAlign, an approach
designed for efficient adaptation to downstream tasks involving knowledge
bases. Our method utilizes iterative training with self-annotated data such as
Q&A pairs and revision suggestions, enabling the model to grasp the knowledge
content efficiently. Experimental results on multiple datasets demonstrate the
effectiveness of our approach, significantly boosting model performance in
downstream tasks that require specific knowledge at a low cost. Notably, our
approach achieves over 90% of the performance improvement that can be obtained
by using GPT-4-turbo annotation, while relying entirely on self-supervision. We
release our experimental data, models, and process analyses to the community
for further exploration (https://github.com/thunlp/KBAlign).",Zheni Zeng
2024-11-22T16:15:50Z,http://arxiv.org/abs/2411.15041v1,"mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for
  Knowledge-Based VQA","Advanced Multimodal Large Language Models (MLLMs) struggle with recent
Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their
limited and frozen knowledge scope, often leading to ambiguous and inaccurate
responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally
introduced to provide MLLMs with comprehensive and up-to-date knowledge,
effectively expanding the knowledge scope. However, current mRAG methods have
inherent drawbacks, including: 1) Performing retrieval even when external
knowledge is not needed. 2) Lacking of identification of evidence that supports
the query. 3) Increasing model complexity due to additional information
filtering modules or rules. To address these shortcomings, we propose a novel
generalized framework called \textbf{m}ultimodal
\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration
(mR$^2$AG), which achieves adaptive retrieval and useful information
localization to enable answers through two easy-to-implement reflection
operations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection
is designed to distinguish different user queries and avoids redundant
retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in
locating beneficial evidence of the retrieved content and generating answers
accordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM
with efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset
(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,
GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while
maintaining the exceptional capabilities of base MLLMs across a wide range of
Visual-dependent tasks.",Tao Zhang
2024-11-20T04:47:42Z,http://arxiv.org/abs/2411.15203v1,"Multimodal large language model for wheat breeding: a new exploration of
  smart breeding","UAV remote sensing technology has become a key technology in crop breeding,
which can achieve high-throughput and non-destructive collection of crop
phenotyping data. However, the multidisciplinary nature of breeding has brought
technical barriers and efficiency challenges to knowledge mining. Therefore, it
is important to develop a smart breeding goal tool to mine cross-domain
multimodal data. Based on different pre-trained open-source multimodal large
language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used
supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and
reinforcement learning from human feedback (RLHF) technologies to inject
cross-domain knowledge into MLLMs, thereby constructing multiple multimodal
large language models for wheat breeding (WBLMs). The above WBLMs were
evaluated using the newly created evaluation benchmark in this study. The
results showed that the WBLM constructed using SFT, RAG and RLHF technologies
and InternVL2-8B has leading performance. Then, subsequent experiments were
conducted using the WBLM. Ablation experiments indicated that the combination
of SFT, RAG, and RLHF technologies can improve the overall generation
performance, enhance the generated quality, balance the timeliness and
adaptability of the generated answer, and reduce hallucinations and biases. The
WBLM performed best in wheat yield prediction using cross-domain data (remote
sensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of
0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate
professional decision support answers for phenotyping estimation, environmental
stress assessment, target germplasm screening, cultivation technique
recommendation, and seed price query tasks.",Guofeng Yang
2024-11-24T03:56:43Z,http://arxiv.org/abs/2411.15700v1,"RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large
  Language Models on Dietary Supplements","\textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.",Zaifu Zhan
2024-11-25T15:35:51Z,http://arxiv.org/abs/2411.16495v2,"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous
  Knowledge Reasoning","Recent advancements in large language models (LLMs) have led to significant
improvements in various natural language processing tasks, but it is still
challenging for LLMs to perform knowledge-intensive complex question answering
due to LLMs' inefficacy in reasoning planning and the hallucination problem. A
typical solution is to employ retrieval-augmented generation (RAG) coupled with
chain-of-thought (CoT) reasoning, which decomposes complex questions into
chain-like sub-questions and applies iterative RAG at each sub-question.
However, prior works exhibit sub-optimal reasoning planning and overlook
dynamic knowledge retrieval from heterogeneous sources. In this paper, we
propose AtomR, a novel heterogeneous knowledge reasoning framework that
conducts multi-source reasoning at the atomic level. Drawing inspiration from
the graph modeling of knowledge, AtomR leverages large language models (LLMs)
to decompose complex questions into combinations of three atomic knowledge
operators, significantly enhancing the reasoning process at both the planning
and execution stages. We also introduce BlendQA, a novel evaluation benchmark
tailored to assess complex heterogeneous knowledge reasoning. Experiments show
that AtomR significantly outperforms state-of-the-art baselines across three
single-source and two multi-source reasoning benchmarks, with notable
performance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",Amy Xin
2024-11-23T18:14:42Z,http://arxiv.org/abs/2411.16740v3,"Document Haystacks: Vision-Language Reasoning Over Piles of 1000+
  Documents","Large multimodal models (LMMs) have achieved impressive progress in
vision-language understanding, yet they face limitations in real-world
applications requiring complex reasoning over a large number of images.
Existing benchmarks for multi-image question-answering are limited in scope,
each question is paired with only up to 30 images, which does not fully capture
the demands of large-scale retrieval tasks encountered in the real-world
usages. To reduce these gaps, we introduce two document haystack benchmarks,
dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on
large-scale visual document retrieval and understanding. Additionally, we
propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)
framework that leverages a suite of multimodal vision encoders, each optimized
for specific strengths, and a dedicated question-document relevance module.
V-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the
challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,
compared to the previous best baseline models. Additionally, integrating V-RAG
with LMMs enables them to efficiently operate across thousands of images,
yielding significant improvements on our DocHaystack and InfoHaystack
benchmarks. Our code and datasets are available at
https://github.com/Vision-CAIR/dochaystacks",Jun Chen
2024-11-28T06:29:46Z,http://arxiv.org/abs/2411.18948v1,"Knowledge Database or Poison Base? Detecting RAG Poisoning Attack
  through LLM Activations","As Large Language Models (LLMs) are progressively deployed across diverse
fields and real-world applications, ensuring the security and robustness of
LLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a
cutting-edge approach designed to address the limitations of large language
models (LLMs). By retrieving information from the relevant knowledge database,
RAG enriches the input to LLMs, enabling them to produce responses that are
more accurate and contextually appropriate. It is worth noting that the
knowledge database, being sourced from publicly available channels such as
Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves
injecting malicious texts into the knowledge database, ultimately leading to
the generation of the attacker's target response (also called poisoned
response). However, there are currently limited methods available for detecting
such poisoning attacks. We aim to bridge the gap in this work. Particularly, we
introduce RevPRAG, a flexible and automated detection pipeline that leverages
the activations of LLMs for poisoned response detection. Our investigation
uncovers distinct patterns in LLMs' activations when generating correct
responses versus poisoned responses. Our results on multiple benchmark datasets
and RAG architectures show our approach could achieve 98% true positive rate,
while maintaining false positive rates close to 1%. We also evaluate recent
backdoor detection methods specifically designed for LLMs and applicable for
identifying poisoned responses in RAG. The results demonstrate that our
approach significantly surpasses them.",Xue Tan
2024-11-28T11:24:43Z,http://arxiv.org/abs/2411.19064v1,"Way to Specialist: Closing Loop Between Specialized LLM and Evolving
  Domain Knowledge Graph","Large language models (LLMs) have demonstrated exceptional performance across
a wide variety of domains. Nonetheless, generalist LLMs continue to fall short
in reasoning tasks necessitating specialized knowledge. Prior investigations
into specialized LLMs focused on domain-specific training, which entails
substantial efforts in domain data acquisition and model parameter fine-tuning.
To address these challenges, this paper proposes the Way-to-Specialist (WTS)
framework, which synergizes retrieval-augmented generation with knowledge
graphs (KGs) to enhance the specialized capability of LLMs in the absence of
specialized training. In distinction to existing paradigms that merely utilize
external knowledge from general KGs or static domain KGs to prompt LLM for
enhanced domain-specific reasoning, WTS proposes an innovative
""LLM$\circlearrowright$KG"" paradigm, which achieves bidirectional enhancement
between specialized LLM and domain knowledge graph (DKG). The proposed paradigm
encompasses two closely coupled components: the DKG-Augmented LLM and the
LLM-Assisted DKG Evolution. The former retrieves question-relevant domain
knowledge from DKG and uses it to prompt LLM to enhance the reasoning
capability for domain-specific tasks; the latter leverages LLM to generate new
domain knowledge from processed tasks and use it to evolve DKG. WTS closes the
loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling
continuous improvement in the domain specialization as it progressively answers
and learns from domain-specific questions. We validate the performance of WTS
on 6 datasets spanning 5 domains. The experimental results show that WTS
surpasses the previous SOTA in 4 specialized domains and achieves a maximum
performance improvement of 11.3%.",Yutong Zhang
2024-11-28T15:53:27Z,http://arxiv.org/abs/2411.19229v2,Habit Coach: Customising RAG-based chatbots to support behavior change,"This paper presents the iterative development of Habit Coach, a GPT-based
chatbot designed to support users in habit change through personalized
interaction. Employing a user-centered design approach, we developed the
chatbot using a Retrieval-Augmented Generation (RAG) system, which enables
behavior personalization without retraining the underlying language model
(GPT-4). The system leverages document retrieval and specialized prompts to
tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and
narrative therapy techniques. A key challenge in the development process was
the difficulty of translating declarative knowledge into effective interaction
behaviors. In the initial phase, the chatbot was provided with declarative
knowledge about CBT via reference textbooks and high-level conversational
goals. However, this approach resulted in imprecise and inefficient behavior,
as the GPT model struggled to convert static information into dynamic and
contextually appropriate interactions. This highlighted the limitations of
relying solely on declarative knowledge to guide chatbot behavior, particularly
in nuanced, therapeutic conversations. Over four iterations, we addressed this
issue by gradually transitioning towards procedural knowledge, refining the
chatbot's interaction strategies, and improving its overall effectiveness. In
the final evaluation, 5 participants engaged with the chatbot over five
consecutive days, receiving individualized CBT interventions. The Self-Report
Habit Index (SRHI) was used to measure habit strength before and after the
intervention, revealing a reduction in habit strength post-intervention. These
results underscore the importance of procedural knowledge in driving effective,
personalized behavior change support in RAG-based systems.",Arian Fooroogh Mand Arabi
2024-11-29T05:31:04Z,http://arxiv.org/abs/2411.19478v1,"Zero-Indexing Internet Search Augmented Generation for Large Language
  Models","Retrieval augmented generation has emerged as an effective method to enhance
large language model performance. This approach typically relies on an internal
retrieval module that uses various indexing mechanisms to manage a static
pre-processed corpus. However, such a paradigm often falls short when it is
necessary to integrate the most up-to-date information that has not been
updated into the corpus during generative inference time. In this paper, we
explore an alternative approach that leverages standard search engine APIs to
dynamically integrate the latest online information (without maintaining any
index for any fixed corpus), thereby improving the quality of generated
content. We design a collaborative LLM-based paradigm, where we include: (i) a
parser-LLM that determines if the Internet augmented generation is demanded and
extracts the search keywords if so with a single inference; (ii) a mixed
ranking strategy that re-ranks the retrieved HTML files to eliminate bias
introduced from the search engine API; and (iii) an extractor-LLM that can
accurately and efficiently extract relevant information from the fresh content
in each HTML file. We conduct extensive empirical studies to evaluate the
performance of this Internet search augmented generation paradigm. The
experimental results demonstrate that our method generates content with
significantly improved quality. Our system has been successfully deployed in a
production environment to serve 01.AI's generative inference requests.",Guangxin He
2024-11-29T07:57:32Z,http://arxiv.org/abs/2411.19528v1,"RAGDiffusion: Faithful Cloth Generation via External Knowledge
  Assimilation","Standard clothing asset generation involves creating forward-facing flat-lay
garment images displayed on a clear background by extracting clothing
information from diverse real-world contexts, which presents significant
challenges due to highly standardized sampling distributions and precise
structural requirements in the generated images. Existing models have limited
spatial perception and often exhibit structural hallucinations in this
high-specification generative task. To address this issue, we propose a novel
Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance
structure determinacy and mitigate hallucinations by assimilating external
knowledge from LLM and databases. RAGDiffusion consists of two core processes:
(1) Retrieval-based structure aggregation, which employs contrastive learning
and a Structure Locally Linear Embedding (SLLE) to derive global structure and
spatial landmarks, providing both soft and hard guidance to counteract
structural ambiguities; and (2) Omni-level faithful garment generation, which
introduces a three-level alignment that ensures fidelity in structural,
pattern, and decoding components within the diffusing. Extensive experiments on
challenging real-world datasets demonstrate that RAGDiffusion synthesizes
structurally and detail-faithful clothing assets with significant performance
improvements, representing a pioneering effort in high-specification faithful
generation with RAG to confront intrinsic hallucinations and enhance fidelity.",Xianfeng Tan
2024-11-29T08:34:07Z,http://arxiv.org/abs/2411.19539v1,Knowledge Management for Automobile Failure Analysis Using Graph RAG,"This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.",Yuta Ojima
2024-11-29T09:07:21Z,http://arxiv.org/abs/2411.19554v1,"Unimib Assistant: designing a student-friendly RAG-based chatbot for all
  their needs","Natural language processing skills of Large Language Models (LLMs) are
unprecedented, having wide diffusion and application in different tasks. This
pilot study focuses on specializing ChatGPT behavior through a
Retrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs
feature. The purpose of our chatbot, called Unimib Assistant, is to provide
information and solutions to the specific needs of University of Milano-Bicocca
(Unimib) students through a question-answering approach. We provided the system
with a prompt highlighting its specific purpose and behavior, as well as
university-related documents and links obtained from an initial need-finding
phase, interviewing six students. After a preliminary customization phase, a
qualitative usability test was conducted with six other students to identify
the strengths and weaknesses of the chatbot, with the goal of improving it in a
subsequent redesign phase. While the chatbot was appreciated for its
user-friendly experience, perceived general reliability, well-structured
responses, and conversational tone, several significant technical and
functional limitations emerged. In particular, the satisfaction and overall
experience of the users was impaired by the system's inability to always
provide fully accurate information. Moreover, it would often neglect to report
relevant information even if present in the materials uploaded and prompt
given. Furthermore, it sometimes generated unclickable links, undermining its
trustworthiness, since providing the source of information was an important
aspect for our users. Further in-depth studies and feedback from other users as
well as implementation iterations are planned to refine our Unimib Assistant.",Chiara Antico
2024-11-29T13:57:07Z,http://arxiv.org/abs/2411.19710v1,"Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating
  RAG Systems","Retrieval Augmented Generation (RAG) systems are a widespread application of
Large Language Models (LLMs) in the industry. While many tools exist empowering
developers to build their own systems, measuring their performance locally,
with datasets reflective of the system's use cases, is a technological
challenge. Solutions to this problem range from non-specific and cheap (most
public datasets) to specific and costly (generating data from local documents).
In this paper, we show that using public question and answer (Q&A) datasets to
assess retrieval performance can lead to non-optimal systems design, and that
common tools for RAG dataset generation can lead to unbalanced data. We propose
solutions to these issues based on the characterization of RAG datasets through
labels and through label-targeted data generation. Finally, we show that
fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that
these observations are invaluable to the know-your-data step of RAG systems
development.",Rafael Teixeira de Lima
2024-11-29T20:13:56Z,http://arxiv.org/abs/2412.00239v1,Generating a Low-code Complete Workflow via Task Decomposition and RAG,"AI technologies are moving rapidly from research to production. With the
popularity of Foundation Models (FMs) that generate text, images, and video,
AI-based systems are increasing their complexity. Compared to traditional
AI-based software, systems employing FMs, or GenAI-based systems, are more
difficult to design due to their scale and versatility. This makes it necessary
to document best practices, known as design patterns in software engineering,
that can be used across GenAI applications. Our first contribution is to
formalize two techniques, Task Decomposition and Retrieval-Augmented Generation
(RAG), as design patterns for GenAI-based systems. We discuss their trade-offs
in terms of software quality attributes and comment on alternative approaches.
We recommend to AI practitioners to consider these techniques not only from a
scientific perspective but also from the standpoint of desired engineering
properties such as flexibility, maintainability, safety, and security. As a
second contribution, we describe our industry experience applying Task
Decomposition and RAG to build a complex real-world GenAI application for
enterprise users: Workflow Generation. The task of generating workflows entails
generating a specific plan using data from the system environment, taking as
input a user requirement. As these two patterns affect the entire AI
development cycle, we explain how they impacted the dataset creation, model
training, model evaluation, and deployment phases.",Orlando Marquez Ayala
2024-11-30T14:32:48Z,http://arxiv.org/abs/2412.00495v1,"Rethinking Strategic Mechanism Design In The Age Of Large Language
  Models: New Directions For Communication Systems","This paper explores the application of large language models (LLMs) in
designing strategic mechanisms -- including auctions, contracts, and games --
for specific purposes in communication networks. Traditionally, strategic
mechanism design in telecommunications has relied on human expertise to craft
solutions based on game theory, auction theory, and contract theory. However,
the evolving landscape of telecom networks, characterized by increasing
abstraction, emerging use cases, and novel value creation opportunities, calls
for more adaptive and efficient approaches. We propose leveraging LLMs to
automate or semi-automate the process of strategic mechanism design, from
intent specification to final formulation. This paradigm shift introduces both
semi-automated and fully-automated design pipelines, raising crucial questions
about faithfulness to intents, incentive compatibility, algorithmic stability,
and the balance between human oversight and artificial intelligence (AI)
autonomy. The paper discusses potential frameworks, such as retrieval-augmented
generation (RAG)-based systems, to implement LLM-driven mechanism design in
communication networks contexts. We examine key challenges, including LLM
limitations in capturing domain-specific constraints, ensuring strategy
proofness, and integrating with evolving telecom standards. By providing an
in-depth analysis of the synergies and tensions between LLMs and strategic
mechanism design within the IoT ecosystem, this work aims to stimulate
discussion on the future of AI-driven information economic mechanisms in
telecommunications and their potential to address complex, dynamic network
management scenarios.",Ismail Lotfi
2024-11-30T23:11:44Z,http://arxiv.org/abs/2412.00608v3,"Leveraging LLM for Automated Ontology Extraction and Knowledge Graph
  Generation","Extracting relevant and structured knowledge from large, complex technical
documents within the Reliability and Maintainability (RAM) domain is
labor-intensive and prone to errors. Our work addresses this challenge by
presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge
Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through
an interactive user interface guided by our adaptive iterative Chain of Thought
(CoT) algorithm to ensure that the ontology extraction process and, thus, KG
generation align with user-specific requirements. Although KG generation
follows a clear, structured path based on the confirmed ontology, there is no
universally correct ontology as it is inherently based on the user's
preferences. OntoKGen recommends an ontology grounded in best practices,
minimizing user effort and providing valuable insights that may have been
overlooked, all while giving the user complete control over the final ontology.
Having generated the KG based on the confirmed ontology, OntoKGen enables
seamless integration into schemeless, non-relational databases like Neo4j. This
integration allows for flexible storage and retrieval of knowledge from
diverse, unstructured sources, facilitating advanced querying, analysis, and
decision-making. Moreover, the generated KG serves as a robust foundation for
future integration into Retrieval Augmented Generation (RAG) systems, offering
enhanced capabilities for developing domain-specific intelligent applications.",Mohammad Sadeq Abolhasani
2024-12-01T15:19:23Z,http://arxiv.org/abs/2412.00846v1,"Improving Multimodal LLMs Ability In Geometry Problem Solving,
  Reasoning, And Multistep Scoring","This paper presents GPSM4K, a comprehensive geometry multimodal dataset
tailored to augment the problem-solving capabilities of Large Vision Language
Models (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs
manually extracted from mathematics textbooks spanning grades 7-12 and is
further augmented to 5340 problems, consisting of both numerical and
theorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which
feature only objective-type questions, GPSM4K offers detailed step-by-step
solutions in a consistent format, facilitating a comprehensive evaluation of
problem-solving approaches. This dataset serves as an excellent benchmark for
assessing the geometric reasoning capabilities of LVLMs. Evaluation of our test
set shows that there is scope for improvement needed in open-source language
models in geometry problem-solving. Finetuning on our training set increases
the geometry problem-solving capabilities of models. Further, We also evaluate
the effectiveness of techniques such as image captioning and Retrieval
Augmentation generation (RAG) on model performance. We leveraged LLM to
automate the task of final answer evaluation by providing ground truth and
predicted solutions. This research will help to assess and improve the
geometric reasoning capabilities of LVLMs.",Avinash Anand
2024-12-03T21:00:10Z,http://arxiv.org/abs/2412.02835v1,"CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural
  Networks","We present CAISSON, a novel hierarchical approach to Retrieval-Augmented
Generation (RAG) that transforms traditional single-vector search into a
multi-view clustering framework. At its core, CAISSON leverages dual
Self-Organizing Maps (SOMs) to create complementary organizational views of the
document space, where each view captures different aspects of document
relationships through specialized embeddings. The first view processes combined
text and metadata embeddings, while the second operates on metadata enriched
with concept embeddings, enabling a comprehensive multi-view analysis that
captures both fine-grained semantic relationships and high-level conceptual
patterns. This dual-view approach enables more nuanced document discovery by
combining evidence from different organizational perspectives. To evaluate
CAISSON, we develop SynFAQA, a framework for generating synthetic financial
analyst notes and question-answer pairs that systematically tests different
aspects of information retrieval capabilities. Drawing on HotPotQA's
methodology for constructing multi-step reasoning questions, SynFAQA generates
controlled test cases where each question is paired with the set of notes
containing its ground-truth answer, progressing from simple single-entity
queries to complex multi-hop retrieval tasks involving multiple entities and
concepts. Our experimental results demonstrate substantial improvements over
both basic and enhanced RAG implementations, particularly for complex
multi-entity queries, while maintaining practical response times suitable for
interactive applications.",Igor Halperin
2024-12-04T03:02:46Z,http://arxiv.org/abs/2412.02987v1,"Advancing Conversational Psychotherapy: Integrating Privacy,
  Dual-Memory, and Domain Expertise with Large Language Models","Mental health has increasingly become a global issue that reveals the
limitations of traditional conversational psychotherapy, constrained by
location, time, expense, and privacy concerns. In response to these challenges,
we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed
to democratize access to psychotherapy. SoulSpeak improves upon the
capabilities of standard LLM-enabled chatbots by incorporating a novel
dual-memory component that combines short-term and long-term context via
Retrieval Augmented Generation (RAG) to offer personalized responses while
ensuring the preservation of user privacy and intimacy through a dedicated
privacy module. In addition, it leverages a counseling chat dataset of
therapist-client interactions and various prompting techniques to align the
generated responses with psychotherapeutic methods. We introduce two fine-tuned
BERT models to evaluate the system against existing LLMs and human therapists:
the Conversational Psychotherapy Preference Model (CPPM) to simulate human
preference among responses and another to assess response relevance to user
input. CPPM is useful for training and evaluating psychotherapy-focused
language models independent from SoulSpeak, helping with the constrained
resources available for psychotherapy. Furthermore, the effectiveness of the
dual-memory component and the robustness of the privacy module are also
examined. Our findings highlight the potential and challenge of enhancing
mental health care by offering an alternative that combines the expertise of
traditional therapy with the advantages of LLMs, providing a promising way to
address the accessibility and personalization gap in current mental health
services.",XiuYu Zhang
2024-12-04T18:26:13Z,http://arxiv.org/abs/2412.03531v1,"A Review on Scientific Knowledge Extraction using Large Language Models
  in Biomedical Sciences","The rapid advancement of large language models (LLMs) has opened new
boundaries in the extraction and synthesis of medical knowledge, particularly
within evidence synthesis. This paper reviews the state-of-the-art applications
of LLMs in the biomedical domain, exploring their effectiveness in automating
complex tasks such as evidence synthesis and data extraction from a biomedical
corpus of documents. While LLMs demonstrate remarkable potential, significant
challenges remain, including issues related to hallucinations, contextual
understanding, and the ability to generalize across diverse medical tasks. We
highlight critical gaps in the current research literature, particularly the
need for unified benchmarks to standardize evaluations and ensure reliability
in real-world applications. In addition, we propose directions for future
research, emphasizing the integration of state-of-the-art techniques such as
retrieval-augmented generation (RAG) to enhance LLM performance in evidence
synthesis. By addressing these challenges and utilizing the strengths of LLMs,
we aim to improve access to medical literature and facilitate meaningful
discoveries in healthcare.",Gabriel Lino Garcia
2024-12-06T03:02:58Z,http://arxiv.org/abs/2412.04741v1,"Question Answering for Decisionmaking in Green Building Design: A
  Multimodal Data Reasoning Method Driven by Large Language Models","In recent years, the critical role of green buildings in addressing energy
consumption and environmental issues has become widely acknowledged. Research
indicates that over 40% of potential energy savings can be achieved during the
early design stage. Therefore, decision-making in green building design (DGBD),
which is based on modeling and performance simulation, is crucial for reducing
building energy costs. However, the field of green building encompasses a broad
range of specialized knowledge, which involves significant learning costs and
results in low decision-making efficiency. Many studies have already applied
artificial intelligence (AI) methods to this field. Based on previous research,
this study innovatively integrates large language models with DGBD, creating
GreenQA, a question answering framework for multimodal data reasoning.
Utilizing Retrieval Augmented Generation, Chain of Thought, and Function Call
methods, GreenQA enables multimodal question answering, including weather data
analysis and visualization, retrieval of green building cases, and knowledge
query. Additionally, this study conducted a user survey using the GreenQA web
platform. The results showed that 96% of users believed the platform helped
improve design efficiency. This study not only effectively supports DGBD but
also provides inspiration for AI-assisted design.",Yihui Li
2024-12-06T17:07:27Z,http://arxiv.org/abs/2412.05187v1,SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot,"Surgical interventions, particularly in neurology, represent complex and
high-stakes scenarios that impose substantial cognitive burdens on surgical
teams. Although deliberate education and practice can enhance cognitive
capabilities, surgical training opportunities remain limited due to patient
safety concerns. To address these cognitive challenges in surgical training and
operation, we propose SurgBox, an agent-driven sandbox framework to
systematically enhance the cognitive capabilities of surgeons in immersive
surgical simulations. Specifically, our SurgBox leverages large language models
(LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically
replicate various surgical roles, enabling realistic training environments for
deliberate practice. In particular, we devise Surgery Copilot, an AI-driven
assistant to actively coordinate the surgical information stream and support
clinical decision-making, thereby diminishing the cognitive workload of
surgical teams during surgery. By incorporating a novel Long-Short Memory
mechanism, our Surgery Copilot can effectively balance immediate procedural
assistance with comprehensive surgical knowledge. Extensive experiments using
real neurosurgical procedure records validate our SurgBox framework in both
enhancing surgical cognitive capabilities and supporting clinical
decision-making. By providing an integrated solution for training and
operational support to address cognitive challenges, our SurgBox framework
advances surgical education and practice, potentially transforming surgical
outcomes and healthcare quality. The code is available at
https://github.com/franciszchen/SurgBox.",Jinlin Wu
2024-12-06T17:35:52Z,http://arxiv.org/abs/2412.05206v1,"ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented
  Argumentation with LLM Judges","Computational argumentation, which involves generating answers or summaries
for controversial topics like abortion bans and vaccination, has become
increasingly important in today's polarized environment. Sophisticated LLM
capabilities offer the potential to provide nuanced, evidence-based answers to
such questions through Retrieval-Augmented Argumentation (RAArg), leveraging
real-world evidence for high-quality, grounded arguments. However, evaluating
RAArg remains challenging, as human evaluation is costly and difficult for
complex, lengthy answers on complicated topics. At the same time, re-using
existing argumentation datasets is no longer sufficient, as they lack long,
complex arguments and realistic evidence from potentially misleading sources,
limiting holistic evaluation of retrieval effectiveness and argument quality.
To address these gaps, we investigate automated evaluation methods using
multiple fine-grained LLM judges, providing better and more interpretable
assessments than traditional single-score metrics and even previously reported
human crowdsourcing. To validate the proposed techniques, we introduce ConQRet,
a new benchmark featuring long and complex human-authored arguments on debated
topics, grounded in real-world websites, allowing an exhaustive evaluation
across retrieval effectiveness, argument quality, and groundedness. We validate
our LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed
LLM Judges and the ConQRet benchmark can enable rapid progress in computational
argumentation and can be naturally extended to other complex
retrieval-augmented generation tasks.",Kaustubh D. Dhole
2024-12-07T08:50:24Z,http://arxiv.org/abs/2412.05587v2,"GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on
  the Google Earth Engine Platform Powered by Large Language Models","As the scale and complexity of spatiotemporal data continue to grow rapidly,
the use of geospatial modeling on the Google Earth Engine (GEE) platform
presents dual challenges: improving the coding efficiency of domain experts and
enhancing the coding capabilities of interdisciplinary users. To address these
challenges and improve the performance of large language models (LLMs) in
geospatial code generation tasks, we propose a framework for building a
geospatial operator knowledge base tailored to the GEE JavaScript API. This
framework consists of an operator syntax knowledge table, an operator
relationship frequency table, an operator frequent pattern knowledge table, and
an operator relationship chain knowledge table. By leveraging Abstract Syntax
Tree (AST) techniques and frequent itemset mining, we systematically extract
operator knowledge from 185,236 real GEE scripts and syntax documentation,
forming a structured knowledge base. Experimental results demonstrate that the
framework achieves over 90% accuracy, recall, and F1 score in operator
knowledge extraction. When integrated with the Retrieval-Augmented Generation
(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge
base improves performance by 20-30%. Ablation studies further quantify the
necessity of each knowledge table in the knowledge base construction. This work
provides robust support for the advancement and application of geospatial code
modeling techniques, offering an innovative approach to constructing
domain-specific knowledge bases that enhance the code generation capabilities
of LLMs, and fostering the deeper integration of generative AI technologies
within the field of geoinformatics.",Shuyang Hou
2024-12-08T23:00:06Z,http://arxiv.org/abs/2412.06099v1,DECO: Life-Cycle Management of Enterprise-Grade Chatbots,"Software engineers frequently grapple with the challenge of accessing
disparate documentation and telemetry data, including Troubleshooting Guides
(TSGs), incident reports, code repositories, and various internal tools
developed by multiple stakeholders. While on-call duties are inevitable,
incident resolution becomes even more daunting due to the obscurity of legacy
sources and the pressures of strict time constraints. To enhance the efficiency
of on-call engineers (OCEs) and streamline their daily workflows, we introduced
DECO -- a comprehensive framework for developing, deploying, and managing
enterprise-grade chatbots tailored to improve productivity in engineering
routines. This paper details the design and implementation of the DECO
framework, emphasizing its innovative NL2SearchQuery functionality and a
hierarchical planner. These features support efficient and customized
retrieval-augmented-generation (RAG) algorithms that not only extract relevant
information from diverse sources but also select the most pertinent toolkits in
response to user queries. This enables the addressing of complex technical
questions and provides seamless, automated access to internal resources.
Additionally, DECO incorporates a robust mechanism for converting unstructured
incident logs into user-friendly, structured guides, effectively bridging the
documentation gap. Feedback from users underscores DECO's pivotal role in
simplifying complex engineering tasks, accelerating incident resolution, and
bolstering organizational productivity. Since its launch in September 2023,
DECO has demonstrated its effectiveness through extensive engagement, with tens
of thousands of interactions from hundreds of active users across multiple
organizations within the company.",Yiwen Zhu
2024-12-09T04:56:43Z,http://arxiv.org/abs/2412.06206v1,SiReRAG: Indexing Similar and Related Information for Multihop Reasoning,"Indexing is an important step towards strong performance in
retrieval-augmented generation (RAG) systems. However, existing methods
organize data based on either semantic similarity (similarity) or related
information (relatedness), but do not cover both perspectives comprehensively.
Our analysis reveals that modeling only one perspective results in insufficient
knowledge synthesis, leading to suboptimal performance on complex tasks
requiring multihop reasoning. In this paper, we propose SiReRAG, a novel RAG
indexing approach that explicitly considers both similar and related
information. On the similarity side, we follow existing work and explore some
variances to construct a similarity tree based on recursive summarization. On
the relatedness side, SiReRAG extracts propositions and entities from texts,
groups propositions via shared entities, and generates recursive summaries to
construct a relatedness tree. We index and flatten both similarity and
relatedness trees into a unified retrieval pool. Our experiments demonstrate
that SiReRAG consistently outperforms state-of-the-art indexing methods on
three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an
average 1.9% improvement in F1 scores. As a reasonably efficient solution,
SiReRAG enhances existing reranking methods significantly, with up to 7.8%
improvement in average F1 scores.",Nan Zhang
2024-12-09T18:59:46Z,http://arxiv.org/abs/2412.06786v1,"Retrieving Semantics from the Deep: an RAG Solution for Gesture
  Synthesis","Non-verbal communication often comprises of semantically rich gestures that
help convey the meaning of an utterance. Producing such semantic co-speech
gestures has been a major challenge for the existing neural systems that can
generate rhythmic beat gestures, but struggle to produce semantically
meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based
gesture generation approach that leverages Retrieval Augmented Generation (RAG)
to produce natural-looking and semantically rich gestures. Our neuro-explicit
gesture generation approach is designed to produce semantic gestures grounded
in interpretable linguistic knowledge. We achieve this by using explicit domain
knowledge to retrieve exemplar motions from a database of co-speech gestures.
Once retrieved, we then inject these semantic exemplar gestures into our
diffusion-based gesture generation pipeline using DDIM inversion and retrieval
guidance at the inference time without any need of training. Further, we
propose a control paradigm for guidance, that allows the users to modulate the
amount of influence each retrieval insertion has over the generated sequence.
Our comparative evaluations demonstrate the validity of our approach against
recent gesture generation approaches. The reader is urged to explore the
results on our project page.",M. Hamza Mughal
2024-12-06T21:17:47Z,http://arxiv.org/abs/2412.06827v1,"Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning
  with Human-AI Feedback","Large Language Models (LLMs) have demonstrated strong capabilities in
text-based tasks but struggle with the complex reasoning required for physics
problems, particularly in advanced arithmetic and conceptual understanding.
While some research has explored ways to enhance LLMs in physics education
using techniques such as prompt engineering and Retrieval Augmentation
Generation (RAG), not enough effort has been made in addressing their
limitations in physics reasoning. This paper presents a novel approach to
improving LLM performance on physics questions using Reinforcement Learning
with Human and Artificial Intelligence Feedback (RLHAIF). We evaluate several
reinforcement learning methods, including Proximal Policy Optimization (PPO),
Direct Preference Optimization (DPO), and Remax optimization. These methods are
chosen to investigate RL policy performance with different settings on the
PhyQA dataset, which includes challenging physics problems from high school
textbooks. Our RLHAIF model, tested on leading LLMs like LLaMA2 and Mistral,
achieved superior results, notably with the MISTRAL-PPO model, demonstrating
marked improvements in reasoning and accuracy. It achieved high scores, with a
58.67 METEOR score and a 0.74 Reasoning score, making it a strong example for
future physics reasoning research in this area.",Avinash Anand
2024-12-07T01:32:13Z,http://arxiv.org/abs/2412.06832v1,"SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to
  Question Answering","Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to
generalize to new information by decoupling reasoning capabilities from static
knowledge bases. Traditional RAG enhancements have explored vertical scaling --
assigning subtasks to specialized modules -- and horizontal scaling --
replicating tasks across multiple agents -- to improve performance. However,
real-world applications impose diverse Service Level Agreements (SLAs) and
Quality of Service (QoS) requirements, involving trade-offs among objectives
such as reducing cost, ensuring answer quality, and adhering to specific
operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG
tailored for real-world Question Answering (QA) applications. By integrating
task-specific non-functional requirements -- such as answer quality, cost, and
latency -- into the system, we enable dynamic reconfiguration to meet diverse
SLAs. Our method maps these Service Level Objectives (SLOs) to system-level
parameters, allowing the generation of optimal results within specified
resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic
re-orchestration of a multi-agent RAG system can effectively manage the
trade-off between answer quality and cost. By adjusting the system based on
query intent and operational conditions, we systematically balance performance
and resource utilization. This approach allows the system to meet SLOs for
various query types, showcasing its practicality for real-world applications.",Michael Iannelli
2024-12-10T14:39:51Z,http://arxiv.org/abs/2412.07548v1,"Automatic Database Configuration Debugging using Retrieval-Augmented
  Language Models","Database management system (DBMS) configuration debugging, e.g., diagnosing
poorly configured DBMS knobs and generating troubleshooting recommendations, is
crucial in optimizing DBMS performance. However, the configuration debugging
process is tedious and, sometimes challenging, even for seasoned database
administrators (DBAs) with sufficient experience in DBMS configurations and
good understandings of the DBMS internals (e.g., MySQL or Oracle). To address
this difficulty, we propose Andromeda, a framework that utilizes large language
models (LLMs) to enable automatic DBMS configuration debugging. Andromeda
serves as a natural surrogate of DBAs to answer a wide range of natural
language (NL) questions on DBMS configuration issues, and to generate
diagnostic suggestions to fix these issues. Nevertheless, directly prompting
LLMs with these professional questions may result in overly generic and often
unsatisfying answers. To this end, we propose a retrieval-augmented generation
(RAG) strategy that effectively provides matched domain-specific contexts for
the question from multiple sources. They come from related historical
questions, troubleshooting manuals and DBMS telemetries, which significantly
improve the performance of configuration debugging. To support the RAG
strategy, we develop a document retrieval mechanism addressing heterogeneous
documents and design an effective method for telemetry analysis. Extensive
experiments on real-world DBMS configuration debugging datasets show that
Andromeda significantly outperforms existing solutions.",Sibei Chen
2024-12-10T16:05:56Z,http://arxiv.org/abs/2412.07626v1,"OmniDocBench: Benchmarking Diverse PDF Document Parsing with
  Comprehensive Annotations","Document content extraction is crucial in computer vision, especially for
meeting the high-quality data needs of large language models (LLMs) and
retrieval-augmented generation (RAG) technologies. However, current document
parsing methods suffer from significant limitations in terms of diversity and
comprehensive evaluation. To address these challenges, we introduce
OmniDocBench, a novel multi-source benchmark designed to advance automated
document content extraction. OmniDocBench includes a meticulously curated and
annotated high-quality evaluation dataset comprising nine diverse document
types, such as academic papers, textbooks, slides, among others. Our benchmark
provides a flexible and comprehensive evaluation framework with 19 layout
category labels and 14 attribute labels, enabling multi-level assessments
across entire datasets, individual modules, or specific data types. Using
OmniDocBench, we perform an exhaustive comparative analysis of existing modular
pipelines and multimodal end-to-end methods, highlighting their limitations in
handling document diversity and ensuring fair evaluation. OmniDocBench
establishes a robust, diverse, and fair evaluation standard for the document
content extraction field, offering crucial insights for future advancements and
fostering the development of document parsing technologies. The codes and
dataset is available in https://github.com/opendatalab/OmniDocBench.",Linke Ouyang
2024-12-10T17:20:47Z,http://arxiv.org/abs/2412.07687v1,"Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions","The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.",Anant Prakash Awasthi
2024-12-10T18:17:02Z,http://arxiv.org/abs/2412.07724v2,Granite Guardian,"We introduce the Granite Guardian models, a suite of safeguards designed to
provide risk detection for prompts and responses, enabling safe and responsible
use in combination with any large language model (LLM). These models offer
comprehensive coverage across multiple risk dimensions, including social bias,
profanity, violence, sexual content, unethical behavior, jailbreaking, and
hallucination-related risks such as context relevance, groundedness, and answer
relevance for retrieval-augmented generation (RAG). Trained on a unique dataset
combining human annotations from diverse sources and synthetic data, Granite
Guardian models address risks typically overlooked by traditional risk
detection models, such as jailbreaks and RAG-specific issues. With AUC scores
of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks
respectively, Granite Guardian is the most generalizable and competitive model
available in the space. Released as open-source, Granite Guardian aims to
promote responsible AI development across the community.
  https://github.com/ibm-granite/granite-guardian",Inkit Padhi
2024-12-11T03:00:24Z,http://arxiv.org/abs/2412.08054v1,Federated In-Context LLM Agent Learning,"Large Language Models (LLMs) have revolutionized intelligent services by
enabling logical reasoning, tool use, and interaction with external systems as
agents. The advancement of LLMs is frequently hindered by the scarcity of
high-quality data, much of which is inherently sensitive. Federated learning
(FL) offers a potential solution by facilitating the collaborative training of
distributed LLMs while safeguarding private data. However, FL frameworks face
significant bandwidth and computational demands, along with challenges from
heterogeneous data distributions. The emerging in-context learning capability
of LLMs offers a promising approach by aggregating natural language rather than
bulky model parameters. Yet, this method risks privacy leakage, as it
necessitates the collection and presentation of data samples from various
clients during aggregation. In this paper, we propose a novel
privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,
which to our best knowledge for the first work unleashes the power of
in-context learning to train diverse LLM agents through FL. In our design,
knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums
Generation (KCG) module are transmitted between clients and the server instead
of model parameters in previous FL methods. Apart from that, an incredible
Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)
module is designed and we incorporate the aggregated global knowledge
compendium as a teacher to teach LLM agents the usage of tools. We conducted
extensive experiments and the results show that FICAL has competitive
performance compared to other SOTA baselines with a significant communication
cost decrease of $\mathbf{3.33\times10^5}$ times.",Panlong Wu
2024-12-13T07:51:32Z,http://arxiv.org/abs/2412.09936v1,"CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven
  Visual Language Models","The obesity phenomenon, known as the heavy issue, is a leading cause of
preventable chronic diseases worldwide. Traditional calorie estimation tools
often rely on specific data formats or complex pipelines, limiting their
practicality in real-world scenarios. Recently, vision-language models (VLMs)
have excelled in understanding real-world contexts and enabling conversational
interactions, making them ideal for downstream tasks such as ingredient
analysis. However, applying VLMs to calorie estimation requires domain-specific
data and alignment strategies. To this end, we curated CalData, a 330K
image-text pair dataset tailored for ingredient recognition and calorie
estimation, combining a large-scale recipe dataset with detailed nutritional
instructions for robust vision-language training. Built upon this dataset, we
present CaLoRAify, a novel VLM framework aligning ingredient recognition and
calorie estimation via training with visual-text pairs. During inference, users
only need a single monocular food image to estimate calories while retaining
the flexibility of agent-based conversational interaction. With Low-rank
Adaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our
system enhances the performance of foundational VLMs in the vertical domain of
calorie estimation. Our code and data are fully open-sourced at
https://github.com/KennyYao2001/16824-CaLORAify.",Dongyu Yao
2024-12-13T21:28:17Z,http://arxiv.org/abs/2412.10571v3,"Evidence Contextualization and Counterfactual Attribution for
  Conversational QA over Heterogeneous Data with RAG Systems","Retrieval Augmented Generation (RAG) works as a backbone for interacting with
an enterprise's own data via Conversational Question Answering (ConvQA). In a
RAG system, a retriever fetches passages from a collection in response to a
question, which are then included in the prompt of a large language model (LLM)
for generating a natural language (NL) answer. However, several RAG systems
today suffer from two shortcomings: (i) retrieved passages usually contain
their raw text and lack appropriate document context, negatively impacting both
retrieval and answering quality; and (ii) attribution strategies that explain
answer generation typically rely only on similarity between the answer and the
retrieved passages, thereby only generating plausible but not causal
explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies
the above concerns by: (i) contextualizing evidence with source metadata and
surrounding text; and (ii) computing counterfactual attribution, a causal
explanation approach where the contribution of an evidence to an answer is
determined by the similarity of the original response to the answer obtained by
removing that evidence. To evaluate our proposals, we release a new benchmark
ConfQuestions: it has 300 hand-created conversational questions, each in
English and German, coupled with ground truth URLs, completed questions, and
answers from 215 public Confluence pages. These documents are typical of
enterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE
on ConfQuestions show the viability of our ideas: contextualization improves
RAG performance, and counterfactual explanations outperform standard
attribution.",Rishiraj Saha Roy
2024-12-15T04:51:30Z,http://arxiv.org/abs/2412.11050v1,"RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous
  Driving with Vision-Language Models","Understanding and addressing corner cases is essential for ensuring the
safety and reliability of autonomous driving systems. Vision-Language Models
(VLMs) play a crucial role in enhancing scenario comprehension, yet they face
significant challenges, such as hallucination and insufficient real-world
grounding, which compromise their performance in critical driving scenarios. In
this work, we propose RAC3, a novel framework designed to improve VLMs' ability
to handle corner cases effectively. The framework integrates
Retrieval-Augmented Generation (RAG) to mitigate hallucination by dynamically
incorporating context-specific external knowledge. A cornerstone of RAC3 is its
cross-modal alignment fine-tuning, which utilizes contrastive learning to embed
image-text pairs into a unified semantic space, enabling robust retrieval of
similar scenarios. We evaluate RAC3 through extensive experiments using a
curated dataset of corner case scenarios, demonstrating its ability to enhance
semantic alignment, improve hallucination mitigation, and achieve superior
performance metrics, such as Cosine Similarity and ROUGE-L scores. For example,
for the LLaVA-v1.6-34B VLM, the cosine similarity between the generated text
and the reference text has increased by 5.22\%. The F1-score in ROUGE-L has
increased by 39.91\%, the Precision has increased by 55.80\%, and the Recall
has increased by 13.74\%. This work underscores the potential of
retrieval-augmented VLMs to advance the robustness and safety of autonomous
driving in complex environments.",Yujin Wang
2024-12-17T10:36:52Z,http://arxiv.org/abs/2412.12775v1,RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service,"Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.",Yihang Cheng
2024-12-17T15:38:42Z,http://arxiv.org/abs/2412.13018v1,"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in
  Financial Domain","As a typical and practical application of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) techniques have gained extensive
attention, particularly in vertical domains where LLMs may lack domain-specific
knowledge. In this paper, we introduce an omnidirectional and automatic RAG
benchmark, OmniEval, in the financial domain. Our benchmark is characterized by
its multi-dimensional evaluation framework, including (1) a matrix-based RAG
scenario evaluation system that categorizes queries into five task classes and
16 financial topics, leading to a structured assessment of diverse query
scenarios; (2) a multi-dimensional evaluation data generation approach, which
combines GPT-4-based automatic generation and human annotation, achieving an
87.47\% acceptance ratio in human evaluations on generated instances; (3) a
multi-stage evaluation system that evaluates both retrieval and generation
performance, result in a comprehensive evaluation on the RAG pipeline; and (4)
robust evaluation metrics derived from rule-based and LLM-based ones, enhancing
the reliability of assessments through manual annotations and supervised
fine-tuning of an LLM evaluator. Our experiments demonstrate the
comprehensiveness of OmniEval, which includes extensive test datasets and
highlights the performance variations of RAG systems across diverse topics and
tasks, revealing significant opportunities for RAG models to improve their
capabilities in vertical domains. We open source the code of our benchmark in
\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",Shuting Wang
2024-12-18T11:28:05Z,http://arxiv.org/abs/2412.13746v1,"RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented
  Generation for Preference Alignment","Despite the significant progress made by existing retrieval augmented
language models (RALMs) in providing trustworthy responses and grounding in
reliable sources, they often overlook effective alignment with human
preferences. In the alignment process, reward models (RMs) act as a crucial
proxy for human values to guide optimization. However, it remains unclear how
to evaluate and select a reliable RM for preference alignment in RALMs. To this
end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG
settings. First, we design four crucial and challenging RAG-specific scenarios
to assess RMs, including multi-hop reasoning, fine-grained citation,
appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG
subsets, six retrievers, and 24 RALMs to increase the diversity of data
sources. Finally, we adopt an LLM-as-a-judge approach to improve preference
annotation efficiency and effectiveness, exhibiting a strong correlation with
human annotations. Based on the RAG-RewardBench, we conduct a comprehensive
evaluation of 45 RMs and uncover their limitations in RAG scenarios.
Additionally, we also reveal that existing trained RALMs show almost no
improvement in preference alignment, highlighting the need for a shift towards
preference-aligned training.We release our benchmark and code publicly at
https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",Zhuoran Jin
2024-12-18T12:11:39Z,http://arxiv.org/abs/2412.13774v1,Designing an LLM-Based Copilot for Manufacturing Equipment Selection,"Effective decision-making in automation equipment selection is critical for
reducing ramp-up time and maintaining production quality, especially in the
face of increasing product variation and market demands. However, limited
expertise and resource constraints often result in inefficiencies during the
ramp-up phase when new products are integrated into production lines. Existing
methods often lack structured and tailored solutions to support automation
engineers in reducing ramp-up time, leading to compromises in quality. This
research investigates whether large-language models (LLMs), combined with
Retrieval-Augmented Generation (RAG), can assist in streamlining equipment
selection in ramp-up planning. We propose a factual-driven copilot integrating
LLMs with structured and semi-structured knowledge retrieval for three
component types (robots, feeders and vision systems), providing a guided and
traceable state-machine process for decision-making in automation equipment
selection. The system was demonstrated to an industrial partner, who tested it
on three internal use-cases. Their feedback affirmed its capability to provide
logical and actionable recommendations for automation equipment. More
specifically, among 22 equipment prompts analyzed, 19 involved selecting the
correct equipment while considering most requirements, and in 6 cases, all
requirements were fully met.",Jonas Werheid
2024-12-18T12:45:55Z,http://arxiv.org/abs/2412.13799v1,"Enhancing Rhetorical Figure Annotation: An Ontology-Based Web
  Application with RAG Integration","Rhetorical figures play an important role in our communication. They are used
to convey subtle, implicit meaning, or to emphasize statements. We notice them
in hate speech, fake news, and propaganda. By improving the systems for
computational detection of rhetorical figures, we can also improve tasks such
as hate speech and fake news detection, sentiment analysis, opinion mining, or
argument mining. Unfortunately, there is a lack of annotated data, as well as
qualified annotators that would help us build large corpora to train machine
learning models for the detection of rhetorical figures. The situation is
particularly difficult in languages other than English, and for rhetorical
figures other than metaphor, sarcasm, and irony. To overcome this issue, we
develop a web application called ""Find your Figure"" that facilitates the
identification and annotation of German rhetorical figures. The application is
based on the German Rhetorical ontology GRhOOT which we have specially adapted
for this purpose. In addition, we improve the user experience with Retrieval
Augmented Generation (RAG). In this paper, we present the restructuring of the
ontology, the development of the web application, and the built-in RAG
pipeline. We also identify the optimal RAG settings for our application. Our
approach is one of the first to practically use rhetorical ontologies in
combination with RAG and shows promising results.",Ramona Kühn
2024-12-14T17:08:34Z,http://arxiv.org/abs/2412.15247v1,"Streamlining Systematic Reviews: A Novel Application of Large Language
  Models","Systematic reviews (SRs) are essential for evidence-based guidelines but are
often limited by the time-consuming nature of literature screening. We propose
and evaluate an in-house system based on Large Language Models (LLMs) for
automating both title/abstract and full-text screening, addressing a critical
gap in the literature. Using a completed SR on Vitamin D and falls (14,439
articles), the LLM-based system employed prompt engineering for title/abstract
screening and Retrieval-Augmented Generation (RAG) for full-text screening. The
system achieved an article exclusion rate (AER) of 99.5%, specificity of 99.6%,
a false negative rate (FNR) of 0%, and a negative predictive value (NPV) of
100%. After screening, only 78 articles required manual review, including all
20 identified by traditional methods, reducing manual screening time by 95.5%.
For comparison, Rayyan, a commercial tool for title/abstract screening,
achieved an AER of 72.1% and FNR of 5% when including articles Rayyan
considered as undecided or likely to include. Lowering Rayyan's inclusion
thresholds improved FNR to 0% but increased screening time. By addressing both
screening phases, the LLM-based system significantly outperformed Rayyan and
traditional methods, reducing total screening time to 25.5 hours while
maintaining high accuracy. These findings highlight the transformative
potential of LLMs in SR workflows by offering a scalable, efficient, and
accurate solution, particularly for the full-text screening phase, which has
lacked automation tools.",Fouad Trad
2024-12-19T22:51:56Z,http://arxiv.org/abs/2412.15443v1,"SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic
  Retrieval","Retrieval-Augmented Generation (RAG) systems have become pivotal in
leveraging vast corpora to generate informed and contextually relevant
responses, notably reducing hallucinations in Large Language Models. Despite
significant advancements, these systems struggle to efficiently process and
retrieve information from large datasets while maintaining a comprehensive
understanding of the context. This paper introduces SKETCH, a novel methodology
that enhances the RAG retrieval process by integrating semantic text retrieval
with knowledge graphs, thereby merging structured and unstructured data for a
more holistic comprehension. SKETCH, demonstrates substantial improvements in
retrieval performance and maintains superior context integrity compared to
traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,
NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline
approaches on key RAGAS metrics such as answer_relevancy, faithfulness,
context_precision and context_recall. Notably, on the Italian Cuisine dataset,
SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,
representing the highest performance across all evaluated metrics. These
results highlight SKETCH's capability in delivering more accurate and
contextually relevant responses, setting new benchmarks for future retrieval
systems.",Aakash Mahalingam
2024-12-21T13:19:15Z,http://arxiv.org/abs/2412.16615v1,"Large Language Model Can Be a Foundation for Hidden Rationale-Based
  Retrieval","Despite the recent advancement in Retrieval-Augmented Generation (RAG)
systems, most retrieval methodologies are often developed for factual
retrieval, which assumes query and positive documents are semantically similar.
In this paper, we instead propose and study a more challenging type of
retrieval task, called hidden rationale retrieval, in which query and document
are not similar but can be inferred by reasoning chains, logic relationships,
or empirical experiences. To address such problems, an instruction-tuned Large
language model (LLM) with a cross-encoder architecture could be a reasonable
choice. To further strengthen pioneering LLM-based retrievers, we design a
special instruction that transforms the retrieval task into a generative task
by prompting LLM to answer a binary-choice question. The model can be
fine-tuned with direct preference optimization (DPO). The framework is also
optimized for computational efficiency with no performance degradation. We name
this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned
performance superiority on Emotional Support Conversation (ESC), compared with
previous retrieval works. Our study suggests the potential to employ LLM as a
foundation for a wider scope of retrieval tasks. Our codes, models, and
datasets are available on https://github.com/flyfree5/LaHoRe.",Luo Ji
2024-12-22T20:03:35Z,http://arxiv.org/abs/2412.17146v1,LLM Agent for Fire Dynamics Simulations,"Significant advances have been achieved in leveraging foundation models, such
as large language models (LLMs), to accelerate complex scientific workflows. In
this work we introduce FoamPilot, a proof-of-concept LLM agent designed to
enhance the usability of FireFOAM, a specialized solver for fire dynamics and
fire suppression simulations built using OpenFOAM, a popular open-source
toolbox for computational fluid dynamics (CFD). FoamPilot provides three core
functionalities: code insight, case configuration and simulation evaluation.
Code insight is an alternative to traditional keyword searching leveraging
retrieval-augmented generation (RAG) and aims to enable efficient navigation
and summarization of the FireFOAM source code for developers and experienced
users. For case configuration, the agent interprets user requests in natural
language and aims to modify existing simulation setups accordingly to support
intermediate users. FoamPilot's job execution functionality seeks to manage the
submission and execution of simulations in high-performance computing (HPC)
environments and provide preliminary analysis of simulation results to support
less experienced users. Promising results were achieved for each functionality,
particularly for simple tasks, and opportunities were identified for
significant further improvement for more complex tasks. The integration of
these functionalities into a single LLM agent is a step aimed at accelerating
the simulation workflow for engineers and scientists employing FireFOAM for
complex simulations critical for improving fire safety.",Leidong Xu
2024-12-24T02:21:09Z,http://arxiv.org/abs/2412.18100v1,EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent,"The rapid growth of scientific techniques and knowledge is reflected in the
exponential increase in new patents filed annually. While these patents drive
innovation, they also present significant burden for researchers and engineers,
especially newcomers. To avoid the tedious work of navigating a vast and
complex landscape to identify trends and breakthroughs, researchers urgently
need efficient tools to summarize, evaluate, and contextualize patents,
revealing their innovative contributions and underlying scientific
principles.To address this need, we present EvoPat, a multi-LLM-based patent
agent designed to assist users in analyzing patents through Retrieval-Augmented
Generation (RAG) and advanced search strategies. EvoPat leverages multiple
Large Language Models (LLMs), each performing specialized roles such as
planning, identifying innovations, and conducting comparative evaluations. The
system integrates data from local databases, including patents, literature,
product catalogous, and company repositories, and online searches to provide
up-to-date insights. The ability to collect information not included in
original database automatically is also implemented. Through extensive testing
in the natural language processing (NLP) domain, we demonstrate that EvoPat
outperforms GPT-4 in tasks such as patent summarization, comparative analysis,
and technical evaluation. EvoPat represents a significant step toward creating
AI-powered tools that empower researchers and engineers to efficiently navigate
the complexities of the patent landscape.",Suyuan Wang
2023-04-28T10:15:25Z,http://arxiv.org/abs/2304.14732v7,"Search-in-the-Chain: Interactively Enhancing Large Language Models with
  Search for Knowledge-intensive Tasks","Making the content generated by Large Language Model (LLM), accurate,
credible and traceable is crucial, especially in complex knowledge-intensive
tasks that require multi-step reasoning and each step needs knowledge to solve.
Retrieval-augmented generation is good potential to solve this problem.
However, where and how to introduce Information Retrieval (IR) to LLM is a big
challenge. Previous work has the problems that wrong knowledge retrieved by IR
misleads the LLM and interaction between IR and LLM breaks the reasoning chain
of LLM. This paper proposes a novel framework named
\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the reasoning chain named
Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer
pair. Second, IR verifies the answer of each node of CoQ. It corrects the
answer that is not consistent with the retrieved information when IR gives high
confidence, which improves the credibility. Third, LLM can indicate its missing
knowledge in CoQ and rely on IR to provide this knowledge to LLM. These
operations improve the accuracy in terms of reasoning and knowledge. Finally,
SearChain generates the reasoning process and marks references to supporting
documents for each reasoning step, which improves traceability. Interaction
with IR in SearChain forms a novel reasoning path based on a tree, which
enables LLM to dynamically modify the direction of reasoning. Experiments show
that SearChain outperforms state-of-the-art baselines on complex
knowledge-intensive tasks including multi-hop Q\&A, slot filling, fact
checking, and long-form Q\&A.",Shicheng Xu
2023-10-18T18:00:11Z,http://arxiv.org/abs/2310.12214v6,"InferDPT: Privacy-Preserving Inference for Black-box Large Language
  Model","Large language models (LLMs), like ChatGPT, have greatly simplified text
generation tasks. However, they have also raised concerns about privacy risks
such as data leakage and unauthorized data collection. Existing solutions for
privacy-preserving inference face practical challenges related to computation
time and communication costs. In this paper, we propose InferDPT, the first
practical framework for the privacy-preserving Inference of black-box LLMs,
implementing Differential Privacy in Text generation. InferDPT comprises two
key modules: the ""perturbation module"" utilizes the exponential mechanism to
generate a perturbed prompt, facilitating privacy-preserving inference with
black-box LLMs, and the ""extraction module"", inspired by knowledge distillation
and retrieval-augmented generation, extracts coherent and consistent text from
the perturbed generation result, ensuring successful text generation
completion. To address privacy concerns related to previous exponential
mechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,
a novel differential privacy mechanism integrated into the perturbation module
of InferDPT, which introduces the concept of ""RANdom adjacency"" for TEXT
perturbation within the prompt. Experimental results across three datasets
demonstrate that the text generation quality of InferDPT is comparable to that
of non-private GPT-4, and RANTEXT surpasses existing state-of-the-art
mechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and
utility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves
an average privacy protection rate exceeding 90% against embedding revision
attacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher
than that of CUSTEXT+.",Meng Tong
2023-11-29T03:07:00Z,http://arxiv.org/abs/2311.17330v2,"Biomedical knowledge graph-optimized prompt generation for large
  language models","Large Language Models (LLMs) are being adopted at an unprecedented rate, yet
still face challenges in knowledge-intensive domains like biomedicine.
Solutions such as pre-training and domain-specific fine-tuning add substantial
computational overhead, requiring further domain expertise. Here, we introduce
a token-optimized and robust Knowledge Graph-based Retrieval Augmented
Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)
with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful
biomedical text rooted in established knowledge. Compared to the existing RAG
technique for Knowledge Graphs, the proposed method utilizes minimal graph
schema for context extraction and uses embedding methods for context pruning.
This optimization in context extraction results in more than 50% reduction in
token consumption without compromising the accuracy, making a cost-effective
and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced
the performance of LLMs across diverse biomedical prompts by generating
responses rooted in established knowledge, accompanied by accurate provenance
and statistical evidence (if available) to substantiate the claims. Further
benchmarking on human curated datasets, such as biomedical true/false and
multiple-choice questions (MCQ), showed a remarkable 71% boost in the
performance of the Llama-2 model on the challenging MCQ dataset, demonstrating
the framework's capacity to empower open-source models with fewer parameters
for domain specific questions. Furthermore, KG-RAG enhanced the performance of
proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM in a token
optimized fashion, thus enhancing the adaptability of general-purpose LLMs to
tackle domain-specific questions in a cost-effective fashion.",Karthik Soman
2023-12-24T23:01:00Z,http://arxiv.org/abs/2312.15561v5,"README: Bridging Medical Jargon and Lay Understanding for Patient
  Education through Data-Centric NLP","The advancement in healthcare has shifted focus toward patient-centric
approaches, particularly in self-care and patient education, facilitated by
access to Electronic Health Records (EHR). However, medical jargon in EHRs
poses significant challenges in patient comprehension. To address this, we
introduce a new task of automatically generating lay definitions, aiming to
simplify complex medical terms into patient-friendly lay language. We first
created the README dataset, an extensive collection of over 50,000 unique
(medical term, lay definition) pairs and 300,000 mentions, each offering
context-aware lay definitions manually annotated by domain experts. We have
also engineered a data-centric Human-AI pipeline that synergizes data
filtering, augmentation, and selection to improve data quality. We then used
README as the training data for models and leveraged a Retrieval-Augmented
Generation method to reduce hallucinations and improve the quality of model
outputs. Our extensive automatic and human evaluations demonstrate that
open-source mobile-friendly models, when fine-tuned with high-quality data, are
capable of matching or even surpassing the performance of state-of-the-art
closed-source large language models like ChatGPT. This research represents a
significant stride in closing the knowledge gap in patient education and
advancing patient-centric healthcare solutions.",Zonghai Yao
2023-12-25T02:32:05Z,http://arxiv.org/abs/2312.15591v5,Privacy-Preserved Neural Graph Databases,"In the era of large language models (LLMs), efficient and accurate data
retrieval has become increasingly crucial for the use of domain-specific or
private data in the retrieval augmented generation (RAG). Neural graph
databases (NGDBs) have emerged as a powerful paradigm that combines the
strengths of graph databases (GDBs) and neural networks to enable efficient
storage, retrieval, and analysis of graph-structured data which can be
adaptively trained with LLMs. The usage of neural embedding storage and Complex
neural logical Query Answering (CQA) provides NGDBs with generalization
ability. When the graph is incomplete, by extracting latent patterns and
representations, neural graph databases can fill gaps in the graph structure,
revealing hidden relationships and enabling accurate query answering.
Nevertheless, this capability comes with inherent trade-offs, as it introduces
additional privacy risks to the domain-specific or private databases. Malicious
attackers can infer more sensitive information in the database using
well-designed queries such as from the answer sets of where Turing Award
winners born before 1950 and after 1940 lived, the living places of Turing
Award winner Hinton are probably exposed, although the living places may have
been deleted in the training stage due to the privacy concerns. In this work,
we propose a privacy-preserved neural graph database (P-NGDB) framework to
alleviate the risks of privacy leakage in NGDBs. We introduce adversarial
training techniques in the training stage to enforce the NGDBs to generate
indistinguishable answers when queried with private information, enhancing the
difficulty of inferring sensitive information through combinations of multiple
innocuous queries.",Qi Hu
2023-12-29T03:23:23Z,http://arxiv.org/abs/2312.17449v2,"DB-GPT: Empowering Database Interactions with Private Large Language
  Models","The recent breakthroughs in large language models (LLMs) are positioned to
transition many areas of software. Database technologies particularly have an
important entanglement with LLMs as efficient and intuitive database
interactions are paramount. In this paper, we present DB-GPT, a revolutionary
and production-ready project that integrates LLMs with traditional database
systems to enhance user experience and accessibility. DB-GPT is designed to
understand natural language queries, provide context-aware responses, and
generate complex SQL queries with high accuracy, making it an indispensable
tool for users ranging from novice to expert. The core innovation in DB-GPT
lies in its private LLM technology, which is fine-tuned on domain-specific
corpora to maintain user privacy and ensure data security while offering the
benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which
includes a novel retrieval augmented generation (RAG) knowledge system, an
adaptive learning mechanism to continuously improve performance based on user
feedback and a service-oriented multi-model framework (SMMF) with powerful
data-driven agents. Our extensive experiments and user studies confirm that
DB-GPT represents a paradigm shift in database interactions, offering a more
natural, efficient, and secure way to engage with data repositories. The paper
concludes with a discussion of the implications of DB-GPT framework on the
future of human-database interaction and outlines potential avenues for further
enhancements and applications in the field. The project code is available at
https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by
installing it with the instructions
https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute
video at https://www.youtube.com/watch?v=KYs4nTDzEhk.",Siqiao Xue
2024-01-02T17:56:30Z,http://arxiv.org/abs/2401.01313v3,"A Comprehensive Survey of Hallucination Mitigation Techniques in Large
  Language Models","As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.",S. M Towhidul Islam Tonmoy
2024-01-05T15:09:57Z,http://arxiv.org/abs/2401.02851v2,"Natural Language Programming in Medicine: Administering Evidence Based
  Clinical Workflows with Autonomous Agents Powered by Generative Large
  Language Models","Generative Large Language Models (LLMs) hold significant promise in
healthcare, demonstrating capabilities such as passing medical licensing exams
and providing clinical knowledge. However, their current use as information
retrieval tools is limited by challenges like data staleness, resource demands,
and occasional generation of incorrect information. This study assessed the
potential of LLMs to function as autonomous agents in a simulated tertiary care
medical center, using real-world clinical cases across multiple specialties.
Both proprietary and open-source LLMs were evaluated, with Retrieval Augmented
Generation (RAG) enhancing contextual relevance. Proprietary models,
particularly GPT-4, generally outperformed open-source models, showing improved
guideline adherence and more accurate responses with RAG. The manual evaluation
by expert clinicians was crucial in validating models' outputs, underscoring
the importance of human oversight in LLM operation. Further, the study
emphasizes Natural Language Programming (NLP) as the appropriate paradigm for
modifying model behavior, allowing for precise adjustments through tailored
prompts and real-world interactions. This approach highlights the potential of
LLMs to significantly enhance and supplement clinical decision-making, while
also emphasizing the value of continuous expert involvement and the flexibility
of NLP to ensure their reliability and effectiveness in healthcare settings.",Akhil Vaid
2024-01-10T02:57:20Z,http://arxiv.org/abs/2401.06800v1,Reinforcement Learning for Optimizing RAG for Domain Chatbots,"With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.",Mandar Kulkarni
2024-01-16T14:44:47Z,http://arxiv.org/abs/2401.08406v3,"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on
  Agriculture","There are two common ways in which developers are incorporating proprietary
and domain-specific data when building applications of Large Language Models
(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the
prompt with the external data, while fine-Tuning incorporates the additional
knowledge into the model itself. However, the pros and cons of both approaches
are not well understood. In this paper, we propose a pipeline for fine-tuning
and RAG, and present the tradeoffs of both for multiple popular LLMs, including
Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,
including extracting information from PDFs, generating questions and answers,
using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We
propose metrics to assess the performance of different stages of the RAG and
fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.
Agriculture as an industry has not seen much penetration of AI, and we study a
potentially disruptive application - what if we could provide location-specific
insights to a farmer? Our results show the effectiveness of our dataset
generation pipeline in capturing geographic-specific knowledge, and the
quantitative and qualitative benefits of RAG and fine-tuning. We see an
accuracy increase of over 6 p.p. when fine-tuning the model and this is
cumulative with RAG, which increases accuracy by 5 p.p. further. In one
particular experiment, we also demonstrate that the fine-tuned model leverages
information from across geographies to answer specific questions, increasing
answer similarity from 47% to 72%. Overall, the results point to how systems
built using LLMs can be adapted to respond and incorporate knowledge across a
dimension that is critical for a specific industry, paving the way for further
applications of LLMs in other industrial domains.",Angels Balaguer
2024-01-27T02:29:42Z,http://arxiv.org/abs/2401.15269v3,"Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models","Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.",Minbyul Jeong
2024-01-27T10:50:11Z,http://arxiv.org/abs/2401.15378v4,"A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM","Challenges exist in learning and understanding religions, such as the
complexity and depth of religious doctrines and teachings. Chatbots as
question-answering systems can help in solving these challenges. LLM chatbots
use NLP techniques to establish connections between topics and accurately
respond to complex questions. These capabilities make it perfect for
enlightenment on religion as a question-answering chatbot. However, LLMs also
tend to generate false information, known as hallucination. Also, the chatbots'
responses can include content that insults personal religious beliefs,
interfaith conflicts, and controversial or sensitive topics. It must avoid such
cases without promoting hate speech or offending certain groups of people or
their beliefs. This study uses a vector database-based Retrieval Augmented
Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our
question-answering system is called ""MufassirQAS"". We created a database
consisting of several open-access books that include Turkish context. These
books contain Turkish translations and interpretations of Islam. This database
is utilized to answer religion-related questions and ensure our answers are
trustworthy. The relevant part of the dataset, which LLM also uses, is
presented along with the answer. We have put careful effort into creating
system prompts that give instructions to prevent harmful, offensive, or
disrespectful responses to respect people's values and provide reliable
results. The system answers and shares additional information, such as the page
number from the respective book and the articles referenced for obtaining the
information. MufassirQAS and ChatGPT are also tested with sensitive questions.
We got better performance with our system. Study and enhancements are still in
progress. Results and future works are given.",Ahmet Yusuf Alan
2024-01-29T16:03:29Z,http://arxiv.org/abs/2402.01741v2,"Development and Testing of a Novel Large Language Model-Based Clinical
  Decision Support Systems for Medication Safety in 12 Clinical Specialties","Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) framework as a Clinical Decision Support Systems (CDSS) to
support safe medication prescription.
  Objective: To evaluate the efficacy of LLM-based CDSS in correctly
identifying medication errors in different patient case vignettes from diverse
medical and surgical sub-disciplines, against a human expert panel derived
ground truth. We compared performance for under 2 different CDSS practical
healthcare integration modalities: LLM-based CDSS alone (fully autonomous mode)
vs junior pharmacist + LLM-based CDSS (co-pilot, assistive mode).
  Design, Setting, and Participants: Utilizing a RAG model with
state-of-the-art medically-related LLMs (GPT-4, Gemini Pro 1.0 and Med-PaLM 2),
this study used 61 prescribing error scenarios embedded into 23 complex
clinical vignettes across 12 different medical and surgical specialties. A
multidisciplinary expert panel assessed these cases for Drug-Related Problems
(DRPs) using the PCNE classification and graded severity / potential for harm
using revised NCC MERP medication error index. We compared.
  Results RAG-LLM performed better compared to LLM alone. When employed in a
co-pilot mode, accuracy, recall, and F1 scores were optimized, indicating
effectiveness in identifying moderate to severe DRPs. The accuracy of DRP
detection with RAG-LLM improved in several categories but at the expense of
lower precision.
  Conclusions This study established that a RAG-LLM based CDSS significantly
boosts the accuracy of medication error identification when used alongside
junior pharmacists (co-pilot), with notable improvements in detecting severe
DRPs. This study also illuminates the comparative performance of current
state-of-the-art LLMs in RAG-based CDSS systems.",Jasmine Chiat Ling Ong
2024-01-30T00:21:41Z,http://arxiv.org/abs/2402.01748v2,"Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems","Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.",Shengzhe Xu
2024-02-20T02:16:16Z,http://arxiv.org/abs/2402.12659v2,FinBen: A Holistic Financial Benchmark for Large Language Models,"LLMs have transformed NLP and shown promise in various fields, yet their
potential in finance is underexplored due to a lack of comprehensive evaluation
benchmarks, the rapid development of LLMs, and the complexity of financial
tasks. In this paper, we introduce FinBen, the first extensive open-source
evaluation benchmark, including 36 datasets spanning 24 financial tasks,
covering seven critical aspects: information extraction (IE), textual analysis,
question answering (QA), text generation, risk management, forecasting, and
decision-making. FinBen offers several key innovations: a broader range of
tasks and datasets, the first evaluation of stock trading, novel agent and
Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source
evaluation datasets for text summarization, question answering, and stock
trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT,
and the latest Gemini, reveals several key findings: While LLMs excel in IE and
textual analysis, they struggle with advanced reasoning and complex tasks like
text generation and forecasting. GPT-4 excels in IE and stock trading, while
Gemini is better at text generation and forecasting. Instruction-tuned LLMs
improve textual analysis but offer limited benefits for complex tasks such as
QA. FinBen has been used to host the first financial LLMs shared task at the
FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel
solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation
in financial LLMs. All datasets, results, and codes are released for the
research community: https://github.com/The-FinAI/PIXIU.",Qianqian Xie
2024-03-11T16:12:34Z,http://arxiv.org/abs/2403.06857v1,"Development of a Reliable and Accessible Caregiving Language Model
  (CaLM)","Unlike professional caregivers, family caregivers often assume this role
without formal preparation or training. Because of this, there is an urgent
need to enhance the capacity of family caregivers to provide quality care.
Large language models can potentially be used as a foundation technology for
supporting caregivers as educational tools or as adjunct to care. This study
aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a
caregiving knowledge base, develop an accessible CaLM using a small FM that
requires fewer computing resources, and evaluate the performance of the model
compared to a large FM. We developed CaLM using the Retrieval Augmented
Generation (RAG) framework combined with FM fine-tuning for improving the
quality of FM answers by grounding the model on a caregiving knowledge base. We
used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B
parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving
knowledge base by gathering various types of documents from the Internet. In
this study, we focused on caregivers of individuals with Alzheimer's Disease
Related Dementias. We evaluated the models' performance using the benchmark
metrics commonly used in evaluating language models and their reliability to
provide accurate references with the answers. The RAG framework improved the
performance of all FMs used in this study across all measures. As expected, the
large FM performed better than small FMs across all metrics. The most
interesting result is that small fine-tuned FMs with RAG performed
significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2
small FM performed better than GPT 3.5 (even with RAG) in returning references
with the answers. The study shows that reliable and accessible CaLM can be
developed by using small FMs with a knowledge base specific to the caregiving
domain.",Bambang Parmanto
2024-03-21T13:05:18Z,http://arxiv.org/abs/2403.14374v1,FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,"Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.",Yuren Mao
2024-03-25T21:37:30Z,http://arxiv.org/abs/2403.17209v4,"Generation of Asset Administration Shell with Large Language Model
  Agents: Toward Semantic Interoperability in Digital Twins in the Context of
  Industry 4.0","This research introduces a novel approach for achieving semantic
interoperability in digital twins and assisting the creation of Asset
Administration Shell (AAS) as digital twin model within the context of Industry
4.0. The foundational idea of our research is that the communication based on
semantics and the generation of meaningful textual data are directly linked,
and we posit that these processes are equivalent if the exchanged information
can be serialized in text form. Based on this, we construct a ""semantic node""
data structure in our research to capture the semantic essence of textual data.
Then, a system powered by large language models is designed and implemented to
process the ""semantic node"" and generate standardized digital twin models from
raw textual data collected from datasheets describing technical assets. Our
evaluation demonstrates an effective generation rate of 62-79%, indicating a
substantial proportion of the information from the source text can be
translated error-free to the target digital twin instance model with the
generative capability of large language models. This result has a direct
application in the context of Industry 4.0, and the designed system is
implemented as a data model generation tool for reducing the manual effort in
creating AAS model. In our evaluation, a comparative analysis of different LLMs
and an in-depth ablation study of Retrieval-Augmented Generation (RAG)
mechanisms provide insights into the effectiveness of LLM systems for
interpreting technical concepts and translating data. Our findings emphasize
LLMs' capability to automate AAS instance creation and contribute to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are presented
on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.",Yuchen Xia
2024-03-28T03:09:42Z,http://arxiv.org/abs/2403.19113v1,FACTOID: FACtual enTailment fOr hallucInation Detection,"The widespread adoption of Large Language Models (LLMs) has facilitated
numerous benefits. However, hallucination is a significant concern. In
response, Retrieval Augmented Generation (RAG) has emerged as a highly
promising paradigm to improve LLM outputs by grounding them in factual
information. RAG relies on textual entailment (TE) or similar methods to check
if the text produced by LLMs is supported or contradicted, compared to
retrieved documents. This paper argues that conventional TE methods are
inadequate for spotting hallucinations in content generated by LLMs. For
instance, consider a prompt about the 'USA's stance on the Ukraine war''. The
AI-generated text states, ...U.S. President Barack Obama says the U.S. will not
put troops in Ukraine...'' However, during the war the U.S. president is Joe
Biden which contradicts factual reality. Moreover, current TE systems are
unable to accurately annotate the given text and identify the exact portion
that is contradicted. To address this, we introduces a new type of TE called
``Factual Entailment (FE).'', aims to detect factual inaccuracies in content
generated by LLMs while also highlighting the specific text segment that
contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation
Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL)
framework for FE, incorporating state-of-the-art (SoTA) long text embeddings
such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The
proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy
on the FACTOID benchmark compared to SoTA TE methods. As FE automatically
detects hallucinations, we assessed 15 modern LLMs and ranked them using our
proposed Auto Hallucination Vulnerability Index (HVI_auto). This index
quantifies and offers a comparative scale to evaluate and rank LLMs according
to their hallucinations.",Vipula Rawte
2024-04-16T00:43:03Z,http://arxiv.org/abs/2404.10198v2,"ClashEval: Quantifying the tug-of-war between an LLM's internal prior
  and external evidence","Retrieval augmented generation (RAG) is frequently used to mitigate
hallucinations and provide up-to-date knowledge for large language models
(LLMs). However, given that document retrieval is an imprecise task and
sometimes results in erroneous or even harmful content being presented in
context, this raises the question of how LLMs handle retrieved information: If
the provided content is incorrect, does the model know to ignore it, or does it
recapitulate the error? Conversely, when the model's initial response is
incorrect, does it always know to use the retrieved information to correct
itself, or does it insist on its wrong prior response? To answer this, we
curate a dataset of over 1200 questions across six domains (e.g., drug dosages,
Olympic records, locations) along with content relevant to answering each
question. We further apply precise perturbations to the answers in the content
that range from subtle to blatant errors. We benchmark six top-performing LLMs,
including GPT-4o, on this dataset and find that LLMs are susceptible to
adopting incorrect retrieved content, overriding their own correct prior
knowledge over 60% of the time. However, the more unrealistic the retrieved
content is (i.e. more deviated from truth), the less likely the model is to
adopt it. Also, the less confident a model is in its initial response (via
measuring token probabilities), the more likely it is to adopt the information
in the retrieved content. We exploit this finding and demonstrate simple
methods for improving model accuracy where there is conflicting retrieved
content. Our results highlight a difficult task and benchmark for LLMs --
namely, their ability to correctly discern when it is wrong in light of correct
retrieved content and to reject cases when the provided content is incorrect.",Kevin Wu
2024-04-18T16:38:02Z,http://arxiv.org/abs/2404.12309v2,iRAG: Advancing RAG for Videos with an Incremental Approach,"Retrieval-augmented generation (RAG) systems combine the strengths of
language generation and information retrieval to power many real-world
applications like chatbots. Use of RAG for understanding of videos is appealing
but there are two critical limitations. One-time, upfront conversion of all
content in large corpus of videos into text descriptions entails high
processing times. Also, not all information in the rich video data is typically
captured in the text descriptions. Since user queries are not known apriori,
developing a system for video to text conversion and interactive querying of
video data is challenging.
  To address these limitations, we propose an incremental RAG system called
iRAG, which augments RAG with a novel incremental workflow to enable
interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG
quickly indexes large repositories of videos, and in the incremental workflow,
it uses the index to opportunistically extract more details from select
portions of the videos to retrieve context relevant to an interactive user
query. Such an incremental workflow avoids long video to text conversion times,
and overcomes information loss issues due to conversion of video to text, by
doing on-demand query-specific extraction of details in video data. This
ensures high quality of responses to interactive user queries that are often
not known apriori. To the best of our knowledge, iRAG is the first system to
augment RAG with an incremental workflow to support efficient interactive
querying of a large corpus of videos. Experimental results on real-world
datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring
that latency and quality of responses to interactive user queries is comparable
to responses from a traditional RAG where all video data is converted to text
upfront before any user querying.",Md Adnan Arefeen
2024-04-22T09:25:05Z,http://arxiv.org/abs/2404.14464v1,"Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for
  Multi-hop Question Answering","Multi-hop question answering is a knowledge-intensive complex problem. Large
Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason
complex problems step by step, and retrieval-augmentation can effectively
alleviate factual errors caused by outdated and unknown knowledge in LLMs.
Recent works have introduced retrieval-augmentation in the CoT reasoning to
solve multi-hop question answering. However, these chain methods have the
following problems: 1) Retrieved irrelevant paragraphs may mislead the
reasoning; 2) An error in the chain structure may lead to a cascade of errors.
  In this paper, we propose a dynamic retrieval framework called Tree of
Reviews (ToR), where the root node is the question, and the other nodes are
paragraphs from retrieval, extending different reasoning paths from the root
node to other nodes. Our framework dynamically decides to initiate a new
search, reject, or accept based on the paragraphs on the reasoning paths.
Compared to related work, we introduce a tree structure to handle each
retrieved paragraph separately, alleviating the misleading effect of irrelevant
paragraphs on the reasoning path; the diversity of reasoning path extension
reduces the impact of a single reasoning error on the whole. We conducted
experiments on three different multi-hop question answering datasets. The
results show that compared to the baseline methods, ToR achieves
state-of-the-art performance in both retrieval and response generation. In
addition, we propose two tree-based search optimization strategies, pruning and
effective expansion, to reduce time overhead and increase the diversity of path
extension. We will release our code.",Li Jiapeng
2024-05-01T05:39:07Z,http://arxiv.org/abs/2405.00330v1,"Integrating A.I. in Higher Education: Protocol for a Pilot Study with
  'SAMCares: An Adaptive Learning Hub'","Learning never ends, and there is no age limit to grow yourself. However, the
educational landscape may face challenges in effectively catering to students'
inclusion and diverse learning needs. These students should have access to
state-of-the-art methods for lecture delivery, online resources, and technology
needs. However, with all the diverse learning sources, it becomes harder for
students to comprehend a large amount of knowledge in a short period of time.
Traditional assistive technologies and learning aids often lack the dynamic
adaptability required for individualized education plans. Large Language Models
(LLM) have been used in language translation, text summarization, and content
generation applications. With rapid growth in AI over the past years,
AI-powered chatbots and virtual assistants have been developed. This research
aims to bridge this gap by introducing an innovative study buddy we will be
calling the 'SAMCares'. The system leverages a Large Language Model (LLM) (in
our case, LLaMa-2 70B as the base model) and Retriever-Augmented Generation
(RAG) to offer real-time, context-aware, and adaptive educational support. The
context of the model will be limited to the knowledge base of Sam Houston State
University (SHSU) course notes. The LLM component enables a chat-like
environment to interact with it to meet the unique learning requirements of
each student. For this, we will build a custom web-based GUI. At the same time,
RAG enhances real-time information retrieval and text generation, in turn
providing more accurate and context-specific assistance. An option to upload
additional study materials in the web GUI is added in case additional knowledge
support is required. The system's efficacy will be evaluated through controlled
trials and iterative feedback mechanisms.",Syed Hasib Akhter Faruqui
2024-05-01T11:06:31Z,http://arxiv.org/abs/2405.00449v1,"RAG-based Explainable Prediction of Road Users Behaviors for Automated
  Driving using Knowledge Graphs and Large Language Models","Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.",Mohamed Manzour Hussien
2024-05-03T16:38:51Z,http://arxiv.org/abs/2405.02228v2,"REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific
  Sentences using Public and Proprietary LLMs","Automatic citation generation for sentences in a document or report is
paramount for intelligence analysts, cybersecurity, news agencies, and
education personnel. In this research, we investigate whether large language
models (LLMs) are capable of generating references based on two forms of
sentence queries: (a) Direct Queries, LLMs are asked to provide author names of
the given research article, and (b) Indirect Queries, LLMs are asked to provide
the title of a mentioned article when given a sentence from a different
article. To demonstrate where LLM stands in this task, we introduce a large
dataset called REASONS comprising abstracts of the 12 most popular domains of
scientific research on arXiv. From around 20K research articles, we make the
following deductions on public and proprietary LLMs: (a) State-of-the-art,
often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass
percentage (PP) to minimize the hallucination rate (HR). When tested with
Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant
metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented
generation (RAG) using Mistral demonstrates consistent and robust citation
support on indirect queries and matched performance to GPT-3.5 and GPT-4. The
HR across all domains and models decreased by an average of 41.93%, and the PP
was reduced to 0% in most cases. In terms of generation quality, the average F1
Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with
adversarial samples showed that LLMs, including the Advance RAG Mistral,
struggle to understand context, but the extent of this issue was small in
Mistral and GPT-4-Preview. Our study contributes valuable insights into the
reliability of RAG for automated citation generation tasks.",Deepa Tilwani
2024-05-03T02:48:55Z,http://arxiv.org/abs/2405.02355v3,"CodeGRAG: Bridging the Gap between Natural Language and Programming
  Language via Graphical Retrieval Augmented Generation","Utilizing large language models to generate codes has shown promising meaning
in software development revolution. Despite the intelligence shown by the
general large language models, their specificity in code generation can still
be improved due to the syntactic gap and mismatched vocabulary existing among
natural language and different programming languages. In this paper, we propose
CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance
the performance of LLMs. CodeGRAG builds the graphical view of code blocks
based on the control flow and data flow of them to fill the gap between
programming languages and natural language, which can facilitate natural
language based LLMs for better understanding of code syntax and serve as a
bridge among different programming languages. To take the extracted structural
knowledge into the foundation models, we propose 1) a hard meta-graph prompt
template to transform the challenging graphical representation into informative
knowledge for tuning-free models and 2) a soft prompting technique that injects
the domain knowledge of programming languages into the model parameters via
finetuning the models with the help of a pretrained GNN expert model. Various
experiments and ablations are done on four datasets including both the C++ and
python languages to validate the hard meta-graph prompt, the soft prompting
technique, and the effectiveness of the objectives for pretrained GNN expert.
CodeGRAG improves the code generation ability of LLMs and can even offer
performance gain for cross-lingual code generation. Code is available at
https://anonymous.4open.science/r/Code-5970/.",Kounianhua Du
2024-05-10T02:48:45Z,http://arxiv.org/abs/2405.06211v3,"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language
  Models","As one of the most advanced techniques in AI, Retrieval-Augmented Generation
(RAG) can offer reliable and up-to-date external knowledge, providing huge
convenience for numerous tasks. Particularly in the era of AI-Generated Content
(AIGC), the powerful capacity of retrieval in providing additional knowledge
enables RAG to assist existing generative AI in producing high-quality outputs.
Recently, Large Language Models (LLMs) have demonstrated revolutionary
abilities in language understanding and generation, while still facing inherent
limitations, such as hallucinations and out-of-date internal knowledge. Given
the powerful abilities of RAG in providing the latest and helpful auxiliary
information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged
to harness external and authoritative knowledge bases, rather than solely
relying on the model's internal knowledge, to augment the generation quality of
LLMs. In this survey, we comprehensively review existing research studies in
RA-LLMs, covering three primary technical perspectives: architectures, training
strategies, and applications. As the preliminary knowledge, we briefly
introduce the foundations and recent advances of LLMs. Then, to illustrate the
practical significance of RAG for LLMs, we systematically review mainstream
relevant work by their architectures, training strategies, and application
areas, detailing specifically the challenges of each and the corresponding
capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss
current limitations and several promising directions for future research.
Updated information about this survey can be found at
https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",Wenqi Fan
2024-05-18T22:43:44Z,http://arxiv.org/abs/2405.11407v2,Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?,"Advancements in deep learning have generated a large-scale interest in the
development of foundational deep learning models. The development of Large
Language Models (LLM) has evolved as a transformative paradigm in
conversational tasks, which has led to its integration and extension even in
the critical domain of healthcare. With LLMs becoming widely popular and their
public access through open-source models and integration with other
applications, there is a need to investigate their potential and limitations.
One such crucial task where LLMs are applied but require a deeper understanding
is that of self-diagnosis of medical conditions based on bias-validating
symptoms in the interest of public health. The widespread integration of Gemini
with Google search and GPT-4.0 with Bing search has led to a shift in the trend
of self-diagnosis using search engines to conversational LLM models. Owing to
the critical nature of the task, it is prudent to investigate and understand
the potential and limitations of public LLMs in the task of self-diagnosis. In
this study, we prepare a prompt engineered dataset of 10000 samples and test
the performance on the general task of self-diagnosis. We compared the
performance of both the state-of-the-art GPT-4.0 and the fee Gemini model on
the task of self-diagnosis and recorded contrasting accuracies of 63.07% and
6.01%, respectively. We also discuss the challenges, limitations, and potential
of both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future
research and towards the broader impact of general public knowledge.
Furthermore, we demonstrate the potential and improvement in performance for
the task of self-diagnosis using Retrieval Augmented Generation.",Nikil Sharan Prabahar Balasubramanian
2024-05-23T13:32:07Z,http://arxiv.org/abs/2405.14554v2,"SearchLVLMs: A Plug-and-Play Framework for Augmenting Large
  Vision-Language Models by Searching Up-to-Date Internet Knowledge","Large vision-language models (LVLMs) are ignorant of the up-to-date
knowledge, such as LLaVA series, because they cannot be updated frequently due
to the large amount of resources required, and therefore fail in many cases.
For example, if a LVLM was released on January 2024, and it wouldn't know the
singer of the theme song for the new Detective Conan movie, which wasn't
released until April 2024. To solve the problem, a promising solution motivated
by retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date
knowledge via internet search during inference, i.e., internet-augmented
generation (IAG), which is already integrated in some closed-source commercial
LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain
a mystery. In this paper, we propose a plug-and-play framework, for augmenting
existing LVLMs in handling visual question answering (VQA) about up-to-date
knowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to
effectively and efficiently find the most helpful content from the websites
returned by a search engine to prompt LVLMs with up-to-date knowledge. To train
the model and evaluate our framework's performance, we propose a pipeline to
automatically generate news-related VQA samples to construct a dataset, dubbed
UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness
of website/content for VQA samples to construct the training set. Experimental
results demonstrate the effectiveness of our framework, outperforming GPT-4V by
about 25% in accuracy.",Chuanhao Li
2024-05-24T17:34:32Z,http://arxiv.org/abs/2405.15739v3,"Large Language Models Reflect Human Citation Patterns with a Heightened
  Citation Bias","Citation practices are crucial in shaping the structure of scientific
knowledge, yet they are often influenced by contemporary norms and biases. The
emergence of Large Language Models (LLMs) introduces a new dynamic to these
practices. Interestingly, the characteristics and potential biases of
references recommended by LLMs that entirely rely on their parametric
knowledge, and not on search or retrieval-augmented generation, remain
unexplored. Here, we analyze these characteristics in an experiment using a
dataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge
cut-off date. In our experiment, LLMs are tasked with suggesting scholarly
references for the anonymized in-text citations within these papers. Our
findings reveal a remarkable similarity between human and LLM citation
patterns, but with a more pronounced high citation bias, which persists even
after controlling for publication year, title length, number of authors, and
venue. The results hold for both GPT-4, and the more capable models GPT-4o and
Claude 3.5 where the papers are part of the training data. Additionally, we
observe a large consistency between the characteristics of LLM's existing and
non-existent generated references, indicating the model's internalization of
citation patterns. By analyzing citation graphs, we show that the references
recommended are embedded in the relevant citation context, suggesting an even
deeper conceptual internalization of the citation networks. While LLMs can aid
in citation generation, they may also amplify existing biases, such as the
Matthew effect, and introduce new ones, potentially skewing scientific
knowledge dissemination.",Andres Algaba
2024-05-30T17:56:05Z,http://arxiv.org/abs/2405.20362v1,"Hallucination-Free? Assessing the Reliability of Leading AI Legal
  Research Tools","Legal practice has witnessed a sharp rise in products incorporating
artificial intelligence (AI). Such tools are designed to assist with a wide
range of core legal tasks, from search and summarization of caselaw to document
drafting. But the large language models used in these tools are prone to
""hallucinate,"" or make up false information, making their use risky in
high-stakes domains. Recently, certain legal research providers have touted
methods such as retrieval-augmented generation (RAG) as ""eliminating""
(Casetext, 2023) or ""avoid[ing]"" hallucinations (Thomson Reuters, 2023), or
guaranteeing ""hallucination-free"" legal citations (LexisNexis, 2023). Because
of the closed nature of these systems, systematically assessing these claims is
challenging. In this article, we design and report on the first preregistered
empirical evaluation of AI-driven legal research tools. We demonstrate that the
providers' claims are overstated. While hallucinations are reduced relative to
general-purpose chatbots (GPT-4), we find that the AI research tools made by
LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and
Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also
document substantial differences between systems in responsiveness and
accuracy. Our article makes four key contributions. It is the first to assess
and report the performance of RAG-based proprietary legal AI tools. Second, it
introduces a comprehensive, preregistered dataset for identifying and
understanding vulnerabilities in these systems. Third, it proposes a clear
typology for differentiating between hallucinations and accurate legal
responses. Last, it provides evidence to inform the responsibilities of legal
professionals in supervising and verifying AI outputs, which remains a central
open question for the responsible integration of AI into law.",Varun Magesh
2024-06-07T08:43:07Z,http://arxiv.org/abs/2406.04744v2,CRAG -- Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.",Xiao Yang
2024-06-26T03:32:35Z,http://arxiv.org/abs/2406.18039v1,"Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model
  with Three Types of Knowledge","Liver cancer has a high incidence rate, but primary healthcare settings often
lack experienced doctors. Advances in large models and AI technologies offer
potential assistance. This work aims to address limitations in liver cancer
diagnosis models, such as poor understanding of medical images, insufficient
consideration of liver blood vessels, and ensuring accurate medical
information. We propose a specialized diagnostic assistant to improve the
diagnostic capabilities of less experienced doctors. Our framework combines
large and small models, using optimized small models for precise patient image
perception. Specifically, a segmentation network iteratively removes ambiguous
pixels for liver tumor segmentation, and a multi-scale, multi-level
differential network segments liver vessels. Features from these segmentations
and medical records form a patient's personalized knowledge base. For
diagnosis, Chain of Thought (COT) technology designs prompts mimicking
experienced doctors' thought patterns, and Retrieval-Augmented Generation (RAG)
technology provides answers based on reliable domain knowledge and trusted
cases. Our small model methods improve liver tumor and vessel segmentation
performance, resulting in more accurate information extraction. The large model
component scores over 1 point higher on a 10-point scale in evaluations by
doctors compared to control methods. Our method enhances semantic perception of
medical images, improves classification of ambiguous pixels, and optimizes
small object perception. It considers blood vessel positions for specific
treatments and improves response credibility and interpretability by mimicking
experienced doctors' thought processes using reliable resources. This approach
has been recognized by doctors and benefits liver cancer auxiliary diagnosis.",Xuzhou Wu
2024-06-26T12:51:37Z,http://arxiv.org/abs/2406.18312v4,AI-native Memory: A Pathway from LLMs Towards AGI,"Large language models (LLMs) have demonstrated the world with the sparks of
artificial general intelligence (AGI). One opinion, especially from some
startups working on LLMs, argues that an LLM with nearly unlimited context
length can realize AGI. However, they might be too optimistic about the
long-context capability of (existing) LLMs -- (1) Recent literature has shown
that their effective context length is significantly smaller than their claimed
context length; and (2) Our reasoning-in-a-haystack experiments further
demonstrate that simultaneously finding the relevant information from a long
context and conducting (simple) reasoning is nearly impossible. In this paper,
we envision a pathway from LLMs to AGI through the integration of
\emph{memory}. We believe that AGI should be a system where LLMs serve as core
processors. In addition to raw data, the memory in this system would store a
large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw
data, this approach not only connects semantically related information closer,
but also simplifies complex inferences at the time of querying. As an
intermediate stage, the memory will likely be in the form of natural language
descriptions, which can be directly consumed by users too. Ultimately, every
agent/person should have its own large personal model, a deep neural network
model (thus \emph{AI-native}) that parameterizes and compresses all types of
memory, even the ones cannot be described by natural languages. Finally, we
discuss the significant potential of AI-native memory as the transformative
infrastructure for (proactive) engagement, personalization, distribution, and
social in the AGI era, as well as the incurred privacy and security challenges
with preliminary solutions.",Jingbo Shang
2024-06-29T15:23:28Z,http://arxiv.org/abs/2407.00466v1,"BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for
  Biomedical Science","Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,
draws increasing attention, where one common approach is to build a copilot
agent driven by Large Language Models (LLMs). However, to evaluate such
systems, people either rely on direct Question-Answering (QA) to the LLM
itself, or in a biomedical experimental manner. How to precisely benchmark
biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of
scientists, understanding the literature, and introduce BioKGBench. In contrast
to traditional evaluation benchmark that only focuses on factual QA, where the
LLMs are known to have hallucination issues, we first disentangle
""Understanding Literature"" into two atomic abilities, i) ""Understanding"" the
unstructured text from research papers by performing scientific claim
verification, and ii) Ability to interact with structured Knowledge-Graph
Question-Answering (KGQA) as a form of ""Literature"" grounding. We then
formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based
Retrieval-Augmented Generation (RAG) to identify the factual errors of existing
large-scale knowledge graph databases. We collect over two thousand data for
two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios
and biomedical ones, have either failed or inferior performance on our
benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors
which provide scenarios for agents to make discoveries and demonstrate the
effectiveness of our approach. The code and data are available at
https://github.com/westlake-autolab/BioKGBench.",Xinna Lin
2024-07-05T12:42:31Z,http://arxiv.org/abs/2407.04472v3,"EventChat: Implementation and user-centric evaluation of a large
  language model-driven conversational recommender system for exploring leisure
  events in an SME context","Large language models (LLMs) present an enormous evolution in the strategic
potential of conversational recommender systems (CRS). Yet to date, research
has predominantly focused upon technical frameworks to implement LLM-driven
CRS, rather than end-user evaluations or strategic implications for firms,
particularly from the perspective of a small to medium enterprises (SME) that
makeup the bedrock of the global economy. In the current paper, we detail the
design of an LLM-driven CRS in an SME setting, and its subsequent performance
in the field using both objective system metrics and subjective user
evaluations. While doing so, we additionally outline a short-form revised
ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly
evolving field. Our results reveal good system performance from a user
experience perspective (85.5% recommendation accuracy) but underscore latency,
cost, and quality issues challenging business viability. Notably, with a median
cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and
response time emerge as crucial areas for achieving a more user-friendly and
economically viable LLM-driven CRS for SME settings. One major driver of these
costs is the use of an advanced LLM as a ranker within the retrieval-augmented
generation (RAG) technique. Our results additionally indicate that relying
solely on approaches such as Prompt-based learning with ChatGPT as the
underlying LLM makes it challenging to achieve satisfying quality in a
production environment. Strategic considerations for SMEs deploying an
LLM-driven CRS are outlined, particularly considering trade-offs in the current
technical landscape.",Hannes Kunstmann
2024-07-10T02:33:09Z,http://arxiv.org/abs/2407.07321v2,"Examining Long-Context Large Language Models for Environmental Review
  Document Comprehension","As LLMs become increasingly ubiquitous, researchers have tried various
techniques to augment the knowledge provided to these models. Long context and
retrieval-augmented generation (RAG) are two such methods that have recently
gained popularity. In this work, we examine the benefits of both of these
techniques by utilizing question answering (QA) task in a niche domain. While
the effectiveness of LLM-based QA systems has already been established at an
acceptable level in popular domains such as trivia and literature, it has not
often been established in niche domains that traditionally require specialized
expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance
of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and
Mistral -- when answering questions originating from Environmental Impact
Statements prepared by U.S. federal government agencies in accordance with the
National Environmental Environmental Act (NEPA). We specifically measure the
ability of LLMs to understand the nuances of legal, technical, and
compliance-related information present in NEPA documents in different
contextual scenarios. We test the LLMs' internal prior NEPA knowledge by
providing questions without any context, as well as assess how LLMs synthesize
the contextual information present in long NEPA documents to facilitate the
question/answering task. We compare the performance of the models in handling
different types of questions (e.g., problem-solving, divergent, etc.). Our
results suggest that RAG powered models significantly outperform those provided
with only the PDF context in terms of answer accuracy, regardless of the choice
of the LLM. Our further analysis reveals that many models perform better
answering closed type questions (Yes/No) than divergent and problem-solving
questions.",Hung Phan
2024-06-05T03:32:06Z,http://arxiv.org/abs/2407.11987v1,SlicerChat: Building a Local Chatbot for 3D Slicer,"3D Slicer is a powerful platform for 3D data visualization and analysis, but
has a significant learning curve for new users. Generative AI applications,
such as ChatGPT, have emerged as a potential method of bridging the gap between
various sources of documentation using natural language. The limited exposure
of LLM services to 3D Slicer documentation, however, means that ChatGPT and
related services tend to suffer from significant hallucination. The objective
of this project is to build a chatbot architecture, called SlicerChat, that is
optimized to answer 3D Slicer related questions and able to run locally using
an open-source model. The core research questions explored in this work revolve
around the answer quality and speed differences due to fine-tuning, model size,
and the type of domain knowledge included in the prompt. A prototype SlicerChat
system was built as a custom extension in 3D Slicer based on the Code-Llama
Instruct architecture. Models of size 1.1B, 7B and 13B were fine-tuned using
Low rank Adaptation, and various sources of 3D Slicer documentation were
compiled for use in a Retrieval Augmented Generation paradigm. Testing
combinations of fine-tuning and model sizes on a benchmark dataset of five 3D
Slicer questions revealed that fine-tuning had no impact on model performance
or speed compared to the base architecture, and that larger models performed
better with a significant speed decrease. Experiments with adding 3D Slicer
documentation to the prompt showed that Python sample code and Markdown
documentation were the most useful information to include, but that adding 3D
Slicer scene data and questions taken from Discourse also improved model
performance. In conclusion, this project shows the potential for integrating a
high quality, local chatbot directly into 3D Slicer to help new users and
experienced developers alike to more efficiently use the software.",Colton Barr
2024-07-19T17:35:47Z,http://arxiv.org/abs/2407.14482v2,"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities","In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K
context window, designed to bridge the gap between open-source LLMs and leading
proprietary models (e.g., GPT-4-Turbo) in long-context understanding and
retrieval-augmented generation (RAG) capabilities. These two capabilities are
essential for LLMs to process large volumes of information that cannot fit into
a single prompt and are complementary to each other, depending on the
downstream tasks and computational budgets. We present a detailed continued
training recipe to extend the context window of Llama3-70B-base from 8K to 128K
tokens, along with a three-stage instruction tuning process to enhance the
model's instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
outperforms most existing state-of-the-art models, including
GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on
ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only
a 4K context window, showing the strong long context capability across varying
sequence lengths. We further provide extensive comparisons between direct
long-context and RAG solutions using the same state-of-the-art long-context
LLMs. Interestingly, we find that the performance of strong long-context LLMs
using RAG improves when retrieving a larger number of chunks. With a large set
of top-k chunks, RAG consistently outperforms direct long-context solution
using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B
and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To
advance research in this field, we open-sourced the model weights, training
data, and the evaluation setup for the for the community:
https://chatqa2-project.github.io/",Peng Xu
2024-07-19T18:08:39Z,http://arxiv.org/abs/2407.14609v1,"Adversarial Databases Improve Success in Retrieval-based Large Language
  Models","Open-source LLMs have shown great potential as fine-tuned chatbots, and
demonstrate robust abilities in reasoning and surpass many existing benchmarks.
Retrieval-Augmented Generation (RAG) is a technique for improving the
performance of LLMs on tasks that the models weren't explicitly trained on, by
leveraging external knowledge databases. Numerous studies have demonstrated the
effectiveness of RAG to more successfully accomplish downstream tasks when
using vector datasets that consist of relevant background information. It has
been implicitly assumed by those in the field that if adversarial background
information is utilized in this context, that the success of using a RAG-based
approach would be nonexistent or even negatively impact the results. To address
this assumption, we tested several open-source LLMs on the ability of RAG to
improve their success in answering multiple-choice questions (MCQ) in the
medical subspecialty field of Nephrology. Unlike previous studies, we examined
the effect of RAG in utilizing both relevant and adversarial background
databases. We set up several open-source LLMs, including Llama 3, Phi-3,
Mixtral 8x7b, Zephyr$\beta$, and Gemma 7B Instruct, in a zero-shot RAG
pipeline. As adversarial sources of information, text from the Bible and a
Random Words generated database were used for comparison. Our data show that
most of the open-source LLMs improve their multiple-choice test-taking success
as expected when incorporating relevant information vector databases.
Surprisingly however, adversarial Bible text significantly improved the success
of many LLMs and even random word text improved test taking ability of some of
the models. In summary, our results demonstrate for the first time the
countertintuitive ability of adversarial information datasets to improve the
RAG-based LLM success.",Sean Wu
2024-07-26T11:00:08Z,http://arxiv.org/abs/2408.00804v1,"ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large
  Language Model","The field of integrated circuit (IC) design is highly specialized, presenting
significant barriers to entry and research and development challenges. Although
large language models (LLMs) have achieved remarkable success in various
domains, existing LLMs often fail to meet the specific needs of students,
engineers, and researchers. Consequently, the potential of LLMs in the IC
design domain remains largely unexplored. To address these issues, we introduce
ChipExpert, the first open-source, instructional LLM specifically tailored for
the IC design field. ChipExpert is trained on one of the current best
open-source base model (Llama-3 8B). The entire training process encompasses
several key stages, including data preparation, continue pre-training,
instruction-guided supervised fine-tuning, preference alignment, and
evaluation. In the data preparation stage, we construct multiple high-quality
custom datasets through manual selection and data synthesis techniques. In the
subsequent two stages, ChipExpert acquires a vast amount of IC design knowledge
and learns how to respond to user queries professionally. ChipExpert also
undergoes an alignment phase, using Direct Preference Optimization, to achieve
a high standard of ethical performance. Finally, to mitigate the hallucinations
of ChipExpert, we have developed a Retrieval-Augmented Generation (RAG) system,
based on the IC design knowledge base. We also released the first IC design
benchmark ChipICD-Bench, to evaluate the capabilities of LLMs across multiple
IC design sub-domains. Through comprehensive experiments conducted on this
benchmark, ChipExpert demonstrated a high level of expertise in IC design
knowledge Question-and-Answer tasks.",Ning Xu
2024-08-02T21:54:13Z,http://arxiv.org/abs/2408.01585v3,"LibreLog: Accurate and Efficient Unsupervised Log Parsing Using
  Open-Source Large Language Models","Log parsing is a critical step that transforms unstructured log data into
structured formats, facilitating subsequent log-based analysis. Traditional
syntax-based log parsers are efficient and effective, but they often experience
decreased accuracy when processing logs that deviate from the predefined rules.
Recently, large language models (LLM) based log parsers have shown superior
parsing accuracy. However, existing LLM-based parsers face three main
challenges: 1)time-consuming and labor-intensive manual labeling for
fine-tuning or in-context learning, 2)increased parsing costs due to the vast
volume of log data and limited context size of LLMs, and 3)privacy risks from
using commercial models like ChatGPT with sensitive log information. To
overcome these limitations, this paper introduces LibreLog, an unsupervised log
parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance
privacy and reduce operational costs while achieving state-of-the-art parsing
accuracy. LibreLog first groups logs with similar static text but varying
dynamic variables using a fixed-depth grouping tree. It then parses logs within
these groups using three components: i)similarity scoring-based retrieval
augmented generation: selects diverse logs within each group based on Jaccard
similarity, helping the LLM distinguish between static text and dynamic
variables; ii)self-reflection: iteratively query LLMs to refine log templates
to improve parsing accuracy; and iii) log template memory: stores parsed
templates to reduce LLM queries for improved parsing efficiency. Our evaluation
on LogHub-2.0 shows that LibreLog achieves 25% higher parsing accuracy and
processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers.
In short, LibreLog addresses privacy and cost concerns of using commercial LLMs
while achieving state-of-the-arts parsing efficiency and accuracy.",Zeyang Ma
2024-08-06T02:09:35Z,http://arxiv.org/abs/2408.02900v1,"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular
  Annotations for Medicine","This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal
dataset for medicine, covering over 25 million images across 10 modalities,
with multigranular annotations for more than 65 diseases. These enriched
annotations encompass both global textual information, such as disease/lesion
type, modality, region-specific descriptions, and inter-regional relationships,
as well as detailed local annotations for regions of interest (ROIs), including
bounding boxes, segmentation masks. Unlike existing approach which is limited
by the availability of image-text pairs, we have developed the first automated
pipeline that scales up multimodal data by generating multigranular visual and
texual annotations (in the form of image-ROI-description triplets) without the
need for any paired text descriptions. Specifically, data from over 90
different sources have been collected, preprocessed, and grounded using
domain-specific expert models to identify ROIs related to abnormal regions. We
then build a comprehensive knowledge base and prompt multimodal large language
models to perform retrieval-augmented generation with the identified ROIs as
guidance, resulting in multigranular texual descriptions. Compared to existing
datasets, MedTrinity-25M provides the most enriched annotations, supporting a
comprehensive range of multimodal tasks such as captioning and report
generation, as well as vision-centric tasks like classification and
segmentation. Pretraining on MedTrinity-25M, our model achieves
state-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal
large language models and other representative SoTA approaches. This dataset
can also be utilized to support large-scale pre-training of multimodal medical
AI models, contributing to the development of future foundation models in the
medical domain.",Yunfei Xie
2024-08-06T14:53:25Z,http://arxiv.org/abs/2408.04665v1,"LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations","The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
literature text has been challenging but crucial for the logical design of new
MOFs with desirable functionality. The recent advent of large language models
(LLMs) provides disruptively new solution to this long-standing problem and
latest researches have reported over 90% F1 in extracting correct conditions
from MOFs literature. We argue in this paper that most existing synthesis
extraction practices with LLMs stay with the primitive zero-shot learning,
which could lead to downgraded extraction and application performance due to
the lack of specialized knowledge. This work pioneers and optimizes the
few-shot in-context learning paradigm for LLM extraction of material synthesis
conditions. First, we propose a human-AI joint data curation process to secure
high-quality ground-truth demonstrations for few-shot learning. Second, we
apply a BM25 algorithm based on the retrieval-augmented generation (RAG)
technique to adaptively select few-shot demonstrations for each MOF's
extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the
proposed few-shot method achieves much higher average F1 performance (0.93 vs.
0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
fully automatic evaluation that are more objective than the previous human
evaluation. The proposed method is further validated through real-world
material experiments: compared with the baseline zero-shot LLM, the proposed
few-shot approach increases the MOFs structural inference performance (R^2) by
29.4% in average.",Lei Shi
2024-08-15T21:09:09Z,http://arxiv.org/abs/2408.08422v1,"Assessing and Enhancing Large Language Models in Rare Disease
  Question-answering","Despite the impressive capabilities of Large Language Models (LLMs) in
general medical domains, questions remain about their performance in diagnosing
rare diseases. To answer this question, we aim to assess the diagnostic
performance of LLMs in rare diseases, and explore methods to enhance their
effectiveness in this area. In this work, we introduce a rare disease
question-answering (ReDis-QA) dataset to evaluate the performance of LLMs in
diagnosing rare diseases. Specifically, we collected 1360 high-quality
question-answer pairs within the ReDis-QA dataset, covering 205 rare diseases.
Additionally, we annotated meta-data for each question, facilitating the
extraction of subsets specific to any given disease and its property. Based on
the ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that
diagnosing rare diseases remains a significant challenge for these models.
  To facilitate retrieval augmentation generation for rare disease diagnosis,
we collect the first rare diseases corpus (ReCOP), sourced from the National
Organization for Rare Disorders (NORD) database. Specifically, we split the
report of each rare disease into multiple chunks, each representing a different
property of the disease, including their overview, symptoms, causes, effects,
related disorders, diagnosis, and standard therapies. This structure ensures
that the information within each chunk aligns consistently with a question.
Experiment results demonstrate that ReCOP can effectively improve the accuracy
of LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly
guides LLMs to generate trustworthy answers and explanations that can be traced
back to existing literature.",Guanchu Wang
2024-08-21T13:34:29Z,http://arxiv.org/abs/2408.11609v2,Xinyu: An Efficient LLM-based System for Commentary Generation,"Commentary provides readers with a deep understanding of events by presenting
diverse arguments and evidence. However, creating commentary is a
time-consuming task, even for skilled commentators. Large language models
(LLMs) have simplified the process of natural language generation, but their
direct application in commentary creation still faces challenges due to unique
task requirements. These requirements can be categorized into two levels: 1)
fundamental requirements, which include creating well-structured and logically
consistent narratives, and 2) advanced requirements, which involve generating
quality arguments and providing convincing evidence. In this paper, we
introduce Xinyu, an efficient LLM-based system designed to assist commentators
in generating Chinese commentaries. To meet the fundamental requirements, we
deconstruct the generation process into sequential steps, proposing targeted
strategies and supervised fine-tuning (SFT) for each step. To address the
advanced requirements, we present an argument ranking model for arguments and
establish a comprehensive evidence database that includes up-to-date events and
classic books, thereby strengthening the substantiation of the evidence with
retrieval augmented generation (RAG) technology. To evaluate the generated
commentaries more fairly, corresponding to the two-level requirements, we
introduce a comprehensive evaluation metric that considers five distinct
perspectives in commentary generation. Our experiments confirm the
effectiveness of our proposed system. We also observe a significant increase in
the efficiency of commentators in real-world scenarios, with the average time
spent on creating a commentary dropping from 4 hours to 20 minutes.
Importantly, such an increase in efficiency does not compromise the quality of
the commentaries.",Yiquan Wu
2024-08-25T13:36:22Z,http://arxiv.org/abs/2408.13833v1,"Biomedical Large Languages Models Seem not to be Superior to Generalist
  Models on Unseen Medical Data","Large language models (LLMs) have shown potential in biomedical applications,
leading to efforts to fine-tune them on domain-specific data. However, the
effectiveness of this approach remains unclear. This study evaluates the
performance of biomedically fine-tuned LLMs against their general-purpose
counterparts on a variety of clinical tasks. We evaluated their performance on
clinical case challenges from the New England Journal of Medicine (NEJM) and
the Journal of the American Medical Association (JAMA) and on several clinical
tasks (e.g., information extraction, document summarization, and clinical
coding). Using benchmarks specifically chosen to be likely outside the
fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly
perform inferior to their general-purpose counterparts, especially on tasks not
focused on medical knowledge. While larger models showed similar performance on
case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA
cases), smaller biomedical models showed more pronounced underperformance
(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).
Similar trends were observed across the CLUE (Clinical Language Understanding
Evaluation) benchmark tasks, with general-purpose models often performing
better on text generation, question answering, and coding tasks. Our results
suggest that fine-tuning LLMs to biomedical data may not provide the expected
benefits and may potentially lead to reduced performance, challenging
prevailing assumptions about domain-specific adaptation of LLMs and
highlighting the need for more rigorous evaluation frameworks in healthcare AI.
Alternative approaches, such as retrieval-augmented generation, may be more
effective in enhancing the biomedical capabilities of LLMs without compromising
their general knowledge.",Felix J. Dorfner
2024-09-06T14:58:30Z,http://arxiv.org/abs/2409.13709v1,"Column Vocabulary Association (CVA): semantic interpretation of dataless
  tables","Traditional Semantic Table Interpretation (STI) methods rely primarily on the
underlying table data to create semantic annotations. This year's SemTab
challenge introduced the ``Metadata to KG'' track, which focuses on performing
STI by using only metadata information, without access to the underlying data.
In response to this new challenge, we introduce a new term: Column Vocabulary
Association (CVA). This term refers to the task of semantic annotation of
column headers solely based on metadata information. In this study, we evaluate
the performance of various methods in executing the CVA task, including a Large
Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as
well as a more traditional similarity approach with SemanticBERT. Our
methodology uses a zero-shot setting, with no pretraining or examples passed to
the Large Language Models (LLMs), as we aim to avoid a domain-specific setting.
  We investigate a total of 7 different LLMs, of which three commercial GPT
models (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source
models (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate
this models with RAG systems, and we explore how variations in temperature
settings affect performances. Moreover, we continue our investigation by
performing the CVA task utilizing SemanticBERT, analyzing how various metadata
information influence its performance.
  Initial findings indicate that LLMs generally perform well at temperatures
below 1.0, achieving an accuracy of 100\% in certain cases. Nevertheless, our
investigation also reveal that the nature of the data significantly influences
CVA task outcomes. In fact, in cases where the input data and glossary are
related (for example by being created by the same organizations) traditional
methods appear to surpass the performance of LLMs.",Margherita Martorana
2024-09-10T02:00:28Z,http://arxiv.org/abs/2409.13731v3,"KAG: Boosting LLMs in Professional Domains via Knowledge Augmented
  Generation","The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.",Lei Liang
2024-09-23T17:22:09Z,http://arxiv.org/abs/2409.15228v3,"A Comprehensive Framework for Evaluating API-oriented Code Generation in
  Large Language Models","Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as
powerful tools for code generation, significantly enhancing productivity and
accelerating software development. However, existing benchmarks primarily focus
on general code generation without considering API-oriented code generation,
i.e., generating code that invokes APIs from specific libraries. Given the
growing demand for API-oriented code generation, there is a pressing need for a
systematic and automated approach to evaluate LLM on API-oriented code
generation. To address this gap, we propose AutoAPIEval, a lightweight and
automated framework designed to evaluate the capabilities of LLMs in
API-oriented code generation. Our framework works with any library that
provides API documentation and focuses on two unit tasks: API recommendation
and code example generation, along with four metrics to evaluate the generated
APIs and code examples, such as the proportion of incorrect API recommendations
for Task 1, and the proportion of code examples where no specific API is
invoked and uncompilable/unexecutable code examples for Task 2. In addition, we
conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)
and Java Runtime Environment 8 to demonstrate the framework's effectiveness.
Our findings reveal substantial variability in LLM performance across tasks,
with ChatGPT adhering better to instructions, while sharing similar
effectiveness in code example generation with its counterparts (i.e., MagiCoder
and DeekSeek Coder). We also identify key factors associated with code quality,
such as API popularity and model confidence, and build classifiers that achieve
high accuracy in detecting incorrect API recommendations and erroneous code
examples. Retrieval-augmented generation enhances the quality of code generated
by LLMs, though its effectiveness varies across different LLMs.",Yixi Wu
2024-09-26T17:30:28Z,http://arxiv.org/abs/2409.18164v2,Data-Prep-Kit: getting your data ready for LLM application development,"Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).",David Wood
2024-09-27T17:17:15Z,http://arxiv.org/abs/2409.18924v2,"AIPatient: Simulating Patients with EHRs and LLM Powered Agentic
  Workflow","Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.",Huizi Yu
2024-10-03T16:25:37Z,http://arxiv.org/abs/2410.02642v1,Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers,"Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.",Shijie Chen
2024-09-28T18:52:16Z,http://arxiv.org/abs/2410.03721v1,"Thematic Analysis with Open-Source Generative AI and Machine Learning: A
  New Method for Inductive Qualitative Codebook Development","This paper aims to answer one central question: to what extent can
open-source generative text models be used in a workflow to approximate
thematic analysis in social science research? To answer this question, we
present the Generative AI-enabled Theme Organization and Structuring (GATOS)
workflow, which uses open-source machine learning techniques, natural language
processing tools, and generative text models to facilitate thematic analysis.
To establish validity of the method, we present three case studies applying the
GATOS workflow, leveraging these models and techniques to inductively create
codebooks similar to traditional procedures using thematic analysis.
Specifically, we investigate the extent to which a workflow comprising
open-source models and tools can inductively produce codebooks that approach
the known space of themes and sub-themes. To address the challenge of gleaning
insights from these texts, we combine open-source generative text models,
retrieval-augmented generation, and prompt engineering to identify codes and
themes in large volumes of text, i.e., generate a qualitative codebook. The
process mimics an inductive coding process that researchers might use in
traditional thematic analysis by reading text one unit of analysis at a time,
considering existing codes already in the codebook, and then deciding whether
or not to generate a new code based on whether the extant codebook provides
adequate thematic coverage. We demonstrate this workflow using three synthetic
datasets from hypothetical organizational research settings: a study of
teammate feedback in teamwork settings, a study of organizational cultures of
ethical behavior, and a study of employee perspectives about returning to their
offices after the pandemic. We show that the GATOS workflow is able to identify
themes in the text that were used to generate the original synthetic datasets.",Andrew Katz
2024-10-09T04:36:47Z,http://arxiv.org/abs/2410.06542v1,"MedImageInsight: An Open-Source Embedding Model for General Domain
  Medical Imaging","In this work, we present MedImageInsight, an open-source medical imaging
embedding model. MedImageInsight is trained on medical images with associated
text and labels across a diverse collection of domains, including X-Ray, CT,
MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and
mammography. Rigorous evaluations demonstrate MedImageInsight's ability to
achieve state-of-the-art (SOTA) or human expert level performance across
classification, image-image search, and fine-tuning tasks. Specifically, on
public datasets, MedImageInsight achieves SOTA in CT 3D medical image
retrieval, as well as SOTA in disease classification and search for chest
X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves
human expert performance in bone age estimation (on both public and partner
data), as well as AUC above 0.9 in most other domains. When paired with a text
decoder, MedImageInsight achieves near SOTA level single image report findings
generation with less than 10\% the parameters of other models. Compared to
fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight
outperforms in clinical metrics, but underperforms on lexical metrics where
GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight
can generate ROC curves, adjust sensitivity and specificity based on clinical
need, and provide evidence-based decision support through image-image search
(which can also enable retrieval augmented generation). In an independent
clinical evaluation of image-image search in chest X-ray, MedImageInsight
outperformed every other publicly available foundation model evaluated by large
margins (over 6 points AUC), and significantly outperformed other models in
terms of AI fairness (across age and gender). We hope releasing MedImageInsight
will help enhance collective progress in medical imaging AI research and
development.",Noel C. F. Codella
2024-10-11T00:34:20Z,http://arxiv.org/abs/2410.08431v1,"oRetrieval Augmented Generation for 10 Large Language Models and its
  Generalizability in Assessing Medical Fitness","Large Language Models (LLMs) show potential for medical applications but
often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)
allows customization with domain-specific information, making it suitable for
healthcare. This study evaluates the accuracy, consistency, and safety of RAG
models in determining fitness for surgery and providing preoperative
instructions. We developed LLM-RAG models using 35 local and 23 international
preoperative guidelines and tested them against human-generated responses. A
total of 3,682 responses were evaluated. Clinical documents were processed
using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were
assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects
of preoperative instructions. Established guidelines and expert judgment were
used to determine correct responses, with human-generated answers serving as
comparisons. The LLM-RAG models generated responses within 20 seconds,
significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model
achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no
hallucinations and producing correct instructions comparable to clinicians.
Results were consistent across both local and international guidelines. This
study demonstrates the potential of LLM-RAG models for preoperative healthcare
tasks, highlighting their efficiency, scalability, and reliability.",Yu He Ke
2024-10-19T21:50:11Z,http://arxiv.org/abs/2410.15222v1,"AutoFLUKA: A Large Language Model Based Framework for Automating Monte
  Carlo Simulations in FLUKA","Monte Carlo (MC) simulations, particularly using FLUKA, are essential for
replicating real-world scenarios across scientific and engineering fields.
Despite the robustness and versatility, FLUKA faces significant limitations in
automation and integration with external post-processing tools, leading to
workflows with a steep learning curve, which are time-consuming and prone to
human errors. Traditional methods involving the use of shell and Python
scripts, MATLAB, and Microsoft Excel require extensive manual intervention and
lack flexibility, adding complexity to evolving scenarios. This study explores
the potential of Large Language Models (LLMs) and AI agents to address these
limitations. AI agents, integrate natural language processing with autonomous
reasoning for decision-making and adaptive planning, making them ideal for
automation. We introduce AutoFLUKA, an AI agent application developed using the
LangChain Python Framework to automate typical MC simulation workflows in
FLUKA. AutoFLUKA can modify FLUKA input files, execute simulations, and
efficiently process results for visualization, significantly reducing human
labor and error. Our case studies demonstrate that AutoFLUKA can handle both
generalized and domain-specific cases, such as Microdosimetry, with an
streamlined automated workflow, showcasing its scalability and flexibility. The
study also highlights the potential of Retrieval Augmentation Generation (RAG)
tools to act as virtual assistants for FLUKA, further improving user
experience, time and efficiency. In conclusion, AutoFLUKA represents a
significant advancement in automating MC simulation workflows, offering a
robust solution to the inherent limitations. This innovation not only saves
time and resources but also opens new paradigms for research and development in
high energy physics, medical physics, nuclear engineering space and
environmental science.",Zavier Ndum Ndum
2024-10-21T03:51:54Z,http://arxiv.org/abs/2410.15621v1,"DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on
  Commercial DRAM-PIMs","Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic
similarity search in large datasets, has become a fundamental component of
critical applications such as information retrieval and retrieval-augmented
generation (RAG). However, ANNS is a well-known I/O-intensive algorithm with a
low compute-to-I/O ratio, often requiring massive storage due to the large
volume of high-dimensional data. This leads to I/O bottlenecks on CPUs and
memory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)
architecture, which offers high bandwidth, large-capacity memory, and the
ability to perform efficient computation in or near the data, presents a
promising solution for ANNS. In this work, we investigate the use of commercial
DRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS
engine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM
exhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup
tables (LUTs) to replace more multiplications with I/O operations. We then
systematically tune ANNS to search optimized configurations with lower
computational load, aligning the compute-to-I/O ratio of ANNS with that of
DRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS
algorithm, we further explore implementation optimizations to fully utilize the
two thousand parallel processing units with private local memory in DRAM-PIMs.
To address the load imbalance caused by ANNS requests distributed across
different clusters of large datasets, we propose a load-balancing strategy that
combines static data layout optimization with dynamic runtime request
scheduling. Experimental results on representative datasets show that DRIM-ANN
achieves an average performance speedup of 2.92x compared to a 32-thread CPU
counterpart.",Mingkai Chen
2024-10-21T17:34:39Z,http://arxiv.org/abs/2410.16229v2,Building A Coding Assistant via the Retrieval-Augmented Language Model,"Pretrained language models have shown strong effectiveness in code-related
tasks, such as code retrieval, code generation, code summarization, and code
completion tasks. In this paper, we propose COde assistaNt viA
retrieval-augmeNted language model (CONAN), which aims to build a code
assistant by mimicking the knowledge-seeking behaviors of humans during coding.
Specifically, it consists of a code structure aware retriever (CONAN-R) and a
dual-view code representation-based retrieval-augmented generation model
(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and
Masked Entity Prediction tasks to make language models code structure-aware and
learn effective representations for code snippets and documentation. Then
CONAN-G designs a dual-view code representation mechanism for implementing a
retrieval-augmented code generation model. CONAN-G regards the code
documentation descriptions as prompts, which help language models better
understand the code semantics. Our experiments show that CONAN achieves
convincing performance on different code generation tasks and significantly
outperforms previous retrieval augmented code generation models. Our further
analyses show that CONAN learns tailored representations for both code snippets
and documentation by aligning code-documentation data pairs and capturing
structural semantics by masking and predicting entities in the code data.
Additionally, the retrieved code snippets and documentation provide necessary
information from both program language and natural language to assist the code
generation process. CONAN can also be used as an assistant for Large Language
Models (LLMs), providing LLMs with external knowledge in shorter code document
lengths to improve their effectiveness on various code tasks. It shows the
ability of CONAN to extract necessary information and help filter out the noise
from retrieved code documents.",Xinze Li
2024-10-21T18:08:42Z,http://arxiv.org/abs/2410.16397v1,"Towards a Reliable Offline Personal AI Assistant for Long Duration
  Spaceflight","As humanity prepares for new missions to the Moon and Mars, astronauts will
need to operate with greater autonomy, given the communication delays that make
real-time support from Earth difficult. For instance, messages between Mars and
Earth can take up to 24 minutes, making quick responses impossible. This
limitation poses a challenge for astronauts who must rely on in-situ tools to
access the large volume of data from spacecraft sensors, rovers, and
satellites, data that is often fragmented and difficult to use. To bridge this
gap, systems like the Mars Exploration Telemetry-Driven Information System
(METIS) are being developed. METIS is an AI assistant designed to handle
routine tasks, monitor spacecraft systems, and detect anomalies, all while
reducing the reliance on mission control. Current Generative Pretrained
Transformer (GPT) Models, while powerful, struggle in safety-critical
environments. They can generate plausible but incorrect responses, a phenomenon
known as ""hallucination,"" which could endanger astronauts. To overcome these
limitations, this paper proposes enhancing systems like METIS by integrating
GPTs, Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and
Augmented Reality (AR). The idea is to allow astronauts to interact with their
data more intuitively, using natural language queries and visualizing real-time
information through AR. KGs will be used to easily access live telemetry and
multimodal data, ensuring that astronauts have the right information at the
right time. By combining AI, KGs, and AR, this new system will empower
astronauts to work more autonomously, safely, and efficiently during future
space missions.",Oliver Bensch
2024-10-28T19:35:47Z,http://arxiv.org/abs/2410.21480v1,"AiSciVision: A Framework for Specializing Large Multimodal Models in
  Scientific Image Classification","Trust and interpretability are crucial for the use of Artificial Intelligence
(AI) in scientific research, but current models often operate as black boxes
offering limited transparency and justifications for their outputs. We
introduce AiSciVision, a framework that specializes Large Multimodal Models
(LMMs) into interactive research partners and classification models for image
classification tasks in niche scientific domains. Our framework uses two key
components: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2)
domain-specific tools utilized in an agentic workflow. To classify a target
image, AiSciVision first retrieves the most similar positive and negative
labeled images as context for the LMM. Then the LMM agent actively selects and
applies tools to manipulate and inspect the target image over multiple rounds,
refining its analysis before making a final prediction. These VisRAG and
tooling components are designed to mirror the processes of domain experts, as
humans often compare new data to similar examples and use specialized tools to
manipulate and inspect images before arriving at a conclusion. Each inference
produces both a prediction and a natural language transcript detailing the
reasoning and tool usage that led to the prediction. We evaluate AiSciVision on
three real-world scientific image classification datasets: detecting the
presence of aquaculture ponds, diseased eelgrass, and solar panels. Across
these datasets, our method outperforms fully supervised models in low and
full-labeled data settings. AiSciVision is actively deployed in real-world use,
specifically for aquaculture research, through a dedicated web application that
displays and allows the expert users to converse with the transcripts. This
work represents a crucial step toward AI systems that are both interpretable
and effective, advancing their use in scientific research and scientific
discovery.",Brendan Hogan
2024-11-11T22:22:21Z,http://arxiv.org/abs/2411.07404v1,Controllable Context Sensitivity and the Knob Behind It,"When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.",Julian Minder
2024-11-23T08:18:55Z,http://arxiv.org/abs/2411.15490v1,"Improving Factuality of 3D Brain MRI Report Generation with Paired
  Image-domain Retrieval and Text-domain Augmentation","Acute ischemic stroke (AIS) requires time-critical management, with hours of
delayed intervention leading to an irreversible disability of the patient.
Since diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)
plays a crucial role in the detection of AIS, automated prediction of AIS from
DWI has been a research topic of clinical importance. While text radiology
reports contain the most relevant clinical information from the image findings,
the difficulty of mapping across different modalities has limited the
factuality of conventional direct DWI-to-report generation methods. Here, we
propose paired image-domain retrieval and text-domain augmentation (PIRTA), a
cross-modal retrieval-augmented generation (RAG) framework for providing
clinician-interpretative AIS radiology reports with improved factuality. PIRTA
mitigates the need for learning cross-modal mapping, which poses difficulty in
image-to-text generation, by casting the cross-modal mapping problem as an
in-domain retrieval of similar DWI images that have paired ground-truth text
radiology reports. By exploiting the retrieved radiology reports to augment the
report generation process of the query image, we show by experiments with
extensive in-house and public datasets that PIRTA can accurately retrieve
relevant reports from 3D DWI images. This approach enables the generation of
radiology reports with significantly higher accuracy compared to direct
image-to-text generation using state-of-the-art multimodal language models.",Junhyeok Lee
2024-01-30T18:58:43Z,http://arxiv.org/abs/2401.17268v1,Weaver: Foundation Models for Creative Writing,"This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.",Tiannan Wang
2024-06-29T22:39:20Z,http://arxiv.org/abs/2407.00541v1,"Answering real-world clinical questions using large language model based
  systems","Evidence to guide healthcare decisions is often limited by a lack of relevant
and trustworthy literature as well as difficulty in contextualizing existing
research for a specific patient. Large language models (LLMs) could potentially
address both challenges by either summarizing published literature or
generating new studies based on real-world data (RWD). We evaluated the ability
of five LLM-based systems in answering 50 clinical questions and had nine
independent physicians review the responses for relevance, reliability, and
actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus,
Gemini Pro 1.5) rarely produced answers that were deemed relevant and
evidence-based (2% - 10%). In contrast, retrieval augmented generation
(RAG)-based and agentic LLM systems produced relevant and evidence-based
answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic
ChatRWD was able to answer novel questions compared to other LLMs (65% vs.
0-9%). These results suggest that while general-purpose LLMs should not be used
as-is, a purpose-built system for evidence summarization based on RAG and one
for generating novel evidence working synergistically would improve
availability of pertinent evidence for patient care.",Yen Sia Low
